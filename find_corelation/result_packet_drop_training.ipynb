{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0 0.2.2\n",
      "TITAN RTX\n",
      "11.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_OREDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "#from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "print(torch.__version__, torchvision.__version__)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomness 제어 \n",
    "# https://hoya012.github.io/blog/reproducible_pytorch/\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20019 782\n"
     ]
    }
   ],
   "source": [
    "# dataset load\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset_path = \"/media/2/Network/Imagenet_dup/\"\n",
    "# imagenet data load\n",
    "train_dataset = dataset.ImageFolder(root=dataset_path+\"train\",\n",
    "                                       transform=transform)\n",
    "val_dataset = dataset.ImageFolder(root=dataset_path+\"val\",\n",
    "                                       transform=transform)\n",
    "'''\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=4)\n",
    "'''\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4) # for using subset\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True, #inital False\n",
    "                                        num_workers=4)\n",
    "print(len(train_dataloader),len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model 로드 \n",
    "vgg16_bn = torchvision.models.vgg16_bn(pretrained=True)\n",
    "\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    vgg16_bn = nn.DataParallel(vgg16_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hard coding way to freeze weigth\\nct = 0\\nlayer_level = 1\\nback_layer = []\\nfor layer in vgg16_bn.children():\\n#    print(ct,layer)\\n    if layer_level is 1:\\n        for layer_1 in layer:    \\n            print(\"layer 1_\",ct,layer_1)\\n            for i,param in enumerate(layer_1.parameters()):\\n                if ct != 34:\\n                    param.requires_grad = False\\n                    #print(ct,\"layer false\")\\n                else :\\n                    if i == 0:\\n                        param.requires_grad = True\\n                        print(\"=====\",param.shape)\\n                        print(\"%d-%d_layer true\"%(ct,i))\\n                    else :\\n                        param.requires_grad = False\\n                        print(\"=====\",param.shape)\\n                        print(\"%d-%d_layer false\"%(ct,i))\\n                        \\n            ct +=1\\n    elif layer_level is 2:\\n    #    print(\"layer 2\",layer)  \\n        for param in layer_1.parameters():\\n            param.requires_grad = False\\n            #print(ct,\"layer false\")\\n        ct +=1\\n    elif layer_level is 3:\\n        for layer_1 in layer: \\n    #        print(\"layer 3\",layer_1) \\n            for param in layer_1.parameters():\\n                param.requires_grad = False\\n                #print(ct,\"layer false\")\\n            ct+=1\\n    layer_level +=1\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only conv5_1 is trainable\n",
    "''' hard coding way to freeze weigth\n",
    "ct = 0\n",
    "layer_level = 1\n",
    "back_layer = []\n",
    "for layer in vgg16_bn.children():\n",
    "#    print(ct,layer)\n",
    "    if layer_level is 1:\n",
    "        for layer_1 in layer:    \n",
    "            print(\"layer 1_\",ct,layer_1)\n",
    "            for i,param in enumerate(layer_1.parameters()):\n",
    "                if ct != 34:\n",
    "                    param.requires_grad = False\n",
    "                    #print(ct,\"layer false\")\n",
    "                else :\n",
    "                    if i == 0:\n",
    "                        param.requires_grad = True\n",
    "                        print(\"=====\",param.shape)\n",
    "                        print(\"%d-%d_layer true\"%(ct,i))\n",
    "                    else :\n",
    "                        param.requires_grad = False\n",
    "                        print(\"=====\",param.shape)\n",
    "                        print(\"%d-%d_layer false\"%(ct,i))\n",
    "                        \n",
    "            ct +=1\n",
    "    elif layer_level is 2:\n",
    "    #    print(\"layer 2\",layer)  \n",
    "        for param in layer_1.parameters():\n",
    "            param.requires_grad = False\n",
    "            #print(ct,\"layer false\")\n",
    "        ct +=1\n",
    "    elif layer_level is 3:\n",
    "        for layer_1 in layer: \n",
    "    #        print(\"layer 3\",layer_1) \n",
    "            for param in layer_1.parameters():\n",
    "                param.requires_grad = False\n",
    "                #print(ct,\"layer false\")\n",
    "            ct+=1\n",
    "    layer_level +=1\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.34 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_output=[] # extract 10 features from conv5_1 input\n",
    "def error_injection(name,num_error,start_index):\n",
    "    def hook(model,input):\n",
    "        start = start_index\n",
    "        end = start_index + num_error\n",
    "        #print(\"error_injection, input_shape:\",input[0][0][start:end].size())\n",
    "        input[0][0][start:end] = 0\n",
    "        #print(\"shape :\",input[0][0].size())\n",
    "        if len(feature_output) < 10:\n",
    "            feature_output.append(input[0][0].cpu())\n",
    "    return hook\n",
    "\n",
    "param_list = []\n",
    "num_error = 128\n",
    "error_index = 0\n",
    "for name,parameter in vgg16_bn.named_parameters():\n",
    "    if \"features.34.weight\" in name:\n",
    "        #print(name,\"size:\",parameter.size())\n",
    "        param_list.append(parameter)\n",
    "for name,layer in vgg16_bn.named_modules():\n",
    "    #print(name)\n",
    "    if \"features.34\" in name  and isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "        print(name,layer) # target layer Conv5_1\n",
    "        handle = layer.register_forward_pre_hook(error_injection(name,num_error,error_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(size_average = None).to(device)\n",
    "#criterion = torch.nn.CrossEntropyLoss(size_average = None).cuda()\n",
    "optimizer = torch.optim.SGD(param_list,lr=0.005,weight_decay=1e-4)\n",
    "lr_sche = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)\n",
    "max_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====epoch 0 start======\n",
      "[1,    30] loss: 0.697\n",
      "[1,    60] loss: 0.757\n",
      "[1,    90] loss: 0.717\n",
      "[1,   120] loss: 0.736\n",
      "[1,   150] loss: 0.705\n",
      "[1,   180] loss: 0.779\n",
      "[1,   210] loss: 0.667\n",
      "[1,   240] loss: 0.731\n",
      "[1,   270] loss: 0.709\n",
      "[1,   300] loss: 0.718\n",
      "[1,   330] loss: 0.640\n",
      "[1,   360] loss: 0.694\n",
      "[1,   390] loss: 0.682\n",
      "=====epoch 1 start======\n",
      "[2,    30] loss: 0.684\n",
      "[2,    60] loss: 0.704\n",
      "[2,    90] loss: 0.721\n",
      "[2,   120] loss: 0.697\n",
      "[2,   150] loss: 0.728\n",
      "[2,   180] loss: 0.662\n",
      "[2,   210] loss: 0.706\n",
      "[2,   240] loss: 0.690\n",
      "[2,   270] loss: 0.732\n",
      "[2,   300] loss: 0.655\n",
      "[2,   330] loss: 0.692\n",
      "[2,   360] loss: 0.731\n",
      "[2,   390] loss: 0.714\n",
      "=====epoch 2 start======\n",
      "[3,    30] loss: 0.706\n",
      "[3,    60] loss: 0.656\n",
      "[3,    90] loss: 0.701\n",
      "[3,   120] loss: 0.788\n",
      "[3,   150] loss: 0.723\n",
      "[3,   180] loss: 0.689\n",
      "[3,   210] loss: 0.678\n",
      "[3,   240] loss: 0.732\n",
      "[3,   270] loss: 0.686\n",
      "[3,   300] loss: 0.714\n",
      "[3,   330] loss: 0.743\n",
      "[3,   360] loss: 0.759\n",
      "[3,   390] loss: 0.703\n",
      "=====epoch 3 start======\n",
      "[4,    30] loss: 0.702\n",
      "[4,    60] loss: 0.706\n",
      "[4,    90] loss: 0.637\n",
      "[4,   120] loss: 0.700\n",
      "[4,   150] loss: 0.707\n",
      "[4,   180] loss: 0.674\n",
      "[4,   210] loss: 0.721\n",
      "[4,   240] loss: 0.704\n",
      "[4,   270] loss: 0.699\n",
      "[4,   300] loss: 0.683\n",
      "[4,   330] loss: 0.683\n",
      "[4,   360] loss: 0.710\n",
      "[4,   390] loss: 0.737\n",
      "=====epoch 4 start======\n",
      "[5,    30] loss: 0.684\n",
      "[5,    60] loss: 0.720\n",
      "[5,    90] loss: 0.734\n",
      "[5,   120] loss: 0.633\n",
      "[5,   150] loss: 0.704\n",
      "[5,   180] loss: 0.688\n",
      "[5,   210] loss: 0.731\n",
      "[5,   240] loss: 0.734\n",
      "[5,   270] loss: 0.708\n",
      "[5,   300] loss: 0.671\n",
      "[5,   330] loss: 0.687\n",
      "[5,   360] loss: 0.726\n",
      "[5,   390] loss: 0.706\n",
      "=====epoch 5 start======\n",
      "[6,    30] loss: 0.688\n",
      "[6,    60] loss: 0.654\n",
      "[6,    90] loss: 0.684\n",
      "[6,   120] loss: 0.688\n",
      "[6,   150] loss: 0.720\n",
      "[6,   180] loss: 0.689\n",
      "[6,   210] loss: 0.733\n",
      "[6,   240] loss: 0.706\n",
      "[6,   270] loss: 0.706\n",
      "[6,   300] loss: 0.673\n",
      "[6,   330] loss: 0.701\n",
      "[6,   360] loss: 0.725\n",
      "[6,   390] loss: 0.666\n",
      "=====epoch 6 start======\n",
      "[7,    30] loss: 0.710\n",
      "[7,    60] loss: 0.753\n",
      "[7,    90] loss: 0.653\n",
      "[7,   120] loss: 0.672\n",
      "[7,   150] loss: 0.707\n",
      "[7,   180] loss: 0.657\n",
      "[7,   210] loss: 0.718\n",
      "[7,   240] loss: 0.715\n",
      "[7,   270] loss: 0.699\n",
      "[7,   300] loss: 0.665\n",
      "[7,   330] loss: 0.731\n",
      "[7,   360] loss: 0.724\n",
      "[7,   390] loss: 0.696\n",
      "=====epoch 7 start======\n",
      "[8,    30] loss: 0.738\n",
      "[8,    60] loss: 0.700\n",
      "[8,    90] loss: 0.683\n",
      "[8,   120] loss: 0.718\n",
      "[8,   150] loss: 0.681\n",
      "[8,   180] loss: 0.725\n",
      "[8,   210] loss: 0.739\n",
      "[8,   240] loss: 0.688\n",
      "[8,   270] loss: 0.649\n",
      "[8,   300] loss: 0.662\n",
      "[8,   330] loss: 0.697\n",
      "[8,   360] loss: 0.722\n",
      "[8,   390] loss: 0.728\n",
      "=====epoch 8 start======\n",
      "[9,    30] loss: 0.700\n",
      "[9,    60] loss: 0.712\n",
      "[9,    90] loss: 0.658\n",
      "[9,   120] loss: 0.738\n",
      "[9,   150] loss: 0.729\n",
      "[9,   180] loss: 0.675\n",
      "[9,   210] loss: 0.720\n",
      "[9,   240] loss: 0.670\n",
      "[9,   270] loss: 0.709\n",
      "[9,   300] loss: 0.665\n",
      "[9,   330] loss: 0.696\n",
      "[9,   360] loss: 0.654\n",
      "[9,   390] loss: 0.719\n",
      "=====epoch 9 start======\n",
      "[10,    30] loss: 0.730\n",
      "[10,    60] loss: 0.705\n",
      "[10,    90] loss: 0.747\n",
      "[10,   120] loss: 0.700\n",
      "[10,   150] loss: 0.721\n",
      "[10,   180] loss: 0.689\n",
      "[10,   210] loss: 0.737\n",
      "[10,   240] loss: 0.703\n",
      "[10,   270] loss: 0.725\n",
      "[10,   300] loss: 0.739\n",
      "[10,   330] loss: 0.713\n",
      "[10,   360] loss: 0.702\n",
      "[10,   390] loss: 0.670\n",
      "=====epoch 10 start======\n",
      "[11,    30] loss: 0.688\n",
      "[11,    60] loss: 0.756\n",
      "[11,    90] loss: 0.730\n",
      "[11,   120] loss: 0.705\n",
      "[11,   150] loss: 0.643\n",
      "[11,   180] loss: 0.757\n",
      "[11,   210] loss: 0.702\n",
      "[11,   240] loss: 0.683\n",
      "[11,   270] loss: 0.740\n",
      "[11,   300] loss: 0.689\n",
      "[11,   330] loss: 0.692\n",
      "[11,   360] loss: 0.724\n",
      "[11,   390] loss: 0.690\n",
      "=====epoch 11 start======\n",
      "[12,    30] loss: 0.692\n",
      "[12,    60] loss: 0.732\n",
      "[12,    90] loss: 0.678\n",
      "[12,   120] loss: 0.680\n",
      "[12,   150] loss: 0.741\n",
      "[12,   180] loss: 0.638\n",
      "[12,   210] loss: 0.693\n",
      "[12,   240] loss: 0.730\n",
      "[12,   270] loss: 0.708\n",
      "[12,   300] loss: 0.742\n",
      "[12,   330] loss: 0.679\n",
      "[12,   360] loss: 0.726\n",
      "[12,   390] loss: 0.661\n",
      "=====epoch 12 start======\n",
      "[13,    30] loss: 0.701\n",
      "[13,    60] loss: 0.815\n",
      "[13,    90] loss: 0.651\n",
      "[13,   120] loss: 0.729\n",
      "[13,   150] loss: 0.740\n",
      "[13,   180] loss: 0.744\n",
      "[13,   210] loss: 0.711\n",
      "[13,   240] loss: 0.718\n",
      "[13,   270] loss: 0.776\n",
      "[13,   300] loss: 0.658\n",
      "[13,   330] loss: 0.711\n",
      "[13,   360] loss: 0.714\n",
      "[13,   390] loss: 0.672\n",
      "=====epoch 13 start======\n",
      "[14,    30] loss: 0.700\n",
      "[14,    60] loss: 0.685\n",
      "[14,    90] loss: 0.695\n",
      "[14,   120] loss: 0.716\n",
      "[14,   150] loss: 0.647\n",
      "[14,   180] loss: 0.722\n",
      "[14,   210] loss: 0.672\n",
      "[14,   240] loss: 0.681\n",
      "[14,   270] loss: 0.735\n",
      "[14,   300] loss: 0.670\n",
      "[14,   330] loss: 0.610\n",
      "[14,   360] loss: 0.656\n",
      "[14,   390] loss: 0.701\n",
      "=====epoch 14 start======\n",
      "[15,    30] loss: 0.632\n",
      "[15,    60] loss: 0.689\n",
      "[15,    90] loss: 0.679\n",
      "[15,   120] loss: 0.699\n",
      "[15,   150] loss: 0.678\n",
      "[15,   180] loss: 0.644\n",
      "[15,   210] loss: 0.676\n",
      "[15,   240] loss: 0.671\n",
      "[15,   270] loss: 0.669\n",
      "[15,   300] loss: 0.670\n",
      "[15,   330] loss: 0.728\n",
      "[15,   360] loss: 0.672\n",
      "[15,   390] loss: 0.736\n",
      "=====epoch 15 start======\n",
      "[16,    30] loss: 0.726\n",
      "[16,    60] loss: 0.673\n",
      "[16,    90] loss: 0.711\n",
      "[16,   120] loss: 0.635\n",
      "[16,   150] loss: 0.717\n",
      "[16,   180] loss: 0.766\n",
      "[16,   210] loss: 0.711\n",
      "[16,   240] loss: 0.690\n",
      "[16,   270] loss: 0.651\n",
      "[16,   300] loss: 0.611\n",
      "[16,   330] loss: 0.727\n",
      "[16,   360] loss: 0.700\n",
      "[16,   390] loss: 0.774\n",
      "=====epoch 16 start======\n",
      "[17,    30] loss: 0.698\n",
      "[17,    60] loss: 0.688\n",
      "[17,    90] loss: 0.693\n",
      "[17,   120] loss: 0.724\n",
      "[17,   150] loss: 0.659\n",
      "[17,   180] loss: 0.665\n",
      "[17,   210] loss: 0.690\n",
      "[17,   240] loss: 0.680\n",
      "[17,   270] loss: 0.746\n",
      "[17,   300] loss: 0.715\n",
      "[17,   330] loss: 0.666\n",
      "[17,   360] loss: 0.676\n",
      "[17,   390] loss: 0.702\n",
      "=====epoch 17 start======\n",
      "[18,    30] loss: 0.706\n",
      "[18,    60] loss: 0.662\n",
      "[18,    90] loss: 0.701\n",
      "[18,   120] loss: 0.649\n",
      "[18,   150] loss: 0.692\n",
      "[18,   180] loss: 0.669\n",
      "[18,   210] loss: 0.747\n",
      "[18,   240] loss: 0.631\n",
      "[18,   270] loss: 0.714\n",
      "[18,   300] loss: 0.711\n",
      "[18,   330] loss: 0.661\n",
      "[18,   360] loss: 0.692\n",
      "[18,   390] loss: 0.728\n",
      "=====epoch 18 start======\n",
      "[19,    30] loss: 0.696\n",
      "[19,    60] loss: 0.686\n",
      "[19,    90] loss: 0.669\n",
      "[19,   120] loss: 0.687\n",
      "[19,   150] loss: 0.704\n",
      "[19,   180] loss: 0.671\n",
      "[19,   210] loss: 0.681\n",
      "[19,   240] loss: 0.722\n",
      "[19,   270] loss: 0.682\n",
      "[19,   300] loss: 0.674\n",
      "[19,   330] loss: 0.655\n",
      "[19,   360] loss: 0.654\n",
      "[19,   390] loss: 0.764\n",
      "=====epoch 19 start======\n",
      "[20,    30] loss: 0.683\n",
      "[20,    60] loss: 0.694\n",
      "[20,    90] loss: 0.691\n",
      "[20,   120] loss: 0.610\n",
      "[20,   150] loss: 0.676\n",
      "[20,   180] loss: 0.666\n",
      "[20,   210] loss: 0.670\n",
      "[20,   240] loss: 0.718\n",
      "[20,   270] loss: 0.638\n",
      "[20,   300] loss: 0.685\n",
      "[20,   330] loss: 0.688\n",
      "[20,   360] loss: 0.674\n",
      "[20,   390] loss: 0.695\n",
      "=====epoch 20 start======\n",
      "[21,    30] loss: 0.676\n",
      "[21,    60] loss: 0.677\n",
      "[21,    90] loss: 0.655\n",
      "[21,   120] loss: 0.716\n",
      "[21,   150] loss: 0.729\n",
      "[21,   180] loss: 0.703\n",
      "[21,   210] loss: 0.640\n",
      "[21,   240] loss: 0.686\n",
      "[21,   270] loss: 0.669\n",
      "[21,   300] loss: 0.683\n",
      "[21,   330] loss: 0.657\n",
      "[21,   360] loss: 0.635\n",
      "[21,   390] loss: 0.749\n",
      "=====epoch 21 start======\n",
      "[22,    30] loss: 0.675\n",
      "[22,    60] loss: 0.705\n",
      "[22,    90] loss: 0.669\n",
      "[22,   120] loss: 0.725\n",
      "[22,   150] loss: 0.655\n",
      "[22,   180] loss: 0.639\n",
      "[22,   210] loss: 0.671\n",
      "[22,   240] loss: 0.713\n",
      "[22,   270] loss: 0.638\n",
      "[22,   300] loss: 0.714\n",
      "[22,   330] loss: 0.668\n",
      "[22,   360] loss: 0.669\n",
      "[22,   390] loss: 0.646\n",
      "=====epoch 22 start======\n",
      "[23,    30] loss: 0.688\n",
      "[23,    60] loss: 0.731\n",
      "[23,    90] loss: 0.712\n",
      "[23,   120] loss: 0.690\n",
      "[23,   150] loss: 0.716\n",
      "[23,   180] loss: 0.708\n",
      "[23,   210] loss: 0.673\n",
      "[23,   240] loss: 0.691\n",
      "[23,   270] loss: 0.650\n",
      "[23,   300] loss: 0.687\n",
      "[23,   330] loss: 0.707\n",
      "[23,   360] loss: 0.654\n",
      "[23,   390] loss: 0.738\n",
      "=====epoch 23 start======\n",
      "[24,    30] loss: 0.709\n",
      "[24,    60] loss: 0.692\n",
      "[24,    90] loss: 0.703\n",
      "[24,   120] loss: 0.696\n",
      "[24,   150] loss: 0.700\n",
      "[24,   180] loss: 0.703\n",
      "[24,   210] loss: 0.689\n",
      "[24,   240] loss: 0.670\n",
      "[24,   270] loss: 0.672\n",
      "[24,   300] loss: 0.697\n",
      "[24,   330] loss: 0.676\n",
      "[24,   360] loss: 0.716\n",
      "[24,   390] loss: 0.730\n",
      "=====epoch 24 start======\n",
      "[25,    30] loss: 0.690\n",
      "[25,    60] loss: 0.672\n",
      "[25,    90] loss: 0.693\n",
      "[25,   120] loss: 0.692\n",
      "[25,   150] loss: 0.686\n",
      "[25,   180] loss: 0.668\n",
      "[25,   210] loss: 0.751\n",
      "[25,   240] loss: 0.675\n",
      "[25,   270] loss: 0.669\n",
      "[25,   300] loss: 0.717\n",
      "[25,   330] loss: 0.724\n",
      "[25,   360] loss: 0.718\n",
      "[25,   390] loss: 0.670\n",
      "=====epoch 25 start======\n",
      "[26,    30] loss: 0.676\n",
      "[26,    60] loss: 0.682\n",
      "[26,    90] loss: 0.709\n",
      "[26,   120] loss: 0.740\n",
      "[26,   150] loss: 0.670\n",
      "[26,   180] loss: 0.668\n",
      "[26,   210] loss: 0.677\n",
      "[26,   240] loss: 0.670\n",
      "[26,   270] loss: 0.700\n",
      "[26,   300] loss: 0.689\n",
      "[26,   330] loss: 0.667\n",
      "[26,   360] loss: 0.673\n",
      "[26,   390] loss: 0.725\n",
      "=====epoch 26 start======\n",
      "[27,    30] loss: 0.720\n",
      "[27,    60] loss: 0.719\n",
      "[27,    90] loss: 0.734\n",
      "[27,   120] loss: 0.697\n",
      "[27,   150] loss: 0.646\n",
      "[27,   180] loss: 0.670\n",
      "[27,   210] loss: 0.637\n",
      "[27,   240] loss: 0.628\n",
      "[27,   270] loss: 0.693\n",
      "[27,   300] loss: 0.740\n",
      "[27,   330] loss: 0.675\n",
      "[27,   360] loss: 0.634\n",
      "[27,   390] loss: 0.694\n",
      "=====epoch 27 start======\n",
      "[28,    30] loss: 0.668\n",
      "[28,    60] loss: 0.640\n",
      "[28,    90] loss: 0.663\n",
      "[28,   120] loss: 0.664\n",
      "[28,   150] loss: 0.695\n",
      "[28,   180] loss: 0.690\n",
      "[28,   210] loss: 0.683\n",
      "[28,   240] loss: 0.636\n",
      "[28,   270] loss: 0.680\n",
      "[28,   300] loss: 0.706\n",
      "[28,   330] loss: 0.694\n",
      "[28,   360] loss: 0.688\n",
      "[28,   390] loss: 0.667\n",
      "=====epoch 28 start======\n",
      "[29,    30] loss: 0.684\n",
      "[29,    60] loss: 0.714\n",
      "[29,    90] loss: 0.698\n",
      "[29,   120] loss: 0.675\n",
      "[29,   150] loss: 0.705\n",
      "[29,   180] loss: 0.691\n",
      "[29,   210] loss: 0.697\n",
      "[29,   240] loss: 0.660\n",
      "[29,   270] loss: 0.700\n",
      "[29,   300] loss: 0.725\n",
      "[29,   330] loss: 0.631\n",
      "[29,   360] loss: 0.672\n",
      "[29,   390] loss: 0.682\n",
      "=====epoch 29 start======\n",
      "[30,    30] loss: 0.610\n",
      "[30,    60] loss: 0.691\n",
      "[30,    90] loss: 0.690\n",
      "[30,   120] loss: 0.678\n",
      "[30,   150] loss: 0.677\n",
      "[30,   180] loss: 0.674\n",
      "[30,   210] loss: 0.678\n",
      "[30,   240] loss: 0.658\n",
      "[30,   270] loss: 0.708\n",
      "[30,   300] loss: 0.684\n",
      "[30,   330] loss: 0.698\n",
      "[30,   360] loss: 0.655\n",
      "[30,   390] loss: 0.698\n",
      "=====epoch 30 start======\n",
      "[31,    30] loss: 0.659\n",
      "[31,    60] loss: 0.688\n",
      "[31,    90] loss: 0.633\n",
      "[31,   120] loss: 0.685\n",
      "[31,   150] loss: 0.678\n",
      "[31,   180] loss: 0.705\n",
      "[31,   210] loss: 0.691\n",
      "[31,   240] loss: 0.707\n",
      "[31,   270] loss: 0.692\n",
      "[31,   300] loss: 0.715\n",
      "[31,   330] loss: 0.628\n",
      "[31,   360] loss: 0.682\n",
      "[31,   390] loss: 0.687\n",
      "=====epoch 31 start======\n",
      "[32,    30] loss: 0.703\n",
      "[32,    60] loss: 0.655\n",
      "[32,    90] loss: 0.749\n",
      "[32,   120] loss: 0.679\n",
      "[32,   150] loss: 0.719\n",
      "[32,   180] loss: 0.670\n",
      "[32,   210] loss: 0.656\n",
      "[32,   240] loss: 0.675\n",
      "[32,   270] loss: 0.691\n",
      "[32,   300] loss: 0.682\n",
      "[32,   330] loss: 0.670\n",
      "[32,   360] loss: 0.663\n",
      "[32,   390] loss: 0.665\n",
      "=====epoch 32 start======\n",
      "[33,    30] loss: 0.674\n",
      "[33,    60] loss: 0.702\n",
      "[33,    90] loss: 0.704\n",
      "[33,   120] loss: 0.632\n",
      "[33,   150] loss: 0.675\n",
      "[33,   180] loss: 0.711\n",
      "[33,   210] loss: 0.739\n",
      "[33,   240] loss: 0.677\n",
      "[33,   270] loss: 0.676\n",
      "[33,   300] loss: 0.685\n",
      "[33,   330] loss: 0.683\n",
      "[33,   360] loss: 0.661\n",
      "[33,   390] loss: 0.653\n",
      "=====epoch 33 start======\n",
      "[34,    30] loss: 0.668\n",
      "[34,    60] loss: 0.738\n",
      "[34,    90] loss: 0.664\n",
      "[34,   120] loss: 0.589\n",
      "[34,   150] loss: 0.633\n",
      "[34,   180] loss: 0.651\n",
      "[34,   210] loss: 0.667\n",
      "[34,   240] loss: 0.637\n",
      "[34,   270] loss: 0.690\n",
      "[34,   300] loss: 0.744\n",
      "[34,   330] loss: 0.650\n",
      "[34,   360] loss: 0.653\n",
      "[34,   390] loss: 0.691\n",
      "=====epoch 34 start======\n",
      "[35,    30] loss: 0.675\n",
      "[35,    60] loss: 0.639\n",
      "[35,    90] loss: 0.692\n",
      "[35,   120] loss: 0.639\n",
      "[35,   150] loss: 0.690\n",
      "[35,   180] loss: 0.650\n",
      "[35,   210] loss: 0.626\n",
      "[35,   240] loss: 0.672\n",
      "[35,   270] loss: 0.734\n",
      "[35,   300] loss: 0.710\n",
      "[35,   330] loss: 0.706\n",
      "[35,   360] loss: 0.701\n",
      "[35,   390] loss: 0.695\n",
      "=====epoch 35 start======\n",
      "[36,    30] loss: 0.673\n",
      "[36,    60] loss: 0.618\n",
      "[36,    90] loss: 0.656\n",
      "[36,   120] loss: 0.637\n",
      "[36,   150] loss: 0.714\n",
      "[36,   180] loss: 0.684\n",
      "[36,   210] loss: 0.740\n",
      "[36,   240] loss: 0.734\n",
      "[36,   270] loss: 0.688\n",
      "[36,   300] loss: 0.660\n",
      "[36,   330] loss: 0.686\n",
      "[36,   360] loss: 0.707\n",
      "[36,   390] loss: 0.715\n",
      "=====epoch 36 start======\n",
      "[37,    30] loss: 0.676\n",
      "[37,    60] loss: 0.714\n",
      "[37,    90] loss: 0.658\n",
      "[37,   120] loss: 0.698\n",
      "[37,   150] loss: 0.653\n",
      "[37,   180] loss: 0.702\n",
      "[37,   210] loss: 0.726\n",
      "[37,   240] loss: 0.704\n",
      "[37,   270] loss: 0.666\n",
      "[37,   300] loss: 0.679\n",
      "[37,   330] loss: 0.664\n",
      "[37,   360] loss: 0.718\n",
      "[37,   390] loss: 0.723\n",
      "=====epoch 37 start======\n",
      "[38,    30] loss: 0.733\n",
      "[38,    60] loss: 0.670\n",
      "[38,    90] loss: 0.644\n",
      "[38,   120] loss: 0.663\n",
      "[38,   150] loss: 0.707\n",
      "[38,   180] loss: 0.682\n",
      "[38,   210] loss: 0.678\n",
      "[38,   240] loss: 0.689\n",
      "[38,   270] loss: 0.690\n",
      "[38,   300] loss: 0.670\n",
      "[38,   330] loss: 0.639\n",
      "[38,   360] loss: 0.644\n",
      "[38,   390] loss: 0.707\n",
      "=====epoch 38 start======\n",
      "[39,    30] loss: 0.695\n",
      "[39,    60] loss: 0.719\n",
      "[39,    90] loss: 0.703\n",
      "[39,   120] loss: 0.743\n",
      "[39,   150] loss: 0.661\n",
      "[39,   180] loss: 0.630\n",
      "[39,   210] loss: 0.720\n",
      "[39,   240] loss: 0.661\n",
      "[39,   270] loss: 0.658\n",
      "[39,   300] loss: 0.671\n",
      "[39,   330] loss: 0.659\n",
      "[39,   360] loss: 0.618\n",
      "[39,   390] loss: 0.690\n",
      "=====epoch 39 start======\n",
      "[40,    30] loss: 0.702\n",
      "[40,    60] loss: 0.672\n",
      "[40,    90] loss: 0.672\n",
      "[40,   120] loss: 0.666\n",
      "[40,   150] loss: 0.678\n",
      "[40,   180] loss: 0.647\n",
      "[40,   210] loss: 0.642\n",
      "[40,   240] loss: 0.709\n",
      "[40,   270] loss: 0.690\n",
      "[40,   300] loss: 0.696\n",
      "[40,   330] loss: 0.687\n",
      "[40,   360] loss: 0.679\n",
      "[40,   390] loss: 0.697\n",
      "=====epoch 40 start======\n",
      "[41,    30] loss: 0.669\n",
      "[41,    60] loss: 0.708\n",
      "[41,    90] loss: 0.695\n",
      "[41,   120] loss: 0.636\n",
      "[41,   150] loss: 0.711\n",
      "[41,   180] loss: 0.668\n",
      "[41,   210] loss: 0.673\n",
      "[41,   240] loss: 0.707\n",
      "[41,   270] loss: 0.628\n",
      "[41,   300] loss: 0.685\n",
      "[41,   330] loss: 0.619\n",
      "[41,   360] loss: 0.723\n",
      "[41,   390] loss: 0.677\n",
      "=====epoch 41 start======\n",
      "[42,    30] loss: 0.685\n",
      "[42,    60] loss: 0.639\n",
      "[42,    90] loss: 0.643\n",
      "[42,   120] loss: 0.647\n",
      "[42,   150] loss: 0.657\n",
      "[42,   180] loss: 0.660\n",
      "[42,   210] loss: 0.665\n",
      "[42,   240] loss: 0.684\n",
      "[42,   270] loss: 0.648\n",
      "[42,   300] loss: 0.644\n",
      "[42,   330] loss: 0.705\n",
      "[42,   360] loss: 0.666\n",
      "[42,   390] loss: 0.689\n",
      "=====epoch 42 start======\n",
      "[43,    30] loss: 0.729\n",
      "[43,    60] loss: 0.716\n",
      "[43,    90] loss: 0.725\n",
      "[43,   120] loss: 0.687\n",
      "[43,   150] loss: 0.674\n",
      "[43,   180] loss: 0.733\n",
      "[43,   210] loss: 0.683\n",
      "[43,   240] loss: 0.671\n",
      "[43,   270] loss: 0.665\n",
      "[43,   300] loss: 0.737\n",
      "[43,   330] loss: 0.657\n",
      "[43,   360] loss: 0.675\n",
      "[43,   390] loss: 0.740\n",
      "=====epoch 43 start======\n",
      "[44,    30] loss: 0.705\n",
      "[44,    60] loss: 0.682\n",
      "[44,    90] loss: 0.694\n",
      "[44,   120] loss: 0.660\n",
      "[44,   150] loss: 0.656\n",
      "[44,   180] loss: 0.685\n",
      "[44,   210] loss: 0.684\n",
      "[44,   240] loss: 0.648\n",
      "[44,   270] loss: 0.690\n",
      "[44,   300] loss: 0.647\n",
      "[44,   330] loss: 0.686\n",
      "[44,   360] loss: 0.689\n",
      "[44,   390] loss: 0.680\n",
      "=====epoch 44 start======\n",
      "[45,    30] loss: 0.722\n",
      "[45,    60] loss: 0.705\n",
      "[45,    90] loss: 0.672\n",
      "[45,   120] loss: 0.638\n",
      "[45,   150] loss: 0.647\n",
      "[45,   180] loss: 0.657\n",
      "[45,   210] loss: 0.609\n",
      "[45,   240] loss: 0.665\n",
      "[45,   270] loss: 0.705\n",
      "[45,   300] loss: 0.655\n",
      "[45,   330] loss: 0.636\n",
      "[45,   360] loss: 0.656\n",
      "[45,   390] loss: 0.621\n",
      "=====epoch 45 start======\n",
      "[46,    30] loss: 0.702\n",
      "[46,    60] loss: 0.716\n",
      "[46,    90] loss: 0.737\n",
      "[46,   120] loss: 0.661\n",
      "[46,   150] loss: 0.688\n",
      "[46,   180] loss: 0.736\n",
      "[46,   210] loss: 0.640\n",
      "[46,   240] loss: 0.671\n",
      "[46,   270] loss: 0.686\n",
      "[46,   300] loss: 0.646\n",
      "[46,   330] loss: 0.675\n",
      "[46,   360] loss: 0.656\n",
      "[46,   390] loss: 0.719\n",
      "=====epoch 46 start======\n",
      "[47,    30] loss: 0.689\n",
      "[47,    60] loss: 0.651\n",
      "[47,    90] loss: 0.709\n",
      "[47,   120] loss: 0.699\n",
      "[47,   150] loss: 0.685\n",
      "[47,   180] loss: 0.673\n",
      "[47,   210] loss: 0.648\n",
      "[47,   240] loss: 0.680\n",
      "[47,   270] loss: 0.692\n",
      "[47,   300] loss: 0.713\n",
      "[47,   330] loss: 0.657\n",
      "[47,   360] loss: 0.689\n",
      "[47,   390] loss: 0.701\n",
      "=====epoch 47 start======\n",
      "[48,    30] loss: 0.708\n",
      "[48,    60] loss: 0.640\n",
      "[48,    90] loss: 0.689\n",
      "[48,   120] loss: 0.702\n",
      "[48,   150] loss: 0.702\n",
      "[48,   180] loss: 0.683\n",
      "[48,   210] loss: 0.724\n",
      "[48,   240] loss: 0.670\n",
      "[48,   270] loss: 0.679\n",
      "[48,   300] loss: 0.631\n",
      "[48,   330] loss: 0.708\n",
      "[48,   360] loss: 0.712\n",
      "[48,   390] loss: 0.684\n",
      "=====epoch 48 start======\n",
      "[49,    30] loss: 0.720\n",
      "[49,    60] loss: 0.702\n",
      "[49,    90] loss: 0.688\n",
      "[49,   120] loss: 0.678\n",
      "[49,   150] loss: 0.663\n",
      "[49,   180] loss: 0.685\n",
      "[49,   210] loss: 0.681\n",
      "[49,   240] loss: 0.697\n",
      "[49,   270] loss: 0.718\n",
      "[49,   300] loss: 0.678\n",
      "[49,   330] loss: 0.638\n",
      "[49,   360] loss: 0.710\n",
      "[49,   390] loss: 0.695\n",
      "=====epoch 49 start======\n",
      "[50,    30] loss: 0.681\n",
      "[50,    60] loss: 0.656\n",
      "[50,    90] loss: 0.679\n",
      "[50,   120] loss: 0.723\n",
      "[50,   150] loss: 0.639\n",
      "[50,   180] loss: 0.674\n",
      "[50,   210] loss: 0.628\n",
      "[50,   240] loss: 0.702\n",
      "[50,   270] loss: 0.670\n",
      "[50,   300] loss: 0.654\n",
      "[50,   330] loss: 0.713\n",
      "[50,   360] loss: 0.658\n",
      "[50,   390] loss: 0.653\n",
      "=====epoch 50 start======\n",
      "[51,    30] loss: 0.731\n",
      "[51,    60] loss: 0.683\n",
      "[51,    90] loss: 0.628\n",
      "[51,   120] loss: 0.705\n",
      "[51,   150] loss: 0.615\n",
      "[51,   180] loss: 0.659\n",
      "[51,   210] loss: 0.659\n",
      "[51,   240] loss: 0.679\n",
      "[51,   270] loss: 0.700\n",
      "[51,   300] loss: 0.671\n",
      "[51,   330] loss: 0.709\n",
      "[51,   360] loss: 0.656\n",
      "[51,   390] loss: 0.743\n",
      "=====epoch 51 start======\n",
      "[52,    30] loss: 0.639\n",
      "[52,    60] loss: 0.688\n",
      "[52,    90] loss: 0.737\n",
      "[52,   120] loss: 0.709\n",
      "[52,   150] loss: 0.679\n",
      "[52,   180] loss: 0.669\n",
      "[52,   210] loss: 0.673\n",
      "[52,   240] loss: 0.680\n",
      "[52,   270] loss: 0.637\n",
      "[52,   300] loss: 0.603\n",
      "[52,   330] loss: 0.613\n",
      "[52,   360] loss: 0.696\n",
      "[52,   390] loss: 0.654\n",
      "=====epoch 52 start======\n",
      "[53,    30] loss: 0.727\n",
      "[53,    60] loss: 0.685\n",
      "[53,    90] loss: 0.699\n",
      "[53,   120] loss: 0.769\n",
      "[53,   150] loss: 0.679\n",
      "[53,   180] loss: 0.713\n",
      "[53,   210] loss: 0.659\n",
      "[53,   240] loss: 0.610\n",
      "[53,   270] loss: 0.667\n",
      "[53,   300] loss: 0.663\n",
      "[53,   330] loss: 0.649\n",
      "[53,   360] loss: 0.687\n",
      "[53,   390] loss: 0.713\n",
      "=====epoch 53 start======\n",
      "[54,    30] loss: 0.674\n",
      "[54,    60] loss: 0.673\n",
      "[54,    90] loss: 0.625\n",
      "[54,   120] loss: 0.661\n",
      "[54,   150] loss: 0.653\n",
      "[54,   180] loss: 0.719\n",
      "[54,   210] loss: 0.675\n",
      "[54,   240] loss: 0.633\n",
      "[54,   270] loss: 0.708\n",
      "[54,   300] loss: 0.673\n",
      "[54,   330] loss: 0.686\n",
      "[54,   360] loss: 0.671\n",
      "[54,   390] loss: 0.684\n",
      "=====epoch 54 start======\n",
      "[55,    30] loss: 0.683\n",
      "[55,    60] loss: 0.679\n",
      "[55,    90] loss: 0.647\n",
      "[55,   120] loss: 0.691\n",
      "[55,   150] loss: 0.676\n",
      "[55,   180] loss: 0.675\n",
      "[55,   210] loss: 0.719\n",
      "[55,   240] loss: 0.747\n",
      "[55,   270] loss: 0.660\n",
      "[55,   300] loss: 0.687\n",
      "[55,   330] loss: 0.630\n",
      "[55,   360] loss: 0.666\n",
      "[55,   390] loss: 0.669\n",
      "=====epoch 55 start======\n",
      "[56,    30] loss: 0.661\n",
      "[56,    60] loss: 0.680\n",
      "[56,    90] loss: 0.710\n",
      "[56,   120] loss: 0.681\n",
      "[56,   150] loss: 0.647\n",
      "[56,   180] loss: 0.682\n",
      "[56,   210] loss: 0.655\n",
      "[56,   240] loss: 0.718\n",
      "[56,   270] loss: 0.665\n",
      "[56,   300] loss: 0.723\n",
      "[56,   330] loss: 0.721\n",
      "[56,   360] loss: 0.695\n",
      "[56,   390] loss: 0.691\n",
      "=====epoch 56 start======\n",
      "[57,    30] loss: 0.696\n",
      "[57,    60] loss: 0.704\n",
      "[57,    90] loss: 0.669\n",
      "[57,   120] loss: 0.686\n",
      "[57,   150] loss: 0.659\n",
      "[57,   180] loss: 0.706\n",
      "[57,   210] loss: 0.673\n",
      "[57,   240] loss: 0.621\n",
      "[57,   270] loss: 0.662\n",
      "[57,   300] loss: 0.728\n",
      "[57,   330] loss: 0.648\n",
      "[57,   360] loss: 0.673\n",
      "[57,   390] loss: 0.691\n",
      "=====epoch 57 start======\n",
      "[58,    30] loss: 0.672\n",
      "[58,    60] loss: 0.622\n",
      "[58,    90] loss: 0.720\n",
      "[58,   120] loss: 0.676\n",
      "[58,   150] loss: 0.634\n",
      "[58,   180] loss: 0.662\n",
      "[58,   210] loss: 0.679\n",
      "[58,   240] loss: 0.601\n",
      "[58,   270] loss: 0.707\n",
      "[58,   300] loss: 0.703\n",
      "[58,   330] loss: 0.641\n",
      "[58,   360] loss: 0.670\n",
      "[58,   390] loss: 0.709\n",
      "=====epoch 58 start======\n",
      "[59,    30] loss: 0.617\n",
      "[59,    60] loss: 0.648\n",
      "[59,    90] loss: 0.665\n",
      "[59,   120] loss: 0.706\n",
      "[59,   150] loss: 0.673\n",
      "[59,   180] loss: 0.691\n",
      "[59,   210] loss: 0.650\n",
      "[59,   240] loss: 0.685\n",
      "[59,   270] loss: 0.728\n",
      "[59,   300] loss: 0.650\n",
      "[59,   330] loss: 0.683\n",
      "[59,   360] loss: 0.701\n",
      "[59,   390] loss: 0.676\n",
      "=====epoch 59 start======\n",
      "[60,    30] loss: 0.693\n",
      "[60,    60] loss: 0.657\n",
      "[60,    90] loss: 0.662\n",
      "[60,   120] loss: 0.720\n",
      "[60,   150] loss: 0.673\n",
      "[60,   180] loss: 0.629\n",
      "[60,   210] loss: 0.668\n",
      "[60,   240] loss: 0.666\n",
      "[60,   270] loss: 0.738\n",
      "[60,   300] loss: 0.713\n",
      "[60,   330] loss: 0.601\n",
      "[60,   360] loss: 0.660\n",
      "[60,   390] loss: 0.686\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "def training(model,train_dataloader,optimizer,loss_fn,max_epochs=30,subset=False):\n",
    "    model.to(device)\n",
    "    for epoch in range(max_epochs):\n",
    "        running_loss = 0.0\n",
    "        print(\"=====epoch %d start======\"%(epoch))\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            y_pred = vgg16_bn(inputs)\n",
    "            # compute loss \n",
    "            loss = loss_fn(y_pred,labels)\n",
    "            #print(epoch,i, loss.item())\n",
    "            \n",
    "            if not torch.isfinite(loss):\n",
    "                print(\"WARNING: non-finite loss, ending training\")\n",
    "                exit(1)\n",
    "            else :\n",
    "                running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 30 == 29: \n",
    "                print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/30)) \n",
    "                running_loss = 0.0\n",
    "            if subset is True and i>=400:\n",
    "                break\n",
    "training(vgg16_bn,train_dataloader,optimizer,loss_fn,max_epochs,True)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval start\n",
      "step : 50 / 781\n",
      "step : 100 / 781\n",
      "step : 150 / 781\n",
      "step : 200 / 781\n",
      "step : 250 / 781\n",
      "step : 300 / 781\n",
      "step : 350 / 781\n",
      "step : 400 / 781\n",
      "step : 450 / 781\n",
      "step : 500 / 781\n",
      "step : 550 / 781\n",
      "step : 600 / 781\n",
      "step : 650 / 781\n",
      "step : 700 / 781\n",
      "step : 750 / 781\n",
      "\n",
      "acc of vgg16 on imagenet : 73.5600 %\n"
     ]
    }
   ],
   "source": [
    "# evaluation phase\n",
    "criterion = torch.nn.CrossEntropyLoss(size_average = None).to(device)\n",
    "\n",
    "vgg16_bn.cuda()\n",
    "vgg16_bn.eval()\n",
    "total = 0\n",
    "correct =0\n",
    "with torch.no_grad():\n",
    "    print(\"eval start\")\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        inputs,labels = data\n",
    "        inputs,labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        y_hat = vgg16_bn(inputs)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if(i%50 == 49):\n",
    "            print(\"step : {} / {}\".format(i + 1, int(len(val_dataset)/labels.size(0))))\n",
    "            #print(\".\",end=\"\")\n",
    "    print(\"\")\n",
    "print(\"acc of vgg16 on imagenet : %.4f %%\" %(100*correct/total))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval start\n",
      "step : 50 / 781\n",
      "step : 100 / 781\n",
      "step : 150 / 781\n",
      "step : 200 / 781\n",
      "step : 250 / 781\n",
      "step : 300 / 781\n",
      "step : 350 / 781\n",
      "step : 400 / 781\n",
      "step : 450 / 781\n",
      "step : 500 / 781\n",
      "step : 550 / 781\n",
      "step : 600 / 781\n",
      "step : 650 / 781\n",
      "step : 700 / 781\n",
      "step : 750 / 781\n",
      "\n",
      "acc of vgg16 not shuffle on imagenet : 73.5540 %\n"
     ]
    }
   ],
   "source": [
    "# evaluation phase\n",
    "criterion = torch.nn.CrossEntropyLoss(size_average = None).to(device)\n",
    "\n",
    "vgg16_bn.cuda()\n",
    "vgg16_bn.eval()\n",
    "total = 0\n",
    "correct =0\n",
    "with torch.no_grad():\n",
    "    print(\"eval start\")\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        inputs,labels = data\n",
    "        inputs,labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        y_hat = vgg16_bn(inputs)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if(i%50 == 49):\n",
    "            print(\"step : {} / {}\".format(i + 1, int(len(val_dataset)/labels.size(0))))\n",
    "            #print(\".\",end=\"\")\n",
    "    print(\"\")\n",
    "print(\"acc of vgg16 not shuffle on imagenet : %.4f %%\" %(100*correct/total))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval start\n",
      "step : 50 / 781\n",
      "step : 100 / 781\n",
      "step : 150 / 781\n",
      "step : 200 / 781\n",
      "step : 250 / 781\n",
      "step : 300 / 781\n",
      "step : 350 / 781\n",
      "step : 400 / 781\n",
      "step : 450 / 781\n",
      "step : 500 / 781\n",
      "step : 550 / 781\n",
      "step : 600 / 781\n",
      "step : 650 / 781\n",
      "step : 700 / 781\n",
      "step : 750 / 781\n",
      "\n",
      "acc of vgg16 not shuffle on imagenet : 73.3600 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vgg16_bn = torchvision.models.vgg16_bn(pretrained=True) # not error inserted\n",
    "# evaluation phase\n",
    "criterion = torch.nn.CrossEntropyLoss(size_average = None).to(device)\n",
    "\n",
    "vgg16_bn.cuda()\n",
    "vgg16_bn.eval()\n",
    "total = 0\n",
    "correct =0\n",
    "with torch.no_grad():\n",
    "    print(\"eval start\")\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        inputs,labels = data\n",
    "        inputs,labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        y_hat = vgg16_bn(inputs)\n",
    "        _, predicted = torch.max(y_hat.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if(i%50 == 49):\n",
    "            print(\"step : {} / {}\".format(i + 1, int(len(val_dataset)/labels.size(0))))\n",
    "            #print(\".\",end=\"\")\n",
    "    print(\"\")\n",
    "print(\"acc of vgg16 not shuffle on imagenet : %.4f %%\" %(100*correct/total))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

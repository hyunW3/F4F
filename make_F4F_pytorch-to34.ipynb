{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0 0.2.2\n",
      "GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy as d_copy\n",
    "import random\n",
    "\n",
    "print(torch.__version__, torchvision.__version__)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#error_index = 0\n",
    "vgg16_bn = torchvision.models.vgg16_bn(pretrained=True)#.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef hook_register(model,error_index,num_error):\\n    param_list = []\\n    handle =[]\\n    for name,parameter in model.named_parameters():\\n        if \"features.34.weight\" in name:\\n        #print(name,\"size:\",parameter.size())\\n            param_list.append(parameter)\\n    for name,layer in model.named_modules():\\n    #print(name)\\n        if \"34\" in name  and isinstance(layer, torch.nn.modules.conv.Conv2d):\\n            print(\"input\",name,layer) # target layer Conv5_1\\n            tmp = layer.register_forward_pre_hook(error_injection(name,num_error,error_index))\\n            handle.append(tmp)\\n        if \"36\" in name :\\n            print(\"output\",name,layer)\\n            tmp = layer.register_forward_pre_hook(name)\\n            handle.append(tmp)\\n    return param_list,handle\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomness 제어 \n",
    "# https://hoya012.github.io/blog/reproducible_pytorch/\n",
    "def set_randomness(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "# func\n",
    "\n",
    "# only apply for feature part (not pooling, classfier)\n",
    "# because of layers.feature \n",
    "def split_layer(model,start,end):\n",
    "    ct = 0\n",
    "    split_model=[] # from start to Conv5_1(include ReLU)\n",
    "    for name,layers in model.named_modules():\n",
    "        #print(name,layer)\n",
    "        #print(layers.features)\n",
    "        for idx,layer in enumerate(layers.features):\n",
    "            #print(idx,layer)\n",
    "            if start <=idx and idx <=end :\n",
    "                split_model.append(layer)\n",
    "        break\n",
    "    return nn.Sequential(*split_model)\n",
    "\n",
    "def error_injection(name,num_error,start_index):\n",
    "    def hook(model,input):\n",
    "        start = start_index\n",
    "        end = start_index + num_error\n",
    "        input[0][:, start:end]=0\n",
    "        print(\"shape :\",input[0][:, start:end].size())\n",
    "    return hook\n",
    "'''\n",
    "def hook_register(model,error_index,num_error):\n",
    "    param_list = []\n",
    "    handle =[]\n",
    "    for name,parameter in model.named_parameters():\n",
    "        if \"features.34.weight\" in name:\n",
    "        #print(name,\"size:\",parameter.size())\n",
    "            param_list.append(parameter)\n",
    "    for name,layer in model.named_modules():\n",
    "    #print(name)\n",
    "        if \"34\" in name  and isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "            print(\"input\",name,layer) # target layer Conv5_1\n",
    "            tmp = layer.register_forward_pre_hook(error_injection(name,num_error,error_index))\n",
    "            handle.append(tmp)\n",
    "        if \"36\" in name :\n",
    "            print(\"output\",name,layer)\n",
    "            tmp = layer.register_forward_pre_hook(name)\n",
    "            handle.append(tmp)\n",
    "    return param_list,handle\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0624_models\t\t      kdw_F4F.ipynb\n",
      "0624_to_34models\t      make_F4F_keras.ipynb\n",
      "0624_to_34models_petImage     make_F4F_pytorch.ipynb\n",
      "acc_log_to34.txt\t      make_F4F_pytorch-to34.ipynb\n",
      "acc_log.txt\t\t      models\n",
      "autoencoder_model.ipynb       model_test\n",
      "F4F_addDense.py\t\t      output.txt\n",
      "F4F_keras.ipynb\t\t      README.md\n",
      "F4F_pytorch.ipynb\t      tmp.txt\n",
      "Filter_for_Filter_result.txt  trythis.ipynb\n",
      "find_corelation\n"
     ]
    }
   ],
   "source": [
    "!ls /media/0/hwbae0326/F4F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 3125\n"
     ]
    }
   ],
   "source": [
    "# dataset load\n",
    "batch_size = 16 # 32~ out of memory in 3080\n",
    "num_train = 128000\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_path = \"/media/1/Imagenet_dup/\"\n",
    "retrain_model_path = \"/media/0/hwbae0326/F4F/0624_to_34models/\"\n",
    "# imagenet data load\n",
    "train_dataset = dataset.ImageFolder(root=dataset_path+\"train\",\n",
    "                                       transform=transform)\n",
    "subset_train_dataset,_ = torch.utils.data.random_split(train_dataset, [num_train,len(train_dataset)-num_train])\n",
    "\n",
    "test_dataset = dataset.ImageFolder(root=dataset_path+\"val\",\n",
    "                                       transform=transform)\n",
    "'''\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=4)\n",
    "'''\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(subset_train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4) # for using subset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4)\n",
    "print(len(train_dataloader),len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(retrain_model_path) is False:\n",
    "    os.mkdir(retrain_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "set_randomness(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external variable in error_index, num_error\n",
    "\n",
    "def make_error_info(error_index, num_error):\n",
    "    data = []\n",
    "    for i in range(511,-1,-1):\n",
    "        if error_index <= i and i < error_index+num_error:\n",
    "            data.append(1)\n",
    "        else :\n",
    "            data.append(0)\n",
    "        #print(data)\n",
    "    error_info = torch.Tensor(data)\n",
    "    error_info  = error_info.unsqueeze(0).repeat(512,1)\n",
    "        #print(error_info)\n",
    "    return error_info # 512,521\n",
    "class F4F(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f4f = nn.Linear(3*3*512+512,3*3*512) # 4167,4608 filter which change feature.34 (Conv5_1)\n",
    "        # 512 x5120 사이즈로 batch 저장\n",
    "        #print(type(self.f4f.weight))\n",
    "        self.f4f_optimizer = torch.optim.SGD([self.f4f.weight,self.f4f.bias],lr=0.0005,weight_decay=1e-4)\n",
    "    def get_f4f_weight(self):\n",
    "        # fc.weight.size(),fc.bias.size()\n",
    "        return self.f4f.weight # torch.Size([4608, 5120])\n",
    "    def forward(self,x):\n",
    "        x = self.f4f(x)\n",
    "        y = torch.tanh(x)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target_model(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def get_layer(self,idx):\n",
    "        #print(self.model._modules['34'])\n",
    "        return self.model._modules[str(idx)]\n",
    "    def apply_f4f(self,f4f,error_info):\n",
    "            #print(len(self.get_layer(34).weight.data))\n",
    "            #print(self.get_layer(34).weight.data.size())\n",
    "        weight = torch.reshape(self.get_layer(34).weight.data,(512,512*3*3)).to(device) # flatten [512,5210] (batch 512)\n",
    "            #print(weight.size(),error_info.size())\n",
    "        data = torch.cat( (weight,error_info), 1 )\n",
    "            #print(data.size())\n",
    "        offset = torch.reshape(f4f(data),(512,512,3,3))\n",
    "        offset = torch.tanh(offset)\n",
    "        self.get_layer(34).weight.data = self.get_layer(34).weight.data + offset\n",
    "    def forward(self,x,f4f,error_info):\n",
    "        # apply_f4f는 매 epoch마다 동일하므로 \n",
    "        self.apply_f4f(f4f,error_info)\n",
    "        y = self.model(x)\n",
    "        return y\n",
    "class Test_model(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def get_layer(self,idx):\n",
    "        return self.model.features._modules[str(idx)]\n",
    "    def apply_f4f(self,f4f,error_info):\n",
    "        weight = torch.reshape(self.get_layer(34).weight.data,(512,512*3*3)).to(device) # flatten [512,5210] (batch 512)\n",
    "        data = torch.cat( (weight,error_info), 1 )\n",
    "        offset = torch.reshape(f4f(data),(512,512,3,3))\n",
    "        offset = torch.tanh(offset)\n",
    "        self.get_layer(34).weight.data = self.get_layer(34).weight.data + offset\n",
    "            #break # for debug\n",
    "    def forward(self,x,f4f,error_info):\n",
    "        # apply_f4f는 매 epoch마다 동일하므로 \n",
    "        self.apply_f4f(f4f,error_info)\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_layer_number = 34 # 34 conv5_1 convolution\n",
    "Out_layer_number = 36 # 36 conv5_1 relu \n",
    "error_index=0\n",
    "max_epochs = 30\n",
    "num_error = 128\n",
    "#optimizer = torch.optim.SGD(param_list,lr=0.01,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_register(model,error_index,num_error):\n",
    "    for name,layer in model.named_modules():\n",
    "        #print(name,layer)\n",
    "        if \"34\" in name  and isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "            print(\"input\",name,layer) # target layer Conv5_1\n",
    "            layer.register_forward_pre_hook(error_injection(name,num_error,error_index))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_model = split_layer(vgg16_bn,0,34)\n",
    "#print(split_model)\n",
    "original_model = d_copy(split_model).to(device)\n",
    "#param_list,handle = hook_register(split_model,error_index,num_error)\n",
    "\n",
    "hook_register(split_model,error_index,num_error)\n",
    "target_model = Target_model(split_model).to(device)\n",
    "target_model.get_layer(In_layer_number)\n",
    "\n",
    "test = d_copy(vgg16_bn).to(device)\n",
    "hook_register(test,error_index,num_error)\n",
    "test_model = Test_model(test).to(device)\n",
    "\n",
    "log_file = \"./acc_log_to34.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation phasetraining\n",
    "def eval(model,dataloader,epoch,f4f,error_info):\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct =0\n",
    "    with torch.no_grad():\n",
    "        print(\"======eval start=======\")\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "            y_hat = model(inputs,f4f,error_info)\n",
    "            _, predicted = torch.max(y_hat.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if(i%200 == 199):\n",
    "                print(\"step : %d / %d acc : %.3f\"\n",
    "                      %(i + 1,int(len(dataloader)), correct*100/total))\n",
    "                #print(\".\",end=\"\")\n",
    "        print(\"\")\n",
    "    acc = 100*correct/total\n",
    "    print(\"%dth epoch acc of %s on imagenet : %.4f %%\" %(epoch, model.__class__.__name__,acc)) \n",
    "    f = open(log_file,\"a\")\n",
    "    print(\"%dth epoch acc of %s on imagenet : %.4f %%\" %(epoch, model.__class__.__name__,acc),file=f) \n",
    "    f.close()\n",
    "    print(\"======eval  end ======\")  \n",
    "    return acc\n",
    "#torch.save(vgg16_bn.state_dict(), retrain_model_path+\"test_vgg16_bn_state_dict.pt\")\n",
    "def model_copy(model):\n",
    "    return d_copy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def training(test_model,target_model,original_model,train_dataloader,test_dataloader,loss_fn,error_idx,num_error,max_epochs=30,subset=False):\n",
    "    f4f = F4F().to(device)\n",
    "    target_model.to(device)\n",
    "    original_model.to(device)\n",
    "    error_info = make_error_info(error_index,num_error).to(device)\n",
    "    first_feature = []\n",
    "    original = []\n",
    "    optimizer = f4f.f4f_optimizer\n",
    "    for epoch in range(max_epochs):\n",
    "        running_loss = 0.0\n",
    "        total_avg_loss = 0.0\n",
    "        print(\"=====epoch %d start======\"%(epoch+1))\n",
    "        f4f.train()\n",
    "        # update f4f filter\n",
    "        #target_model.apply_f4f(f4f,error_info)\n",
    "    \n",
    "        # compare\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            print(\".\",end=\"\")\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            original_out = original_model(inputs)\n",
    "            target_out = target_model(inputs,f4f,error_info)\n",
    "            if i == 0:\n",
    "                first_feature.append(target_out[0])\n",
    "                original.append(original_out[0])\n",
    "                \n",
    "            loss = loss_fn(original_out,target_out)\n",
    "            #print(loss.size())\n",
    "            running_loss += loss.item()\n",
    "            target_model.model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 99: \n",
    "                total_avg_loss += running_loss\n",
    "                print(\"\")\n",
    "                print('[%d, %5d] loss: %.6f' % (epoch+1, i+1, running_loss/100)) \n",
    "                running_loss = 0.0\n",
    "        # save weight\n",
    "        #print((len(train_dataloader)/batch_size))\n",
    "        total_avg_loss /= int(len(train_dataloader)/batch_size)\n",
    "        acc = eval(test_model,test_dataloader,epoch,f4f,error_info)\n",
    "        \n",
    "        torch.save(f4f.get_f4f_weight(), \n",
    "               retrain_model_path+\"%s~%s_pkt_err_f4f_epoch_%s_acc_%.4f_loss_%.4f.pt\"\n",
    "               %(str(error_idx).zfill(3),str(error_idx+num_error).zfill(3),\n",
    "                str(epoch+1).zfill(2),acc,total_avg_loss))    \n",
    "    return original_out,first_feature\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********error_idx :0, num error :128**********\n",
      "=====epoch 1 start======\n",
      "....................................................................................................\n",
      "[1,   100] loss: 0.093273\n",
      "....................................................................................................\n",
      "[1,   200] loss: 0.091334\n",
      "....................................................................................................\n",
      "[1,   300] loss: 0.092237\n",
      "....................................................................................................\n",
      "[1,   400] loss: 0.092689\n",
      "....................................................................................................\n",
      "[1,   500] loss: 0.093073\n",
      "....................................................................................................\n",
      "[1,   600] loss: 0.093970\n",
      "....................................................................................................\n",
      "[1,   700] loss: 0.093563\n",
      "....................................................................................................\n",
      "[1,   800] loss: 0.093767\n",
      "....................................................................................................\n",
      "[1,   900] loss: 0.093430\n",
      "....................................................................................................\n",
      "[1,  1000] loss: 0.093331\n",
      "....................................................................................................\n",
      "[1,  1100] loss: 0.093986\n",
      "....................................................................................................\n",
      "[1,  1200] loss: 0.093767\n",
      "....................................................................................................\n",
      "[1,  1300] loss: 0.094175\n",
      "....................................................................................................\n",
      "[1,  1400] loss: 0.094355\n",
      "....................................................................................................\n",
      "[1,  1500] loss: 0.094350\n",
      "....................................................................................................\n",
      "[1,  1600] loss: 0.094706\n",
      "....................................................................................................\n",
      "[1,  1700] loss: 0.094513\n",
      "....................................................................................................\n",
      "[1,  1800] loss: 0.094121\n",
      "....................................................................................................\n",
      "[1,  1900] loss: 0.093954\n",
      "....................................................................................................\n",
      "[1,  2000] loss: 0.094430\n",
      "....................................................................................................\n",
      "[1,  2100] loss: 0.094353\n",
      "....................................................................................................\n",
      "[1,  2200] loss: 0.094347\n",
      "....................................................................................................\n",
      "[1,  2300] loss: 0.094828\n",
      "....................................................................................................\n",
      "[1,  2400] loss: 0.094441\n",
      "....................................................................................................\n",
      "[1,  2500] loss: 0.094424\n",
      "....................................................................................................\n",
      "[1,  2600] loss: 0.095068\n",
      "....................................................................................................\n",
      "[1,  2700] loss: 0.094298\n",
      "....................................................................................................\n",
      "[1,  2800] loss: 0.095521\n",
      "....................................................................................................\n",
      "[1,  2900] loss: 0.095128\n",
      "....................................................................................................\n",
      "[1,  3000] loss: 0.094674\n",
      "....................................................................................................\n",
      "[1,  3100] loss: 0.094511\n",
      "....................................................................................................\n",
      "[1,  3200] loss: 0.094651\n",
      "....................................................................................................\n",
      "[1,  3300] loss: 0.094841\n",
      "....................................................................................................\n",
      "[1,  3400] loss: 0.094866\n",
      "....................................................................................................\n",
      "[1,  3500] loss: 0.094136\n",
      "....................................................................................................\n",
      "[1,  3600] loss: 0.095409\n",
      "....................................................................................................\n",
      "[1,  3700] loss: 0.094881\n",
      "....................................................................................................\n",
      "[1,  3800] loss: 0.094735\n",
      "....................................................................................................\n",
      "[1,  3900] loss: 0.094775\n",
      "....................................................................................................\n",
      "[1,  4000] loss: 0.094701\n",
      "....................................................................................................\n",
      "[1,  4100] loss: 0.094863\n",
      "....................................................................................................\n",
      "[1,  4200] loss: 0.094578\n",
      "....................................................................................................\n",
      "[1,  4300] loss: 0.094479\n",
      "....................................................................................................\n",
      "[1,  4400] loss: 0.095376\n",
      "....................................................................................................\n",
      "[1,  4500] loss: 0.094963\n",
      "....................................................................................................\n",
      "[1,  4600] loss: 0.094729\n",
      "....................................................................................................\n",
      "[1,  4700] loss: 0.094970\n",
      "....................................................................................................\n",
      "[1,  4800] loss: 0.094319\n",
      "....................................................................................................\n",
      "[1,  4900] loss: 0.094749\n",
      "....................................................................................................\n",
      "[1,  5000] loss: 0.095025\n",
      "....................................................................................................\n",
      "[1,  5100] loss: 0.095062\n",
      "....................................................................................................\n",
      "[1,  5200] loss: 0.095070\n",
      "....................................................................................................\n",
      "[1,  5300] loss: 0.095065\n",
      "....................................................................................................\n",
      "[1,  5400] loss: 0.095352\n",
      "....................................................................................................\n",
      "[1,  5500] loss: 0.094728\n",
      "....................................................................................................\n",
      "[1,  5600] loss: 0.095822\n",
      "....................................................................................................\n",
      "[1,  5700] loss: 0.094759\n",
      "....................................................................................................\n",
      "[1,  5800] loss: 0.094920\n",
      "....................................................................................................\n",
      "[1,  5900] loss: 0.095584\n",
      "....................................................................................................\n",
      "[1,  6000] loss: 0.095120\n",
      "....................................................................................................\n",
      "[1,  6100] loss: 0.094889\n",
      "....................................................................................................\n",
      "[1,  6200] loss: 0.094303\n",
      "....................................................................................................\n",
      "[1,  6300] loss: 0.094137\n",
      "....................................................................................................\n",
      "[1,  6400] loss: 0.095139\n",
      "....................................................................................................\n",
      "[1,  6500] loss: 0.094851\n",
      "....................................................................................................\n",
      "[1,  6600] loss: 0.093979\n",
      "....................................................................................................\n",
      "[1,  6700] loss: 0.094892\n",
      "....................................................................................................\n",
      "[1,  6800] loss: 0.095426\n",
      "....................................................................................................\n",
      "[1,  6900] loss: 0.094568\n",
      "....................................................................................................\n",
      "[1,  7000] loss: 0.095110\n",
      "....................................................................................................\n",
      "[1,  7100] loss: 0.095489\n",
      "....................................................................................................\n",
      "[1,  7200] loss: 0.095415\n",
      "....................................................................................................\n",
      "[1,  7300] loss: 0.094153\n",
      "....................................................................................................\n",
      "[1,  7400] loss: 0.095067\n",
      "..............................................................................."
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss().to(device)\n",
    "#optimizer = torch.optim.SGD(param_list,lr=0.01,weight_decay=1e-4)\n",
    "first_feature = []\n",
    "original_out = []\n",
    "f = open(log_file,\"w\")\n",
    "f.close()\n",
    "for error_idx in range(0,512,num_error):\n",
    "    print(\"**********error_idx :%d, num error :%d**********\"%(error_idx, num_error))\n",
    "    f = open(log_file,\"a\")\n",
    "    print(\"**********error_idx :%d, num error :%d**********\"%(error_idx, num_error),file=f)\n",
    "    f.close()\n",
    "    split_model = split_layer(vgg16_bn,0,36)\n",
    "    #original_model = d_copy(split_model).to(device)\n",
    "    hook_register(split_model,error_index,num_error)\n",
    "    target_model = Target_model(split_model).to(device)\n",
    "    \n",
    "    tmp= training(test_model,target_model,original_model,train_dataloader,test_dataloader,loss_fn,error_idx,num_error,10,True)\n",
    "    first_feature.append(tmp[1])\n",
    "    original_out.append(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 feature 그림 보기 위한 것들입니다.\n",
    "len(first_feature),len(original_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_out[0].size())\n",
    "w = 10\n",
    "h = 10\n",
    "cols = 32\n",
    "rows = 16\n",
    "def feature_print(pic):\n",
    "    print(\"test with 'after pooling 4 feature'\")\n",
    "    fig = plt.figure(figsize=(64,32))\n",
    "    ax = []\n",
    "    for i in range(cols*rows):\n",
    "        ch = pic[i,:,:]\n",
    "        ax.append(fig.add_subplot(rows,cols,i+1))\n",
    "        ax[-1].set_title(str(i)+\"th ch (14x14)\")\n",
    "        plt.imshow(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델 (에러없이, f4f없이)을 통과한 결과\n",
    "feature_print(original_out[0][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 1\n",
    "%matplotlib inline\n",
    "feature_print(first_feature[0][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 9\n",
    "print(\"epoch 9\")\n",
    "feature_print(first_feature[0][9].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14x14 의 feature 모두 합한 결과\n",
    "tmp = first_feature[0][6][0]\n",
    "for i in range(1,512):\n",
    "    tmp += first_feature[0][6][i]\n",
    "%matplotlib inline\n",
    "plt.imshow(tmp.cpu().detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14x14 의 feature 모두 합한 결과\n",
    "print(\"original\")\n",
    "tmp1 = original_out[0][6][0]\n",
    "for i in range(1,512):\n",
    "    tmp1 += original_out[0][6][i]\n",
    "%matplotlib inline\n",
    "plt.imshow(tmp1.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 6\n",
    "print(\"epoch 6\")\n",
    "feature_print(first_feature[0][6].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

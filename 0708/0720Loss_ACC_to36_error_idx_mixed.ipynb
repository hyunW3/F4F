{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0 0.2.2\n",
      "TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy as d_copy\n",
    "import random\n",
    "\n",
    "print(torch.__version__, torchvision.__version__)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#error_index = 0\n",
    "vgg16_bn = torchvision.models.vgg16_bn(pretrained=True)#.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_layer_number = 34 # 34 conv5_1 convolution\n",
    "Out_layer_number = 36 # 36 conv5_1 relu \n",
    "error_index=0\n",
    "max_epochs = 30\n",
    "num_error = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomness 제어 \n",
    "# https://hoya012.github.io/blog/reproducible_pytorch/\n",
    "def set_randomness(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "# func\n",
    "\n",
    "# only apply for feature part (not pooling, classfier)\n",
    "# because of layers.feature \n",
    "def split_layer(model,start,end):\n",
    "    ct = 0\n",
    "    split_model=[] # from start to Conv5_1(include ReLU)\n",
    "    for name,layers in model.named_modules():\n",
    "        #print(name,layer)\n",
    "        #print(layers.features)\n",
    "        for idx,layer in enumerate(layers.features):\n",
    "            #print(idx,layer)\n",
    "            if start <=idx and idx <=end :\n",
    "                split_model.append(layer)\n",
    "        break\n",
    "    return nn.Sequential(*split_model)\n",
    "\n",
    "def error_injection(name,num_error,start_index):\n",
    "    def hook(model,input):\n",
    "        start = start_index\n",
    "        end = start_index + num_error\n",
    "        input[0][:, start:end]=0\n",
    "        print(\"shape :\",input[0][:, start:end].size())\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0720Loss_ACC_to36_error_idx_mixed.ipynb\n",
      "acc_log_to34.txt\n",
      "acc_log_toEnd.txt\n",
      "F4F_pytorch-to36_error_idx_mixed-Copy1.ipynb\n",
      "Loss_ACC_to36_error_idx_mixed.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls /media/2/hwbae0326/F4F/0708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 3125\n"
     ]
    }
   ],
   "source": [
    "# dataset load\n",
    "batch_size = 16 # 32~ out of memory in 3080\n",
    "num_train = 128000\n",
    "#num_train = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_path = \"/media/2/Network/Imagenet_dup/\"\n",
    "retrain_model_path = \"/media/0/Network/0708_to_34models/\"\n",
    "# imagenet data load\n",
    "train_dataset = dataset.ImageFolder(root=dataset_path+\"train\",\n",
    "                                       transform=transform)\n",
    "subset_train_dataset,_ = torch.utils.data.random_split(train_dataset, [num_train,len(train_dataset)-num_train])\n",
    "\n",
    "test_dataset = dataset.ImageFolder(root=dataset_path+\"val\",\n",
    "                                       transform=transform)\n",
    "'''\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=4)\n",
    "'''\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(subset_train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4) # for using subset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4)\n",
    "print(len(train_dataloader),len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(retrain_model_path) is False:\n",
    "    os.mkdir(retrain_model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make_F4F_pytorch-to34_F4Famended.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'extracted_feature',\n",
       " 'VGG16',\n",
       " 'start.py',\n",
       " 'acc_log_to34.txt',\n",
       " 'make_F4F_pytorch-to34.ipynb',\n",
       " '0708_to_34models',\n",
       " '0624_to_34models']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(retrain_model_path+\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "set_randomness(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external variable in error_index, num_error\n",
    "\n",
    "def make_error_info(error_index, num_error):\n",
    "    data = []\n",
    "    for i in range(511,-1,-1):\n",
    "        if error_index <= i and i < error_index+num_error:\n",
    "            data.append(1)\n",
    "        else :\n",
    "            data.append(0)\n",
    "        #print(data)\n",
    "    error_info = torch.Tensor(data)\n",
    "    error_info  = error_info.unsqueeze(0).repeat(512,1)\n",
    "    #print(\"error_info :\",error_info)\n",
    "    return error_info # 512,521\n",
    "class F4F(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f4f = nn.Linear(3*3*512+512,3*3*512) # 4167,4608 filter which change feature.34 (Conv5_1)\n",
    "        # 512 x5120 사이즈로 batch 저장\n",
    "    def get_f4f_weight(self):\n",
    "        # fc.weight.size(),fc.bias.size()\n",
    "        return self.f4f.weight # torch.Size([4608, 5120])\n",
    "    def forward(self,x):\n",
    "        x = self.f4f(x)\n",
    "        y = torch.tanh(x)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_register(model,error_index,num_error):\n",
    "    for name,layer in model.named_modules():\n",
    "        #print(name,layer)\n",
    "        if \"34\" in name  and isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "            print(\"input\",name,layer) # target layer Conv5_1\n",
    "            layer.register_forward_pre_hook(error_injection(name,num_error,error_index))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target_model(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def get_layer(self,idx):\n",
    "        #print(self.model._modules['34'])\n",
    "        layer =None\n",
    "        try : # target model\n",
    "            layer = self.model._modules[str(idx)]\n",
    "        except KeyError: # test_model\n",
    "            layer = self.model.features._modules[str(idx)]\n",
    "        return layer\n",
    "    def apply_f4f(self,f4f,error_info):\n",
    "        \n",
    "        weight = torch.reshape(self.get_layer(34).weight.data,(512,512*3*3)).to(device) # flatten [512,5210] (batch 512)\n",
    "        \n",
    "        data = torch.cat( (weight,error_info), 1 )\n",
    "        offset = torch.reshape(f4f(data),(512,512,3,3))\n",
    "        self.get_layer(34).weight.data = self.get_layer(34).weight.data + offset\n",
    "    def forward(self,x,f4f,error_info):\n",
    "        # apply_f4f는 매 epoch마다 동일하므로 \n",
    "        self.apply_f4f(f4f,error_info)\n",
    "        y = self.model(x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation phasetraining\n",
    "def eval(model,dataloader,epoch,f4f,error_info):\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct =0\n",
    "    with torch.no_grad():\n",
    "        print(\"======eval start=======\")\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "            y_hat = model(inputs,f4f,error_info)\n",
    "            _, predicted = torch.max(y_hat.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if(i%200 == 199):\n",
    "                print(\"step : %d / %d acc : %.3f\"\n",
    "                      %(i + 1,int(len(dataloader)), correct*100/total))\n",
    "                #print(\".\",end=\"\")\n",
    "        print(\"\")\n",
    "    acc = 100*correct/total\n",
    "    print(\"%dth epoch acc of %s on imagenet : %.4f %%\" %(epoch, model.__class__.__name__,acc)) \n",
    "    f = open(log_file,\"a\")\n",
    "    print(\"%dth epoch acc of %s on imagenet : %.4f %%\" %(epoch, model.__class__.__name__,acc),file=f) \n",
    "    f.close()\n",
    "    print(\"======eval  end ======\")  \n",
    "    return acc\n",
    "#torch.save(vgg16_bn.state_dict(), retrain_model_path+\"test_vgg16_bn_state_dict.pt\")\n",
    "def model_copy(model):\n",
    "    return d_copy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def training(f4f,target_model,\n",
    "             train_dataloader,test_dataloader,\n",
    "             loss_fn,optimizer,\n",
    "             error_idx,num_error,\n",
    "             max_epochs=30,subset=False):\n",
    "    \n",
    "    target_model.to(device)\n",
    "    original_model.to(device)\n",
    "    error_info = make_error_info(error_index,num_error).to(device)\n",
    "    first_feature = []\n",
    "    \n",
    "    feature_num = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        running_loss = 0.0\n",
    "        total_loss = []\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        f4f.train()\n",
    "        # update f4f filter\n",
    "        #target_model.apply_f4f(f4f,error_info)\n",
    "    \n",
    "        # compare\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            if i % 10 == 0:\n",
    "                print(\".\",end=\"\")\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            target_out = target_model(inputs,f4f,error_info)\n",
    "            #print(original_out[0][0][0],target_out[0][0][0])\n",
    "            #exit(0)\n",
    "            \n",
    "            first_feature.append(target_out)\n",
    "            if len(first_feature) > feature_num:\n",
    "                first_feature.pop(0)\n",
    "            _,predicted = torch.max(target_out.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            \n",
    "            #print(labels.size(),target_out.size())\n",
    "            loss = loss_fn(target_out,labels)\n",
    "            #print(loss.size())\n",
    "            running_loss += loss.item()\n",
    "            target_model.model.zero_grad()\n",
    "            f4f.f4f.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 99: \n",
    "                total_loss.append(running_loss/100)\n",
    "                print(\"\")\n",
    "                print('[%d, %5d] loss: %.6f' % (epoch+1, i+1, running_loss/100)) \n",
    "                running_loss = 0.0\n",
    "        # save weight\n",
    "        #print((len(train_dataloader)/batch_size))\n",
    "        total_avg_loss = sum(total_loss)/len(total_loss)\n",
    "        acc = 100*correct/total\n",
    "        print(\"total average loss : %.3f\" %(total_avg_loss))\n",
    "        print(\"train acc : %.4f\" %(acc))\n",
    "        acc = eval(target_model,test_dataloader,epoch,f4f,error_info)\n",
    "        \n",
    "        torch.save(f4f.get_f4f_weight(), \n",
    "               retrain_model_path+\"%s~%s_pkt_err_f4f_epoch_%s_acc_%.4f_loss_%.4f.pt\"\n",
    "               %(str(error_idx).zfill(3),str(error_idx+num_error).zfill(3),\n",
    "                str(epoch+1).zfill(2),acc,total_avg_loss))    \n",
    "    return original_out,first_feature\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"./acc_log_toEnd.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_model = split_layer(vgg16_bn,0,Out_layer_number)\n",
    "\n",
    "original_model = d_copy(vgg16_bn).to(device)\n",
    "# subset of vgg16 (whole layer) with f4f\n",
    "hook_register(vgg16_bn,error_index,num_error)\n",
    "target_model = Target_model(vgg16_bn).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4f = F4F().to(device)\n",
    "optimizer = torch.optim.SGD(f4f.parameters(),lr=0.1e-5,weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= epoch  0 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 7.626556\n",
      "..........\n",
      "[1,   200] loss: 7.580555\n",
      "..........\n",
      "[1,   300] loss: 7.600811\n",
      "..........\n",
      "[1,   400] loss: 7.514917\n",
      "..........\n",
      "[1,   500] loss: 7.581228\n",
      "..........\n",
      "[1,   600] loss: 7.530646\n",
      "..........\n",
      "[1,   700] loss: 7.568056\n",
      "..........\n",
      "[1,   800] loss: 7.589793\n",
      "..........\n",
      "[1,   900] loss: 7.540020\n",
      "..........\n",
      "[1,  1000] loss: 7.567878\n",
      "..........\n",
      "[1,  1100] loss: 7.490357\n",
      "..........\n",
      "[1,  1200] loss: 7.524303\n",
      "..........\n",
      "[1,  1300] loss: 7.505941\n",
      "..........\n",
      "[1,  1400] loss: 7.493628\n",
      "..........\n",
      "[1,  1500] loss: 7.522469\n",
      "..........\n",
      "[1,  1600] loss: 7.482189\n",
      "..........\n",
      "[1,  1700] loss: 7.476092\n",
      "..........\n",
      "[1,  1800] loss: 7.491617\n",
      "..........\n",
      "[1,  1900] loss: 7.540546\n",
      "..........\n",
      "[1,  2000] loss: 7.518159\n",
      "..........\n",
      "[1,  2100] loss: 7.441951\n",
      "..........\n",
      "[1,  2200] loss: 7.479742\n",
      "..........\n",
      "[1,  2300] loss: 7.477429\n",
      "..........\n",
      "[1,  2400] loss: 7.506167\n",
      "..........\n",
      "[1,  2500] loss: 7.524424\n",
      "..........\n",
      "[1,  2600] loss: 7.475516\n",
      "..........\n",
      "[1,  2700] loss: 7.536121\n",
      "..........\n",
      "[1,  2800] loss: 7.492571\n",
      "..........\n",
      "[1,  2900] loss: 7.506754\n",
      "..........\n",
      "[1,  3000] loss: 7.491890\n",
      "..........\n",
      "[1,  3100] loss: 7.515835\n",
      "..........\n",
      "[1,  3200] loss: 7.459973\n",
      "..........\n",
      "[1,  3300] loss: 7.478101\n",
      "..........\n",
      "[1,  3400] loss: 7.503762\n",
      "..........\n",
      "[1,  3500] loss: 7.502965\n",
      "..........\n",
      "[1,  3600] loss: 7.507764\n",
      "..........\n",
      "[1,  3700] loss: 7.424179\n",
      "..........\n",
      "[1,  3800] loss: 7.483693\n",
      "..........\n",
      "[1,  3900] loss: 7.494723\n",
      "..........\n",
      "[1,  4000] loss: 7.518414\n",
      "..........\n",
      "[1,  4100] loss: 7.480279\n",
      "..........\n",
      "[1,  4200] loss: 7.507098\n",
      "..........\n",
      "[1,  4300] loss: 7.457459\n",
      "..........\n",
      "[1,  4400] loss: 7.533950\n",
      "..........\n",
      "[1,  4500] loss: 7.515527\n",
      "..........\n",
      "[1,  4600] loss: 7.474873\n",
      "..........\n",
      "[1,  4700] loss: 7.465197\n",
      "..........\n",
      "[1,  4800] loss: 7.547848\n",
      "..........\n",
      "[1,  4900] loss: 7.510626\n",
      "..........\n",
      "[1,  5000] loss: 7.529262\n",
      "..........\n",
      "[1,  5100] loss: 7.496787\n",
      "..........\n",
      "[1,  5200] loss: 7.501088\n",
      "..........\n",
      "[1,  5300] loss: 7.499959\n",
      "..........\n",
      "[1,  5400] loss: 7.486800\n",
      "..........\n",
      "[1,  5500] loss: 7.479052\n",
      "..........\n",
      "[1,  5600] loss: 7.497518\n",
      "..........\n",
      "[1,  5700] loss: 7.459357\n",
      "..........\n",
      "[1,  5800] loss: 7.476686\n",
      "..........\n",
      "[1,  5900] loss: 7.500745\n",
      "..........\n",
      "[1,  6000] loss: 7.547710\n",
      "..........\n",
      "[1,  6100] loss: 7.476173\n",
      "..........\n",
      "[1,  6200] loss: 7.514951\n",
      "..........\n",
      "[1,  6300] loss: 7.464556\n",
      "..........\n",
      "[1,  6400] loss: 7.482912\n",
      "..........\n",
      "[1,  6500] loss: 7.506069\n",
      "..........\n",
      "[1,  6600] loss: 7.462989\n",
      "..........\n",
      "[1,  6700] loss: 7.519810\n",
      "..........\n",
      "[1,  6800] loss: 7.521309\n",
      "..........\n",
      "[1,  6900] loss: 7.516261\n",
      "..........\n",
      "[1,  7000] loss: 7.502748\n",
      "..........\n",
      "[1,  7100] loss: 7.492041\n",
      "..........\n",
      "[1,  7200] loss: 7.489214\n",
      "..........\n",
      "[1,  7300] loss: 7.497189\n",
      "..........\n",
      "[1,  7400] loss: 7.464400\n",
      "..........\n",
      "[1,  7500] loss: 7.458296\n",
      "..........\n",
      "[1,  7600] loss: 7.463192\n",
      "..........\n",
      "[1,  7700] loss: 7.466643\n",
      "..........\n",
      "[1,  7800] loss: 7.496557\n",
      "..........\n",
      "[1,  7900] loss: 7.490683\n",
      "..........\n",
      "[1,  8000] loss: 7.530310\n",
      "total average loss : 7.504\n",
      "train acc : 0.1086\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.146\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.094\n",
      "step : 1400 / 3125 acc : 0.098\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.103\n",
      "step : 2200 / 3125 acc : 0.108\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.120\n",
      "step : 2800 / 3125 acc : 0.114\n",
      "step : 3000 / 3125 acc : 0.110\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1060 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 7.545403\n",
      "..........\n",
      "[1,   200] loss: 7.499840\n",
      "..........\n",
      "[1,   300] loss: 7.513311\n",
      "..........\n",
      "[1,   400] loss: 7.526897\n",
      "..........\n",
      "[1,   500] loss: 7.554485\n",
      "..........\n",
      "[1,   600] loss: 7.538632\n",
      "..........\n",
      "[1,   700] loss: 7.535390\n",
      "..........\n",
      "[1,   800] loss: 7.604081\n",
      "..........\n",
      "[1,   900] loss: 7.595367\n",
      "..........\n",
      "[1,  1000] loss: 7.607134\n",
      "..........\n",
      "[1,  1100] loss: 7.601413\n",
      "..........\n",
      "[1,  1200] loss: 7.571323\n",
      "..........\n",
      "[1,  1300] loss: 7.618728\n",
      "..........\n",
      "[1,  1400] loss: 7.569607\n",
      "..........\n",
      "[1,  1500] loss: 7.563759\n",
      "..........\n",
      "[1,  1600] loss: 7.595694\n",
      "..........\n",
      "[1,  1700] loss: 7.606837\n",
      "..........\n",
      "[1,  1800] loss: 7.636967\n",
      "..........\n",
      "[1,  1900] loss: 7.612980\n",
      "..........\n",
      "[1,  2000] loss: 7.615365\n",
      "..........\n",
      "[1,  2100] loss: 7.605069\n",
      "..........\n",
      "[1,  2200] loss: 7.619403\n",
      "..........\n",
      "[1,  2300] loss: 7.654965\n",
      "..........\n",
      "[1,  2400] loss: 7.638170\n",
      "..........\n",
      "[1,  2500] loss: 7.705146\n",
      "..........\n",
      "[1,  2600] loss: 7.702304\n",
      "..........\n",
      "[1,  2700] loss: 7.715607\n",
      "..........\n",
      "[1,  2800] loss: 7.666463\n",
      "..........\n",
      "[1,  2900] loss: 7.688836\n",
      "..........\n",
      "[1,  3000] loss: 7.706160\n",
      "..........\n",
      "[1,  3100] loss: 7.777449\n",
      "..........\n",
      "[1,  3200] loss: 7.699073\n",
      "..........\n",
      "[1,  3300] loss: 7.675605\n",
      "..........\n",
      "[1,  3400] loss: 7.727474\n",
      "..........\n",
      "[1,  3500] loss: 7.755933\n",
      "..........\n",
      "[1,  3600] loss: 7.770196\n",
      "..........\n",
      "[1,  3700] loss: 7.746027\n",
      "..........\n",
      "[1,  3800] loss: 7.810358\n",
      "..........\n",
      "[1,  3900] loss: 7.817619\n",
      "..........\n",
      "[1,  4000] loss: 7.786881\n",
      "..........\n",
      "[1,  4100] loss: 7.800946\n",
      "..........\n",
      "[1,  4200] loss: 7.796058\n",
      "..........\n",
      "[1,  4300] loss: 7.811574\n",
      "..........\n",
      "[1,  4400] loss: 7.768526\n",
      "..........\n",
      "[1,  4500] loss: 7.879128\n",
      "..........\n",
      "[1,  4600] loss: 7.874857\n",
      "..........\n",
      "[1,  4700] loss: 7.836764\n",
      "..........\n",
      "[1,  4800] loss: 7.877261\n",
      "..........\n",
      "[1,  4900] loss: 7.902412\n",
      "..........\n",
      "[1,  5000] loss: 7.949393\n",
      "..........\n",
      "[1,  5100] loss: 7.963029\n",
      "..........\n",
      "[1,  5200] loss: 7.922372\n",
      "..........\n",
      "[1,  5300] loss: 7.884771\n",
      "..........\n",
      "[1,  5400] loss: 7.938400\n",
      "..........\n",
      "[1,  5500] loss: 8.003803\n",
      "..........\n",
      "[1,  5600] loss: 8.062308\n",
      "..........\n",
      "[1,  5700] loss: 8.007154\n",
      "..........\n",
      "[1,  5800] loss: 7.983846\n",
      "..........\n",
      "[1,  5900] loss: 8.043469\n",
      "..........\n",
      "[1,  6000] loss: 7.994466\n",
      "..........\n",
      "[1,  6100] loss: 8.008112\n",
      "..........\n",
      "[1,  6200] loss: 8.014038\n",
      "..........\n",
      "[1,  6300] loss: 8.141089\n",
      "..........\n",
      "[1,  6400] loss: 8.056575\n",
      "..........\n",
      "[1,  6500] loss: 8.156413\n",
      "..........\n",
      "[1,  6600] loss: 8.127031\n",
      "..........\n",
      "[1,  6700] loss: 8.167893\n",
      "..........\n",
      "[1,  6800] loss: 8.205973\n",
      "..........\n",
      "[1,  6900] loss: 8.195699\n",
      "..........\n",
      "[1,  7000] loss: 8.167928\n",
      "..........\n",
      "[1,  7100] loss: 8.225662\n",
      "..........\n",
      "[1,  7200] loss: 8.182926\n",
      "..........\n",
      "[1,  7300] loss: 8.250059\n",
      "..........\n",
      "[1,  7400] loss: 8.270336\n",
      "..........\n",
      "[1,  7500] loss: 8.357234\n",
      "..........\n",
      "[1,  7600] loss: 8.364976\n",
      "..........\n",
      "[1,  7700] loss: 8.174215\n",
      "..........\n",
      "[1,  7800] loss: 8.242556\n",
      "..........\n",
      "[1,  7900] loss: 8.305708\n",
      "..........\n",
      "[1,  8000] loss: 8.344953\n",
      "total average loss : 7.851\n",
      "train acc : 0.1289\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.138\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.122\n",
      "step : 2000 / 3125 acc : 0.128\n",
      "step : 2200 / 3125 acc : 0.131\n",
      "step : 2400 / 3125 acc : 0.133\n",
      "step : 2600 / 3125 acc : 0.132\n",
      "step : 2800 / 3125 acc : 0.129\n",
      "step : 3000 / 3125 acc : 0.123\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1220 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 8.946055\n",
      "..........\n",
      "[1,   200] loss: 9.088365\n",
      "..........\n",
      "[1,   300] loss: 9.059997\n",
      "..........\n",
      "[1,   400] loss: 9.109000\n",
      "..........\n",
      "[1,   500] loss: 9.100725\n",
      "..........\n",
      "[1,   600] loss: 9.125667\n",
      "..........\n",
      "[1,   700] loss: 9.071086\n",
      "..........\n",
      "[1,   800] loss: 9.105544\n",
      "..........\n",
      "[1,   900] loss: 9.146331\n",
      "..........\n",
      "[1,  1000] loss: 9.182293\n",
      "..........\n",
      "[1,  1100] loss: 9.285552\n",
      "..........\n",
      "[1,  1200] loss: 9.205613\n",
      "..........\n",
      "[1,  1300] loss: 9.168539\n",
      "..........\n",
      "[1,  1400] loss: 9.338849\n",
      "..........\n",
      "[1,  1500] loss: 9.261275\n",
      "..........\n",
      "[1,  1600] loss: 9.292536\n",
      "..........\n",
      "[1,  1700] loss: 9.271537\n",
      "..........\n",
      "[1,  1800] loss: 9.407743\n",
      "..........\n",
      "[1,  1900] loss: 9.501665\n",
      "..........\n",
      "[1,  2000] loss: 9.406356\n",
      "..........\n",
      "[1,  2100] loss: 9.457757\n",
      "..........\n",
      "[1,  2200] loss: 9.397815\n",
      "..........\n",
      "[1,  2300] loss: 9.329557\n",
      "..........\n",
      "[1,  2400] loss: 9.577631\n",
      "..........\n",
      "[1,  2500] loss: 9.352196\n",
      "..........\n",
      "[1,  2600] loss: 9.540824\n",
      "..........\n",
      "[1,  2700] loss: 9.623679\n",
      "..........\n",
      "[1,  2800] loss: 9.573108\n",
      "..........\n",
      "[1,  2900] loss: 9.616666\n",
      "..........\n",
      "[1,  3000] loss: 9.497231\n",
      "..........\n",
      "[1,  3100] loss: 9.749390\n",
      "..........\n",
      "[1,  3200] loss: 9.730407\n",
      "..........\n",
      "[1,  3300] loss: 9.733559\n",
      "..........\n",
      "[1,  3400] loss: 9.759911\n",
      "..........\n",
      "[1,  3500] loss: 9.802114\n",
      "..........\n",
      "[1,  3600] loss: 9.760113\n",
      "..........\n",
      "[1,  3700] loss: 9.837794\n",
      "..........\n",
      "[1,  3800] loss: 9.817725\n",
      "..........\n",
      "[1,  3900] loss: 9.888242\n",
      "..........\n",
      "[1,  4000] loss: 9.919457\n",
      "..........\n",
      "[1,  4100] loss: 9.993284\n",
      "..........\n",
      "[1,  4200] loss: 9.925061\n",
      "..........\n",
      "[1,  4300] loss: 9.953199\n",
      "..........\n",
      "[1,  4400] loss: 10.061539\n",
      "..........\n",
      "[1,  4500] loss: 10.041023\n",
      "..........\n",
      "[1,  4600] loss: 10.153263\n",
      "..........\n",
      "[1,  4700] loss: 10.132004\n",
      "..........\n",
      "[1,  4800] loss: 10.142190\n",
      "..........\n",
      "[1,  4900] loss: 10.243498\n",
      "..........\n",
      "[1,  5000] loss: 10.404515\n",
      "..........\n",
      "[1,  5100] loss: 10.240349\n",
      "..........\n",
      "[1,  5200] loss: 10.402205\n",
      "..........\n",
      "[1,  5300] loss: 10.265902\n",
      "..........\n",
      "[1,  5400] loss: 10.170321\n",
      "..........\n",
      "[1,  5500] loss: 10.248101\n",
      "..........\n",
      "[1,  5600] loss: 10.452315\n",
      "..........\n",
      "[1,  5700] loss: 10.512202\n",
      "..........\n",
      "[1,  5800] loss: 10.413115\n",
      "..........\n",
      "[1,  5900] loss: 10.542067\n",
      "..........\n",
      "[1,  6000] loss: 10.563209\n",
      "..........\n",
      "[1,  6100] loss: 10.398811\n",
      "..........\n",
      "[1,  6200] loss: 10.546739\n",
      "..........\n",
      "[1,  6300] loss: 10.471522\n",
      "..........\n",
      "[1,  6400] loss: 10.559920\n",
      "..........\n",
      "[1,  6500] loss: 10.608540\n",
      "..........\n",
      "[1,  6600] loss: 10.721024\n",
      "..........\n",
      "[1,  6700] loss: 10.722544\n",
      "..........\n",
      "[1,  6800] loss: 10.786999\n",
      "..........\n",
      "[1,  6900] loss: 10.729120\n",
      "..........\n",
      "[1,  7000] loss: 10.907472\n",
      "..........\n",
      "[1,  7100] loss: 10.909172\n",
      "..........\n",
      "[1,  7200] loss: 10.624552\n",
      "..........\n",
      "[1,  7300] loss: 10.900903\n",
      "..........\n",
      "[1,  7400] loss: 10.942280\n",
      "..........\n",
      "[1,  7500] loss: 10.895307\n",
      "..........\n",
      "[1,  7600] loss: 11.043514\n",
      "..........\n",
      "[1,  7700] loss: 11.092739\n",
      "..........\n",
      "[1,  7800] loss: 11.088390\n",
      "..........\n",
      "[1,  7900] loss: 10.852236\n",
      "..........\n",
      "[1,  8000] loss: 11.044080\n",
      "total average loss : 9.972\n",
      "train acc : 0.1461\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.083\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.098\n",
      "step : 1800 / 3125 acc : 0.090\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.094\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.099\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.108\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1160 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 11.899829\n",
      "..........\n",
      "[1,   200] loss: 12.066776\n",
      "..........\n",
      "[1,   300] loss: 11.933702\n",
      "..........\n",
      "[1,   400] loss: 12.076016\n",
      "..........\n",
      "[1,   500] loss: 12.195451\n",
      "..........\n",
      "[1,   600] loss: 12.125590\n",
      "..........\n",
      "[1,   700] loss: 12.141972\n",
      "..........\n",
      "[1,   800] loss: 12.091333\n",
      "..........\n",
      "[1,   900] loss: 12.382440\n",
      "..........\n",
      "[1,  1000] loss: 11.955927\n",
      "..........\n",
      "[1,  1100] loss: 12.236403\n",
      "..........\n",
      "[1,  1200] loss: 12.430548\n",
      "..........\n",
      "[1,  1300] loss: 12.478448\n",
      "..........\n",
      "[1,  1400] loss: 12.262475\n",
      "..........\n",
      "[1,  1500] loss: 12.289127\n",
      "..........\n",
      "[1,  1600] loss: 12.417178\n",
      "..........\n",
      "[1,  1700] loss: 12.525194\n",
      "..........\n",
      "[1,  1800] loss: 12.450056\n",
      "..........\n",
      "[1,  1900] loss: 12.566460\n",
      "..........\n",
      "[1,  2000] loss: 12.376970\n",
      "..........\n",
      "[1,  2100] loss: 12.390343\n",
      "..........\n",
      "[1,  2200] loss: 12.500483\n",
      "..........\n",
      "[1,  2300] loss: 12.661397\n",
      "..........\n",
      "[1,  2400] loss: 12.656565\n",
      "..........\n",
      "[1,  2500] loss: 12.649530\n",
      "..........\n",
      "[1,  2600] loss: 12.835914\n",
      "..........\n",
      "[1,  2700] loss: 12.714010\n",
      "..........\n",
      "[1,  2800] loss: 12.699836\n",
      "..........\n",
      "[1,  2900] loss: 12.627827\n",
      "..........\n",
      "[1,  3000] loss: 12.715358\n",
      "..........\n",
      "[1,  3100] loss: 12.983396\n",
      "..........\n",
      "[1,  3200] loss: 13.101356\n",
      "..........\n",
      "[1,  3300] loss: 13.112500\n",
      "..........\n",
      "[1,  3400] loss: 12.694903\n",
      "..........\n",
      "[1,  3500] loss: 13.073665\n",
      "..........\n",
      "[1,  3600] loss: 12.902272\n",
      "..........\n",
      "[1,  3700] loss: 12.871428\n",
      "..........\n",
      "[1,  3800] loss: 12.987013\n",
      "..........\n",
      "[1,  3900] loss: 13.072899\n",
      "..........\n",
      "[1,  4000] loss: 13.234632\n",
      "..........\n",
      "[1,  4100] loss: 13.046102\n",
      "..........\n",
      "[1,  4200] loss: 13.237545\n",
      "..........\n",
      "[1,  4300] loss: 13.293910\n",
      "..........\n",
      "[1,  4400] loss: 13.224450\n",
      "..........\n",
      "[1,  4500] loss: 13.204697\n",
      "..........\n",
      "[1,  4600] loss: 13.255829\n",
      "..........\n",
      "[1,  4700] loss: 13.293188\n",
      "..........\n",
      "[1,  4800] loss: 13.343003\n",
      "..........\n",
      "[1,  4900] loss: 13.603093\n",
      "..........\n",
      "[1,  5000] loss: 13.486319\n",
      "..........\n",
      "[1,  5100] loss: 13.475523\n",
      "..........\n",
      "[1,  5200] loss: 13.480453\n",
      "..........\n",
      "[1,  5300] loss: 13.508410\n",
      "..........\n",
      "[1,  5400] loss: 13.490962\n",
      "..........\n",
      "[1,  5500] loss: 13.711657\n",
      "..........\n",
      "[1,  5600] loss: 13.712766\n",
      "..........\n",
      "[1,  5700] loss: 13.467264\n",
      "..........\n",
      "[1,  5800] loss: 13.739521\n",
      "..........\n",
      "[1,  5900] loss: 13.837939\n",
      "..........\n",
      "[1,  6000] loss: 13.662886\n",
      "..........\n",
      "[1,  6100] loss: 13.859065\n",
      "..........\n",
      "[1,  6200] loss: 13.818142\n",
      "..........\n",
      "[1,  6300] loss: 13.849185\n",
      "..........\n",
      "[1,  6400] loss: 14.006511\n",
      "..........\n",
      "[1,  6500] loss: 13.997259\n",
      "..........\n",
      "[1,  6600] loss: 13.893376\n",
      "..........\n",
      "[1,  6700] loss: 14.081197\n",
      "..........\n",
      "[1,  6800] loss: 14.188862\n",
      "..........\n",
      "[1,  6900] loss: 14.354079\n",
      "..........\n",
      "[1,  7000] loss: 14.006194\n",
      "..........\n",
      "[1,  7100] loss: 14.064662\n",
      "..........\n",
      "[1,  7200] loss: 14.091261\n",
      "..........\n",
      "[1,  7300] loss: 14.244193\n",
      "..........\n",
      "[1,  7400] loss: 14.078402\n",
      "..........\n",
      "[1,  7500] loss: 14.281703\n",
      "..........\n",
      "[1,  7600] loss: 14.307189\n",
      "..........\n",
      "[1,  7700] loss: 14.309875\n",
      "..........\n",
      "[1,  7800] loss: 14.407010\n",
      "..........\n",
      "[1,  7900] loss: 14.310203\n",
      "..........\n",
      "[1,  8000] loss: 14.228616\n",
      "total average loss : 13.160\n",
      "train acc : 0.1133\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.125\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.113\n",
      "step : 1800 / 3125 acc : 0.111\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.117\n",
      "step : 2600 / 3125 acc : 0.123\n",
      "step : 2800 / 3125 acc : 0.123\n",
      "step : 3000 / 3125 acc : 0.125\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1260 %\n",
      "======eval  end ======\n",
      "======= epoch  1 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 15.694590\n",
      "..........\n",
      "[1,   200] loss: 15.650677\n",
      "..........\n",
      "[1,   300] loss: 15.773224\n",
      "..........\n",
      "[1,   400] loss: 15.489867\n",
      "..........\n",
      "[1,   500] loss: 15.910236\n",
      "..........\n",
      "[1,   600] loss: 15.815064\n",
      "..........\n",
      "[1,   700] loss: 15.783457\n",
      "..........\n",
      "[1,   800] loss: 15.725615\n",
      "..........\n",
      "[1,   900] loss: 15.867452\n",
      "..........\n",
      "[1,  1000] loss: 15.762853\n",
      "..........\n",
      "[1,  1100] loss: 15.797177\n",
      "..........\n",
      "[1,  1200] loss: 15.684523\n",
      "..........\n",
      "[1,  1300] loss: 16.011545\n",
      "..........\n",
      "[1,  1400] loss: 16.110836\n",
      "..........\n",
      "[1,  1500] loss: 15.924219\n",
      "..........\n",
      "[1,  1600] loss: 16.303664\n",
      "..........\n",
      "[1,  1700] loss: 16.169649\n",
      "..........\n",
      "[1,  1800] loss: 16.165104\n",
      "..........\n",
      "[1,  1900] loss: 16.345203\n",
      "..........\n",
      "[1,  2000] loss: 16.297148\n",
      "..........\n",
      "[1,  2100] loss: 16.353824\n",
      "..........\n",
      "[1,  2200] loss: 16.047219\n",
      "..........\n",
      "[1,  2300] loss: 16.614171\n",
      "..........\n",
      "[1,  2400] loss: 16.367900\n",
      "..........\n",
      "[1,  2500] loss: 16.489363\n",
      "..........\n",
      "[1,  2600] loss: 16.495360\n",
      "..........\n",
      "[1,  2700] loss: 16.494689\n",
      "..........\n",
      "[1,  2800] loss: 16.529945\n",
      "..........\n",
      "[1,  2900] loss: 16.566679\n",
      "..........\n",
      "[1,  3000] loss: 16.963407\n",
      "..........\n",
      "[1,  3100] loss: 16.739028\n",
      "..........\n",
      "[1,  3200] loss: 16.574764\n",
      "..........\n",
      "[1,  3300] loss: 16.797200\n",
      "..........\n",
      "[1,  3400] loss: 16.905455\n",
      "..........\n",
      "[1,  3500] loss: 16.806535\n",
      "..........\n",
      "[1,  3600] loss: 16.687620\n",
      "..........\n",
      "[1,  3700] loss: 16.957189\n",
      "..........\n",
      "[1,  3800] loss: 16.758914\n",
      "..........\n",
      "[1,  3900] loss: 16.564387\n",
      "..........\n",
      "[1,  4000] loss: 17.001208\n",
      "..........\n",
      "[1,  4100] loss: 16.881600\n",
      "..........\n",
      "[1,  4200] loss: 16.950861\n",
      "..........\n",
      "[1,  4300] loss: 17.223915\n",
      "..........\n",
      "[1,  4400] loss: 17.321645\n",
      "..........\n",
      "[1,  4500] loss: 16.938006\n",
      "..........\n",
      "[1,  4600] loss: 17.528830\n",
      "..........\n",
      "[1,  4700] loss: 17.297963\n",
      "..........\n",
      "[1,  4800] loss: 17.340046\n",
      "..........\n",
      "[1,  4900] loss: 17.467802\n",
      "..........\n",
      "[1,  5000] loss: 16.969083\n",
      "..........\n",
      "[1,  5100] loss: 17.563575\n",
      "..........\n",
      "[1,  5200] loss: 17.298675\n",
      "..........\n",
      "[1,  5300] loss: 17.277791\n",
      "..........\n",
      "[1,  5400] loss: 17.790250\n",
      "..........\n",
      "[1,  5500] loss: 17.659903\n",
      "..........\n",
      "[1,  5600] loss: 17.320338\n",
      "..........\n",
      "[1,  5700] loss: 17.582069\n",
      "..........\n",
      "[1,  5800] loss: 17.377517\n",
      "..........\n",
      "[1,  5900] loss: 17.277036\n",
      "..........\n",
      "[1,  6000] loss: 17.714262\n",
      "..........\n",
      "[1,  6100] loss: 18.108500\n",
      "..........\n",
      "[1,  6200] loss: 17.634723\n",
      "..........\n",
      "[1,  6300] loss: 17.951242\n",
      "..........\n",
      "[1,  6400] loss: 17.824941\n",
      "..........\n",
      "[1,  6500] loss: 18.046019\n",
      "..........\n",
      "[1,  6600] loss: 17.828640\n",
      "..........\n",
      "[1,  6700] loss: 17.772223\n",
      "..........\n",
      "[1,  6800] loss: 17.977277\n",
      "..........\n",
      "[1,  6900] loss: 18.055596\n",
      "..........\n",
      "[1,  7000] loss: 18.068623\n",
      "..........\n",
      "[1,  7100] loss: 17.973693\n",
      "..........\n",
      "[1,  7200] loss: 18.025839\n",
      "..........\n",
      "[1,  7300] loss: 18.307944\n",
      "..........\n",
      "[1,  7400] loss: 18.292414\n",
      "..........\n",
      "[1,  7500] loss: 18.124852\n",
      "..........\n",
      "[1,  7600] loss: 18.384148\n",
      "..........\n",
      "[1,  7700] loss: 18.409816\n",
      "..........\n",
      "[1,  7800] loss: 18.236430\n",
      "..........\n",
      "[1,  7900] loss: 18.218276\n",
      "..........\n",
      "[1,  8000] loss: 18.682127\n",
      "total average loss : 16.996\n",
      "train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.000\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.150\n",
      "step : 1200 / 3125 acc : 0.130\n",
      "step : 1400 / 3125 acc : 0.129\n",
      "step : 1600 / 3125 acc : 0.148\n",
      "step : 1800 / 3125 acc : 0.142\n",
      "step : 2000 / 3125 acc : 0.134\n",
      "step : 2200 / 3125 acc : 0.131\n",
      "step : 2400 / 3125 acc : 0.130\n",
      "step : 2600 / 3125 acc : 0.125\n",
      "step : 2800 / 3125 acc : 0.129\n",
      "step : 3000 / 3125 acc : 0.127\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1300 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 19.771349\n",
      "..........\n",
      "[1,   200] loss: 20.222913\n",
      "..........\n",
      "[1,   300] loss: 19.815503\n",
      "..........\n",
      "[1,   400] loss: 19.730967\n",
      "..........\n",
      "[1,   500] loss: 19.797060\n",
      "..........\n",
      "[1,   600] loss: 19.871950\n",
      "..........\n",
      "[1,   700] loss: 20.160923\n",
      "..........\n",
      "[1,   800] loss: 19.596517\n",
      "..........\n",
      "[1,   900] loss: 20.153331\n",
      "..........\n",
      "[1,  1000] loss: 19.951906\n",
      "..........\n",
      "[1,  1100] loss: 19.786320\n",
      "..........\n",
      "[1,  1200] loss: 20.260707\n",
      "..........\n",
      "[1,  1300] loss: 20.216559\n",
      "..........\n",
      "[1,  1400] loss: 20.298001\n",
      "..........\n",
      "[1,  1500] loss: 19.819577\n",
      "..........\n",
      "[1,  1600] loss: 20.107681\n",
      "..........\n",
      "[1,  1700] loss: 20.296933\n",
      "..........\n",
      "[1,  1800] loss: 19.927442\n",
      "..........\n",
      "[1,  1900] loss: 20.128012\n",
      "..........\n",
      "[1,  2000] loss: 20.570573\n",
      "..........\n",
      "[1,  2100] loss: 20.639168\n",
      "..........\n",
      "[1,  2200] loss: 20.197788\n",
      "..........\n",
      "[1,  2300] loss: 20.863812\n",
      "..........\n",
      "[1,  2400] loss: 20.630126\n",
      "..........\n",
      "[1,  2500] loss: 20.858096\n",
      "..........\n",
      "[1,  2600] loss: 20.487535\n",
      "..........\n",
      "[1,  2700] loss: 20.861400\n",
      "..........\n",
      "[1,  2800] loss: 20.430566\n",
      "..........\n",
      "[1,  2900] loss: 20.394305\n",
      "..........\n",
      "[1,  3000] loss: 20.712677\n",
      "..........\n",
      "[1,  3100] loss: 20.849060\n",
      "..........\n",
      "[1,  3200] loss: 20.618918\n",
      "..........\n",
      "[1,  3300] loss: 20.470906\n",
      "..........\n",
      "[1,  3400] loss: 21.155800\n",
      "..........\n",
      "[1,  3500] loss: 20.797338\n",
      "..........\n",
      "[1,  3600] loss: 20.644958\n",
      "..........\n",
      "[1,  3700] loss: 20.707623\n",
      "..........\n",
      "[1,  3800] loss: 21.038651\n",
      "..........\n",
      "[1,  3900] loss: 21.126687\n",
      "..........\n",
      "[1,  4000] loss: 21.061663\n",
      "..........\n",
      "[1,  4100] loss: 21.195243\n",
      "..........\n",
      "[1,  4200] loss: 20.951220\n",
      "..........\n",
      "[1,  4300] loss: 21.466803\n",
      "..........\n",
      "[1,  4400] loss: 21.279095\n",
      "..........\n",
      "[1,  4500] loss: 21.046469\n",
      "..........\n",
      "[1,  4600] loss: 21.525904\n",
      "..........\n",
      "[1,  4700] loss: 21.460816\n",
      "..........\n",
      "[1,  4800] loss: 21.344259\n",
      "..........\n",
      "[1,  4900] loss: 21.730313\n",
      "..........\n",
      "[1,  5000] loss: 21.662994\n",
      "..........\n",
      "[1,  5100] loss: 21.208270\n",
      "..........\n",
      "[1,  5200] loss: 21.339716\n",
      "..........\n",
      "[1,  5300] loss: 21.690572\n",
      "..........\n",
      "[1,  5400] loss: 21.661580\n",
      "..........\n",
      "[1,  5500] loss: 21.008394\n",
      "..........\n",
      "[1,  5600] loss: 21.533445\n",
      "..........\n",
      "[1,  5700] loss: 21.644067\n",
      "..........\n",
      "[1,  5800] loss: 21.095070\n",
      "..........\n",
      "[1,  5900] loss: 21.339282\n",
      "..........\n",
      "[1,  6000] loss: 21.490487\n",
      "..........\n",
      "[1,  6100] loss: 21.534000\n",
      "..........\n",
      "[1,  6200] loss: 21.602586\n",
      "..........\n",
      "[1,  6300] loss: 21.959874\n",
      "..........\n",
      "[1,  6400] loss: 21.645678\n",
      "..........\n",
      "[1,  6500] loss: 21.860688\n",
      "..........\n",
      "[1,  6600] loss: 21.558039\n",
      "..........\n",
      "[1,  6700] loss: 21.817945\n",
      "..........\n",
      "[1,  6800] loss: 21.994017\n",
      "..........\n",
      "[1,  6900] loss: 21.980071\n",
      "..........\n",
      "[1,  7000] loss: 22.021605\n",
      "..........\n",
      "[1,  7100] loss: 21.824030\n",
      "..........\n",
      "[1,  7200] loss: 22.051293\n",
      "..........\n",
      "[1,  7300] loss: 22.214309\n",
      "..........\n",
      "[1,  7400] loss: 22.089061\n",
      "..........\n",
      "[1,  7500] loss: 22.224794\n",
      "..........\n",
      "[1,  7600] loss: 22.341588\n",
      "..........\n",
      "[1,  7700] loss: 22.392519\n",
      "..........\n",
      "[1,  7800] loss: 22.493797\n",
      "..........\n",
      "[1,  7900] loss: 22.185270\n",
      "..........\n",
      "[1,  8000] loss: 22.129142\n",
      "total average loss : 21.033\n",
      "train acc : 0.1000\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.070\n",
      "step : 1000 / 3125 acc : 0.069\n",
      "step : 1200 / 3125 acc : 0.057\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.086\n",
      "step : 1800 / 3125 acc : 0.090\n",
      "step : 2000 / 3125 acc : 0.091\n",
      "step : 2200 / 3125 acc : 0.094\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.107\n",
      "step : 3000 / 3125 acc : 0.104\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 23.190518\n",
      "..........\n",
      "[1,   200] loss: 23.565864\n",
      "..........\n",
      "[1,   300] loss: 22.983807\n",
      "..........\n",
      "[1,   400] loss: 23.250133\n",
      "..........\n",
      "[1,   500] loss: 23.537181\n",
      "..........\n",
      "[1,   600] loss: 23.100327\n",
      "..........\n",
      "[1,   700] loss: 23.757317\n",
      "..........\n",
      "[1,   800] loss: 23.111744\n",
      "..........\n",
      "[1,   900] loss: 23.513799\n",
      "..........\n",
      "[1,  1000] loss: 23.761780\n",
      "..........\n",
      "[1,  1100] loss: 23.955322\n",
      "..........\n",
      "[1,  1200] loss: 23.575601\n",
      "..........\n",
      "[1,  1300] loss: 23.515650\n",
      "..........\n",
      "[1,  1400] loss: 23.958450\n",
      "..........\n",
      "[1,  1500] loss: 23.275982\n",
      "..........\n",
      "[1,  1600] loss: 23.593812\n",
      "..........\n",
      "[1,  1700] loss: 23.383709\n",
      "..........\n",
      "[1,  1800] loss: 23.089354\n",
      "..........\n",
      "[1,  1900] loss: 23.773880\n",
      "..........\n",
      "[1,  2000] loss: 23.853189\n",
      "..........\n",
      "[1,  2100] loss: 24.219683\n",
      "..........\n",
      "[1,  2200] loss: 23.976317\n",
      "..........\n",
      "[1,  2300] loss: 24.075929\n",
      "..........\n",
      "[1,  2400] loss: 23.607637\n",
      "..........\n",
      "[1,  2500] loss: 23.548195\n",
      "..........\n",
      "[1,  2600] loss: 23.922882\n",
      "..........\n",
      "[1,  2700] loss: 23.779957\n",
      "..........\n",
      "[1,  2800] loss: 24.205073\n",
      "..........\n",
      "[1,  2900] loss: 23.731264\n",
      "..........\n",
      "[1,  3000] loss: 23.678079\n",
      "..........\n",
      "[1,  3100] loss: 23.889024\n",
      "..........\n",
      "[1,  3200] loss: 24.272904\n",
      "..........\n",
      "[1,  3300] loss: 24.542456\n",
      "..........\n",
      "[1,  3400] loss: 23.981026\n",
      "..........\n",
      "[1,  3500] loss: 24.361919\n",
      "..........\n",
      "[1,  3600] loss: 24.411771\n",
      "..........\n",
      "[1,  3700] loss: 24.264517\n",
      "..........\n",
      "[1,  3800] loss: 24.254412\n",
      "..........\n",
      "[1,  3900] loss: 24.181326\n",
      "..........\n",
      "[1,  4000] loss: 23.767260\n",
      "..........\n",
      "[1,  4100] loss: 24.226943\n",
      "..........\n",
      "[1,  4200] loss: 24.193940\n",
      "..........\n",
      "[1,  4300] loss: 23.661730\n",
      "..........\n",
      "[1,  4400] loss: 24.292811\n",
      "..........\n",
      "[1,  4500] loss: 24.455488\n",
      "..........\n",
      "[1,  4600] loss: 25.021165\n",
      "..........\n",
      "[1,  4700] loss: 24.129016\n",
      "..........\n",
      "[1,  4800] loss: 24.785469\n",
      "..........\n",
      "[1,  4900] loss: 24.389758\n",
      "..........\n",
      "[1,  5000] loss: 24.862293\n",
      "..........\n",
      "[1,  5100] loss: 24.552882\n",
      "..........\n",
      "[1,  5200] loss: 24.658330\n",
      "..........\n",
      "[1,  5300] loss: 24.727742\n",
      "..........\n",
      "[1,  5400] loss: 24.701053\n",
      "..........\n",
      "[1,  5500] loss: 24.994148\n",
      "..........\n",
      "[1,  5600] loss: 25.170276\n",
      "..........\n",
      "[1,  5700] loss: 24.852778\n",
      "..........\n",
      "[1,  5800] loss: 24.891182\n",
      "..........\n",
      "[1,  5900] loss: 24.320816\n",
      "..........\n",
      "[1,  6000] loss: 25.287471\n",
      "..........\n",
      "[1,  6100] loss: 24.786358\n",
      "..........\n",
      "[1,  6200] loss: 24.372767\n",
      "..........\n",
      "[1,  6300] loss: 25.094197\n",
      "..........\n",
      "[1,  6400] loss: 24.763161\n",
      "..........\n",
      "[1,  6500] loss: 25.123670\n",
      "..........\n",
      "[1,  6600] loss: 25.258263\n",
      "..........\n",
      "[1,  6700] loss: 24.468924\n",
      "..........\n",
      "[1,  6800] loss: 24.837464\n",
      "..........\n",
      "[1,  6900] loss: 24.863082\n",
      "..........\n",
      "[1,  7000] loss: 24.959591\n",
      "..........\n",
      "[1,  7100] loss: 24.806830\n",
      "..........\n",
      "[1,  7200] loss: 25.119261\n",
      "..........\n",
      "[1,  7300] loss: 24.918150\n",
      "..........\n",
      "[1,  7400] loss: 25.170330\n",
      "..........\n",
      "[1,  7500] loss: 24.803510\n",
      "..........\n",
      "[1,  7600] loss: 25.404015\n",
      "..........\n",
      "[1,  7700] loss: 25.300783\n",
      "..........\n",
      "[1,  7800] loss: 25.096699\n",
      "..........\n",
      "[1,  7900] loss: 25.005829\n",
      "..........\n",
      "[1,  8000] loss: 25.156618\n",
      "total average loss : 24.274\n",
      "train acc : 0.1008\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.094\n",
      "step : 1400 / 3125 acc : 0.089\n",
      "step : 1600 / 3125 acc : 0.098\n",
      "step : 1800 / 3125 acc : 0.094\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.097\n",
      "step : 2400 / 3125 acc : 0.104\n",
      "step : 2600 / 3125 acc : 0.108\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1080 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 25.914175\n",
      "..........\n",
      "[1,   200] loss: 25.821706\n",
      "..........\n",
      "[1,   300] loss: 25.527186\n",
      "..........\n",
      "[1,   400] loss: 26.743120\n",
      "..........\n",
      "[1,   500] loss: 25.931373\n",
      "..........\n",
      "[1,   600] loss: 25.946315\n",
      "..........\n",
      "[1,   700] loss: 26.363617\n",
      "..........\n",
      "[1,   800] loss: 26.196380\n",
      "..........\n",
      "[1,   900] loss: 25.594823\n",
      "..........\n",
      "[1,  1000] loss: 25.760863\n",
      "..........\n",
      "[1,  1100] loss: 26.353586\n",
      "..........\n",
      "[1,  1200] loss: 25.884536\n",
      "..........\n",
      "[1,  1300] loss: 25.976818\n",
      "..........\n",
      "[1,  1400] loss: 25.479411\n",
      "..........\n",
      "[1,  1500] loss: 26.365507\n",
      "..........\n",
      "[1,  1600] loss: 26.721334\n",
      "..........\n",
      "[1,  1700] loss: 26.820297\n",
      "..........\n",
      "[1,  1800] loss: 25.846147\n",
      "..........\n",
      "[1,  1900] loss: 26.699636\n",
      "..........\n",
      "[1,  2000] loss: 26.318948\n",
      "..........\n",
      "[1,  2100] loss: 26.190054\n",
      "..........\n",
      "[1,  2200] loss: 26.261644\n",
      "..........\n",
      "[1,  2300] loss: 26.291971\n",
      "..........\n",
      "[1,  2400] loss: 26.490672\n",
      "..........\n",
      "[1,  2500] loss: 26.315548\n",
      "..........\n",
      "[1,  2600] loss: 26.269295\n",
      "..........\n",
      "[1,  2700] loss: 26.258390\n",
      "..........\n",
      "[1,  2800] loss: 26.912005\n",
      "..........\n",
      "[1,  2900] loss: 26.844058\n",
      "..........\n",
      "[1,  3000] loss: 26.912614\n",
      "..........\n",
      "[1,  3100] loss: 26.753288\n",
      "..........\n",
      "[1,  3200] loss: 26.839558\n",
      "..........\n",
      "[1,  3300] loss: 26.614898\n",
      "..........\n",
      "[1,  3400] loss: 26.907211\n",
      "..........\n",
      "[1,  3500] loss: 26.268280\n",
      "..........\n",
      "[1,  3600] loss: 26.539297\n",
      "..........\n",
      "[1,  3700] loss: 27.290966\n",
      "..........\n",
      "[1,  3800] loss: 26.858805\n",
      "..........\n",
      "[1,  3900] loss: 27.262088\n",
      "..........\n",
      "[1,  4000] loss: 26.702514\n",
      "..........\n",
      "[1,  4100] loss: 26.421234\n",
      "..........\n",
      "[1,  4200] loss: 27.115194\n",
      "..........\n",
      "[1,  4300] loss: 27.119348\n",
      "..........\n",
      "[1,  4400] loss: 27.558236\n",
      "..........\n",
      "[1,  4500] loss: 27.396927\n",
      "..........\n",
      "[1,  4600] loss: 26.527995\n",
      "..........\n",
      "[1,  4700] loss: 27.163260\n",
      "..........\n",
      "[1,  4800] loss: 26.317433\n",
      "..........\n",
      "[1,  4900] loss: 26.765060\n",
      "..........\n",
      "[1,  5000] loss: 26.651694\n",
      "..........\n",
      "[1,  5100] loss: 26.855575\n",
      "..........\n",
      "[1,  5200] loss: 27.184977\n",
      "..........\n",
      "[1,  5300] loss: 26.769610\n",
      "..........\n",
      "[1,  5400] loss: 27.084077\n",
      "..........\n",
      "[1,  5500] loss: 28.047464\n",
      "..........\n",
      "[1,  5600] loss: 27.368563\n",
      "..........\n",
      "[1,  5700] loss: 26.897104\n",
      "..........\n",
      "[1,  5800] loss: 26.471225\n",
      "..........\n",
      "[1,  5900] loss: 27.272463\n",
      "..........\n",
      "[1,  6000] loss: 27.594801\n",
      "..........\n",
      "[1,  6100] loss: 28.059688\n",
      "..........\n",
      "[1,  6200] loss: 26.904136\n",
      "..........\n",
      "[1,  6300] loss: 27.997015\n",
      "..........\n",
      "[1,  6400] loss: 27.375905\n",
      "..........\n",
      "[1,  6500] loss: 26.854973\n",
      "..........\n",
      "[1,  6600] loss: 27.578249\n",
      "..........\n",
      "[1,  6700] loss: 27.923427\n",
      "..........\n",
      "[1,  6800] loss: 27.540722\n",
      "..........\n",
      "[1,  6900] loss: 27.448407\n",
      "..........\n",
      "[1,  7000] loss: 27.844521\n",
      "..........\n",
      "[1,  7100] loss: 27.252338\n",
      "..........\n",
      "[1,  7200] loss: 27.993132\n",
      "..........\n",
      "[1,  7300] loss: 26.955885\n",
      "..........\n",
      "[1,  7400] loss: 27.899913\n",
      "..........\n",
      "[1,  7500] loss: 28.004547\n",
      "..........\n",
      "[1,  7600] loss: 27.467523\n",
      "..........\n",
      "[1,  7700] loss: 27.678711\n",
      "..........\n",
      "[1,  7800] loss: 28.119003\n",
      "..........\n",
      "[1,  7900] loss: 28.336039\n",
      "..........\n",
      "[1,  8000] loss: 27.812683\n",
      "total average loss : 26.858\n",
      "train acc : 0.1000\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.125\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.129\n",
      "step : 1600 / 3125 acc : 0.121\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.119\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.117\n",
      "step : 2600 / 3125 acc : 0.115\n",
      "step : 2800 / 3125 acc : 0.112\n",
      "step : 3000 / 3125 acc : 0.121\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1200 %\n",
      "======eval  end ======\n",
      "======= epoch  2 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 28.406897\n",
      "..........\n",
      "[1,   200] loss: 28.664914\n",
      "..........\n",
      "[1,   300] loss: 28.131187\n",
      "..........\n",
      "[1,   400] loss: 27.949578\n",
      "..........\n",
      "[1,   500] loss: 28.341965\n",
      "..........\n",
      "[1,   600] loss: 27.787564\n",
      "..........\n",
      "[1,   700] loss: 28.407038\n",
      "..........\n",
      "[1,   800] loss: 28.254129\n",
      "..........\n",
      "[1,   900] loss: 27.984050\n",
      "..........\n",
      "[1,  1000] loss: 28.451166\n",
      "..........\n",
      "[1,  1100] loss: 28.397430\n",
      "..........\n",
      "[1,  1200] loss: 27.923066\n",
      "..........\n",
      "[1,  1300] loss: 28.953074\n",
      "..........\n",
      "[1,  1400] loss: 28.319385\n",
      "..........\n",
      "[1,  1500] loss: 28.293248\n",
      "..........\n",
      "[1,  1600] loss: 28.583966\n",
      "..........\n",
      "[1,  1700] loss: 29.158628\n",
      "..........\n",
      "[1,  1800] loss: 29.187778\n",
      "..........\n",
      "[1,  1900] loss: 29.105453\n",
      "..........\n",
      "[1,  2000] loss: 28.857566\n",
      "..........\n",
      "[1,  2100] loss: 28.371905\n",
      "..........\n",
      "[1,  2200] loss: 28.580776\n",
      "..........\n",
      "[1,  2300] loss: 29.535068\n",
      "..........\n",
      "[1,  2400] loss: 28.400642\n",
      "..........\n",
      "[1,  2500] loss: 28.788683\n",
      "..........\n",
      "[1,  2600] loss: 28.595951\n",
      "..........\n",
      "[1,  2700] loss: 28.988584\n",
      "..........\n",
      "[1,  2800] loss: 28.924881\n",
      "..........\n",
      "[1,  2900] loss: 28.839777\n",
      "..........\n",
      "[1,  3000] loss: 28.959203\n",
      "..........\n",
      "[1,  3100] loss: 28.479548\n",
      "..........\n",
      "[1,  3200] loss: 28.644310\n",
      "..........\n",
      "[1,  3300] loss: 29.303583\n",
      "..........\n",
      "[1,  3400] loss: 29.136040\n",
      "..........\n",
      "[1,  3500] loss: 28.995458\n",
      "..........\n",
      "[1,  3600] loss: 28.853017\n",
      "..........\n",
      "[1,  3700] loss: 28.764037\n",
      "..........\n",
      "[1,  3800] loss: 29.482413\n",
      "..........\n",
      "[1,  3900] loss: 28.761781\n",
      "..........\n",
      "[1,  4000] loss: 29.329574\n",
      "..........\n",
      "[1,  4100] loss: 29.016296\n",
      "..........\n",
      "[1,  4200] loss: 29.417665\n",
      "..........\n",
      "[1,  4300] loss: 29.546619\n",
      "..........\n",
      "[1,  4400] loss: 29.450522\n",
      "..........\n",
      "[1,  4500] loss: 29.163313\n",
      "..........\n",
      "[1,  4600] loss: 28.539187\n",
      "..........\n",
      "[1,  4700] loss: 29.456991\n",
      "..........\n",
      "[1,  4800] loss: 29.244387\n",
      "..........\n",
      "[1,  4900] loss: 29.531353\n",
      "..........\n",
      "[1,  5000] loss: 29.518140\n",
      "..........\n",
      "[1,  5100] loss: 28.979170\n",
      "..........\n",
      "[1,  5200] loss: 28.724144\n",
      "..........\n",
      "[1,  5300] loss: 29.571041\n",
      "..........\n",
      "[1,  5400] loss: 29.883232\n",
      "..........\n",
      "[1,  5500] loss: 29.422265\n",
      "..........\n",
      "[1,  5600] loss: 29.784972\n",
      "..........\n",
      "[1,  5700] loss: 28.894368\n",
      "..........\n",
      "[1,  5800] loss: 28.924142\n",
      "..........\n",
      "[1,  5900] loss: 29.426904\n",
      "..........\n",
      "[1,  6000] loss: 29.563512\n",
      "..........\n",
      "[1,  6100] loss: 29.756596\n",
      "..........\n",
      "[1,  6200] loss: 29.742606\n",
      "..........\n",
      "[1,  6300] loss: 29.524233\n",
      "..........\n",
      "[1,  6400] loss: 30.008835\n",
      "..........\n",
      "[1,  6500] loss: 29.747195\n",
      "..........\n",
      "[1,  6600] loss: 28.947009\n",
      "..........\n",
      "[1,  6700] loss: 29.104540\n",
      "..........\n",
      "[1,  6800] loss: 29.795355\n",
      "..........\n",
      "[1,  6900] loss: 29.378527\n",
      "..........\n",
      "[1,  7000] loss: 29.402944\n",
      "..........\n",
      "[1,  7100] loss: 29.913837\n",
      "..........\n",
      "[1,  7200] loss: 29.993820\n",
      "..........\n",
      "[1,  7300] loss: 29.944471\n",
      "..........\n",
      "[1,  7400] loss: 29.261746\n",
      "..........\n",
      "[1,  7500] loss: 30.484582\n",
      "..........\n",
      "[1,  7600] loss: 30.073405\n",
      "..........\n",
      "[1,  7700] loss: 29.894398\n",
      "..........\n",
      "[1,  7800] loss: 29.758075\n",
      "..........\n",
      "[1,  7900] loss: 30.227673\n",
      "..........\n",
      "[1,  8000] loss: 29.703286\n",
      "total average loss : 29.095\n",
      "train acc : 0.0938\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.131\n",
      "step : 1200 / 3125 acc : 0.130\n",
      "step : 1400 / 3125 acc : 0.134\n",
      "step : 1600 / 3125 acc : 0.133\n",
      "step : 1800 / 3125 acc : 0.125\n",
      "step : 2000 / 3125 acc : 0.138\n",
      "step : 2200 / 3125 acc : 0.128\n",
      "step : 2400 / 3125 acc : 0.125\n",
      "step : 2600 / 3125 acc : 0.120\n",
      "step : 2800 / 3125 acc : 0.125\n",
      "step : 3000 / 3125 acc : 0.123\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1240 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 31.053934\n",
      "..........\n",
      "[1,   200] loss: 30.389813\n",
      "..........\n",
      "[1,   300] loss: 30.776466\n",
      "..........\n",
      "[1,   400] loss: 30.814986\n",
      "..........\n",
      "[1,   500] loss: 30.575213\n",
      "..........\n",
      "[1,   600] loss: 30.071659\n",
      "..........\n",
      "[1,   700] loss: 30.062513\n",
      "..........\n",
      "[1,   800] loss: 31.005961\n",
      "..........\n",
      "[1,   900] loss: 30.456748\n",
      "..........\n",
      "[1,  1000] loss: 30.511255\n",
      "..........\n",
      "[1,  1100] loss: 30.785294\n",
      "..........\n",
      "[1,  1200] loss: 30.629518\n",
      "..........\n",
      "[1,  1300] loss: 31.016350\n",
      "..........\n",
      "[1,  1400] loss: 30.400827\n",
      "..........\n",
      "[1,  1500] loss: 30.908011\n",
      "..........\n",
      "[1,  1600] loss: 30.543578\n",
      "..........\n",
      "[1,  1700] loss: 31.349733\n",
      "..........\n",
      "[1,  1800] loss: 31.329090\n",
      "..........\n",
      "[1,  1900] loss: 30.927344\n",
      "..........\n",
      "[1,  2000] loss: 30.858400\n",
      "..........\n",
      "[1,  2100] loss: 30.757481\n",
      "..........\n",
      "[1,  2200] loss: 31.071883\n",
      "..........\n",
      "[1,  2300] loss: 30.755306\n",
      "..........\n",
      "[1,  2400] loss: 30.598451\n",
      "..........\n",
      "[1,  2500] loss: 31.267955\n",
      "..........\n",
      "[1,  2600] loss: 30.546128\n",
      "..........\n",
      "[1,  2700] loss: 31.472292\n",
      "..........\n",
      "[1,  2800] loss: 31.036988\n",
      "..........\n",
      "[1,  2900] loss: 31.025200\n",
      "..........\n",
      "[1,  3000] loss: 31.507769\n",
      "..........\n",
      "[1,  3100] loss: 30.806686\n",
      "..........\n",
      "[1,  3200] loss: 31.221078\n",
      "..........\n",
      "[1,  3300] loss: 30.841848\n",
      "..........\n",
      "[1,  3400] loss: 31.102006\n",
      "..........\n",
      "[1,  3500] loss: 30.932723\n",
      "..........\n",
      "[1,  3600] loss: 31.145650\n",
      "..........\n",
      "[1,  3700] loss: 31.109042\n",
      "..........\n",
      "[1,  3800] loss: 31.654422\n",
      "..........\n",
      "[1,  3900] loss: 31.226110\n",
      "..........\n",
      "[1,  4000] loss: 30.983123\n",
      "..........\n",
      "[1,  4100] loss: 31.593390\n",
      "..........\n",
      "[1,  4200] loss: 31.811444\n",
      "..........\n",
      "[1,  4300] loss: 31.248506\n",
      "..........\n",
      "[1,  4400] loss: 31.494622\n",
      "..........\n",
      "[1,  4500] loss: 31.464202\n",
      "..........\n",
      "[1,  4600] loss: 31.290242\n",
      "..........\n",
      "[1,  4700] loss: 31.789482\n",
      "..........\n",
      "[1,  4800] loss: 30.981232\n",
      "..........\n",
      "[1,  4900] loss: 32.181266\n",
      "..........\n",
      "[1,  5000] loss: 31.064669\n",
      "..........\n",
      "[1,  5100] loss: 31.881710\n",
      "..........\n",
      "[1,  5200] loss: 31.910884\n",
      "..........\n",
      "[1,  5300] loss: 31.453127\n",
      "..........\n",
      "[1,  5400] loss: 31.377354\n",
      "..........\n",
      "[1,  5500] loss: 31.034266\n",
      "..........\n",
      "[1,  5600] loss: 31.362685\n",
      "..........\n",
      "[1,  5700] loss: 31.153650\n",
      "..........\n",
      "[1,  5800] loss: 31.499445\n",
      "..........\n",
      "[1,  5900] loss: 30.638616\n",
      "..........\n",
      "[1,  6000] loss: 31.590533\n",
      "..........\n",
      "[1,  6100] loss: 31.627041\n",
      "..........\n",
      "[1,  6200] loss: 32.454682\n",
      "..........\n",
      "[1,  6300] loss: 31.575636\n",
      "..........\n",
      "[1,  6400] loss: 31.806312\n",
      "..........\n",
      "[1,  6500] loss: 31.551150\n",
      "..........\n",
      "[1,  6600] loss: 31.671674\n",
      "..........\n",
      "[1,  6700] loss: 31.977413\n",
      "..........\n",
      "[1,  6800] loss: 31.170916\n",
      "..........\n",
      "[1,  6900] loss: 31.440635\n",
      "..........\n",
      "[1,  7000] loss: 31.694809\n",
      "..........\n",
      "[1,  7100] loss: 32.142654\n",
      "..........\n",
      "[1,  7200] loss: 31.413526\n",
      "..........\n",
      "[1,  7300] loss: 31.695341\n",
      "..........\n",
      "[1,  7400] loss: 31.986368\n",
      "..........\n",
      "[1,  7500] loss: 31.918590\n",
      "..........\n",
      "[1,  7600] loss: 32.182861\n",
      "..........\n",
      "[1,  7700] loss: 31.903044\n",
      "..........\n",
      "[1,  7800] loss: 32.552555\n",
      "..........\n",
      "[1,  7900] loss: 31.619962\n",
      "..........\n",
      "[1,  8000] loss: 31.583770\n",
      "total average loss : 31.254\n",
      "train acc : 0.0883\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.130\n",
      "step : 1400 / 3125 acc : 0.134\n",
      "step : 1600 / 3125 acc : 0.137\n",
      "step : 1800 / 3125 acc : 0.146\n",
      "step : 2000 / 3125 acc : 0.134\n",
      "step : 2200 / 3125 acc : 0.134\n",
      "step : 2400 / 3125 acc : 0.128\n",
      "step : 2600 / 3125 acc : 0.127\n",
      "step : 2800 / 3125 acc : 0.123\n",
      "step : 3000 / 3125 acc : 0.119\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1180 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 32.934420\n",
      "..........\n",
      "[1,   200] loss: 32.455888\n",
      "..........\n",
      "[1,   300] loss: 32.867978\n",
      "..........\n",
      "[1,   400] loss: 33.343949\n",
      "..........\n",
      "[1,   500] loss: 33.099012\n",
      "..........\n",
      "[1,   600] loss: 32.228752\n",
      "..........\n",
      "[1,   700] loss: 32.555617\n",
      "..........\n",
      "[1,   800] loss: 32.709670\n",
      "..........\n",
      "[1,   900] loss: 32.477809\n",
      "..........\n",
      "[1,  1000] loss: 32.632061\n",
      "..........\n",
      "[1,  1100] loss: 32.397123\n",
      "..........\n",
      "[1,  1200] loss: 32.751246\n",
      "..........\n",
      "[1,  1300] loss: 32.854741\n",
      "..........\n",
      "[1,  1400] loss: 33.355278\n",
      "..........\n",
      "[1,  1500] loss: 31.922809\n",
      "..........\n",
      "[1,  1600] loss: 32.681367\n",
      "..........\n",
      "[1,  1700] loss: 32.743894\n",
      "..........\n",
      "[1,  1800] loss: 32.723057\n",
      "..........\n",
      "[1,  1900] loss: 33.229233\n",
      "..........\n",
      "[1,  2000] loss: 33.594141\n",
      "..........\n",
      "[1,  2100] loss: 32.567763\n",
      "..........\n",
      "[1,  2200] loss: 33.524528\n",
      "..........\n",
      "[1,  2300] loss: 32.617754\n",
      "..........\n",
      "[1,  2400] loss: 33.004420\n",
      "..........\n",
      "[1,  2500] loss: 32.898039\n",
      "..........\n",
      "[1,  2600] loss: 33.246314\n",
      "..........\n",
      "[1,  2700] loss: 32.358795\n",
      "..........\n",
      "[1,  2800] loss: 32.683917\n",
      "..........\n",
      "[1,  2900] loss: 33.357430\n",
      "..........\n",
      "[1,  3000] loss: 32.969045\n",
      "..........\n",
      "[1,  3100] loss: 33.104531\n",
      "..........\n",
      "[1,  3200] loss: 33.371902\n",
      "..........\n",
      "[1,  3300] loss: 32.930613\n",
      "..........\n",
      "[1,  3400] loss: 33.784306\n",
      "..........\n",
      "[1,  3500] loss: 32.741894\n",
      "..........\n",
      "[1,  3600] loss: 33.456605\n",
      "..........\n",
      "[1,  3700] loss: 33.420845\n",
      "..........\n",
      "[1,  3800] loss: 32.924937\n",
      "..........\n",
      "[1,  3900] loss: 33.220110\n",
      "..........\n",
      "[1,  4000] loss: 33.493333\n",
      "..........\n",
      "[1,  4100] loss: 33.330464\n",
      "..........\n",
      "[1,  4200] loss: 33.693277\n",
      "..........\n",
      "[1,  4300] loss: 32.839831\n",
      "..........\n",
      "[1,  4400] loss: 33.238194\n",
      "..........\n",
      "[1,  4500] loss: 33.642023\n",
      "..........\n",
      "[1,  4600] loss: 33.563206\n",
      "..........\n",
      "[1,  4700] loss: 33.433404\n",
      "..........\n",
      "[1,  4800] loss: 33.800363\n",
      "..........\n",
      "[1,  4900] loss: 33.785303\n",
      "..........\n",
      "[1,  5000] loss: 33.304663\n",
      "..........\n",
      "[1,  5100] loss: 33.200352\n",
      "..........\n",
      "[1,  5200] loss: 33.787791\n",
      "..........\n",
      "[1,  5300] loss: 33.695552\n",
      "..........\n",
      "[1,  5400] loss: 33.261450\n",
      "..........\n",
      "[1,  5500] loss: 33.944994\n",
      "..........\n",
      "[1,  5600] loss: 33.429356\n",
      "..........\n",
      "[1,  5700] loss: 33.516358\n",
      "..........\n",
      "[1,  5800] loss: 33.911283\n",
      "..........\n",
      "[1,  5900] loss: 33.908109\n",
      "..........\n",
      "[1,  6000] loss: 33.862479\n",
      "..........\n",
      "[1,  6100] loss: 33.814821\n",
      "..........\n",
      "[1,  6200] loss: 33.432778\n",
      "..........\n",
      "[1,  6300] loss: 34.660180\n",
      "..........\n",
      "[1,  6400] loss: 33.769614\n",
      "..........\n",
      "[1,  6500] loss: 34.144648\n",
      "..........\n",
      "[1,  6600] loss: 34.103006\n",
      "..........\n",
      "[1,  6700] loss: 34.357131\n",
      "..........\n",
      "[1,  6800] loss: 33.447174\n",
      "..........\n",
      "[1,  6900] loss: 33.981326\n",
      "..........\n",
      "[1,  7000] loss: 33.894688\n",
      "..........\n",
      "[1,  7100] loss: 33.833054\n",
      "..........\n",
      "[1,  7200] loss: 34.151966\n",
      "..........\n",
      "[1,  7300] loss: 34.108590\n",
      "..........\n",
      "[1,  7400] loss: 33.797322\n",
      "..........\n",
      "[1,  7500] loss: 33.991138\n",
      "..........\n",
      "[1,  7600] loss: 33.892975\n",
      "..........\n",
      "[1,  7700] loss: 34.295173\n",
      "..........\n",
      "[1,  7800] loss: 34.073241\n",
      "..........\n",
      "[1,  7900] loss: 33.836537\n",
      "..........\n",
      "[1,  8000] loss: 34.109234\n",
      "total average loss : 33.351\n",
      "train acc : 0.0914\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.146\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.131\n",
      "step : 1200 / 3125 acc : 0.141\n",
      "step : 1400 / 3125 acc : 0.138\n",
      "step : 1600 / 3125 acc : 0.137\n",
      "step : 1800 / 3125 acc : 0.132\n",
      "step : 2000 / 3125 acc : 0.125\n",
      "step : 2200 / 3125 acc : 0.116\n",
      "step : 2400 / 3125 acc : 0.112\n",
      "step : 2600 / 3125 acc : 0.120\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.113\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1140 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 34.810915\n",
      "..........\n",
      "[1,   200] loss: 34.659135\n",
      "..........\n",
      "[1,   300] loss: 34.434642\n",
      "..........\n",
      "[1,   400] loss: 34.642919\n",
      "..........\n",
      "[1,   500] loss: 33.712731\n",
      "..........\n",
      "[1,   600] loss: 35.132925\n",
      "..........\n",
      "[1,   700] loss: 34.911426\n",
      "..........\n",
      "[1,   800] loss: 34.755377\n",
      "..........\n",
      "[1,   900] loss: 34.783902\n",
      "..........\n",
      "[1,  1000] loss: 35.259268\n",
      "..........\n",
      "[1,  1100] loss: 35.131014\n",
      "..........\n",
      "[1,  1200] loss: 35.493540\n",
      "..........\n",
      "[1,  1300] loss: 35.049470\n",
      "..........\n",
      "[1,  1400] loss: 34.908906\n",
      "..........\n",
      "[1,  1500] loss: 35.350251\n",
      "..........\n",
      "[1,  1600] loss: 35.603701\n",
      "..........\n",
      "[1,  1700] loss: 34.957135\n",
      "..........\n",
      "[1,  1800] loss: 35.390409\n",
      "..........\n",
      "[1,  1900] loss: 35.996219\n",
      "..........\n",
      "[1,  2000] loss: 34.984378\n",
      "..........\n",
      "[1,  2100] loss: 34.708410\n",
      "..........\n",
      "[1,  2200] loss: 34.774301\n",
      "..........\n",
      "[1,  2300] loss: 35.526297\n",
      "..........\n",
      "[1,  2400] loss: 34.709986\n",
      "..........\n",
      "[1,  2500] loss: 35.168883\n",
      "..........\n",
      "[1,  2600] loss: 35.518787\n",
      "..........\n",
      "[1,  2700] loss: 35.285577\n",
      "..........\n",
      "[1,  2800] loss: 35.626283\n",
      "..........\n",
      "[1,  2900] loss: 35.395135\n",
      "..........\n",
      "[1,  3000] loss: 34.710217\n",
      "..........\n",
      "[1,  3100] loss: 35.364379\n",
      "..........\n",
      "[1,  3200] loss: 35.179804\n",
      "..........\n",
      "[1,  3300] loss: 35.009873\n",
      "..........\n",
      "[1,  3400] loss: 35.715396\n",
      "..........\n",
      "[1,  3500] loss: 35.136209\n",
      "..........\n",
      "[1,  3600] loss: 35.471652\n",
      "..........\n",
      "[1,  3700] loss: 35.734754\n",
      "..........\n",
      "[1,  3800] loss: 35.177264\n",
      "..........\n",
      "[1,  3900] loss: 35.740182\n",
      "..........\n",
      "[1,  4000] loss: 35.728739\n",
      "..........\n",
      "[1,  4100] loss: 35.738074\n",
      "..........\n",
      "[1,  4200] loss: 35.221830\n",
      "..........\n",
      "[1,  4300] loss: 35.528603\n",
      "..........\n",
      "[1,  4400] loss: 35.493253\n",
      "..........\n",
      "[1,  4500] loss: 36.051198\n",
      "..........\n",
      "[1,  4600] loss: 35.522148\n",
      "..........\n",
      "[1,  4700] loss: 35.888981\n",
      "..........\n",
      "[1,  4800] loss: 35.758140\n",
      "..........\n",
      "[1,  4900] loss: 35.420739\n",
      "..........\n",
      "[1,  5000] loss: 36.542840\n",
      "..........\n",
      "[1,  5100] loss: 35.943040\n",
      "..........\n",
      "[1,  5200] loss: 35.614197\n",
      "..........\n",
      "[1,  5300] loss: 35.835416\n",
      "..........\n",
      "[1,  5400] loss: 35.384775\n",
      "..........\n",
      "[1,  5500] loss: 36.462859\n",
      "..........\n",
      "[1,  5600] loss: 36.133222\n",
      "..........\n",
      "[1,  5700] loss: 36.765690\n",
      "..........\n",
      "[1,  5800] loss: 36.368084\n",
      "..........\n",
      "[1,  5900] loss: 36.246700\n",
      "..........\n",
      "[1,  6000] loss: 35.919838\n",
      "..........\n",
      "[1,  6100] loss: 36.222101\n",
      "..........\n",
      "[1,  6200] loss: 36.215793\n",
      "..........\n",
      "[1,  6300] loss: 36.373861\n",
      "..........\n",
      "[1,  6400] loss: 36.686542\n",
      "..........\n",
      "[1,  6500] loss: 36.591347\n",
      "..........\n",
      "[1,  6600] loss: 36.043096\n",
      "..........\n",
      "[1,  6700] loss: 36.582279\n",
      "..........\n",
      "[1,  6800] loss: 36.320723\n",
      "..........\n",
      "[1,  6900] loss: 36.418312\n",
      "..........\n",
      "[1,  7000] loss: 36.462712\n",
      "..........\n",
      "[1,  7100] loss: 36.379590\n",
      "..........\n",
      "[1,  7200] loss: 36.970549\n",
      "..........\n",
      "[1,  7300] loss: 36.772923\n",
      "..........\n",
      "[1,  7400] loss: 36.309483\n",
      "..........\n",
      "[1,  7500] loss: 36.900249\n",
      "..........\n",
      "[1,  7600] loss: 36.563159\n",
      "..........\n",
      "[1,  7700] loss: 37.137495\n",
      "..........\n",
      "[1,  7800] loss: 36.767217\n",
      "..........\n",
      "[1,  7900] loss: 36.997910\n",
      "..........\n",
      "[1,  8000] loss: 36.447351\n",
      "total average loss : 35.683\n",
      "train acc : 0.0914\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.167\n",
      "step : 800 / 3125 acc : 0.164\n",
      "step : 1000 / 3125 acc : 0.188\n",
      "step : 1200 / 3125 acc : 0.182\n",
      "step : 1400 / 3125 acc : 0.161\n",
      "step : 1600 / 3125 acc : 0.152\n",
      "step : 1800 / 3125 acc : 0.142\n",
      "step : 2000 / 3125 acc : 0.134\n",
      "step : 2200 / 3125 acc : 0.134\n",
      "step : 2400 / 3125 acc : 0.130\n",
      "step : 2600 / 3125 acc : 0.125\n",
      "step : 2800 / 3125 acc : 0.121\n",
      "step : 3000 / 3125 acc : 0.115\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "======= epoch  3 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 37.276446\n",
      "..........\n",
      "[1,   200] loss: 37.123344\n",
      "..........\n",
      "[1,   300] loss: 37.189560\n",
      "..........\n",
      "[1,   400] loss: 37.203689\n",
      "..........\n",
      "[1,   500] loss: 37.720542\n",
      "..........\n",
      "[1,   600] loss: 37.948627\n",
      "..........\n",
      "[1,   700] loss: 38.318729\n",
      "..........\n",
      "[1,   800] loss: 37.421630\n",
      "..........\n",
      "[1,   900] loss: 37.627402\n",
      "..........\n",
      "[1,  1000] loss: 38.270973\n",
      "..........\n",
      "[1,  1100] loss: 38.141655\n",
      "..........\n",
      "[1,  1200] loss: 37.920965\n",
      "..........\n",
      "[1,  1300] loss: 37.405762\n",
      "..........\n",
      "[1,  1400] loss: 37.355354\n",
      "..........\n",
      "[1,  1500] loss: 37.985441\n",
      "..........\n",
      "[1,  1600] loss: 37.965668\n",
      "..........\n",
      "[1,  1700] loss: 37.625029\n",
      "..........\n",
      "[1,  1800] loss: 38.110435\n",
      "..........\n",
      "[1,  1900] loss: 38.244925\n",
      "..........\n",
      "[1,  2000] loss: 38.155611\n",
      "..........\n",
      "[1,  2100] loss: 37.902387\n",
      "..........\n",
      "[1,  2200] loss: 38.407815\n",
      "..........\n",
      "[1,  2300] loss: 38.346066\n",
      "..........\n",
      "[1,  2400] loss: 38.298078\n",
      "..........\n",
      "[1,  2500] loss: 37.809868\n",
      "..........\n",
      "[1,  2600] loss: 38.261656\n",
      "..........\n",
      "[1,  2700] loss: 38.465694\n",
      "..........\n",
      "[1,  2800] loss: 38.277961\n",
      "..........\n",
      "[1,  2900] loss: 38.017758\n",
      "..........\n",
      "[1,  3000] loss: 38.097514\n",
      "..........\n",
      "[1,  3100] loss: 37.967638\n",
      "..........\n",
      "[1,  3200] loss: 39.201302\n",
      "..........\n",
      "[1,  3300] loss: 38.049324\n",
      "..........\n",
      "[1,  3400] loss: 38.844609\n",
      "..........\n",
      "[1,  3500] loss: 37.745207\n",
      "..........\n",
      "[1,  3600] loss: 39.126922\n",
      "..........\n",
      "[1,  3700] loss: 38.359316\n",
      "..........\n",
      "[1,  3800] loss: 38.169957\n",
      "..........\n",
      "[1,  3900] loss: 38.031293\n",
      "..........\n",
      "[1,  4000] loss: 38.277325\n",
      "..........\n",
      "[1,  4100] loss: 38.499738\n",
      "..........\n",
      "[1,  4200] loss: 38.523804\n",
      "..........\n",
      "[1,  4300] loss: 38.414010\n",
      "..........\n",
      "[1,  4400] loss: 38.850172\n",
      "..........\n",
      "[1,  4500] loss: 39.040099\n",
      "..........\n",
      "[1,  4600] loss: 38.720130\n",
      "..........\n",
      "[1,  4700] loss: 38.772048\n",
      "..........\n",
      "[1,  4800] loss: 38.368701\n",
      "..........\n",
      "[1,  4900] loss: 38.843332\n",
      "..........\n",
      "[1,  5000] loss: 38.399401\n",
      "..........\n",
      "[1,  5100] loss: 38.354978\n",
      "..........\n",
      "[1,  5200] loss: 39.299349\n",
      "..........\n",
      "[1,  5300] loss: 38.841546\n",
      "..........\n",
      "[1,  5400] loss: 39.357688\n",
      "..........\n",
      "[1,  5500] loss: 38.541034\n",
      "..........\n",
      "[1,  5600] loss: 39.823572\n",
      "..........\n",
      "[1,  5700] loss: 39.015339\n",
      "..........\n",
      "[1,  5800] loss: 38.577470\n",
      "..........\n",
      "[1,  5900] loss: 39.198636\n",
      "..........\n",
      "[1,  6000] loss: 38.747353\n",
      "..........\n",
      "[1,  6100] loss: 38.594378\n",
      "..........\n",
      "[1,  6200] loss: 38.514072\n",
      "..........\n",
      "[1,  6300] loss: 39.103174\n",
      "..........\n",
      "[1,  6400] loss: 39.363112\n",
      "..........\n",
      "[1,  6500] loss: 39.644352\n",
      "..........\n",
      "[1,  6600] loss: 38.414604\n",
      "..........\n",
      "[1,  6700] loss: 38.687541\n",
      "..........\n",
      "[1,  6800] loss: 38.609975\n",
      "..........\n",
      "[1,  6900] loss: 39.213065\n",
      "..........\n",
      "[1,  7000] loss: 39.002260\n",
      "..........\n",
      "[1,  7100] loss: 38.319504\n",
      "..........\n",
      "[1,  7200] loss: 38.300223\n",
      "..........\n",
      "[1,  7300] loss: 40.342339\n",
      "..........\n",
      "[1,  7400] loss: 39.132972\n",
      "..........\n",
      "[1,  7500] loss: 38.924492\n",
      "..........\n",
      "[1,  7600] loss: 39.334165\n",
      "..........\n",
      "[1,  7700] loss: 39.819761\n",
      "..........\n",
      "[1,  7800] loss: 39.125331\n",
      "..........\n",
      "[1,  7900] loss: 39.184615\n",
      "..........\n",
      "[1,  8000] loss: 38.908699\n",
      "total average loss : 38.467\n",
      "train acc : 0.0977\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.098\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.104\n",
      "step : 2000 / 3125 acc : 0.103\n",
      "step : 2200 / 3125 acc : 0.099\n",
      "step : 2400 / 3125 acc : 0.091\n",
      "step : 2600 / 3125 acc : 0.111\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.113\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1080 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 41.159284\n",
      "..........\n",
      "[1,   200] loss: 40.191532\n",
      "..........\n",
      "[1,   300] loss: 41.107847\n",
      "..........\n",
      "[1,   400] loss: 40.896135\n",
      "..........\n",
      "[1,   500] loss: 40.993168\n",
      "..........\n",
      "[1,   600] loss: 39.900518\n",
      "..........\n",
      "[1,   700] loss: 40.356197\n",
      "..........\n",
      "[1,   800] loss: 40.482708\n",
      "..........\n",
      "[1,   900] loss: 40.479658\n",
      "..........\n",
      "[1,  1000] loss: 40.472193\n",
      "..........\n",
      "[1,  1100] loss: 40.895776\n",
      "..........\n",
      "[1,  1200] loss: 40.119786\n",
      "..........\n",
      "[1,  1300] loss: 40.799992\n",
      "..........\n",
      "[1,  1400] loss: 41.135469\n",
      "..........\n",
      "[1,  1500] loss: 40.362036\n",
      "..........\n",
      "[1,  1600] loss: 41.559572\n",
      "..........\n",
      "[1,  1700] loss: 40.745615\n",
      "..........\n",
      "[1,  1800] loss: 41.499233\n",
      "..........\n",
      "[1,  1900] loss: 40.706735\n",
      "..........\n",
      "[1,  2000] loss: 41.105817\n",
      "..........\n",
      "[1,  2100] loss: 40.185055\n",
      "..........\n",
      "[1,  2200] loss: 40.222417\n",
      "..........\n",
      "[1,  2300] loss: 40.794724\n",
      "..........\n",
      "[1,  2400] loss: 40.732446\n",
      "..........\n",
      "[1,  2500] loss: 41.232026\n",
      "..........\n",
      "[1,  2600] loss: 40.921808\n",
      "..........\n",
      "[1,  2700] loss: 41.178400\n",
      "..........\n",
      "[1,  2800] loss: 41.547621\n",
      "..........\n",
      "[1,  2900] loss: 39.843157\n",
      "..........\n",
      "[1,  3000] loss: 41.084403\n",
      "..........\n",
      "[1,  3100] loss: 41.080877\n",
      "..........\n",
      "[1,  3200] loss: 40.623076\n",
      "..........\n",
      "[1,  3300] loss: 40.561128\n",
      "..........\n",
      "[1,  3400] loss: 41.091395\n",
      "..........\n",
      "[1,  3500] loss: 42.022441\n",
      "..........\n",
      "[1,  3600] loss: 40.951631\n",
      "..........\n",
      "[1,  3700] loss: 41.207999\n",
      "..........\n",
      "[1,  3800] loss: 41.300469\n",
      "..........\n",
      "[1,  3900] loss: 41.681268\n",
      "..........\n",
      "[1,  4000] loss: 40.431085\n",
      "..........\n",
      "[1,  4100] loss: 41.232687\n",
      "..........\n",
      "[1,  4200] loss: 41.070575\n",
      "..........\n",
      "[1,  4300] loss: 41.871387\n",
      "..........\n",
      "[1,  4400] loss: 41.491459\n",
      "..........\n",
      "[1,  4500] loss: 40.645245\n",
      "..........\n",
      "[1,  4600] loss: 41.478392\n",
      "..........\n",
      "[1,  4700] loss: 41.164326\n",
      "..........\n",
      "[1,  4800] loss: 41.429268\n",
      "..........\n",
      "[1,  4900] loss: 41.218399\n",
      "..........\n",
      "[1,  5000] loss: 41.897702\n",
      "..........\n",
      "[1,  5100] loss: 41.194247\n",
      "..........\n",
      "[1,  5200] loss: 41.648936\n",
      "..........\n",
      "[1,  5300] loss: 42.042673\n",
      "..........\n",
      "[1,  5400] loss: 42.351149\n",
      "..........\n",
      "[1,  5500] loss: 42.057718\n",
      "..........\n",
      "[1,  5600] loss: 41.940132\n",
      "..........\n",
      "[1,  5700] loss: 43.019890\n",
      "..........\n",
      "[1,  5800] loss: 40.961847\n",
      "..........\n",
      "[1,  5900] loss: 42.478128\n",
      "..........\n",
      "[1,  6000] loss: 42.381726\n",
      "..........\n",
      "[1,  6100] loss: 42.559230\n",
      "..........\n",
      "[1,  6200] loss: 42.220904\n",
      "..........\n",
      "[1,  6300] loss: 41.964575\n",
      "..........\n",
      "[1,  6400] loss: 41.743956\n",
      "..........\n",
      "[1,  6500] loss: 41.727357\n",
      "..........\n",
      "[1,  6600] loss: 41.707305\n",
      "..........\n",
      "[1,  6700] loss: 42.745531\n",
      "..........\n",
      "[1,  6800] loss: 42.021594\n",
      "..........\n",
      "[1,  6900] loss: 42.420931\n",
      "..........\n",
      "[1,  7000] loss: 42.337859\n",
      "..........\n",
      "[1,  7100] loss: 42.187239\n",
      "..........\n",
      "[1,  7200] loss: 42.106890\n",
      "..........\n",
      "[1,  7300] loss: 42.875701\n",
      "..........\n",
      "[1,  7400] loss: 42.552288\n",
      "..........\n",
      "[1,  7500] loss: 42.912214\n",
      "..........\n",
      "[1,  7600] loss: 42.085692\n",
      "..........\n",
      "[1,  7700] loss: 41.951966\n",
      "..........\n",
      "[1,  7800] loss: 42.725237\n",
      "..........\n",
      "[1,  7900] loss: 42.179876\n",
      "..........\n",
      "[1,  8000] loss: 42.411208\n",
      "total average loss : 41.408\n",
      "train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.188\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.094\n",
      "step : 1400 / 3125 acc : 0.094\n",
      "step : 1600 / 3125 acc : 0.090\n",
      "step : 1800 / 3125 acc : 0.080\n",
      "step : 2000 / 3125 acc : 0.081\n",
      "step : 2200 / 3125 acc : 0.082\n",
      "step : 2400 / 3125 acc : 0.086\n",
      "step : 2600 / 3125 acc : 0.096\n",
      "step : 2800 / 3125 acc : 0.094\n",
      "step : 3000 / 3125 acc : 0.098\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0980 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 43.596544\n",
      "..........\n",
      "[1,   200] loss: 44.183451\n",
      "..........\n",
      "[1,   300] loss: 43.525022\n",
      "..........\n",
      "[1,   400] loss: 43.280958\n",
      "..........\n",
      "[1,   500] loss: 44.071671\n",
      "..........\n",
      "[1,   600] loss: 43.424486\n",
      "..........\n",
      "[1,   700] loss: 43.956839\n",
      "..........\n",
      "[1,   800] loss: 43.422455\n",
      "..........\n",
      "[1,   900] loss: 42.838997\n",
      "..........\n",
      "[1,  1000] loss: 43.616846\n",
      "..........\n",
      "[1,  1100] loss: 43.251377\n",
      "..........\n",
      "[1,  1200] loss: 43.356838\n",
      "..........\n",
      "[1,  1300] loss: 43.417765\n",
      "..........\n",
      "[1,  1400] loss: 44.063359\n",
      "..........\n",
      "[1,  1500] loss: 43.370758\n",
      "..........\n",
      "[1,  1600] loss: 43.136786\n",
      "..........\n",
      "[1,  1700] loss: 44.037811\n",
      "..........\n",
      "[1,  1800] loss: 43.912560\n",
      "..........\n",
      "[1,  1900] loss: 44.255239\n",
      "..........\n",
      "[1,  2000] loss: 42.943381\n",
      "..........\n",
      "[1,  2100] loss: 43.672251\n",
      "..........\n",
      "[1,  2200] loss: 43.861738\n",
      "..........\n",
      "[1,  2300] loss: 44.348973\n",
      "..........\n",
      "[1,  2400] loss: 43.909783\n",
      "..........\n",
      "[1,  2500] loss: 44.360096\n",
      "..........\n",
      "[1,  2600] loss: 44.017392\n",
      "..........\n",
      "[1,  2700] loss: 44.305880\n",
      "..........\n",
      "[1,  2800] loss: 44.005722\n",
      "..........\n",
      "[1,  2900] loss: 44.239150\n",
      "..........\n",
      "[1,  3000] loss: 44.336190\n",
      "..........\n",
      "[1,  3100] loss: 43.930185\n",
      "..........\n",
      "[1,  3200] loss: 44.185192\n",
      "..........\n",
      "[1,  3300] loss: 44.611384\n",
      "..........\n",
      "[1,  3400] loss: 44.524728\n",
      "..........\n",
      "[1,  3500] loss: 44.696618\n",
      "..........\n",
      "[1,  3600] loss: 44.277620\n",
      "..........\n",
      "[1,  3700] loss: 44.841559\n",
      "..........\n",
      "[1,  3800] loss: 44.938278\n",
      "..........\n",
      "[1,  3900] loss: 44.127989\n",
      "..........\n",
      "[1,  4000] loss: 43.804984\n",
      "..........\n",
      "[1,  4100] loss: 44.198159\n",
      "..........\n",
      "[1,  4200] loss: 44.501505\n",
      "..........\n",
      "[1,  4300] loss: 44.118104\n",
      "..........\n",
      "[1,  4400] loss: 45.114688\n",
      "..........\n",
      "[1,  4500] loss: 43.832292\n",
      "..........\n",
      "[1,  4600] loss: 44.378988\n",
      "..........\n",
      "[1,  4700] loss: 44.099469\n",
      "..........\n",
      "[1,  4800] loss: 44.946076\n",
      "..........\n",
      "[1,  4900] loss: 44.662167\n",
      "..........\n",
      "[1,  5000] loss: 44.765122\n",
      "..........\n",
      "[1,  5100] loss: 44.761287\n",
      "..........\n",
      "[1,  5200] loss: 44.466202\n",
      "..........\n",
      "[1,  5300] loss: 44.419624\n",
      "..........\n",
      "[1,  5400] loss: 44.709190\n",
      "..........\n",
      "[1,  5500] loss: 44.991976\n",
      "..........\n",
      "[1,  5600] loss: 45.680147\n",
      "..........\n",
      "[1,  5700] loss: 45.357660\n",
      "..........\n",
      "[1,  5800] loss: 45.247008\n",
      "..........\n",
      "[1,  5900] loss: 44.814886\n",
      "..........\n",
      "[1,  6000] loss: 45.505165\n",
      "..........\n",
      "[1,  6100] loss: 45.025300\n",
      "..........\n",
      "[1,  6200] loss: 45.308942\n",
      "..........\n",
      "[1,  6300] loss: 45.166872\n",
      "..........\n",
      "[1,  6400] loss: 43.927449\n",
      "..........\n",
      "[1,  6500] loss: 45.185761\n",
      "..........\n",
      "[1,  6600] loss: 45.460233\n",
      "..........\n",
      "[1,  6700] loss: 44.644530\n",
      "..........\n",
      "[1,  6800] loss: 45.706514\n",
      "..........\n",
      "[1,  6900] loss: 45.679524\n",
      "..........\n",
      "[1,  7000] loss: 44.986462\n",
      "..........\n",
      "[1,  7100] loss: 46.277404\n",
      "..........\n",
      "[1,  7200] loss: 44.644885\n",
      "..........\n",
      "[1,  7300] loss: 45.030151\n",
      "..........\n",
      "[1,  7400] loss: 45.133100\n",
      "..........\n",
      "[1,  7500] loss: 45.521629\n",
      "..........\n",
      "[1,  7600] loss: 46.726660\n",
      "..........\n",
      "[1,  7700] loss: 45.242267\n",
      "..........\n",
      "[1,  7800] loss: 47.081108\n",
      "..........\n",
      "[1,  7900] loss: 45.420452\n",
      "..........\n",
      "[1,  8000] loss: 44.815478\n",
      "total average loss : 44.478\n",
      "train acc : 0.0922\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.087\n",
      "step : 1200 / 3125 acc : 0.078\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.074\n",
      "step : 1800 / 3125 acc : 0.087\n",
      "step : 2000 / 3125 acc : 0.084\n",
      "step : 2200 / 3125 acc : 0.085\n",
      "step : 2400 / 3125 acc : 0.083\n",
      "step : 2600 / 3125 acc : 0.087\n",
      "step : 2800 / 3125 acc : 0.094\n",
      "step : 3000 / 3125 acc : 0.100\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1080 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 45.859355\n",
      "..........\n",
      "[1,   200] loss: 46.893653\n",
      "..........\n",
      "[1,   300] loss: 46.549029\n",
      "..........\n",
      "[1,   400] loss: 46.249845\n",
      "..........\n",
      "[1,   500] loss: 47.001157\n",
      "..........\n",
      "[1,   600] loss: 47.572536\n",
      "..........\n",
      "[1,   700] loss: 46.157065\n",
      "..........\n",
      "[1,   800] loss: 46.977884\n",
      "..........\n",
      "[1,   900] loss: 48.071471\n",
      "..........\n",
      "[1,  1000] loss: 47.026950\n",
      "..........\n",
      "[1,  1100] loss: 46.600500\n",
      "..........\n",
      "[1,  1200] loss: 47.240760\n",
      "..........\n",
      "[1,  1300] loss: 46.954809\n",
      "..........\n",
      "[1,  1400] loss: 46.585605\n",
      "..........\n",
      "[1,  1500] loss: 46.561116\n",
      "..........\n",
      "[1,  1600] loss: 46.374833\n",
      "..........\n",
      "[1,  1700] loss: 47.146971\n",
      "..........\n",
      "[1,  1800] loss: 46.774835\n",
      "..........\n",
      "[1,  1900] loss: 45.960005\n",
      "..........\n",
      "[1,  2000] loss: 46.066008\n",
      "..........\n",
      "[1,  2100] loss: 48.127037\n",
      "..........\n",
      "[1,  2200] loss: 47.897483\n",
      "..........\n",
      "[1,  2300] loss: 47.554254\n",
      "..........\n",
      "[1,  2400] loss: 46.781874\n",
      "..........\n",
      "[1,  2500] loss: 46.524087\n",
      "..........\n",
      "[1,  2600] loss: 48.125904\n",
      "..........\n",
      "[1,  2700] loss: 46.836646\n",
      "..........\n",
      "[1,  2800] loss: 47.150272\n",
      "..........\n",
      "[1,  2900] loss: 46.822134\n",
      "..........\n",
      "[1,  3000] loss: 47.835646\n",
      "..........\n",
      "[1,  3100] loss: 47.773508\n",
      "..........\n",
      "[1,  3200] loss: 48.409497\n",
      "..........\n",
      "[1,  3300] loss: 47.627140\n",
      "..........\n",
      "[1,  3400] loss: 47.904834\n",
      "..........\n",
      "[1,  3500] loss: 48.143403\n",
      "..........\n",
      "[1,  3600] loss: 48.085417\n",
      "..........\n",
      "[1,  3700] loss: 47.972823\n",
      "..........\n",
      "[1,  3800] loss: 46.881970\n",
      "..........\n",
      "[1,  3900] loss: 47.935605\n",
      "..........\n",
      "[1,  4000] loss: 48.229080\n",
      "..........\n",
      "[1,  4100] loss: 47.719385\n",
      "..........\n",
      "[1,  4200] loss: 48.373264\n",
      "..........\n",
      "[1,  4300] loss: 47.587770\n",
      "..........\n",
      "[1,  4400] loss: 47.424903\n",
      "..........\n",
      "[1,  4500] loss: 47.181964\n",
      "..........\n",
      "[1,  4600] loss: 48.165571\n",
      "..........\n",
      "[1,  4700] loss: 47.296425\n",
      "..........\n",
      "[1,  4800] loss: 47.971710\n",
      "..........\n",
      "[1,  4900] loss: 47.223941\n",
      "..........\n",
      "[1,  5000] loss: 48.201958\n",
      "..........\n",
      "[1,  5100] loss: 48.278540\n",
      "..........\n",
      "[1,  5200] loss: 48.413004\n",
      "..........\n",
      "[1,  5300] loss: 48.176804\n",
      "..........\n",
      "[1,  5400] loss: 48.220342\n",
      "..........\n",
      "[1,  5500] loss: 47.244059\n",
      "..........\n",
      "[1,  5600] loss: 49.555091\n",
      "..........\n",
      "[1,  5700] loss: 49.304558\n",
      "..........\n",
      "[1,  5800] loss: 48.271833\n",
      "..........\n",
      "[1,  5900] loss: 48.394148\n",
      "..........\n",
      "[1,  6000] loss: 48.343357\n",
      "..........\n",
      "[1,  6100] loss: 47.547923\n",
      "..........\n",
      "[1,  6200] loss: 48.721240\n",
      "..........\n",
      "[1,  6300] loss: 46.742360\n",
      "..........\n",
      "[1,  6400] loss: 48.021772\n",
      "..........\n",
      "[1,  6500] loss: 48.281951\n",
      "..........\n",
      "[1,  6600] loss: 48.718847\n",
      "..........\n",
      "[1,  6700] loss: 48.802143\n",
      "..........\n",
      "[1,  6800] loss: 48.390268\n",
      "..........\n",
      "[1,  6900] loss: 48.914395\n",
      "..........\n",
      "[1,  7000] loss: 48.556055\n",
      "..........\n",
      "[1,  7100] loss: 48.993182\n",
      "..........\n",
      "[1,  7200] loss: 49.160092\n",
      "..........\n",
      "[1,  7300] loss: 49.004478\n",
      "..........\n",
      "[1,  7400] loss: 47.427650\n",
      "..........\n",
      "[1,  7500] loss: 47.668602\n",
      "..........\n",
      "[1,  7600] loss: 49.396147\n",
      "..........\n",
      "[1,  7700] loss: 48.484085\n",
      "..........\n",
      "[1,  7800] loss: 47.595837\n",
      "..........\n",
      "[1,  7900] loss: 49.129546\n",
      "..........\n",
      "[1,  8000] loss: 49.135436\n",
      "total average loss : 47.716\n",
      "train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.094\n",
      "step : 1600 / 3125 acc : 0.094\n",
      "step : 1800 / 3125 acc : 0.094\n",
      "step : 2000 / 3125 acc : 0.091\n",
      "step : 2200 / 3125 acc : 0.091\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.099\n",
      "step : 2800 / 3125 acc : 0.094\n",
      "step : 3000 / 3125 acc : 0.102\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "======= epoch  4 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 49.985147\n",
      "..........\n",
      "[1,   200] loss: 50.349045\n",
      "..........\n",
      "[1,   300] loss: 50.252754\n",
      "..........\n",
      "[1,   400] loss: 49.742184\n",
      "..........\n",
      "[1,   500] loss: 51.172620\n",
      "..........\n",
      "[1,   600] loss: 50.932761\n",
      "..........\n",
      "[1,   700] loss: 49.876842\n",
      "..........\n",
      "[1,   800] loss: 49.617561\n",
      "..........\n",
      "[1,   900] loss: 50.198949\n",
      "..........\n",
      "[1,  1000] loss: 50.429716\n",
      "..........\n",
      "[1,  1100] loss: 50.095233\n",
      "..........\n",
      "[1,  1200] loss: 50.613632\n",
      "..........\n",
      "[1,  1300] loss: 50.224812\n",
      "..........\n",
      "[1,  1400] loss: 49.349278\n",
      "..........\n",
      "[1,  1500] loss: 51.694783\n",
      "..........\n",
      "[1,  1600] loss: 50.182998\n",
      "..........\n",
      "[1,  1700] loss: 50.538433\n",
      "..........\n",
      "[1,  1800] loss: 51.210792\n",
      "..........\n",
      "[1,  1900] loss: 50.504243\n",
      "..........\n",
      "[1,  2000] loss: 50.450044\n",
      "..........\n",
      "[1,  2100] loss: 51.520518\n",
      "..........\n",
      "[1,  2200] loss: 50.811970\n",
      "..........\n",
      "[1,  2300] loss: 50.304664\n",
      "..........\n",
      "[1,  2400] loss: 50.579378\n",
      "..........\n",
      "[1,  2500] loss: 50.546665\n",
      "..........\n",
      "[1,  2600] loss: 50.141663\n",
      "..........\n",
      "[1,  2700] loss: 49.819359\n",
      "..........\n",
      "[1,  2800] loss: 50.492823\n",
      "..........\n",
      "[1,  2900] loss: 49.466067\n",
      "..........\n",
      "[1,  3000] loss: 49.970844\n",
      "..........\n",
      "[1,  3100] loss: 50.975542\n",
      "..........\n",
      "[1,  3200] loss: 50.276589\n",
      "..........\n",
      "[1,  3300] loss: 50.913273\n",
      "..........\n",
      "[1,  3400] loss: 50.368375\n",
      "..........\n",
      "[1,  3500] loss: 51.289353\n",
      "..........\n",
      "[1,  3600] loss: 50.968976\n",
      "..........\n",
      "[1,  3700] loss: 50.351600\n",
      "..........\n",
      "[1,  3800] loss: 51.317113\n",
      "..........\n",
      "[1,  3900] loss: 51.450392\n",
      "..........\n",
      "[1,  4000] loss: 50.111930\n",
      "..........\n",
      "[1,  4100] loss: 50.802634\n",
      "..........\n",
      "[1,  4200] loss: 50.726970\n",
      "..........\n",
      "[1,  4300] loss: 51.019911\n",
      "..........\n",
      "[1,  4400] loss: 51.497770\n",
      "..........\n",
      "[1,  4500] loss: 51.062057\n",
      "..........\n",
      "[1,  4600] loss: 50.342644\n",
      "..........\n",
      "[1,  4700] loss: 50.854460\n",
      "..........\n",
      "[1,  4800] loss: 50.113022\n",
      "..........\n",
      "[1,  4900] loss: 51.662902\n",
      "..........\n",
      "[1,  5000] loss: 52.147285\n",
      "..........\n",
      "[1,  5100] loss: 50.651726\n",
      "..........\n",
      "[1,  5200] loss: 50.234831\n",
      "..........\n",
      "[1,  5300] loss: 51.252428\n",
      "..........\n",
      "[1,  5400] loss: 50.631531\n",
      "..........\n",
      "[1,  5500] loss: 50.614416\n",
      "..........\n",
      "[1,  5600] loss: 51.138997\n",
      "..........\n",
      "[1,  5700] loss: 50.894547\n",
      "..........\n",
      "[1,  5800] loss: 52.358012\n",
      "..........\n",
      "[1,  5900] loss: 51.104540\n",
      "..........\n",
      "[1,  6000] loss: 51.814351\n",
      "..........\n",
      "[1,  6100] loss: 51.603246\n",
      "..........\n",
      "[1,  6200] loss: 52.239904\n",
      "..........\n",
      "[1,  6300] loss: 50.430859\n",
      "..........\n",
      "[1,  6400] loss: 51.690549\n",
      "..........\n",
      "[1,  6500] loss: 51.878984\n",
      "..........\n",
      "[1,  6600] loss: 51.150638\n",
      "..........\n",
      "[1,  6700] loss: 52.231968\n",
      "..........\n",
      "[1,  6800] loss: 51.365262\n",
      "..........\n",
      "[1,  6900] loss: 52.775299\n",
      "..........\n",
      "[1,  7000] loss: 51.504789\n",
      "..........\n",
      "[1,  7100] loss: 51.967472\n",
      "..........\n",
      "[1,  7200] loss: 51.720885\n",
      "..........\n",
      "[1,  7300] loss: 51.105281\n",
      "..........\n",
      "[1,  7400] loss: 51.856487\n",
      "..........\n",
      "[1,  7500] loss: 51.906413\n",
      "..........\n",
      "[1,  7600] loss: 52.774610\n",
      "..........\n",
      "[1,  7700] loss: 53.082798\n",
      "..........\n",
      "[1,  7800] loss: 51.594629\n",
      "..........\n",
      "[1,  7900] loss: 51.682389\n",
      "..........\n",
      "[1,  8000] loss: 52.251015\n",
      "total average loss : 50.960\n",
      "train acc : 0.0875\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.000\n",
      "step : 400 / 3125 acc : 0.047\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.105\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.103\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.102\n",
      "step : 2600 / 3125 acc : 0.106\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.108\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1040 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 53.982216\n",
      "..........\n",
      "[1,   200] loss: 53.317631\n",
      "..........\n",
      "[1,   300] loss: 52.505867\n",
      "..........\n",
      "[1,   400] loss: 53.249263\n",
      "..........\n",
      "[1,   500] loss: 53.433255\n",
      "..........\n",
      "[1,   600] loss: 53.711886\n",
      "..........\n",
      "[1,   700] loss: 53.130108\n",
      "..........\n",
      "[1,   800] loss: 53.439334\n",
      "..........\n",
      "[1,   900] loss: 53.367854\n",
      "..........\n",
      "[1,  1000] loss: 53.501322\n",
      "..........\n",
      "[1,  1100] loss: 53.449999\n",
      "..........\n",
      "[1,  1200] loss: 53.516883\n",
      "..........\n",
      "[1,  1300] loss: 54.341494\n",
      "..........\n",
      "[1,  1400] loss: 53.872996\n",
      "..........\n",
      "[1,  1500] loss: 53.523671\n",
      "..........\n",
      "[1,  1600] loss: 52.044351\n",
      "..........\n",
      "[1,  1700] loss: 53.748016\n",
      "..........\n",
      "[1,  1800] loss: 54.278949\n",
      "..........\n",
      "[1,  1900] loss: 54.134287\n",
      "..........\n",
      "[1,  2000] loss: 53.289619\n",
      "..........\n",
      "[1,  2100] loss: 53.446394\n",
      "..........\n",
      "[1,  2200] loss: 53.921895\n",
      "..........\n",
      "[1,  2300] loss: 53.718735\n",
      "..........\n",
      "[1,  2400] loss: 53.958182\n",
      "..........\n",
      "[1,  2500] loss: 53.885316\n",
      "..........\n",
      "[1,  2600] loss: 53.697565\n",
      "..........\n",
      "[1,  2700] loss: 53.635863\n",
      "..........\n",
      "[1,  2800] loss: 54.879696\n",
      "..........\n",
      "[1,  2900] loss: 54.400976\n",
      "..........\n",
      "[1,  3000] loss: 53.960674\n",
      "..........\n",
      "[1,  3100] loss: 53.190104\n",
      "..........\n",
      "[1,  3200] loss: 53.733367\n",
      "..........\n",
      "[1,  3300] loss: 55.321953\n",
      "..........\n",
      "[1,  3400] loss: 54.308044\n",
      "..........\n",
      "[1,  3500] loss: 54.148971\n",
      "..........\n",
      "[1,  3600] loss: 54.186877\n",
      "..........\n",
      "[1,  3700] loss: 54.200916\n",
      "..........\n",
      "[1,  3800] loss: 53.636900\n",
      "..........\n",
      "[1,  3900] loss: 54.018742\n",
      "..........\n",
      "[1,  4000] loss: 54.472329\n",
      "..........\n",
      "[1,  4100] loss: 55.252056\n",
      "..........\n",
      "[1,  4200] loss: 53.397345\n",
      "..........\n",
      "[1,  4300] loss: 53.634792\n",
      "..........\n",
      "[1,  4400] loss: 55.135509\n",
      "..........\n",
      "[1,  4500] loss: 54.377519\n",
      "..........\n",
      "[1,  4600] loss: 54.200175\n",
      "..........\n",
      "[1,  4700] loss: 55.298975\n",
      "..........\n",
      "[1,  4800] loss: 54.664644\n",
      "..........\n",
      "[1,  4900] loss: 55.023118\n",
      "..........\n",
      "[1,  5000] loss: 53.790953\n",
      "..........\n",
      "[1,  5100] loss: 55.518275\n",
      "..........\n",
      "[1,  5200] loss: 54.790693\n",
      "..........\n",
      "[1,  5300] loss: 53.860193\n",
      "..........\n",
      "[1,  5400] loss: 54.767013\n",
      "..........\n",
      "[1,  5500] loss: 56.019879\n",
      "..........\n",
      "[1,  5600] loss: 55.305561\n",
      "..........\n",
      "[1,  5700] loss: 54.412019\n",
      "..........\n",
      "[1,  5800] loss: 55.257523\n",
      "..........\n",
      "[1,  5900] loss: 55.525175\n",
      "..........\n",
      "[1,  6000] loss: 55.339035\n",
      "..........\n",
      "[1,  6100] loss: 54.217893\n",
      "..........\n",
      "[1,  6200] loss: 56.128873\n",
      "..........\n",
      "[1,  6300] loss: 55.153781\n",
      "..........\n",
      "[1,  6400] loss: 54.410828\n",
      "..........\n",
      "[1,  6500] loss: 54.264429\n",
      "..........\n",
      "[1,  6600] loss: 55.027871\n",
      "..........\n",
      "[1,  6700] loss: 55.819771\n",
      "..........\n",
      "[1,  6800] loss: 55.076665\n",
      "..........\n",
      "[1,  6900] loss: 54.799874\n",
      "..........\n",
      "[1,  7000] loss: 53.942619\n",
      "..........\n",
      "[1,  7100] loss: 54.246354\n",
      "..........\n",
      "[1,  7200] loss: 56.279786\n",
      "..........\n",
      "[1,  7300] loss: 56.161947\n",
      "..........\n",
      "[1,  7400] loss: 55.462434\n",
      "..........\n",
      "[1,  7500] loss: 54.510761\n",
      "..........\n",
      "[1,  7600] loss: 55.932494\n",
      "..........\n",
      "[1,  7700] loss: 55.091261\n",
      "..........\n",
      "[1,  7800] loss: 55.868490\n",
      "..........\n",
      "[1,  7900] loss: 55.376334\n",
      "..........\n",
      "[1,  8000] loss: 55.363566\n",
      "total average loss : 54.366\n",
      "train acc : 0.0844\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.089\n",
      "step : 1400 / 3125 acc : 0.085\n",
      "step : 1600 / 3125 acc : 0.094\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.113\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.125\n",
      "step : 2800 / 3125 acc : 0.118\n",
      "step : 3000 / 3125 acc : 0.115\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 57.855683\n",
      "..........\n",
      "[1,   200] loss: 57.296056\n",
      "..........\n",
      "[1,   300] loss: 57.420976\n",
      "..........\n",
      "[1,   400] loss: 56.205940\n",
      "..........\n",
      "[1,   500] loss: 55.762392\n",
      "..........\n",
      "[1,   600] loss: 57.403753\n",
      "..........\n",
      "[1,   700] loss: 56.452332\n",
      "..........\n",
      "[1,   800] loss: 56.261884\n",
      "..........\n",
      "[1,   900] loss: 56.242697\n",
      "..........\n",
      "[1,  1000] loss: 57.294689\n",
      "..........\n",
      "[1,  1100] loss: 57.889544\n",
      "..........\n",
      "[1,  1200] loss: 56.601467\n",
      "..........\n",
      "[1,  1300] loss: 55.879047\n",
      "..........\n",
      "[1,  1400] loss: 57.684138\n",
      "..........\n",
      "[1,  1500] loss: 55.902884\n",
      "..........\n",
      "[1,  1600] loss: 57.625375\n",
      "..........\n",
      "[1,  1700] loss: 58.508178\n",
      "..........\n",
      "[1,  1800] loss: 56.967965\n",
      "..........\n",
      "[1,  1900] loss: 56.584531\n",
      "..........\n",
      "[1,  2000] loss: 57.161590\n",
      "..........\n",
      "[1,  2100] loss: 57.664130\n",
      "..........\n",
      "[1,  2200] loss: 58.483973\n",
      "..........\n",
      "[1,  2300] loss: 57.223781\n",
      "..........\n",
      "[1,  2400] loss: 57.277397\n",
      "..........\n",
      "[1,  2500] loss: 57.842390\n",
      "..........\n",
      "[1,  2600] loss: 57.128581\n",
      "..........\n",
      "[1,  2700] loss: 57.689429\n",
      "..........\n",
      "[1,  2800] loss: 57.389220\n",
      "..........\n",
      "[1,  2900] loss: 57.049314\n",
      "..........\n",
      "[1,  3000] loss: 55.583513\n",
      "..........\n",
      "[1,  3100] loss: 56.280311\n",
      "..........\n",
      "[1,  3200] loss: 58.257505\n",
      "..........\n",
      "[1,  3300] loss: 57.346319\n",
      "..........\n",
      "[1,  3400] loss: 57.899284\n",
      "..........\n",
      "[1,  3500] loss: 58.273380\n",
      "..........\n",
      "[1,  3600] loss: 57.315789\n",
      "..........\n",
      "[1,  3700] loss: 58.549324\n",
      "..........\n",
      "[1,  3800] loss: 57.307967\n",
      "..........\n",
      "[1,  3900] loss: 59.188916\n",
      "..........\n",
      "[1,  4000] loss: 57.181097\n",
      "..........\n",
      "[1,  4100] loss: 57.645520\n",
      "..........\n",
      "[1,  4200] loss: 58.204907\n",
      "..........\n",
      "[1,  4300] loss: 58.692158\n",
      "..........\n",
      "[1,  4400] loss: 58.183117\n",
      "..........\n",
      "[1,  4500] loss: 57.610214\n",
      "..........\n",
      "[1,  4600] loss: 57.339906\n",
      "..........\n",
      "[1,  4700] loss: 58.220199\n",
      "..........\n",
      "[1,  4800] loss: 57.994677\n",
      "..........\n",
      "[1,  4900] loss: 57.314376\n",
      "..........\n",
      "[1,  5000] loss: 58.102744\n",
      "..........\n",
      "[1,  5100] loss: 57.882092\n",
      "..........\n",
      "[1,  5200] loss: 57.280745\n",
      "..........\n",
      "[1,  5300] loss: 57.911405\n",
      "..........\n",
      "[1,  5400] loss: 57.389116\n",
      "..........\n",
      "[1,  5500] loss: 58.590577\n",
      "..........\n",
      "[1,  5600] loss: 59.484806\n",
      "..........\n",
      "[1,  5700] loss: 58.951770\n",
      "..........\n",
      "[1,  5800] loss: 58.260492\n",
      "..........\n",
      "[1,  5900] loss: 58.908719\n",
      "..........\n",
      "[1,  6000] loss: 58.466261\n",
      "..........\n",
      "[1,  6100] loss: 58.659740\n",
      "..........\n",
      "[1,  6200] loss: 58.518581\n",
      "..........\n",
      "[1,  6300] loss: 58.224012\n",
      "..........\n",
      "[1,  6400] loss: 59.551731\n",
      "..........\n",
      "[1,  6500] loss: 58.690484\n",
      "..........\n",
      "[1,  6600] loss: 59.010395\n",
      "..........\n",
      "[1,  6700] loss: 57.806752\n",
      "..........\n",
      "[1,  6800] loss: 57.320186\n",
      "..........\n",
      "[1,  6900] loss: 58.324360\n",
      "..........\n",
      "[1,  7000] loss: 59.248089\n",
      "..........\n",
      "[1,  7100] loss: 60.080342\n",
      "..........\n",
      "[1,  7200] loss: 59.021490\n",
      "..........\n",
      "[1,  7300] loss: 59.141550\n",
      "..........\n",
      "[1,  7400] loss: 58.078397\n",
      "..........\n",
      "[1,  7500] loss: 58.922874\n",
      "..........\n",
      "[1,  7600] loss: 59.750011\n",
      "..........\n",
      "[1,  7700] loss: 59.647136\n",
      "..........\n",
      "[1,  7800] loss: 59.444552\n",
      "..........\n",
      "[1,  7900] loss: 58.563931\n",
      "..........\n",
      "[1,  8000] loss: 59.896196\n",
      "total average loss : 57.878\n",
      "train acc : 0.0852\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.000\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.078\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.089\n",
      "step : 1400 / 3125 acc : 0.094\n",
      "step : 1600 / 3125 acc : 0.090\n",
      "step : 1800 / 3125 acc : 0.083\n",
      "step : 2000 / 3125 acc : 0.084\n",
      "step : 2200 / 3125 acc : 0.088\n",
      "step : 2400 / 3125 acc : 0.096\n",
      "step : 2600 / 3125 acc : 0.106\n",
      "step : 2800 / 3125 acc : 0.107\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1100 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 59.791441\n",
      "..........\n",
      "[1,   200] loss: 60.302084\n",
      "..........\n",
      "[1,   300] loss: 60.500249\n",
      "..........\n",
      "[1,   400] loss: 59.818223\n",
      "..........\n",
      "[1,   500] loss: 60.519282\n",
      "..........\n",
      "[1,   600] loss: 60.638875\n",
      "..........\n",
      "[1,   700] loss: 59.692455\n",
      "..........\n",
      "[1,   800] loss: 61.497172\n",
      "..........\n",
      "[1,   900] loss: 59.837038\n",
      "..........\n",
      "[1,  1000] loss: 60.959573\n",
      "..........\n",
      "[1,  1100] loss: 60.698274\n",
      "..........\n",
      "[1,  1200] loss: 60.419609\n",
      "..........\n",
      "[1,  1300] loss: 61.084358\n",
      "..........\n",
      "[1,  1400] loss: 60.007569\n",
      "..........\n",
      "[1,  1500] loss: 61.859381\n",
      "..........\n",
      "[1,  1600] loss: 60.360107\n",
      "..........\n",
      "[1,  1700] loss: 60.853010\n",
      "..........\n",
      "[1,  1800] loss: 60.955169\n",
      "..........\n",
      "[1,  1900] loss: 60.591431\n",
      "..........\n",
      "[1,  2000] loss: 61.790026\n",
      "..........\n",
      "[1,  2100] loss: 61.270367\n",
      "..........\n",
      "[1,  2200] loss: 59.817298\n",
      "..........\n",
      "[1,  2300] loss: 60.631083\n",
      "..........\n",
      "[1,  2400] loss: 61.617627\n",
      "..........\n",
      "[1,  2500] loss: 62.000357\n",
      "..........\n",
      "[1,  2600] loss: 61.484269\n",
      "..........\n",
      "[1,  2700] loss: 60.743313\n",
      "..........\n",
      "[1,  2800] loss: 60.891909\n",
      "..........\n",
      "[1,  2900] loss: 60.503630\n",
      "..........\n",
      "[1,  3000] loss: 61.968239\n",
      "..........\n",
      "[1,  3100] loss: 59.784041\n",
      "..........\n",
      "[1,  3200] loss: 62.535927\n",
      "..........\n",
      "[1,  3300] loss: 62.059589\n",
      "..........\n",
      "[1,  3400] loss: 60.955335\n",
      "..........\n",
      "[1,  3500] loss: 62.143345\n",
      "..........\n",
      "[1,  3600] loss: 60.944446\n",
      "..........\n",
      "[1,  3700] loss: 60.609741\n",
      "..........\n",
      "[1,  3800] loss: 61.376533\n",
      "..........\n",
      "[1,  3900] loss: 61.790968\n",
      "..........\n",
      "[1,  4000] loss: 62.305089\n",
      "..........\n",
      "[1,  4100] loss: 61.736667\n",
      "..........\n",
      "[1,  4200] loss: 62.082780\n",
      "..........\n",
      "[1,  4300] loss: 63.030584\n",
      "..........\n",
      "[1,  4400] loss: 62.159622\n",
      "..........\n",
      "[1,  4500] loss: 61.145527\n",
      "..........\n",
      "[1,  4600] loss: 61.893907\n",
      "..........\n",
      "[1,  4700] loss: 62.355475\n",
      "..........\n",
      "[1,  4800] loss: 62.771146\n",
      "..........\n",
      "[1,  4900] loss: 62.655087\n",
      "..........\n",
      "[1,  5000] loss: 62.165249\n",
      "..........\n",
      "[1,  5100] loss: 62.385075\n",
      "..........\n",
      "[1,  5200] loss: 60.887306\n",
      "..........\n",
      "[1,  5300] loss: 62.336634\n",
      "..........\n",
      "[1,  5400] loss: 62.944726\n",
      "..........\n",
      "[1,  5500] loss: 64.003178\n",
      "..........\n",
      "[1,  5600] loss: 61.260515\n",
      "..........\n",
      "[1,  5700] loss: 62.858785\n",
      "..........\n",
      "[1,  5800] loss: 61.893860\n",
      "..........\n",
      "[1,  5900] loss: 62.665896\n",
      "..........\n",
      "[1,  6000] loss: 62.119389\n",
      "..........\n",
      "[1,  6100] loss: 62.034244\n",
      "..........\n",
      "[1,  6200] loss: 61.326595\n",
      "..........\n",
      "[1,  6300] loss: 62.467207\n",
      "..........\n",
      "[1,  6400] loss: 62.379562\n",
      "..........\n",
      "[1,  6500] loss: 62.222383\n",
      "..........\n",
      "[1,  6600] loss: 62.873636\n",
      "..........\n",
      "[1,  6700] loss: 61.897882\n",
      "..........\n",
      "[1,  6800] loss: 60.724936\n",
      "..........\n",
      "[1,  6900] loss: 61.885095\n",
      "..........\n",
      "[1,  7000] loss: 61.036456\n",
      "..........\n",
      "[1,  7100] loss: 62.193285\n",
      "..........\n",
      "[1,  7200] loss: 62.022067\n",
      "..........\n",
      "[1,  7300] loss: 63.530467\n",
      "..........\n",
      "[1,  7400] loss: 61.543055\n",
      "..........\n",
      "[1,  7500] loss: 62.828565\n",
      "..........\n",
      "[1,  7600] loss: 63.287091\n",
      "..........\n",
      "[1,  7700] loss: 63.579154\n",
      "..........\n",
      "[1,  7800] loss: 62.876410\n",
      "..........\n",
      "[1,  7900] loss: 62.297506\n",
      "..........\n",
      "[1,  8000] loss: 63.797736\n",
      "total average loss : 61.622\n",
      "train acc : 0.0852\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.122\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.108\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.118\n",
      "step : 3000 / 3125 acc : 0.117\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1180 %\n",
      "======eval  end ======\n",
      "======= epoch  5 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 63.283841\n",
      "..........\n",
      "[1,   200] loss: 63.440015\n",
      "..........\n",
      "[1,   300] loss: 64.292680\n",
      "..........\n",
      "[1,   400] loss: 63.977266\n",
      "..........\n",
      "[1,   500] loss: 63.366255\n",
      "..........\n",
      "[1,   600] loss: 63.746746\n",
      "..........\n",
      "[1,   700] loss: 64.249815\n",
      "..........\n",
      "[1,   800] loss: 62.304406\n",
      "..........\n",
      "[1,   900] loss: 64.172457\n",
      "..........\n",
      "[1,  1000] loss: 63.705595\n",
      "..........\n",
      "[1,  1100] loss: 65.195600\n",
      "..........\n",
      "[1,  1200] loss: 64.212758\n",
      "..........\n",
      "[1,  1300] loss: 64.272518\n",
      "..........\n",
      "[1,  1400] loss: 63.909750\n",
      "..........\n",
      "[1,  1500] loss: 65.665638\n",
      "..........\n",
      "[1,  1600] loss: 65.211692\n",
      "..........\n",
      "[1,  1700] loss: 65.252854\n",
      "..........\n",
      "[1,  1800] loss: 64.288374\n",
      "..........\n",
      "[1,  1900] loss: 65.044328\n",
      "..........\n",
      "[1,  2000] loss: 63.392258\n",
      "..........\n",
      "[1,  2100] loss: 64.761689\n",
      "..........\n",
      "[1,  2200] loss: 63.698125\n",
      "..........\n",
      "[1,  2300] loss: 64.438142\n",
      "..........\n",
      "[1,  2400] loss: 63.515349\n",
      "..........\n",
      "[1,  2500] loss: 65.647479\n",
      "..........\n",
      "[1,  2600] loss: 65.414853\n",
      "..........\n",
      "[1,  2700] loss: 64.824362\n",
      "..........\n",
      "[1,  2800] loss: 64.982422\n",
      "..........\n",
      "[1,  2900] loss: 65.380334\n",
      "..........\n",
      "[1,  3000] loss: 64.529545\n",
      "..........\n",
      "[1,  3100] loss: 66.081423\n",
      "..........\n",
      "[1,  3200] loss: 66.255612\n",
      "..........\n",
      "[1,  3300] loss: 64.928077\n",
      "..........\n",
      "[1,  3400] loss: 64.614491\n",
      "..........\n",
      "[1,  3500] loss: 64.199598\n",
      "..........\n",
      "[1,  3600] loss: 64.874938\n",
      "..........\n",
      "[1,  3700] loss: 65.138244\n",
      "..........\n",
      "[1,  3800] loss: 64.430749\n",
      "..........\n",
      "[1,  3900] loss: 65.287412\n",
      "..........\n",
      "[1,  4000] loss: 64.752697\n",
      "..........\n",
      "[1,  4100] loss: 65.473824\n",
      "..........\n",
      "[1,  4200] loss: 64.981417\n",
      "..........\n",
      "[1,  4300] loss: 65.302677\n",
      "..........\n",
      "[1,  4400] loss: 66.338034\n",
      "..........\n",
      "[1,  4500] loss: 64.933867\n",
      "..........\n",
      "[1,  4600] loss: 64.797225\n",
      "..........\n",
      "[1,  4700] loss: 64.379977\n",
      "..........\n",
      "[1,  4800] loss: 65.604231\n",
      "..........\n",
      "[1,  4900] loss: 65.884952\n",
      "..........\n",
      "[1,  5000] loss: 66.141375\n",
      "..........\n",
      "[1,  5100] loss: 66.700279\n",
      "..........\n",
      "[1,  5200] loss: 66.600257\n",
      "..........\n",
      "[1,  5300] loss: 66.445059\n",
      "..........\n",
      "[1,  5400] loss: 66.048202\n",
      "..........\n",
      "[1,  5500] loss: 66.063564\n",
      "..........\n",
      "[1,  5600] loss: 65.678111\n",
      "..........\n",
      "[1,  5700] loss: 65.840768\n",
      "..........\n",
      "[1,  5800] loss: 65.905655\n",
      "..........\n",
      "[1,  5900] loss: 66.783651\n",
      "..........\n",
      "[1,  6000] loss: 66.693375\n",
      "..........\n",
      "[1,  6100] loss: 66.542843\n",
      "..........\n",
      "[1,  6200] loss: 65.519672\n",
      "..........\n",
      "[1,  6300] loss: 66.139998\n",
      "..........\n",
      "[1,  6400] loss: 67.157644\n",
      "..........\n",
      "[1,  6500] loss: 66.024076\n",
      "..........\n",
      "[1,  6600] loss: 65.906873\n",
      "..........\n",
      "[1,  6700] loss: 65.256540\n",
      "..........\n",
      "[1,  6800] loss: 65.804860\n",
      "..........\n",
      "[1,  6900] loss: 66.029701\n",
      "..........\n",
      "[1,  7000] loss: 66.501431\n",
      "..........\n",
      "[1,  7100] loss: 66.493090\n",
      "..........\n",
      "[1,  7200] loss: 65.753200\n",
      "..........\n",
      "[1,  7300] loss: 65.075783\n",
      "..........\n",
      "[1,  7400] loss: 66.692348\n",
      "..........\n",
      "[1,  7500] loss: 65.521439\n",
      "..........\n",
      "[1,  7600] loss: 67.407329\n",
      "..........\n",
      "[1,  7700] loss: 65.957364\n",
      "..........\n",
      "[1,  7800] loss: 68.206120\n",
      "..........\n",
      "[1,  7900] loss: 66.102740\n",
      "..........\n",
      "[1,  8000] loss: 65.901248\n",
      "total average loss : 65.267\n",
      "train acc : 0.0852\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.141\n",
      "step : 1000 / 3125 acc : 0.138\n",
      "step : 1200 / 3125 acc : 0.130\n",
      "step : 1400 / 3125 acc : 0.129\n",
      "step : 1600 / 3125 acc : 0.121\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.109\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.117\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.118\n",
      "step : 3000 / 3125 acc : 0.119\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1200 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 67.581148\n",
      "..........\n",
      "[1,   200] loss: 66.230064\n",
      "..........\n",
      "[1,   300] loss: 68.937700\n",
      "..........\n",
      "[1,   400] loss: 66.862960\n",
      "..........\n",
      "[1,   500] loss: 67.011665\n",
      "..........\n",
      "[1,   600] loss: 67.905165\n",
      "..........\n",
      "[1,   700] loss: 67.892334\n",
      "..........\n",
      "[1,   800] loss: 67.072143\n",
      "..........\n",
      "[1,   900] loss: 66.750507\n",
      "..........\n",
      "[1,  1000] loss: 67.484764\n",
      "..........\n",
      "[1,  1100] loss: 66.636312\n",
      "..........\n",
      "[1,  1200] loss: 69.083374\n",
      "..........\n",
      "[1,  1300] loss: 68.703754\n",
      "..........\n",
      "[1,  1400] loss: 68.171184\n",
      "..........\n",
      "[1,  1500] loss: 66.599288\n",
      "..........\n",
      "[1,  1600] loss: 68.958717\n",
      "..........\n",
      "[1,  1700] loss: 67.212955\n",
      "..........\n",
      "[1,  1800] loss: 67.228442\n",
      "..........\n",
      "[1,  1900] loss: 67.913277\n",
      "..........\n",
      "[1,  2000] loss: 68.160737\n",
      "..........\n",
      "[1,  2100] loss: 67.697393\n",
      "..........\n",
      "[1,  2200] loss: 67.628187\n",
      "..........\n",
      "[1,  2300] loss: 69.033430\n",
      "..........\n",
      "[1,  2400] loss: 68.501894\n",
      "..........\n",
      "[1,  2500] loss: 68.506460\n",
      "..........\n",
      "[1,  2600] loss: 68.849606\n",
      "..........\n",
      "[1,  2700] loss: 68.050314\n",
      "..........\n",
      "[1,  2800] loss: 67.619320\n",
      "..........\n",
      "[1,  2900] loss: 67.781403\n",
      "..........\n",
      "[1,  3000] loss: 67.166579\n",
      "..........\n",
      "[1,  3100] loss: 68.122404\n",
      "..........\n",
      "[1,  3200] loss: 68.318708\n",
      "..........\n",
      "[1,  3300] loss: 68.725071\n",
      "..........\n",
      "[1,  3400] loss: 69.204236\n",
      "..........\n",
      "[1,  3500] loss: 68.351190\n",
      "..........\n",
      "[1,  3600] loss: 68.839905\n",
      "..........\n",
      "[1,  3700] loss: 68.472248\n",
      "..........\n",
      "[1,  3800] loss: 67.973097\n",
      "..........\n",
      "[1,  3900] loss: 68.884499\n",
      "..........\n",
      "[1,  4000] loss: 67.669029\n",
      "..........\n",
      "[1,  4100] loss: 67.660266\n",
      "..........\n",
      "[1,  4200] loss: 70.104024\n",
      "..........\n",
      "[1,  4300] loss: 68.728389\n",
      "..........\n",
      "[1,  4400] loss: 68.211087\n",
      "..........\n",
      "[1,  4500] loss: 67.972347\n",
      "..........\n",
      "[1,  4600] loss: 68.172919\n",
      "..........\n",
      "[1,  4700] loss: 68.558745\n",
      "..........\n",
      "[1,  4800] loss: 68.563322\n",
      "..........\n",
      "[1,  4900] loss: 68.984939\n",
      "..........\n",
      "[1,  5000] loss: 69.985145\n",
      "..........\n",
      "[1,  5100] loss: 69.033765\n",
      "..........\n",
      "[1,  5200] loss: 68.633202\n",
      "..........\n",
      "[1,  5300] loss: 69.825406\n",
      "..........\n",
      "[1,  5400] loss: 69.027957\n",
      "..........\n",
      "[1,  5500] loss: 69.044670\n",
      "..........\n",
      "[1,  5600] loss: 69.367676\n",
      "..........\n",
      "[1,  5700] loss: 69.569864\n",
      "..........\n",
      "[1,  5800] loss: 69.936823\n",
      "..........\n",
      "[1,  5900] loss: 68.249874\n",
      "..........\n",
      "[1,  6000] loss: 69.955612\n",
      "..........\n",
      "[1,  6100] loss: 67.997983\n",
      "..........\n",
      "[1,  6200] loss: 69.084303\n",
      "..........\n",
      "[1,  6300] loss: 70.513529\n",
      "..........\n",
      "[1,  6400] loss: 70.057471\n",
      "..........\n",
      "[1,  6500] loss: 67.277303\n",
      "..........\n",
      "[1,  6600] loss: 70.321883\n",
      "..........\n",
      "[1,  6700] loss: 68.991649\n",
      "..........\n",
      "[1,  6800] loss: 69.848846\n",
      "..........\n",
      "[1,  6900] loss: 69.295451\n",
      "..........\n",
      "[1,  7000] loss: 69.870741\n",
      "..........\n",
      "[1,  7100] loss: 68.725970\n",
      "..........\n",
      "[1,  7200] loss: 69.347994\n",
      "..........\n",
      "[1,  7300] loss: 69.268781\n",
      "..........\n",
      "[1,  7400] loss: 69.828915\n",
      "..........\n",
      "[1,  7500] loss: 69.566995\n",
      "..........\n",
      "[1,  7600] loss: 70.427487\n",
      "..........\n",
      "[1,  7700] loss: 70.767543\n",
      "..........\n",
      "[1,  7800] loss: 71.666272\n",
      "..........\n",
      "[1,  7900] loss: 68.489396\n",
      "..........\n",
      "[1,  8000] loss: 70.622481\n",
      "total average loss : 68.617\n",
      "train acc : 0.0852\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.135\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.113\n",
      "step : 1800 / 3125 acc : 0.128\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.120\n",
      "step : 2800 / 3125 acc : 0.123\n",
      "step : 3000 / 3125 acc : 0.123\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1240 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 70.237232\n",
      "..........\n",
      "[1,   200] loss: 71.485185\n",
      "..........\n",
      "[1,   300] loss: 71.007357\n",
      "..........\n",
      "[1,   400] loss: 70.696269\n",
      "..........\n",
      "[1,   500] loss: 71.213269\n",
      "..........\n",
      "[1,   600] loss: 70.840306\n",
      "..........\n",
      "[1,   700] loss: 69.515585\n",
      "..........\n",
      "[1,   800] loss: 71.022063\n",
      "..........\n",
      "[1,   900] loss: 70.491680\n",
      "..........\n",
      "[1,  1000] loss: 69.769792\n",
      "..........\n",
      "[1,  1100] loss: 71.266539\n",
      "..........\n",
      "[1,  1200] loss: 70.573548\n",
      "..........\n",
      "[1,  1300] loss: 71.184692\n",
      "..........\n",
      "[1,  1400] loss: 70.941027\n",
      "..........\n",
      "[1,  1500] loss: 70.649665\n",
      "..........\n",
      "[1,  1600] loss: 71.480621\n",
      "..........\n",
      "[1,  1700] loss: 71.183334\n",
      "..........\n",
      "[1,  1800] loss: 71.705401\n",
      "..........\n",
      "[1,  1900] loss: 72.304691\n",
      "..........\n",
      "[1,  2000] loss: 72.609726\n",
      "..........\n",
      "[1,  2100] loss: 70.962792\n",
      "..........\n",
      "[1,  2200] loss: 71.941069\n",
      "..........\n",
      "[1,  2300] loss: 69.607132\n",
      "..........\n",
      "[1,  2400] loss: 70.946583\n",
      "..........\n",
      "[1,  2500] loss: 69.604533\n",
      "..........\n",
      "[1,  2600] loss: 71.966936\n",
      "..........\n",
      "[1,  2700] loss: 71.351955\n",
      "..........\n",
      "[1,  2800] loss: 71.001922\n",
      "..........\n",
      "[1,  2900] loss: 71.222135\n",
      "..........\n",
      "[1,  3000] loss: 72.449247\n",
      "..........\n",
      "[1,  3100] loss: 71.236921\n",
      "..........\n",
      "[1,  3200] loss: 69.781218\n",
      "..........\n",
      "[1,  3300] loss: 72.067669\n",
      "..........\n",
      "[1,  3400] loss: 72.989046\n",
      "..........\n",
      "[1,  3500] loss: 72.804633\n",
      "..........\n",
      "[1,  3600] loss: 71.197708\n",
      "..........\n",
      "[1,  3700] loss: 71.544226\n",
      "..........\n",
      "[1,  3800] loss: 71.833746\n",
      "..........\n",
      "[1,  3900] loss: 71.819551\n",
      "..........\n",
      "[1,  4000] loss: 72.212075\n",
      "..........\n",
      "[1,  4100] loss: 72.197660\n",
      "..........\n",
      "[1,  4200] loss: 70.808550\n",
      "..........\n",
      "[1,  4300] loss: 71.398270\n",
      "..........\n",
      "[1,  4400] loss: 72.327783\n",
      "..........\n",
      "[1,  4500] loss: 73.763626\n",
      "..........\n",
      "[1,  4600] loss: 72.268783\n",
      "..........\n",
      "[1,  4700] loss: 71.632522\n",
      "..........\n",
      "[1,  4800] loss: 73.032300\n",
      "..........\n",
      "[1,  4900] loss: 70.726644\n",
      "..........\n",
      "[1,  5000] loss: 71.421929\n",
      "..........\n",
      "[1,  5100] loss: 71.632572\n",
      "..........\n",
      "[1,  5200] loss: 70.930946\n",
      "..........\n",
      "[1,  5300] loss: 72.581263\n",
      "..........\n",
      "[1,  5400] loss: 71.375246\n",
      "..........\n",
      "[1,  5500] loss: 73.257638\n",
      "..........\n",
      "[1,  5600] loss: 74.307628\n",
      "..........\n",
      "[1,  5700] loss: 72.721199\n",
      "..........\n",
      "[1,  5800] loss: 72.114715\n",
      "..........\n",
      "[1,  5900] loss: 71.392934\n",
      "..........\n",
      "[1,  6000] loss: 74.561823\n",
      "..........\n",
      "[1,  6100] loss: 73.263117\n",
      "..........\n",
      "[1,  6200] loss: 70.616502\n",
      "..........\n",
      "[1,  6300] loss: 73.336472\n",
      "..........\n",
      "[1,  6400] loss: 71.314914\n",
      "..........\n",
      "[1,  6500] loss: 73.652884\n",
      "..........\n",
      "[1,  6600] loss: 71.680866\n",
      "..........\n",
      "[1,  6700] loss: 70.611187\n",
      "..........\n",
      "[1,  6800] loss: 73.820971\n",
      "..........\n",
      "[1,  6900] loss: 71.284188\n",
      "..........\n",
      "[1,  7000] loss: 72.699307\n",
      "..........\n",
      "[1,  7100] loss: 72.862637\n",
      "..........\n",
      "[1,  7200] loss: 73.300503\n",
      "..........\n",
      "[1,  7300] loss: 73.313100\n",
      "..........\n",
      "[1,  7400] loss: 71.724113\n",
      "..........\n",
      "[1,  7500] loss: 72.797850\n",
      "..........\n",
      "[1,  7600] loss: 72.548535\n",
      "..........\n",
      "[1,  7700] loss: 73.605487\n",
      "..........\n",
      "[1,  7800] loss: 73.290096\n",
      "..........\n",
      "[1,  7900] loss: 72.448032\n",
      "..........\n",
      "[1,  8000] loss: 72.944852\n",
      "total average loss : 71.804\n",
      "train acc : 0.0875\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.250\n",
      "step : 400 / 3125 acc : 0.172\n",
      "step : 600 / 3125 acc : 0.146\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.098\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.115\n",
      "step : 2800 / 3125 acc : 0.118\n",
      "step : 3000 / 3125 acc : 0.117\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1160 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 73.156155\n",
      "..........\n",
      "[1,   200] loss: 74.246352\n",
      "..........\n",
      "[1,   300] loss: 73.786502\n",
      "..........\n",
      "[1,   400] loss: 74.300316\n",
      "..........\n",
      "[1,   500] loss: 73.802824\n",
      "..........\n",
      "[1,   600] loss: 71.183430\n",
      "..........\n",
      "[1,   700] loss: 74.222685\n",
      "..........\n",
      "[1,   800] loss: 73.244442\n",
      "..........\n",
      "[1,   900] loss: 74.425290\n",
      "..........\n",
      "[1,  1000] loss: 74.340275\n",
      "..........\n",
      "[1,  1100] loss: 73.726105\n",
      "..........\n",
      "[1,  1200] loss: 74.706247\n",
      "..........\n",
      "[1,  1300] loss: 73.243064\n",
      "..........\n",
      "[1,  1400] loss: 73.696651\n",
      "..........\n",
      "[1,  1500] loss: 74.149326\n",
      "..........\n",
      "[1,  1600] loss: 73.160433\n",
      "..........\n",
      "[1,  1700] loss: 73.914688\n",
      "..........\n",
      "[1,  1800] loss: 73.423695\n",
      "..........\n",
      "[1,  1900] loss: 74.277072\n",
      "..........\n",
      "[1,  2000] loss: 73.219994\n",
      "..........\n",
      "[1,  2100] loss: 74.509944\n",
      "..........\n",
      "[1,  2200] loss: 75.920322\n",
      "..........\n",
      "[1,  2300] loss: 75.110654\n",
      "..........\n",
      "[1,  2400] loss: 73.206581\n",
      "..........\n",
      "[1,  2500] loss: 74.997612\n",
      "..........\n",
      "[1,  2600] loss: 75.192271\n",
      "..........\n",
      "[1,  2700] loss: 75.888125\n",
      "..........\n",
      "[1,  2800] loss: 75.081926\n",
      "..........\n",
      "[1,  2900] loss: 75.207986\n",
      "..........\n",
      "[1,  3000] loss: 73.798890\n",
      "..........\n",
      "[1,  3100] loss: 75.297281\n",
      "..........\n",
      "[1,  3200] loss: 74.346356\n",
      "..........\n",
      "[1,  3300] loss: 74.716379\n",
      "..........\n",
      "[1,  3400] loss: 74.457485\n",
      "..........\n",
      "[1,  3500] loss: 73.876827\n",
      "..........\n",
      "[1,  3600] loss: 75.231397\n",
      "..........\n",
      "[1,  3700] loss: 75.595463\n",
      "..........\n",
      "[1,  3800] loss: 74.399945\n",
      "..........\n",
      "[1,  3900] loss: 73.565270\n",
      "..........\n",
      "[1,  4000] loss: 74.175520\n",
      "..........\n",
      "[1,  4100] loss: 74.165133\n",
      "..........\n",
      "[1,  4200] loss: 74.691788\n",
      "..........\n",
      "[1,  4300] loss: 73.927192\n",
      "..........\n",
      "[1,  4400] loss: 74.977495\n",
      "..........\n",
      "[1,  4500] loss: 74.755873\n",
      "..........\n",
      "[1,  4600] loss: 74.675936\n",
      "..........\n",
      "[1,  4700] loss: 73.527504\n",
      "..........\n",
      "[1,  4800] loss: 74.713425\n",
      "..........\n",
      "[1,  4900] loss: 75.198722\n",
      "..........\n",
      "[1,  5000] loss: 74.406406\n",
      "..........\n",
      "[1,  5100] loss: 75.528274\n",
      "..........\n",
      "[1,  5200] loss: 74.806912\n",
      "..........\n",
      "[1,  5300] loss: 74.676247\n",
      "..........\n",
      "[1,  5400] loss: 73.758883\n",
      "..........\n",
      "[1,  5500] loss: 75.583930\n",
      "..........\n",
      "[1,  5600] loss: 75.885748\n",
      "..........\n",
      "[1,  5700] loss: 75.697179\n",
      "..........\n",
      "[1,  5800] loss: 76.184027\n",
      "..........\n",
      "[1,  5900] loss: 75.018111\n",
      "..........\n",
      "[1,  6000] loss: 75.407925\n",
      "..........\n",
      "[1,  6100] loss: 75.963709\n",
      "..........\n",
      "[1,  6200] loss: 74.696992\n",
      "..........\n",
      "[1,  6300] loss: 75.830170\n",
      "..........\n",
      "[1,  6400] loss: 75.175089\n",
      "..........\n",
      "[1,  6500] loss: 74.684214\n",
      "..........\n",
      "[1,  6600] loss: 75.262732\n",
      "..........\n",
      "[1,  6700] loss: 75.249252\n",
      "..........\n",
      "[1,  6800] loss: 75.297048\n",
      "..........\n",
      "[1,  6900] loss: 74.909962\n",
      "..........\n",
      "[1,  7000] loss: 76.257611\n",
      "..........\n",
      "[1,  7100] loss: 74.281607\n",
      "..........\n",
      "[1,  7200] loss: 75.242213\n",
      "..........\n",
      "[1,  7300] loss: 75.442090\n",
      "..........\n",
      "[1,  7400] loss: 74.858220\n",
      "..........\n",
      "[1,  7500] loss: 75.620627\n",
      "..........\n",
      "[1,  7600] loss: 74.305207\n",
      "..........\n",
      "[1,  7700] loss: 75.361271\n",
      "..........\n",
      "[1,  7800] loss: 75.153115\n",
      "..........\n",
      "[1,  7900] loss: 73.880383\n",
      "..........\n",
      "[1,  8000] loss: 74.708011\n",
      "total average loss : 74.607\n",
      "train acc : 0.0914\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.120\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.125\n",
      "step : 1800 / 3125 acc : 0.128\n",
      "step : 2000 / 3125 acc : 0.131\n",
      "step : 2200 / 3125 acc : 0.125\n",
      "step : 2400 / 3125 acc : 0.120\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.110\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "======= epoch  6 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 75.609006\n",
      "..........\n",
      "[1,   200] loss: 78.359093\n",
      "..........\n",
      "[1,   300] loss: 74.941623\n",
      "..........\n",
      "[1,   400] loss: 77.807182\n",
      "..........\n",
      "[1,   500] loss: 77.543481\n",
      "..........\n",
      "[1,   600] loss: 76.858714\n",
      "..........\n",
      "[1,   700] loss: 75.660628\n",
      "..........\n",
      "[1,   800] loss: 75.296972\n",
      "..........\n",
      "[1,   900] loss: 76.625110\n",
      "..........\n",
      "[1,  1000] loss: 77.849944\n",
      "..........\n",
      "[1,  1100] loss: 76.600797\n",
      "..........\n",
      "[1,  1200] loss: 76.687756\n",
      "..........\n",
      "[1,  1300] loss: 77.140056\n",
      "..........\n",
      "[1,  1400] loss: 77.367946\n",
      "..........\n",
      "[1,  1500] loss: 75.258137\n",
      "..........\n",
      "[1,  1600] loss: 78.132863\n",
      "..........\n",
      "[1,  1700] loss: 76.898788\n",
      "..........\n",
      "[1,  1800] loss: 77.061660\n",
      "..........\n",
      "[1,  1900] loss: 76.246352\n",
      "..........\n",
      "[1,  2000] loss: 76.809473\n",
      "..........\n",
      "[1,  2100] loss: 77.752959\n",
      "..........\n",
      "[1,  2200] loss: 76.835434\n",
      "..........\n",
      "[1,  2300] loss: 76.258961\n",
      "..........\n",
      "[1,  2400] loss: 76.776122\n",
      "..........\n",
      "[1,  2500] loss: 76.323794\n",
      "..........\n",
      "[1,  2600] loss: 76.197890\n",
      "..........\n",
      "[1,  2700] loss: 76.176688\n",
      "..........\n",
      "[1,  2800] loss: 77.136563\n",
      "..........\n",
      "[1,  2900] loss: 78.519688\n",
      "..........\n",
      "[1,  3000] loss: 77.895950\n",
      "..........\n",
      "[1,  3100] loss: 77.234036\n",
      "..........\n",
      "[1,  3200] loss: 78.384859\n",
      "..........\n",
      "[1,  3300] loss: 78.896556\n",
      "..........\n",
      "[1,  3400] loss: 76.651842\n",
      "..........\n",
      "[1,  3500] loss: 76.483825\n",
      "..........\n",
      "[1,  3600] loss: 77.147858\n",
      "..........\n",
      "[1,  3700] loss: 75.862438\n",
      "..........\n",
      "[1,  3800] loss: 78.201048\n",
      "..........\n",
      "[1,  3900] loss: 77.163459\n",
      "..........\n",
      "[1,  4000] loss: 77.487502\n",
      "..........\n",
      "[1,  4100] loss: 77.613110\n",
      "..........\n",
      "[1,  4200] loss: 77.656487\n",
      "..........\n",
      "[1,  4300] loss: 77.554271\n",
      "..........\n",
      "[1,  4400] loss: 77.360724\n",
      "..........\n",
      "[1,  4500] loss: 78.296376\n",
      "..........\n",
      "[1,  4600] loss: 78.251123\n",
      "..........\n",
      "[1,  4700] loss: 75.740288\n",
      "..........\n",
      "[1,  4800] loss: 77.720005\n",
      "..........\n",
      "[1,  4900] loss: 77.213861\n",
      "..........\n",
      "[1,  5000] loss: 77.915949\n",
      "..........\n",
      "[1,  5100] loss: 78.259971\n",
      "..........\n",
      "[1,  5200] loss: 77.808803\n",
      "..........\n",
      "[1,  5300] loss: 77.066318\n",
      "..........\n",
      "[1,  5400] loss: 77.355783\n",
      "..........\n",
      "[1,  5500] loss: 77.104402\n",
      "..........\n",
      "[1,  5600] loss: 77.205249\n",
      "..........\n",
      "[1,  5700] loss: 78.275226\n",
      "..........\n",
      "[1,  5800] loss: 77.253155\n",
      "..........\n",
      "[1,  5900] loss: 76.867901\n",
      "..........\n",
      "[1,  6000] loss: 77.080604\n",
      "..........\n",
      "[1,  6100] loss: 78.225735\n",
      "..........\n",
      "[1,  6200] loss: 78.290742\n",
      "..........\n",
      "[1,  6300] loss: 77.389045\n",
      "..........\n",
      "[1,  6400] loss: 77.651730\n",
      "..........\n",
      "[1,  6500] loss: 78.367466\n",
      "..........\n",
      "[1,  6600] loss: 78.031051\n",
      "..........\n",
      "[1,  6700] loss: 77.600119\n",
      "..........\n",
      "[1,  6800] loss: 78.415157\n",
      "..........\n",
      "[1,  6900] loss: 78.118294\n",
      "..........\n",
      "[1,  7000] loss: 78.321831\n",
      "..........\n",
      "[1,  7100] loss: 78.061125\n",
      "..........\n",
      "[1,  7200] loss: 79.045917\n",
      "..........\n",
      "[1,  7300] loss: 79.262311\n",
      "..........\n",
      "[1,  7400] loss: 76.405136\n",
      "..........\n",
      "[1,  7500] loss: 77.865559\n",
      "..........\n",
      "[1,  7600] loss: 78.360277\n",
      "..........\n",
      "[1,  7700] loss: 77.686375\n",
      "..........\n",
      "[1,  7800] loss: 78.423048\n",
      "..........\n",
      "[1,  7900] loss: 78.179892\n",
      "..........\n",
      "[1,  8000] loss: 77.653123\n",
      "total average loss : 77.364\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.094\n",
      "step : 1400 / 3125 acc : 0.089\n",
      "step : 1600 / 3125 acc : 0.094\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.107\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.098\n",
      "step : 3000 / 3125 acc : 0.098\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1060 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 79.076423\n",
      "..........\n",
      "[1,   200] loss: 78.540651\n",
      "..........\n",
      "[1,   300] loss: 77.433398\n",
      "..........\n",
      "[1,   400] loss: 78.577855\n",
      "..........\n",
      "[1,   500] loss: 78.971926\n",
      "..........\n",
      "[1,   600] loss: 79.652929\n",
      "..........\n",
      "[1,   700] loss: 79.401957\n",
      "..........\n",
      "[1,   800] loss: 80.273747\n",
      "..........\n",
      "[1,   900] loss: 79.331461\n",
      "..........\n",
      "[1,  1000] loss: 78.654452\n",
      "..........\n",
      "[1,  1100] loss: 79.007697\n",
      "..........\n",
      "[1,  1200] loss: 78.946102\n",
      "..........\n",
      "[1,  1300] loss: 78.970069\n",
      "..........\n",
      "[1,  1400] loss: 78.024368\n",
      "..........\n",
      "[1,  1500] loss: 80.228352\n",
      "..........\n",
      "[1,  1600] loss: 79.543369\n",
      "..........\n",
      "[1,  1700] loss: 79.091739\n",
      "..........\n",
      "[1,  1800] loss: 79.340071\n",
      "..........\n",
      "[1,  1900] loss: 79.136738\n",
      "..........\n",
      "[1,  2000] loss: 79.301049\n",
      "..........\n",
      "[1,  2100] loss: 79.626330\n",
      "..........\n",
      "[1,  2200] loss: 79.284402\n",
      "..........\n",
      "[1,  2300] loss: 81.908455\n",
      "..........\n",
      "[1,  2400] loss: 79.296907\n",
      "..........\n",
      "[1,  2500] loss: 79.675809\n",
      "..........\n",
      "[1,  2600] loss: 80.290792\n",
      "..........\n",
      "[1,  2700] loss: 80.570364\n",
      "..........\n",
      "[1,  2800] loss: 80.432298\n",
      "..........\n",
      "[1,  2900] loss: 80.859086\n",
      "..........\n",
      "[1,  3000] loss: 79.643358\n",
      "..........\n",
      "[1,  3100] loss: 77.413678\n",
      "..........\n",
      "[1,  3200] loss: 81.373748\n",
      "..........\n",
      "[1,  3300] loss: 78.718713\n",
      "..........\n",
      "[1,  3400] loss: 79.195040\n",
      "..........\n",
      "[1,  3500] loss: 81.890287\n",
      "..........\n",
      "[1,  3600] loss: 80.962974\n",
      "..........\n",
      "[1,  3700] loss: 80.147993\n",
      "..........\n",
      "[1,  3800] loss: 80.382279\n",
      "..........\n",
      "[1,  3900] loss: 81.183739\n",
      "..........\n",
      "[1,  4000] loss: 78.888443\n",
      "..........\n",
      "[1,  4100] loss: 80.454257\n",
      "..........\n",
      "[1,  4200] loss: 79.782761\n",
      "..........\n",
      "[1,  4300] loss: 80.031475\n",
      "..........\n",
      "[1,  4400] loss: 80.827116\n",
      "..........\n",
      "[1,  4500] loss: 80.344941\n",
      "..........\n",
      "[1,  4600] loss: 80.538070\n",
      "..........\n",
      "[1,  4700] loss: 78.795523\n",
      "..........\n",
      "[1,  4800] loss: 80.336165\n",
      "..........\n",
      "[1,  4900] loss: 79.977598\n",
      "..........\n",
      "[1,  5000] loss: 80.498263\n",
      "..........\n",
      "[1,  5100] loss: 80.142169\n",
      "..........\n",
      "[1,  5200] loss: 80.629379\n",
      "..........\n",
      "[1,  5300] loss: 80.053012\n",
      "..........\n",
      "[1,  5400] loss: 80.910869\n",
      "..........\n",
      "[1,  5500] loss: 81.426296\n",
      "..........\n",
      "[1,  5600] loss: 78.696989\n",
      "..........\n",
      "[1,  5700] loss: 80.136872\n",
      "..........\n",
      "[1,  5800] loss: 81.542008\n",
      "..........\n",
      "[1,  5900] loss: 79.934804\n",
      "..........\n",
      "[1,  6000] loss: 81.097482\n",
      "..........\n",
      "[1,  6100] loss: 79.507073\n",
      "..........\n",
      "[1,  6200] loss: 81.826692\n",
      "..........\n",
      "[1,  6300] loss: 80.790521\n",
      "..........\n",
      "[1,  6400] loss: 81.162434\n",
      "..........\n",
      "[1,  6500] loss: 81.100375\n",
      "..........\n",
      "[1,  6600] loss: 82.173033\n",
      "..........\n",
      "[1,  6700] loss: 79.561905\n",
      "..........\n",
      "[1,  6800] loss: 79.783968\n",
      "..........\n",
      "[1,  6900] loss: 80.728725\n",
      "..........\n",
      "[1,  7000] loss: 80.519804\n",
      "..........\n",
      "[1,  7100] loss: 79.533946\n",
      "..........\n",
      "[1,  7200] loss: 80.652921\n",
      "..........\n",
      "[1,  7300] loss: 81.789911\n",
      "..........\n",
      "[1,  7400] loss: 80.790286\n",
      "..........\n",
      "[1,  7500] loss: 81.560450\n",
      "..........\n",
      "[1,  7600] loss: 82.027848\n",
      "..........\n",
      "[1,  7700] loss: 82.005020\n",
      "..........\n",
      "[1,  7800] loss: 80.973645\n",
      "..........\n",
      "[1,  7900] loss: 82.266154\n",
      "..........\n",
      "[1,  8000] loss: 81.479661\n",
      "total average loss : 80.120\n",
      "train acc : 0.0922\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.070\n",
      "step : 1000 / 3125 acc : 0.069\n",
      "step : 1200 / 3125 acc : 0.073\n",
      "step : 1400 / 3125 acc : 0.098\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.107\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.108\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1080 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 81.513271\n",
      "..........\n",
      "[1,   200] loss: 82.743109\n",
      "..........\n",
      "[1,   300] loss: 81.453268\n",
      "..........\n",
      "[1,   400] loss: 81.390003\n",
      "..........\n",
      "[1,   500] loss: 81.510744\n",
      "..........\n",
      "[1,   600] loss: 81.529990\n",
      "..........\n",
      "[1,   700] loss: 81.985250\n",
      "..........\n",
      "[1,   800] loss: 81.942139\n",
      "..........\n",
      "[1,   900] loss: 82.067527\n",
      "..........\n",
      "[1,  1000] loss: 81.579583\n",
      "..........\n",
      "[1,  1100] loss: 82.763698\n",
      "..........\n",
      "[1,  1200] loss: 81.961882\n",
      "..........\n",
      "[1,  1300] loss: 82.890864\n",
      "..........\n",
      "[1,  1400] loss: 83.778484\n",
      "..........\n",
      "[1,  1500] loss: 83.377328\n",
      "..........\n",
      "[1,  1600] loss: 82.161259\n",
      "..........\n",
      "[1,  1700] loss: 82.042736\n",
      "..........\n",
      "[1,  1800] loss: 82.429659\n",
      "..........\n",
      "[1,  1900] loss: 80.736093\n",
      "..........\n",
      "[1,  2000] loss: 82.290611\n",
      "..........\n",
      "[1,  2100] loss: 82.570717\n",
      "..........\n",
      "[1,  2200] loss: 81.405896\n",
      "..........\n",
      "[1,  2300] loss: 81.049284\n",
      "..........\n",
      "[1,  2400] loss: 82.140382\n",
      "..........\n",
      "[1,  2500] loss: 81.854854\n",
      "..........\n",
      "[1,  2600] loss: 82.517410\n",
      "..........\n",
      "[1,  2700] loss: 83.811741\n",
      "..........\n",
      "[1,  2800] loss: 80.290782\n",
      "..........\n",
      "[1,  2900] loss: 84.246147\n",
      "..........\n",
      "[1,  3000] loss: 82.400172\n",
      "..........\n",
      "[1,  3100] loss: 83.307281\n",
      "..........\n",
      "[1,  3200] loss: 81.464256\n",
      "..........\n",
      "[1,  3300] loss: 81.204200\n",
      "..........\n",
      "[1,  3400] loss: 83.473238\n",
      "..........\n",
      "[1,  3500] loss: 82.902073\n",
      "..........\n",
      "[1,  3600] loss: 84.150350\n",
      "..........\n",
      "[1,  3700] loss: 82.164934\n",
      "..........\n",
      "[1,  3800] loss: 84.407560\n",
      "..........\n",
      "[1,  3900] loss: 83.118095\n",
      "..........\n",
      "[1,  4000] loss: 81.702813\n",
      "..........\n",
      "[1,  4100] loss: 84.017272\n",
      "..........\n",
      "[1,  4200] loss: 81.847662\n",
      "..........\n",
      "[1,  4300] loss: 82.905859\n",
      "..........\n",
      "[1,  4400] loss: 82.869365\n",
      "..........\n",
      "[1,  4500] loss: 84.180110\n",
      "..........\n",
      "[1,  4600] loss: 82.931996\n",
      "..........\n",
      "[1,  4700] loss: 82.873511\n",
      "..........\n",
      "[1,  4800] loss: 83.499561\n",
      "..........\n",
      "[1,  4900] loss: 82.406900\n",
      "..........\n",
      "[1,  5000] loss: 84.350269\n",
      "..........\n",
      "[1,  5100] loss: 83.409236\n",
      "..........\n",
      "[1,  5200] loss: 84.569181\n",
      "..........\n",
      "[1,  5300] loss: 84.748931\n",
      "..........\n",
      "[1,  5400] loss: 82.892062\n",
      "..........\n",
      "[1,  5500] loss: 84.051340\n",
      "..........\n",
      "[1,  5600] loss: 84.379256\n",
      "..........\n",
      "[1,  5700] loss: 84.076724\n",
      "..........\n",
      "[1,  5800] loss: 84.000003\n",
      "..........\n",
      "[1,  5900] loss: 84.234092\n",
      "..........\n",
      "[1,  6000] loss: 83.332723\n",
      "..........\n",
      "[1,  6100] loss: 83.473978\n",
      "..........\n",
      "[1,  6200] loss: 82.452646\n",
      "..........\n",
      "[1,  6300] loss: 84.642683\n",
      "..........\n",
      "[1,  6400] loss: 84.669448\n",
      "..........\n",
      "[1,  6500] loss: 83.618409\n",
      "..........\n",
      "[1,  6600] loss: 84.859636\n",
      "..........\n",
      "[1,  6700] loss: 83.465911\n",
      "..........\n",
      "[1,  6800] loss: 86.190425\n",
      "..........\n",
      "[1,  6900] loss: 83.298933\n",
      "..........\n",
      "[1,  7000] loss: 83.406669\n",
      "..........\n",
      "[1,  7100] loss: 85.288973\n",
      "..........\n",
      "[1,  7200] loss: 82.035519\n",
      "..........\n",
      "[1,  7300] loss: 85.152235\n",
      "..........\n",
      "[1,  7400] loss: 83.458856\n",
      "..........\n",
      "[1,  7500] loss: 83.938757\n",
      "..........\n",
      "[1,  7600] loss: 83.635092\n",
      "..........\n",
      "[1,  7700] loss: 84.846892\n",
      "..........\n",
      "[1,  7800] loss: 84.627926\n",
      "..........\n",
      "[1,  7900] loss: 83.249158\n",
      "..........\n",
      "[1,  8000] loss: 84.720108\n",
      "total average loss : 83.062\n",
      "train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.109\n",
      "step : 1400 / 3125 acc : 0.121\n",
      "step : 1600 / 3125 acc : 0.129\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.122\n",
      "step : 2400 / 3125 acc : 0.120\n",
      "step : 2600 / 3125 acc : 0.115\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.113\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1100 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 83.411626\n",
      "..........\n",
      "[1,   200] loss: 84.176432\n",
      "..........\n",
      "[1,   300] loss: 83.779097\n",
      "..........\n",
      "[1,   400] loss: 86.424144\n",
      "..........\n",
      "[1,   500] loss: 86.713662\n",
      "..........\n",
      "[1,   600] loss: 85.499033\n",
      "..........\n",
      "[1,   700] loss: 84.401623\n",
      "..........\n",
      "[1,   800] loss: 86.102535\n",
      "..........\n",
      "[1,   900] loss: 85.091120\n",
      "..........\n",
      "[1,  1000] loss: 84.518767\n",
      "..........\n",
      "[1,  1100] loss: 85.215645\n",
      "..........\n",
      "[1,  1200] loss: 86.320692\n",
      "..........\n",
      "[1,  1300] loss: 84.526810\n",
      "..........\n",
      "[1,  1400] loss: 87.103575\n",
      "..........\n",
      "[1,  1500] loss: 83.996927\n",
      "..........\n",
      "[1,  1600] loss: 85.751669\n",
      "..........\n",
      "[1,  1700] loss: 85.195768\n",
      "..........\n",
      "[1,  1800] loss: 85.748155\n",
      "..........\n",
      "[1,  1900] loss: 84.924908\n",
      "..........\n",
      "[1,  2000] loss: 87.155438\n",
      "..........\n",
      "[1,  2100] loss: 86.242780\n",
      "..........\n",
      "[1,  2200] loss: 85.888148\n",
      "..........\n",
      "[1,  2300] loss: 86.779935\n",
      "..........\n",
      "[1,  2400] loss: 88.151493\n",
      "..........\n",
      "[1,  2500] loss: 86.881178\n",
      "..........\n",
      "[1,  2600] loss: 86.107618\n",
      "..........\n",
      "[1,  2700] loss: 85.148006\n",
      "..........\n",
      "[1,  2800] loss: 86.093576\n",
      "..........\n",
      "[1,  2900] loss: 87.701760\n",
      "..........\n",
      "[1,  3000] loss: 86.118892\n",
      "..........\n",
      "[1,  3100] loss: 85.319036\n",
      "..........\n",
      "[1,  3200] loss: 86.552072\n",
      "..........\n",
      "[1,  3300] loss: 85.976122\n",
      "..........\n",
      "[1,  3400] loss: 87.088040\n",
      "..........\n",
      "[1,  3500] loss: 84.710799\n",
      "..........\n",
      "[1,  3600] loss: 86.363259\n",
      "..........\n",
      "[1,  3700] loss: 86.431142\n",
      "..........\n",
      "[1,  3800] loss: 86.578866\n",
      "..........\n",
      "[1,  3900] loss: 86.453217\n",
      "..........\n",
      "[1,  4000] loss: 84.948663\n",
      "..........\n",
      "[1,  4100] loss: 88.110199\n",
      "..........\n",
      "[1,  4200] loss: 85.622313\n",
      "..........\n",
      "[1,  4300] loss: 86.575700\n",
      "..........\n",
      "[1,  4400] loss: 87.010003\n",
      "..........\n",
      "[1,  4500] loss: 85.054429\n",
      "..........\n",
      "[1,  4600] loss: 85.105074\n",
      "..........\n",
      "[1,  4700] loss: 85.415755\n",
      "..........\n",
      "[1,  4800] loss: 86.287251\n",
      "..........\n",
      "[1,  4900] loss: 86.242524\n",
      "..........\n",
      "[1,  5000] loss: 85.735177\n",
      "..........\n",
      "[1,  5100] loss: 87.571811\n",
      "..........\n",
      "[1,  5200] loss: 87.470600\n",
      "..........\n",
      "[1,  5300] loss: 87.216596\n",
      "..........\n",
      "[1,  5400] loss: 84.932178\n",
      "..........\n",
      "[1,  5500] loss: 87.610372\n",
      "..........\n",
      "[1,  5600] loss: 85.204290\n",
      "..........\n",
      "[1,  5700] loss: 86.811833\n",
      "..........\n",
      "[1,  5800] loss: 84.719925\n",
      "..........\n",
      "[1,  5900] loss: 84.322987\n",
      "..........\n",
      "[1,  6000] loss: 87.411451\n",
      "..........\n",
      "[1,  6100] loss: 87.858159\n",
      "..........\n",
      "[1,  6200] loss: 85.970197\n",
      "..........\n",
      "[1,  6300] loss: 87.029890\n",
      "..........\n",
      "[1,  6400] loss: 86.121657\n",
      "..........\n",
      "[1,  6500] loss: 86.788146\n",
      "..........\n",
      "[1,  6600] loss: 87.824813\n",
      "..........\n",
      "[1,  6700] loss: 87.149920\n",
      "..........\n",
      "[1,  6800] loss: 86.961552\n",
      "..........\n",
      "[1,  6900] loss: 87.469338\n",
      "..........\n",
      "[1,  7000] loss: 86.950627\n",
      "..........\n",
      "[1,  7100] loss: 86.741275\n",
      "..........\n",
      "[1,  7200] loss: 85.772632\n",
      "..........\n",
      "[1,  7300] loss: 87.315520\n",
      "..........\n",
      "[1,  7400] loss: 86.778245\n",
      "..........\n",
      "[1,  7500] loss: 88.318946\n",
      "..........\n",
      "[1,  7600] loss: 86.345327\n",
      "..........\n",
      "[1,  7700] loss: 86.509301\n",
      "..........\n",
      "[1,  7800] loss: 85.952253\n",
      "..........\n",
      "[1,  7900] loss: 87.478938\n",
      "..........\n",
      "[1,  8000] loss: 86.668508\n",
      "total average loss : 86.175\n",
      "train acc : 0.0992\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.131\n",
      "step : 1200 / 3125 acc : 0.125\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.111\n",
      "step : 2400 / 3125 acc : 0.117\n",
      "step : 2600 / 3125 acc : 0.113\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.108\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1060 %\n",
      "======eval  end ======\n",
      "======= epoch  7 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 87.103911\n",
      "..........\n",
      "[1,   200] loss: 90.567848\n",
      "..........\n",
      "[1,   300] loss: 88.063018\n",
      "..........\n",
      "[1,   400] loss: 87.539583\n",
      "..........\n",
      "[1,   500] loss: 87.495656\n",
      "..........\n",
      "[1,   600] loss: 88.212526\n",
      "..........\n",
      "[1,   700] loss: 87.629737\n",
      "..........\n",
      "[1,   800] loss: 89.467074\n",
      "..........\n",
      "[1,   900] loss: 89.560589\n",
      "..........\n",
      "[1,  1000] loss: 89.882252\n",
      "..........\n",
      "[1,  1100] loss: 88.449446\n",
      "..........\n",
      "[1,  1200] loss: 88.371703\n",
      "..........\n",
      "[1,  1300] loss: 89.175024\n",
      "..........\n",
      "[1,  1400] loss: 89.764680\n",
      "..........\n",
      "[1,  1500] loss: 89.768459\n",
      "..........\n",
      "[1,  1600] loss: 88.692403\n",
      "..........\n",
      "[1,  1700] loss: 88.077963\n",
      "..........\n",
      "[1,  1800] loss: 88.678731\n",
      "..........\n",
      "[1,  1900] loss: 88.605631\n",
      "..........\n",
      "[1,  2000] loss: 88.031293\n",
      "..........\n",
      "[1,  2100] loss: 89.807011\n",
      "..........\n",
      "[1,  2200] loss: 89.323236\n",
      "..........\n",
      "[1,  2300] loss: 87.643109\n",
      "..........\n",
      "[1,  2400] loss: 89.387660\n",
      "..........\n",
      "[1,  2500] loss: 89.553719\n",
      "..........\n",
      "[1,  2600] loss: 88.787962\n",
      "..........\n",
      "[1,  2700] loss: 89.473964\n",
      "..........\n",
      "[1,  2800] loss: 88.197219\n",
      "..........\n",
      "[1,  2900] loss: 86.510813\n",
      "..........\n",
      "[1,  3000] loss: 88.619259\n",
      "..........\n",
      "[1,  3100] loss: 89.253537\n",
      "..........\n",
      "[1,  3200] loss: 89.075398\n",
      "..........\n",
      "[1,  3300] loss: 88.551600\n",
      "..........\n",
      "[1,  3400] loss: 89.236136\n",
      "..........\n",
      "[1,  3500] loss: 89.934774\n",
      "..........\n",
      "[1,  3600] loss: 90.308452\n",
      "..........\n",
      "[1,  3700] loss: 88.394885\n",
      "..........\n",
      "[1,  3800] loss: 88.591234\n",
      "..........\n",
      "[1,  3900] loss: 88.818439\n",
      "..........\n",
      "[1,  4000] loss: 89.083306\n",
      "..........\n",
      "[1,  4100] loss: 90.795948\n",
      "..........\n",
      "[1,  4200] loss: 88.570562\n",
      "..........\n",
      "[1,  4300] loss: 90.916811\n",
      "..........\n",
      "[1,  4400] loss: 88.787817\n",
      "..........\n",
      "[1,  4500] loss: 89.090237\n",
      "..........\n",
      "[1,  4600] loss: 90.002213\n",
      "..........\n",
      "[1,  4700] loss: 87.585087\n",
      "..........\n",
      "[1,  4800] loss: 89.382720\n",
      "..........\n",
      "[1,  4900] loss: 91.070332\n",
      "..........\n",
      "[1,  5000] loss: 90.179075\n",
      "..........\n",
      "[1,  5100] loss: 90.059115\n",
      "..........\n",
      "[1,  5200] loss: 89.370824\n",
      "..........\n",
      "[1,  5300] loss: 90.483074\n",
      "..........\n",
      "[1,  5400] loss: 90.209959\n",
      "..........\n",
      "[1,  5500] loss: 90.019841\n",
      "..........\n",
      "[1,  5600] loss: 90.627262\n",
      "..........\n",
      "[1,  5700] loss: 88.322024\n",
      "..........\n",
      "[1,  5800] loss: 89.962625\n",
      "..........\n",
      "[1,  5900] loss: 89.394696\n",
      "..........\n",
      "[1,  6000] loss: 89.197501\n",
      "..........\n",
      "[1,  6100] loss: 88.821652\n",
      "..........\n",
      "[1,  6200] loss: 91.402916\n",
      "..........\n",
      "[1,  6300] loss: 89.658929\n",
      "..........\n",
      "[1,  6400] loss: 91.562152\n",
      "..........\n",
      "[1,  6500] loss: 90.374141\n",
      "..........\n",
      "[1,  6600] loss: 89.304261\n",
      "..........\n",
      "[1,  6700] loss: 91.235853\n",
      "..........\n",
      "[1,  6800] loss: 91.513497\n",
      "..........\n",
      "[1,  6900] loss: 90.710901\n",
      "..........\n",
      "[1,  7000] loss: 90.473511\n",
      "..........\n",
      "[1,  7100] loss: 89.000650\n",
      "..........\n",
      "[1,  7200] loss: 90.306054\n",
      "..........\n",
      "[1,  7300] loss: 90.170569\n",
      "..........\n",
      "[1,  7400] loss: 88.837856\n",
      "..........\n",
      "[1,  7500] loss: 89.929549\n",
      "..........\n",
      "[1,  7600] loss: 89.871670\n",
      "..........\n",
      "[1,  7700] loss: 90.568077\n",
      "..........\n",
      "[1,  7800] loss: 91.011615\n",
      "..........\n",
      "[1,  7900] loss: 93.805180\n",
      "..........\n",
      "[1,  8000] loss: 90.644508\n",
      "total average loss : 89.437\n",
      "train acc : 0.0977\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.146\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.105\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.099\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.096\n",
      "step : 2800 / 3125 acc : 0.092\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1080 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 90.821688\n",
      "..........\n",
      "[1,   200] loss: 91.298385\n",
      "..........\n",
      "[1,   300] loss: 92.694627\n",
      "..........\n",
      "[1,   400] loss: 92.336473\n",
      "..........\n",
      "[1,   500] loss: 92.213163\n",
      "..........\n",
      "[1,   600] loss: 90.872426\n",
      "..........\n",
      "[1,   700] loss: 90.481260\n",
      "..........\n",
      "[1,   800] loss: 91.876979\n",
      "..........\n",
      "[1,   900] loss: 91.556029\n",
      "..........\n",
      "[1,  1000] loss: 91.310378\n",
      "..........\n",
      "[1,  1100] loss: 92.122234\n",
      "..........\n",
      "[1,  1200] loss: 89.950282\n",
      "..........\n",
      "[1,  1300] loss: 91.074146\n",
      "..........\n",
      "[1,  1400] loss: 91.320246\n",
      "..........\n",
      "[1,  1500] loss: 92.256900\n",
      "..........\n",
      "[1,  1600] loss: 91.837648\n",
      "..........\n",
      "[1,  1700] loss: 92.192248\n",
      "..........\n",
      "[1,  1800] loss: 92.155125\n",
      "..........\n",
      "[1,  1900] loss: 92.337105\n",
      "..........\n",
      "[1,  2000] loss: 92.148432\n",
      "..........\n",
      "[1,  2100] loss: 92.696177\n",
      "..........\n",
      "[1,  2200] loss: 90.627611\n",
      "..........\n",
      "[1,  2300] loss: 92.882384\n",
      "..........\n",
      "[1,  2400] loss: 91.984462\n",
      "..........\n",
      "[1,  2500] loss: 92.514283\n",
      "..........\n",
      "[1,  2600] loss: 91.557669\n",
      "..........\n",
      "[1,  2700] loss: 92.704411\n",
      "..........\n",
      "[1,  2800] loss: 91.710509\n",
      "..........\n",
      "[1,  2900] loss: 92.573791\n",
      "..........\n",
      "[1,  3000] loss: 92.087469\n",
      "..........\n",
      "[1,  3100] loss: 90.856306\n",
      "..........\n",
      "[1,  3200] loss: 92.217718\n",
      "..........\n",
      "[1,  3300] loss: 92.031231\n",
      "..........\n",
      "[1,  3400] loss: 91.922232\n",
      "..........\n",
      "[1,  3500] loss: 92.313721\n",
      "..........\n",
      "[1,  3600] loss: 94.037164\n",
      "..........\n",
      "[1,  3700] loss: 93.251142\n",
      "..........\n",
      "[1,  3800] loss: 94.020965\n",
      "..........\n",
      "[1,  3900] loss: 92.588698\n",
      "..........\n",
      "[1,  4000] loss: 93.516293\n",
      "..........\n",
      "[1,  4100] loss: 92.150717\n",
      "..........\n",
      "[1,  4200] loss: 92.088357\n",
      "..........\n",
      "[1,  4300] loss: 92.850088\n",
      "..........\n",
      "[1,  4400] loss: 94.421610\n",
      "..........\n",
      "[1,  4500] loss: 92.777088\n",
      "..........\n",
      "[1,  4600] loss: 92.989125\n",
      "..........\n",
      "[1,  4700] loss: 93.539758\n",
      "..........\n",
      "[1,  4800] loss: 92.065419\n",
      "..........\n",
      "[1,  4900] loss: 92.848786\n",
      "..........\n",
      "[1,  5000] loss: 91.887500\n",
      "..........\n",
      "[1,  5100] loss: 94.623987\n",
      "..........\n",
      "[1,  5200] loss: 90.705447\n",
      "..........\n",
      "[1,  5300] loss: 92.756586\n",
      "..........\n",
      "[1,  5400] loss: 91.761072\n",
      "..........\n",
      "[1,  5500] loss: 92.843499\n",
      "..........\n",
      "[1,  5600] loss: 92.826734\n",
      "..........\n",
      "[1,  5700] loss: 92.538817\n",
      "..........\n",
      "[1,  5800] loss: 92.642100\n",
      "..........\n",
      "[1,  5900] loss: 92.326207\n",
      "..........\n",
      "[1,  6000] loss: 91.898008\n",
      "..........\n",
      "[1,  6100] loss: 92.915546\n",
      "..........\n",
      "[1,  6200] loss: 91.728751\n",
      "..........\n",
      "[1,  6300] loss: 92.770496\n",
      "..........\n",
      "[1,  6400] loss: 93.355049\n",
      "..........\n",
      "[1,  6500] loss: 93.854363\n",
      "..........\n",
      "[1,  6600] loss: 92.719809\n",
      "..........\n",
      "[1,  6700] loss: 93.948783\n",
      "..........\n",
      "[1,  6800] loss: 93.002405\n",
      "..........\n",
      "[1,  6900] loss: 92.267139\n",
      "..........\n",
      "[1,  7000] loss: 93.098267\n",
      "..........\n",
      "[1,  7100] loss: 91.607108\n",
      "..........\n",
      "[1,  7200] loss: 93.236366\n",
      "..........\n",
      "[1,  7300] loss: 93.900537\n",
      "..........\n",
      "[1,  7400] loss: 93.866495\n",
      "..........\n",
      "[1,  7500] loss: 92.137265\n",
      "..........\n",
      "[1,  7600] loss: 93.210655\n",
      "..........\n",
      "[1,  7700] loss: 93.950124\n",
      "..........\n",
      "[1,  7800] loss: 95.143284\n",
      "..........\n",
      "[1,  7900] loss: 94.289840\n",
      "..........\n",
      "[1,  8000] loss: 94.889250\n",
      "total average loss : 92.492\n",
      "train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.156\n",
      "step : 800 / 3125 acc : 0.141\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.120\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.129\n",
      "step : 1800 / 3125 acc : 0.125\n",
      "step : 2000 / 3125 acc : 0.119\n",
      "step : 2200 / 3125 acc : 0.116\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.113\n",
      "step : 2800 / 3125 acc : 0.114\n",
      "step : 3000 / 3125 acc : 0.108\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1080 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 94.510899\n",
      "..........\n",
      "[1,   200] loss: 96.715058\n",
      "..........\n",
      "[1,   300] loss: 95.027557\n",
      "..........\n",
      "[1,   400] loss: 94.838489\n",
      "..........\n",
      "[1,   500] loss: 94.076706\n",
      "..........\n",
      "[1,   600] loss: 93.785501\n",
      "..........\n",
      "[1,   700] loss: 93.275807\n",
      "..........\n",
      "[1,   800] loss: 94.983084\n",
      "..........\n",
      "[1,   900] loss: 93.390805\n",
      "..........\n",
      "[1,  1000] loss: 93.690260\n",
      "..........\n",
      "[1,  1100] loss: 95.490346\n",
      "..........\n",
      "[1,  1200] loss: 94.100110\n",
      "..........\n",
      "[1,  1300] loss: 95.421213\n",
      "..........\n",
      "[1,  1400] loss: 92.687218\n",
      "..........\n",
      "[1,  1500] loss: 93.966387\n",
      "..........\n",
      "[1,  1600] loss: 95.225137\n",
      "..........\n",
      "[1,  1700] loss: 95.137094\n",
      "..........\n",
      "[1,  1800] loss: 93.979704\n",
      "..........\n",
      "[1,  1900] loss: 93.719640\n",
      "..........\n",
      "[1,  2000] loss: 94.637929\n",
      "..........\n",
      "[1,  2100] loss: 95.007913\n",
      "..........\n",
      "[1,  2200] loss: 95.808577\n",
      "..........\n",
      "[1,  2300] loss: 94.230253\n",
      "..........\n",
      "[1,  2400] loss: 96.697920\n",
      "..........\n",
      "[1,  2500] loss: 96.539855\n",
      "..........\n",
      "[1,  2600] loss: 93.786095\n",
      "..........\n",
      "[1,  2700] loss: 95.487743\n",
      "..........\n",
      "[1,  2800] loss: 95.017816\n",
      "..........\n",
      "[1,  2900] loss: 94.478216\n",
      "..........\n",
      "[1,  3000] loss: 94.116063\n",
      "..........\n",
      "[1,  3100] loss: 95.341970\n",
      "..........\n",
      "[1,  3200] loss: 96.263957\n",
      "..........\n",
      "[1,  3300] loss: 94.507624\n",
      "..........\n",
      "[1,  3400] loss: 96.906405\n",
      "..........\n",
      "[1,  3500] loss: 96.599361\n",
      "..........\n",
      "[1,  3600] loss: 96.070822\n",
      "..........\n",
      "[1,  3700] loss: 97.199380\n",
      "..........\n",
      "[1,  3800] loss: 95.679172\n",
      "..........\n",
      "[1,  3900] loss: 94.779058\n",
      "..........\n",
      "[1,  4000] loss: 95.887869\n",
      "..........\n",
      "[1,  4100] loss: 97.436290\n",
      "..........\n",
      "[1,  4200] loss: 95.214644\n",
      "..........\n",
      "[1,  4300] loss: 95.404110\n",
      "..........\n",
      "[1,  4400] loss: 94.008704\n",
      "..........\n",
      "[1,  4500] loss: 98.173444\n",
      "..........\n",
      "[1,  4600] loss: 97.612202\n",
      "..........\n",
      "[1,  4700] loss: 98.992309\n",
      "..........\n",
      "[1,  4800] loss: 96.344539\n",
      "..........\n",
      "[1,  4900] loss: 94.737369\n",
      "..........\n",
      "[1,  5000] loss: 95.991113\n",
      "..........\n",
      "[1,  5100] loss: 95.786480\n",
      "..........\n",
      "[1,  5200] loss: 96.638913\n",
      "..........\n",
      "[1,  5300] loss: 96.213826\n",
      "..........\n",
      "[1,  5400] loss: 96.738137\n",
      "..........\n",
      "[1,  5500] loss: 95.718867\n",
      "..........\n",
      "[1,  5600] loss: 96.492303\n",
      "..........\n",
      "[1,  5700] loss: 95.759562\n",
      "..........\n",
      "[1,  5800] loss: 96.626647\n",
      "..........\n",
      "[1,  5900] loss: 96.436334\n",
      "..........\n",
      "[1,  6000] loss: 94.845532\n",
      "..........\n",
      "[1,  6100] loss: 97.285539\n",
      "..........\n",
      "[1,  6200] loss: 95.197006\n",
      "..........\n",
      "[1,  6300] loss: 94.698690\n",
      "..........\n",
      "[1,  6400] loss: 96.388834\n",
      "..........\n",
      "[1,  6500] loss: 95.836227\n",
      "..........\n",
      "[1,  6600] loss: 96.913724\n",
      "..........\n",
      "[1,  6700] loss: 95.525986\n",
      "..........\n",
      "[1,  6800] loss: 97.051628\n",
      "..........\n",
      "[1,  6900] loss: 96.296570\n",
      "..........\n",
      "[1,  7000] loss: 96.448739\n",
      "..........\n",
      "[1,  7100] loss: 96.895216\n",
      "..........\n",
      "[1,  7200] loss: 95.957964\n",
      "..........\n",
      "[1,  7300] loss: 96.774105\n",
      "..........\n",
      "[1,  7400] loss: 95.906702\n",
      "..........\n",
      "[1,  7500] loss: 96.570538\n",
      "..........\n",
      "[1,  7600] loss: 96.723174\n",
      "..........\n",
      "[1,  7700] loss: 97.614685\n",
      "..........\n",
      "[1,  7800] loss: 96.055261\n",
      "..........\n",
      "[1,  7900] loss: 97.422325\n",
      "..........\n",
      "[1,  8000] loss: 96.934390\n",
      "total average loss : 95.659\n",
      "train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.089\n",
      "step : 1600 / 3125 acc : 0.082\n",
      "step : 1800 / 3125 acc : 0.087\n",
      "step : 2000 / 3125 acc : 0.097\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.104\n",
      "step : 2600 / 3125 acc : 0.106\n",
      "step : 2800 / 3125 acc : 0.105\n",
      "step : 3000 / 3125 acc : 0.098\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 98.473458\n",
      "..........\n",
      "[1,   200] loss: 96.707081\n",
      "..........\n",
      "[1,   300] loss: 98.121642\n",
      "..........\n",
      "[1,   400] loss: 99.926931\n",
      "..........\n",
      "[1,   500] loss: 96.877910\n",
      "..........\n",
      "[1,   600] loss: 98.724946\n",
      "..........\n",
      "[1,   700] loss: 95.521131\n",
      "..........\n",
      "[1,   800] loss: 95.280007\n",
      "..........\n",
      "[1,   900] loss: 100.192029\n",
      "..........\n",
      "[1,  1000] loss: 98.596009\n",
      "..........\n",
      "[1,  1100] loss: 97.999953\n",
      "..........\n",
      "[1,  1200] loss: 97.256625\n",
      "..........\n",
      "[1,  1300] loss: 98.891964\n",
      "..........\n",
      "[1,  1400] loss: 97.925482\n",
      "..........\n",
      "[1,  1500] loss: 96.856939\n",
      "..........\n",
      "[1,  1600] loss: 98.801055\n",
      "..........\n",
      "[1,  1700] loss: 96.910163\n",
      "..........\n",
      "[1,  1800] loss: 96.043362\n",
      "..........\n",
      "[1,  1900] loss: 97.803306\n",
      "..........\n",
      "[1,  2000] loss: 99.249276\n",
      "..........\n",
      "[1,  2100] loss: 98.172683\n",
      "..........\n",
      "[1,  2200] loss: 98.267505\n",
      "..........\n",
      "[1,  2300] loss: 98.679057\n",
      "..........\n",
      "[1,  2400] loss: 96.461761\n",
      "..........\n",
      "[1,  2500] loss: 98.892867\n",
      "..........\n",
      "[1,  2600] loss: 98.497346\n",
      "..........\n",
      "[1,  2700] loss: 97.974106\n",
      "..........\n",
      "[1,  2800] loss: 98.814569\n",
      "..........\n",
      "[1,  2900] loss: 98.949228\n",
      "..........\n",
      "[1,  3000] loss: 100.764961\n",
      "..........\n",
      "[1,  3100] loss: 99.668618\n",
      "..........\n",
      "[1,  3200] loss: 98.660185\n",
      "..........\n",
      "[1,  3300] loss: 99.413753\n",
      "..........\n",
      "[1,  3400] loss: 99.879399\n",
      "..........\n",
      "[1,  3500] loss: 98.638218\n",
      "..........\n",
      "[1,  3600] loss: 100.480162\n",
      "..........\n",
      "[1,  3700] loss: 97.703671\n",
      "..........\n",
      "[1,  3800] loss: 99.958235\n",
      "..........\n",
      "[1,  3900] loss: 98.667461\n",
      "..........\n",
      "[1,  4000] loss: 98.619975\n",
      "..........\n",
      "[1,  4100] loss: 98.162021\n",
      "..........\n",
      "[1,  4200] loss: 99.746367\n",
      "..........\n",
      "[1,  4300] loss: 97.889718\n",
      "..........\n",
      "[1,  4400] loss: 97.190415\n",
      "..........\n",
      "[1,  4500] loss: 98.635384\n",
      "..........\n",
      "[1,  4600] loss: 97.891321\n",
      "..........\n",
      "[1,  4700] loss: 99.168137\n",
      "..........\n",
      "[1,  4800] loss: 97.021864\n",
      "..........\n",
      "[1,  4900] loss: 98.649312\n",
      "..........\n",
      "[1,  5000] loss: 99.442380\n",
      "..........\n",
      "[1,  5100] loss: 99.419177\n",
      "..........\n",
      "[1,  5200] loss: 99.251190\n",
      "..........\n",
      "[1,  5300] loss: 100.630544\n",
      "..........\n",
      "[1,  5400] loss: 99.720021\n",
      "..........\n",
      "[1,  5500] loss: 98.885982\n",
      "..........\n",
      "[1,  5600] loss: 100.114334\n",
      "..........\n",
      "[1,  5700] loss: 99.961660\n",
      "..........\n",
      "[1,  5800] loss: 98.861086\n",
      "..........\n",
      "[1,  5900] loss: 97.343750\n",
      "..........\n",
      "[1,  6000] loss: 98.736093\n",
      "..........\n",
      "[1,  6100] loss: 99.827765\n",
      "..........\n",
      "[1,  6200] loss: 99.994954\n",
      "..........\n",
      "[1,  6300] loss: 98.976347\n",
      "..........\n",
      "[1,  6400] loss: 97.189192\n",
      "..........\n",
      "[1,  6500] loss: 100.441760\n",
      "..........\n",
      "[1,  6600] loss: 100.543688\n",
      "..........\n",
      "[1,  6700] loss: 100.271723\n",
      "..........\n",
      "[1,  6800] loss: 99.078287\n",
      "..........\n",
      "[1,  6900] loss: 100.388618\n",
      "..........\n",
      "[1,  7000] loss: 98.774504\n",
      "..........\n",
      "[1,  7100] loss: 100.657027\n",
      "..........\n",
      "[1,  7200] loss: 98.142569\n",
      "..........\n",
      "[1,  7300] loss: 99.950593\n",
      "..........\n",
      "[1,  7400] loss: 100.560536\n",
      "..........\n",
      "[1,  7500] loss: 101.766025\n",
      "..........\n",
      "[1,  7600] loss: 99.700630\n",
      "..........\n",
      "[1,  7700] loss: 99.369442\n",
      "..........\n",
      "[1,  7800] loss: 101.685889\n",
      "..........\n",
      "[1,  7900] loss: 99.569732\n",
      "..........\n",
      "[1,  8000] loss: 99.563632\n",
      "total average loss : 98.831\n",
      "train acc : 0.0945\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.062\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.087\n",
      "step : 1200 / 3125 acc : 0.089\n",
      "step : 1400 / 3125 acc : 0.089\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.109\n",
      "step : 2200 / 3125 acc : 0.108\n",
      "step : 2400 / 3125 acc : 0.107\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.102\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "======= epoch  8 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 101.355699\n",
      "..........\n",
      "[1,   200] loss: 99.658143\n",
      "..........\n",
      "[1,   300] loss: 102.402312\n",
      "..........\n",
      "[1,   400] loss: 101.521874\n",
      "..........\n",
      "[1,   500] loss: 99.498329\n",
      "..........\n",
      "[1,   600] loss: 99.917958\n",
      "..........\n",
      "[1,   700] loss: 101.388725\n",
      "..........\n",
      "[1,   800] loss: 101.452326\n",
      "..........\n",
      "[1,   900] loss: 100.845021\n",
      "..........\n",
      "[1,  1000] loss: 102.902720\n",
      "..........\n",
      "[1,  1100] loss: 102.403523\n",
      "..........\n",
      "[1,  1200] loss: 101.578088\n",
      "..........\n",
      "[1,  1300] loss: 100.636119\n",
      "..........\n",
      "[1,  1400] loss: 101.215328\n",
      "..........\n",
      "[1,  1500] loss: 101.137270\n",
      "..........\n",
      "[1,  1600] loss: 101.457018\n",
      "..........\n",
      "[1,  1700] loss: 101.761806\n",
      "..........\n",
      "[1,  1800] loss: 102.667815\n",
      "..........\n",
      "[1,  1900] loss: 100.743027\n",
      "..........\n",
      "[1,  2000] loss: 100.730287\n",
      "..........\n",
      "[1,  2100] loss: 103.426120\n",
      "..........\n",
      "[1,  2200] loss: 101.357005\n",
      "..........\n",
      "[1,  2300] loss: 102.148566\n",
      "..........\n",
      "[1,  2400] loss: 101.764455\n",
      "..........\n",
      "[1,  2500] loss: 100.969673\n",
      "..........\n",
      "[1,  2600] loss: 102.267912\n",
      "..........\n",
      "[1,  2700] loss: 102.065461\n",
      "..........\n",
      "[1,  2800] loss: 102.649790\n",
      "..........\n",
      "[1,  2900] loss: 102.992030\n",
      "..........\n",
      "[1,  3000] loss: 102.848620\n",
      "..........\n",
      "[1,  3100] loss: 100.872941\n",
      "..........\n",
      "[1,  3200] loss: 102.180748\n",
      "..........\n",
      "[1,  3300] loss: 102.902108\n",
      "..........\n",
      "[1,  3400] loss: 100.225713\n",
      "..........\n",
      "[1,  3500] loss: 100.658032\n",
      "..........\n",
      "[1,  3600] loss: 103.009474\n",
      "..........\n",
      "[1,  3700] loss: 101.931170\n",
      "..........\n",
      "[1,  3800] loss: 101.167460\n",
      "..........\n",
      "[1,  3900] loss: 103.139412\n",
      "..........\n",
      "[1,  4000] loss: 101.398904\n",
      "..........\n",
      "[1,  4100] loss: 103.245650\n",
      "..........\n",
      "[1,  4200] loss: 102.343845\n",
      "..........\n",
      "[1,  4300] loss: 102.021540\n",
      "..........\n",
      "[1,  4400] loss: 103.011974\n",
      "..........\n",
      "[1,  4500] loss: 101.660303\n",
      "..........\n",
      "[1,  4600] loss: 103.280957\n",
      "..........\n",
      "[1,  4700] loss: 102.012260\n",
      "..........\n",
      "[1,  4800] loss: 101.172557\n",
      "..........\n",
      "[1,  4900] loss: 102.491286\n",
      "..........\n",
      "[1,  5000] loss: 102.163048\n",
      "..........\n",
      "[1,  5100] loss: 102.343331\n",
      "..........\n",
      "[1,  5200] loss: 101.371114\n",
      "..........\n",
      "[1,  5300] loss: 102.052166\n",
      "..........\n",
      "[1,  5400] loss: 101.606618\n",
      "..........\n",
      "[1,  5500] loss: 101.415686\n",
      "..........\n",
      "[1,  5600] loss: 103.068075\n",
      "..........\n",
      "[1,  5700] loss: 103.427392\n",
      "..........\n",
      "[1,  5800] loss: 102.322920\n",
      "..........\n",
      "[1,  5900] loss: 102.493105\n",
      "..........\n",
      "[1,  6000] loss: 103.843302\n",
      "..........\n",
      "[1,  6100] loss: 101.076591\n",
      "..........\n",
      "[1,  6200] loss: 103.093714\n",
      "..........\n",
      "[1,  6300] loss: 102.220703\n",
      "..........\n",
      "[1,  6400] loss: 100.900521\n",
      "..........\n",
      "[1,  6500] loss: 104.348341\n",
      "..........\n",
      "[1,  6600] loss: 103.569658\n",
      "..........\n",
      "[1,  6700] loss: 101.755776\n",
      "..........\n",
      "[1,  6800] loss: 101.434663\n",
      "..........\n",
      "[1,  6900] loss: 101.696471\n",
      "..........\n",
      "[1,  7000] loss: 102.834648\n",
      "..........\n",
      "[1,  7100] loss: 102.543028\n",
      "..........\n",
      "[1,  7200] loss: 103.082547\n",
      "..........\n",
      "[1,  7300] loss: 102.750459\n",
      "..........\n",
      "[1,  7400] loss: 101.682235\n",
      "..........\n",
      "[1,  7500] loss: 103.870509\n",
      "..........\n",
      "[1,  7600] loss: 100.840959\n",
      "..........\n",
      "[1,  7700] loss: 103.637658\n",
      "..........\n",
      "[1,  7800] loss: 103.940064\n",
      "..........\n",
      "[1,  7900] loss: 102.912116\n",
      "..........\n",
      "[1,  8000] loss: 102.830869\n",
      "total average loss : 102.045\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.113\n",
      "step : 1800 / 3125 acc : 0.104\n",
      "step : 2000 / 3125 acc : 0.097\n",
      "step : 2200 / 3125 acc : 0.091\n",
      "step : 2400 / 3125 acc : 0.091\n",
      "step : 2600 / 3125 acc : 0.091\n",
      "step : 2800 / 3125 acc : 0.094\n",
      "step : 3000 / 3125 acc : 0.102\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1020 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 104.237539\n",
      "..........\n",
      "[1,   200] loss: 104.112596\n",
      "..........\n",
      "[1,   300] loss: 104.556750\n",
      "..........\n",
      "[1,   400] loss: 104.473380\n",
      "..........\n",
      "[1,   500] loss: 102.403511\n",
      "..........\n",
      "[1,   600] loss: 105.039158\n",
      "..........\n",
      "[1,   700] loss: 106.000109\n",
      "..........\n",
      "[1,   800] loss: 103.146406\n",
      "..........\n",
      "[1,   900] loss: 104.328003\n",
      "..........\n",
      "[1,  1000] loss: 104.557592\n",
      "..........\n",
      "[1,  1100] loss: 102.831918\n",
      "..........\n",
      "[1,  1200] loss: 105.644927\n",
      "..........\n",
      "[1,  1300] loss: 102.407259\n",
      "..........\n",
      "[1,  1400] loss: 103.458404\n",
      "..........\n",
      "[1,  1500] loss: 104.643247\n",
      "..........\n",
      "[1,  1600] loss: 103.881272\n",
      "..........\n",
      "[1,  1700] loss: 105.044999\n",
      "..........\n",
      "[1,  1800] loss: 106.260747\n",
      "..........\n",
      "[1,  1900] loss: 102.570198\n",
      "..........\n",
      "[1,  2000] loss: 103.894211\n",
      "..........\n",
      "[1,  2100] loss: 105.494889\n",
      "..........\n",
      "[1,  2200] loss: 104.336917\n",
      "..........\n",
      "[1,  2300] loss: 103.961615\n",
      "..........\n",
      "[1,  2400] loss: 106.286644\n",
      "..........\n",
      "[1,  2500] loss: 104.656641\n",
      "..........\n",
      "[1,  2600] loss: 106.473109\n",
      "..........\n",
      "[1,  2700] loss: 102.862919\n",
      "..........\n",
      "[1,  2800] loss: 104.825326\n",
      "..........\n",
      "[1,  2900] loss: 103.104903\n",
      "..........\n",
      "[1,  3000] loss: 103.438850\n",
      "..........\n",
      "[1,  3100] loss: 106.122969\n",
      "..........\n",
      "[1,  3200] loss: 104.004481\n",
      "..........\n",
      "[1,  3300] loss: 105.771345\n",
      "..........\n",
      "[1,  3400] loss: 103.984064\n",
      "..........\n",
      "[1,  3500] loss: 106.348155\n",
      "..........\n",
      "[1,  3600] loss: 106.001993\n",
      "..........\n",
      "[1,  3700] loss: 105.419438\n",
      "..........\n",
      "[1,  3800] loss: 104.312459\n",
      "..........\n",
      "[1,  3900] loss: 106.015048\n",
      "..........\n",
      "[1,  4000] loss: 105.750888\n",
      "..........\n",
      "[1,  4100] loss: 105.935522\n",
      "..........\n",
      "[1,  4200] loss: 105.695234\n",
      "..........\n",
      "[1,  4300] loss: 104.720261\n",
      "..........\n",
      "[1,  4400] loss: 105.902190\n",
      "..........\n",
      "[1,  4500] loss: 105.651303\n",
      "..........\n",
      "[1,  4600] loss: 104.436624\n",
      "..........\n",
      "[1,  4700] loss: 107.075671\n",
      "..........\n",
      "[1,  4800] loss: 103.033390\n",
      "..........\n",
      "[1,  4900] loss: 103.878749\n",
      "..........\n",
      "[1,  5000] loss: 104.789456\n",
      "..........\n",
      "[1,  5100] loss: 105.224946\n",
      "..........\n",
      "[1,  5200] loss: 106.430040\n",
      "..........\n",
      "[1,  5300] loss: 105.629313\n",
      "..........\n",
      "[1,  5400] loss: 105.863301\n",
      "..........\n",
      "[1,  5500] loss: 104.376569\n",
      "..........\n",
      "[1,  5600] loss: 104.606640\n",
      "..........\n",
      "[1,  5700] loss: 107.486660\n",
      "..........\n",
      "[1,  5800] loss: 104.563313\n",
      "..........\n",
      "[1,  5900] loss: 105.266272\n",
      "..........\n",
      "[1,  6000] loss: 106.609404\n",
      "..........\n",
      "[1,  6100] loss: 104.777481\n",
      "..........\n",
      "[1,  6200] loss: 104.714657\n",
      "..........\n",
      "[1,  6300] loss: 105.475740\n",
      "..........\n",
      "[1,  6400] loss: 107.349883\n",
      "..........\n",
      "[1,  6500] loss: 108.053790\n",
      "..........\n",
      "[1,  6600] loss: 106.945092\n",
      "..........\n",
      "[1,  6700] loss: 105.084265\n",
      "..........\n",
      "[1,  6800] loss: 108.470951\n",
      "..........\n",
      "[1,  6900] loss: 106.151533\n",
      "..........\n",
      "[1,  7000] loss: 106.081365\n",
      "..........\n",
      "[1,  7100] loss: 107.251355\n",
      "..........\n",
      "[1,  7200] loss: 106.927953\n",
      "..........\n",
      "[1,  7300] loss: 106.967900\n",
      "..........\n",
      "[1,  7400] loss: 107.465282\n",
      "..........\n",
      "[1,  7500] loss: 107.459456\n",
      "..........\n",
      "[1,  7600] loss: 108.787532\n",
      "..........\n",
      "[1,  7700] loss: 105.835758\n",
      "..........\n",
      "[1,  7800] loss: 106.481904\n",
      "..........\n",
      "[1,  7900] loss: 106.023265\n",
      "..........\n",
      "[1,  8000] loss: 107.567698\n",
      "total average loss : 105.296\n",
      "train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.078\n",
      "step : 1400 / 3125 acc : 0.076\n",
      "step : 1600 / 3125 acc : 0.078\n",
      "step : 1800 / 3125 acc : 0.076\n",
      "step : 2000 / 3125 acc : 0.087\n",
      "step : 2200 / 3125 acc : 0.099\n",
      "step : 2400 / 3125 acc : 0.096\n",
      "step : 2600 / 3125 acc : 0.089\n",
      "step : 2800 / 3125 acc : 0.096\n",
      "step : 3000 / 3125 acc : 0.100\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1020 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 107.076420\n",
      "..........\n",
      "[1,   200] loss: 106.872920\n",
      "..........\n",
      "[1,   300] loss: 107.567422\n",
      "..........\n",
      "[1,   400] loss: 107.204658\n",
      "..........\n",
      "[1,   500] loss: 107.169122\n",
      "..........\n",
      "[1,   600] loss: 107.658947\n",
      "..........\n",
      "[1,   700] loss: 107.274598\n",
      "..........\n",
      "[1,   800] loss: 107.116874\n",
      "..........\n",
      "[1,   900] loss: 106.837221\n",
      "..........\n",
      "[1,  1000] loss: 108.303124\n",
      "..........\n",
      "[1,  1100] loss: 107.626932\n",
      "..........\n",
      "[1,  1200] loss: 107.273401\n",
      "..........\n",
      "[1,  1300] loss: 106.609552\n",
      "..........\n",
      "[1,  1400] loss: 107.942836\n",
      "..........\n",
      "[1,  1500] loss: 108.069145\n",
      "..........\n",
      "[1,  1600] loss: 107.833032\n",
      "..........\n",
      "[1,  1700] loss: 107.484464\n",
      "..........\n",
      "[1,  1800] loss: 107.016608\n",
      "..........\n",
      "[1,  1900] loss: 109.685321\n",
      "..........\n",
      "[1,  2000] loss: 107.731408\n",
      "..........\n",
      "[1,  2100] loss: 106.201294\n",
      "..........\n",
      "[1,  2200] loss: 108.506756\n",
      "..........\n",
      "[1,  2300] loss: 109.501604\n",
      "..........\n",
      "[1,  2400] loss: 108.392748\n",
      "..........\n",
      "[1,  2500] loss: 105.989971\n",
      "..........\n",
      "[1,  2600] loss: 108.297393\n",
      "..........\n",
      "[1,  2700] loss: 110.684364\n",
      "..........\n",
      "[1,  2800] loss: 109.221008\n",
      "..........\n",
      "[1,  2900] loss: 109.206787\n",
      "..........\n",
      "[1,  3000] loss: 106.937201\n",
      "..........\n",
      "[1,  3100] loss: 108.157724\n",
      "..........\n",
      "[1,  3200] loss: 110.039874\n",
      "..........\n",
      "[1,  3300] loss: 108.079192\n",
      "..........\n",
      "[1,  3400] loss: 108.119214\n",
      "..........\n",
      "[1,  3500] loss: 107.713536\n",
      "..........\n",
      "[1,  3600] loss: 109.075521\n",
      "..........\n",
      "[1,  3700] loss: 107.026319\n",
      "..........\n",
      "[1,  3800] loss: 110.540334\n",
      "..........\n",
      "[1,  3900] loss: 107.027862\n",
      "..........\n",
      "[1,  4000] loss: 108.138867\n",
      "..........\n",
      "[1,  4100] loss: 106.590842\n",
      "..........\n",
      "[1,  4200] loss: 105.647612\n",
      "..........\n",
      "[1,  4300] loss: 107.511657\n",
      "..........\n",
      "[1,  4400] loss: 108.113114\n",
      "..........\n",
      "[1,  4500] loss: 108.821228\n",
      "..........\n",
      "[1,  4600] loss: 108.227507\n",
      "..........\n",
      "[1,  4700] loss: 105.957564\n",
      "..........\n",
      "[1,  4800] loss: 107.719491\n",
      "..........\n",
      "[1,  4900] loss: 110.744026\n",
      "..........\n",
      "[1,  5000] loss: 108.301508\n",
      "..........\n",
      "[1,  5100] loss: 107.975404\n",
      "..........\n",
      "[1,  5200] loss: 109.677618\n",
      "..........\n",
      "[1,  5300] loss: 108.704548\n",
      "..........\n",
      "[1,  5400] loss: 107.843210\n",
      "..........\n",
      "[1,  5500] loss: 109.446758\n",
      "..........\n",
      "[1,  5600] loss: 108.946958\n",
      "..........\n",
      "[1,  5700] loss: 109.339798\n",
      "..........\n",
      "[1,  5800] loss: 108.204984\n",
      "..........\n",
      "[1,  5900] loss: 108.957089\n",
      "..........\n",
      "[1,  6000] loss: 107.250323\n",
      "..........\n",
      "[1,  6100] loss: 108.205625\n",
      "..........\n",
      "[1,  6200] loss: 109.049385\n",
      "..........\n",
      "[1,  6300] loss: 111.194018\n",
      "..........\n",
      "[1,  6400] loss: 107.897030\n",
      "..........\n",
      "[1,  6500] loss: 108.779210\n",
      "..........\n",
      "[1,  6600] loss: 107.323828\n",
      "..........\n",
      "[1,  6700] loss: 108.645794\n",
      "..........\n",
      "[1,  6800] loss: 109.902486\n",
      "..........\n",
      "[1,  6900] loss: 109.845075\n",
      "..........\n",
      "[1,  7000] loss: 108.729846\n",
      "..........\n",
      "[1,  7100] loss: 108.688222\n",
      "..........\n",
      "[1,  7200] loss: 108.552232\n",
      "..........\n",
      "[1,  7300] loss: 109.115798\n",
      "..........\n",
      "[1,  7400] loss: 108.868708\n",
      "..........\n",
      "[1,  7500] loss: 110.763586\n",
      "..........\n",
      "[1,  7600] loss: 109.181353\n",
      "..........\n",
      "[1,  7700] loss: 109.656409\n",
      "..........\n",
      "[1,  7800] loss: 110.797989\n",
      "..........\n",
      "[1,  7900] loss: 110.697799\n",
      "..........\n",
      "[1,  8000] loss: 110.210044\n",
      "total average loss : 108.341\n",
      "train acc : 0.0945\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.078\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.098\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.109\n",
      "step : 2200 / 3125 acc : 0.108\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.106\n",
      "step : 2800 / 3125 acc : 0.107\n",
      "step : 3000 / 3125 acc : 0.104\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1020 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 110.828975\n",
      "..........\n",
      "[1,   200] loss: 111.038604\n",
      "..........\n",
      "[1,   300] loss: 111.001245\n",
      "..........\n",
      "[1,   400] loss: 111.391990\n",
      "..........\n",
      "[1,   500] loss: 110.901307\n",
      "..........\n",
      "[1,   600] loss: 111.233843\n",
      "..........\n",
      "[1,   700] loss: 111.498600\n",
      "..........\n",
      "[1,   800] loss: 108.875360\n",
      "..........\n",
      "[1,   900] loss: 110.834952\n",
      "..........\n",
      "[1,  1000] loss: 111.122553\n",
      "..........\n",
      "[1,  1100] loss: 109.562794\n",
      "..........\n",
      "[1,  1200] loss: 110.282657\n",
      "..........\n",
      "[1,  1300] loss: 112.046636\n",
      "..........\n",
      "[1,  1400] loss: 109.014240\n",
      "..........\n",
      "[1,  1500] loss: 109.714125\n",
      "..........\n",
      "[1,  1600] loss: 109.632017\n",
      "..........\n",
      "[1,  1700] loss: 111.150971\n",
      "..........\n",
      "[1,  1800] loss: 108.993276\n",
      "..........\n",
      "[1,  1900] loss: 111.191133\n",
      "..........\n",
      "[1,  2000] loss: 110.811990\n",
      "..........\n",
      "[1,  2100] loss: 111.498158\n",
      "..........\n",
      "[1,  2200] loss: 110.776590\n",
      "..........\n",
      "[1,  2300] loss: 108.326574\n",
      "..........\n",
      "[1,  2400] loss: 111.151039\n",
      "..........\n",
      "[1,  2500] loss: 111.795271\n",
      "..........\n",
      "[1,  2600] loss: 111.776081\n",
      "..........\n",
      "[1,  2700] loss: 111.124499\n",
      "..........\n",
      "[1,  2800] loss: 111.839109\n",
      "..........\n",
      "[1,  2900] loss: 110.749597\n",
      "..........\n",
      "[1,  3000] loss: 111.112323\n",
      "..........\n",
      "[1,  3100] loss: 108.696712\n",
      "..........\n",
      "[1,  3200] loss: 112.255314\n",
      "..........\n",
      "[1,  3300] loss: 110.647790\n",
      "..........\n",
      "[1,  3400] loss: 110.342442\n",
      "..........\n",
      "[1,  3500] loss: 109.018265\n",
      "..........\n",
      "[1,  3600] loss: 109.752690\n",
      "..........\n",
      "[1,  3700] loss: 111.138554\n",
      "..........\n",
      "[1,  3800] loss: 112.738554\n",
      "..........\n",
      "[1,  3900] loss: 109.868822\n",
      "..........\n",
      "[1,  4000] loss: 109.511368\n",
      "..........\n",
      "[1,  4100] loss: 111.574111\n",
      "..........\n",
      "[1,  4200] loss: 110.754298\n",
      "..........\n",
      "[1,  4300] loss: 110.394667\n",
      "..........\n",
      "[1,  4400] loss: 110.548041\n",
      "..........\n",
      "[1,  4500] loss: 110.760705\n",
      "..........\n",
      "[1,  4600] loss: 111.838662\n",
      "..........\n",
      "[1,  4700] loss: 112.517431\n",
      "..........\n",
      "[1,  4800] loss: 113.253226\n",
      "..........\n",
      "[1,  4900] loss: 110.933532\n",
      "..........\n",
      "[1,  5000] loss: 111.273899\n",
      "..........\n",
      "[1,  5100] loss: 111.623579\n",
      "..........\n",
      "[1,  5200] loss: 112.210380\n",
      "..........\n",
      "[1,  5300] loss: 110.578205\n",
      "..........\n",
      "[1,  5400] loss: 110.229850\n",
      "..........\n",
      "[1,  5500] loss: 112.508255\n",
      "..........\n",
      "[1,  5600] loss: 111.569643\n",
      "..........\n",
      "[1,  5700] loss: 112.196962\n",
      "..........\n",
      "[1,  5800] loss: 111.220681\n",
      "..........\n",
      "[1,  5900] loss: 113.480270\n",
      "..........\n",
      "[1,  6000] loss: 112.317961\n",
      "..........\n",
      "[1,  6100] loss: 111.612126\n",
      "..........\n",
      "[1,  6200] loss: 111.932633\n",
      "..........\n",
      "[1,  6300] loss: 110.674945\n",
      "..........\n",
      "[1,  6400] loss: 110.089920\n",
      "..........\n",
      "[1,  6500] loss: 112.140241\n",
      "..........\n",
      "[1,  6600] loss: 109.503499\n",
      "..........\n",
      "[1,  6700] loss: 113.689530\n",
      "..........\n",
      "[1,  6800] loss: 112.925045\n",
      "..........\n",
      "[1,  6900] loss: 110.877259\n",
      "..........\n",
      "[1,  7000] loss: 113.756036\n",
      "..........\n",
      "[1,  7100] loss: 112.658193\n",
      "..........\n",
      "[1,  7200] loss: 113.170526\n",
      "..........\n",
      "[1,  7300] loss: 111.965970\n",
      "..........\n",
      "[1,  7400] loss: 113.570964\n",
      "..........\n",
      "[1,  7500] loss: 114.055624\n",
      "..........\n",
      "[1,  7600] loss: 110.970807\n",
      "..........\n",
      "[1,  7700] loss: 112.719372\n",
      "..........\n",
      "[1,  7800] loss: 112.300500\n",
      "..........\n",
      "[1,  7900] loss: 112.558104\n",
      "..........\n",
      "[1,  8000] loss: 110.332375\n",
      "total average loss : 111.232\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.188\n",
      "step : 400 / 3125 acc : 0.188\n",
      "step : 600 / 3125 acc : 0.177\n",
      "step : 800 / 3125 acc : 0.141\n",
      "step : 1000 / 3125 acc : 0.156\n",
      "step : 1200 / 3125 acc : 0.151\n",
      "step : 1400 / 3125 acc : 0.134\n",
      "step : 1600 / 3125 acc : 0.133\n",
      "step : 1800 / 3125 acc : 0.125\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.096\n",
      "step : 2600 / 3125 acc : 0.099\n",
      "step : 2800 / 3125 acc : 0.105\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1040 %\n",
      "======eval  end ======\n",
      "======= epoch  9 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 111.941662\n",
      "..........\n",
      "[1,   200] loss: 113.136784\n",
      "..........\n",
      "[1,   300] loss: 112.336921\n",
      "..........\n",
      "[1,   400] loss: 115.027624\n",
      "..........\n",
      "[1,   500] loss: 114.240658\n",
      "..........\n",
      "[1,   600] loss: 112.256591\n",
      "..........\n",
      "[1,   700] loss: 113.603181\n",
      "..........\n",
      "[1,   800] loss: 113.279311\n",
      "..........\n",
      "[1,   900] loss: 111.237862\n",
      "..........\n",
      "[1,  1000] loss: 114.110775\n",
      "..........\n",
      "[1,  1100] loss: 113.340329\n",
      "..........\n",
      "[1,  1200] loss: 115.381500\n",
      "..........\n",
      "[1,  1300] loss: 113.813139\n",
      "..........\n",
      "[1,  1400] loss: 111.475710\n",
      "..........\n",
      "[1,  1500] loss: 113.566053\n",
      "..........\n",
      "[1,  1600] loss: 113.698641\n",
      "..........\n",
      "[1,  1700] loss: 114.146279\n",
      "..........\n",
      "[1,  1800] loss: 111.987009\n",
      "..........\n",
      "[1,  1900] loss: 112.997669\n",
      "..........\n",
      "[1,  2000] loss: 112.295213\n",
      "..........\n",
      "[1,  2100] loss: 115.294686\n",
      "..........\n",
      "[1,  2200] loss: 113.980759\n",
      "..........\n",
      "[1,  2300] loss: 113.510982\n",
      "..........\n",
      "[1,  2400] loss: 113.924271\n",
      "..........\n",
      "[1,  2500] loss: 112.111296\n",
      "..........\n",
      "[1,  2600] loss: 112.490558\n",
      "..........\n",
      "[1,  2700] loss: 111.864002\n",
      "..........\n",
      "[1,  2800] loss: 111.578127\n",
      "..........\n",
      "[1,  2900] loss: 111.960996\n",
      "..........\n",
      "[1,  3000] loss: 114.108904\n",
      "..........\n",
      "[1,  3100] loss: 113.283272\n",
      "..........\n",
      "[1,  3200] loss: 113.324849\n",
      "..........\n",
      "[1,  3300] loss: 114.488450\n",
      "..........\n",
      "[1,  3400] loss: 115.536987\n",
      "..........\n",
      "[1,  3500] loss: 114.763131\n",
      "..........\n",
      "[1,  3600] loss: 113.197907\n",
      "..........\n",
      "[1,  3700] loss: 112.573104\n",
      "..........\n",
      "[1,  3800] loss: 114.611533\n",
      "..........\n",
      "[1,  3900] loss: 114.759803\n",
      "..........\n",
      "[1,  4000] loss: 117.062572\n",
      "..........\n",
      "[1,  4100] loss: 115.657593\n",
      "..........\n",
      "[1,  4200] loss: 113.213715\n",
      "..........\n",
      "[1,  4300] loss: 115.282089\n",
      "..........\n",
      "[1,  4400] loss: 114.250377\n",
      "..........\n",
      "[1,  4500] loss: 114.566053\n",
      "..........\n",
      "[1,  4600] loss: 113.787633\n",
      "..........\n",
      "[1,  4700] loss: 113.965183\n",
      "..........\n",
      "[1,  4800] loss: 114.220066\n",
      "..........\n",
      "[1,  4900] loss: 114.302180\n",
      "..........\n",
      "[1,  5000] loss: 114.195437\n",
      "..........\n",
      "[1,  5100] loss: 114.155904\n",
      "..........\n",
      "[1,  5200] loss: 114.707016\n",
      "..........\n",
      "[1,  5300] loss: 114.006222\n",
      "..........\n",
      "[1,  5400] loss: 114.821827\n",
      "..........\n",
      "[1,  5500] loss: 114.748865\n",
      "..........\n",
      "[1,  5600] loss: 114.341571\n",
      "..........\n",
      "[1,  5700] loss: 115.551892\n",
      "..........\n",
      "[1,  5800] loss: 116.151616\n",
      "..........\n",
      "[1,  5900] loss: 116.457312\n",
      "..........\n",
      "[1,  6000] loss: 114.101086\n",
      "..........\n",
      "[1,  6100] loss: 111.386464\n",
      "..........\n",
      "[1,  6200] loss: 113.378115\n",
      "..........\n",
      "[1,  6300] loss: 114.772857\n",
      "..........\n",
      "[1,  6400] loss: 114.297150\n",
      "..........\n",
      "[1,  6500] loss: 117.365118\n",
      "..........\n",
      "[1,  6600] loss: 115.214478\n",
      "..........\n",
      "[1,  6700] loss: 114.199066\n",
      "..........\n",
      "[1,  6800] loss: 112.300940\n",
      "..........\n",
      "[1,  6900] loss: 115.783990\n",
      "..........\n",
      "[1,  7000] loss: 115.064595\n",
      "..........\n",
      "[1,  7100] loss: 114.877263\n",
      "..........\n",
      "[1,  7200] loss: 114.566072\n",
      "..........\n",
      "[1,  7300] loss: 113.933381\n",
      "..........\n",
      "[1,  7400] loss: 113.270448\n",
      "..........\n",
      "[1,  7500] loss: 115.881894\n",
      "..........\n",
      "[1,  7600] loss: 113.645630\n",
      "..........\n",
      "[1,  7700] loss: 115.713815\n",
      "..........\n",
      "[1,  7800] loss: 114.619127\n",
      "..........\n",
      "[1,  7900] loss: 113.946286\n",
      "..........\n",
      "[1,  8000] loss: 115.254572\n",
      "total average loss : 114.003\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.116\n",
      "step : 2400 / 3125 acc : 0.107\n",
      "step : 2600 / 3125 acc : 0.101\n",
      "step : 2800 / 3125 acc : 0.100\n",
      "step : 3000 / 3125 acc : 0.096\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0980 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 115.692359\n",
      "..........\n",
      "[1,   200] loss: 116.047815\n",
      "..........\n",
      "[1,   300] loss: 116.694853\n",
      "..........\n",
      "[1,   400] loss: 114.233654\n",
      "..........\n",
      "[1,   500] loss: 116.794299\n",
      "..........\n",
      "[1,   600] loss: 115.293344\n",
      "..........\n",
      "[1,   700] loss: 114.209721\n",
      "..........\n",
      "[1,   800] loss: 116.111697\n",
      "..........\n",
      "[1,   900] loss: 116.399086\n",
      "..........\n",
      "[1,  1000] loss: 115.705094\n",
      "..........\n",
      "[1,  1100] loss: 117.082368\n",
      "..........\n",
      "[1,  1200] loss: 116.050160\n",
      "..........\n",
      "[1,  1300] loss: 115.790172\n",
      "..........\n",
      "[1,  1400] loss: 117.294575\n",
      "..........\n",
      "[1,  1500] loss: 116.102135\n",
      "..........\n",
      "[1,  1600] loss: 116.359762\n",
      "..........\n",
      "[1,  1700] loss: 115.695285\n",
      "..........\n",
      "[1,  1800] loss: 116.584286\n",
      "..........\n",
      "[1,  1900] loss: 114.609188\n",
      "..........\n",
      "[1,  2000] loss: 116.885676\n",
      "..........\n",
      "[1,  2100] loss: 113.429540\n",
      "..........\n",
      "[1,  2200] loss: 117.112003\n",
      "..........\n",
      "[1,  2300] loss: 116.302130\n",
      "..........\n",
      "[1,  2400] loss: 115.018622\n",
      "..........\n",
      "[1,  2500] loss: 115.046235\n",
      "..........\n",
      "[1,  2600] loss: 116.010615\n",
      "..........\n",
      "[1,  2700] loss: 115.998821\n",
      "..........\n",
      "[1,  2800] loss: 115.422826\n",
      "..........\n",
      "[1,  2900] loss: 116.611346\n",
      "..........\n",
      "[1,  3000] loss: 116.283327\n",
      "..........\n",
      "[1,  3100] loss: 118.135055\n",
      "..........\n",
      "[1,  3200] loss: 116.903337\n",
      "..........\n",
      "[1,  3300] loss: 116.180489\n",
      "..........\n",
      "[1,  3400] loss: 116.298012\n",
      "..........\n",
      "[1,  3500] loss: 117.325112\n",
      "..........\n",
      "[1,  3600] loss: 114.200371\n",
      "..........\n",
      "[1,  3700] loss: 114.955439\n",
      "..........\n",
      "[1,  3800] loss: 115.877907\n",
      "..........\n",
      "[1,  3900] loss: 114.840564\n",
      "..........\n",
      "[1,  4000] loss: 116.417194\n",
      "..........\n",
      "[1,  4100] loss: 116.697536\n",
      "..........\n",
      "[1,  4200] loss: 117.756714\n",
      "..........\n",
      "[1,  4300] loss: 116.646577\n",
      "..........\n",
      "[1,  4400] loss: 117.066872\n",
      "..........\n",
      "[1,  4500] loss: 115.918923\n",
      "..........\n",
      "[1,  4600] loss: 115.785549\n",
      "..........\n",
      "[1,  4700] loss: 116.663465\n",
      "..........\n",
      "[1,  4800] loss: 116.672522\n",
      "..........\n",
      "[1,  4900] loss: 116.217791\n",
      "..........\n",
      "[1,  5000] loss: 117.445143\n",
      "..........\n",
      "[1,  5100] loss: 117.352530\n",
      "..........\n",
      "[1,  5200] loss: 117.389608\n",
      "..........\n",
      "[1,  5300] loss: 117.453570\n",
      "..........\n",
      "[1,  5400] loss: 116.900487\n",
      "..........\n",
      "[1,  5500] loss: 117.581158\n",
      "..........\n",
      "[1,  5600] loss: 117.512246\n",
      "..........\n",
      "[1,  5700] loss: 120.095162\n",
      "..........\n",
      "[1,  5800] loss: 118.218750\n",
      "..........\n",
      "[1,  5900] loss: 116.673215\n",
      "..........\n",
      "[1,  6000] loss: 116.521814\n",
      "..........\n",
      "[1,  6100] loss: 119.039270\n",
      "..........\n",
      "[1,  6200] loss: 118.494510\n",
      "..........\n",
      "[1,  6300] loss: 118.523248\n",
      "..........\n",
      "[1,  6400] loss: 117.644604\n",
      "..........\n",
      "[1,  6500] loss: 116.393131\n",
      "..........\n",
      "[1,  6600] loss: 116.606842\n",
      "..........\n",
      "[1,  6700] loss: 117.293521\n",
      "..........\n",
      "[1,  6800] loss: 114.338279\n",
      "..........\n",
      "[1,  6900] loss: 115.288595\n",
      "..........\n",
      "[1,  7000] loss: 116.407469\n",
      "..........\n",
      "[1,  7100] loss: 118.176863\n",
      "..........\n",
      "[1,  7200] loss: 116.507577\n",
      "..........\n",
      "[1,  7300] loss: 119.318461\n",
      "..........\n",
      "[1,  7400] loss: 119.036967\n",
      "..........\n",
      "[1,  7500] loss: 115.633592\n",
      "..........\n",
      "[1,  7600] loss: 115.685169\n",
      "..........\n",
      "[1,  7700] loss: 116.585623\n",
      "..........\n",
      "[1,  7800] loss: 117.683670\n",
      "..........\n",
      "[1,  7900] loss: 117.943697\n",
      "..........\n",
      "[1,  8000] loss: 115.957263\n",
      "total average loss : 116.539\n",
      "train acc : 0.0961\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.188\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.119\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.100\n",
      "step : 2200 / 3125 acc : 0.097\n",
      "step : 2400 / 3125 acc : 0.096\n",
      "step : 2600 / 3125 acc : 0.099\n",
      "step : 2800 / 3125 acc : 0.094\n",
      "step : 3000 / 3125 acc : 0.100\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0960 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 119.057426\n",
      "..........\n",
      "[1,   200] loss: 119.139504\n",
      "..........\n",
      "[1,   300] loss: 116.590453\n",
      "..........\n",
      "[1,   400] loss: 120.457898\n",
      "..........\n",
      "[1,   500] loss: 119.801947\n",
      "..........\n",
      "[1,   600] loss: 114.827243\n",
      "..........\n",
      "[1,   700] loss: 118.545018\n",
      "..........\n",
      "[1,   800] loss: 116.832502\n",
      "..........\n",
      "[1,   900] loss: 118.730777\n",
      "..........\n",
      "[1,  1000] loss: 119.153187\n",
      "..........\n",
      "[1,  1100] loss: 117.662977\n",
      "..........\n",
      "[1,  1200] loss: 118.139156\n",
      "..........\n",
      "[1,  1300] loss: 118.535774\n",
      "..........\n",
      "[1,  1400] loss: 118.004917\n",
      "..........\n",
      "[1,  1500] loss: 120.074396\n",
      "..........\n",
      "[1,  1600] loss: 118.670103\n",
      "..........\n",
      "[1,  1700] loss: 116.442165\n",
      "..........\n",
      "[1,  1800] loss: 118.796873\n",
      "..........\n",
      "[1,  1900] loss: 118.840638\n",
      "..........\n",
      "[1,  2000] loss: 118.898162\n",
      "..........\n",
      "[1,  2100] loss: 116.925874\n",
      "..........\n",
      "[1,  2200] loss: 119.446905\n",
      "..........\n",
      "[1,  2300] loss: 117.589786\n",
      "..........\n",
      "[1,  2400] loss: 118.940193\n",
      "..........\n",
      "[1,  2500] loss: 119.578251\n",
      "..........\n",
      "[1,  2600] loss: 115.982851\n",
      "..........\n",
      "[1,  2700] loss: 119.003078\n",
      "..........\n",
      "[1,  2800] loss: 116.841907\n",
      "..........\n",
      "[1,  2900] loss: 116.699248\n",
      "..........\n",
      "[1,  3000] loss: 118.710132\n",
      "..........\n",
      "[1,  3100] loss: 118.993046\n",
      "..........\n",
      "[1,  3200] loss: 122.626494\n",
      "..........\n",
      "[1,  3300] loss: 119.782474\n",
      "..........\n",
      "[1,  3400] loss: 117.724975\n",
      "..........\n",
      "[1,  3500] loss: 118.364850\n",
      "..........\n",
      "[1,  3600] loss: 119.455679\n",
      "..........\n",
      "[1,  3700] loss: 121.595984\n",
      "..........\n",
      "[1,  3800] loss: 117.627317\n",
      "..........\n",
      "[1,  3900] loss: 119.722381\n",
      "..........\n",
      "[1,  4000] loss: 118.547342\n",
      "..........\n",
      "[1,  4100] loss: 118.496907\n",
      "..........\n",
      "[1,  4200] loss: 118.267542\n",
      "..........\n",
      "[1,  4300] loss: 120.037741\n",
      "..........\n",
      "[1,  4400] loss: 117.595497\n",
      "..........\n",
      "[1,  4500] loss: 117.477286\n",
      "..........\n",
      "[1,  4600] loss: 118.945831\n",
      "..........\n",
      "[1,  4700] loss: 119.054609\n",
      "..........\n",
      "[1,  4800] loss: 117.722052\n",
      "..........\n",
      "[1,  4900] loss: 117.572231\n",
      "..........\n",
      "[1,  5000] loss: 120.366065\n",
      "..........\n",
      "[1,  5100] loss: 117.246408\n",
      "..........\n",
      "[1,  5200] loss: 119.279443\n",
      "..........\n",
      "[1,  5300] loss: 119.723028\n",
      "..........\n",
      "[1,  5400] loss: 118.943172\n",
      "..........\n",
      "[1,  5500] loss: 121.021550\n",
      "..........\n",
      "[1,  5600] loss: 119.707745\n",
      "..........\n",
      "[1,  5700] loss: 120.143323\n",
      "..........\n",
      "[1,  5800] loss: 119.569419\n",
      "..........\n",
      "[1,  5900] loss: 121.881198\n",
      "..........\n",
      "[1,  6000] loss: 119.510410\n",
      "..........\n",
      "[1,  6100] loss: 120.211392\n",
      "..........\n",
      "[1,  6200] loss: 118.074368\n",
      "..........\n",
      "[1,  6300] loss: 117.243647\n",
      "..........\n",
      "[1,  6400] loss: 119.444317\n",
      "..........\n",
      "[1,  6500] loss: 120.338656\n",
      "..........\n",
      "[1,  6600] loss: 121.437622\n",
      "..........\n",
      "[1,  6700] loss: 118.440194\n",
      "..........\n",
      "[1,  6800] loss: 118.310848\n",
      "..........\n",
      "[1,  6900] loss: 118.644038\n",
      "..........\n",
      "[1,  7000] loss: 120.242913\n",
      "..........\n",
      "[1,  7100] loss: 119.443476\n",
      "..........\n",
      "[1,  7200] loss: 120.996457\n",
      "..........\n",
      "[1,  7300] loss: 118.338105\n",
      "..........\n",
      "[1,  7400] loss: 119.139532\n",
      "..........\n",
      "[1,  7500] loss: 117.286452\n",
      "..........\n",
      "[1,  7600] loss: 119.042021\n",
      "..........\n",
      "[1,  7700] loss: 118.908891\n",
      "..........\n",
      "[1,  7800] loss: 117.669104\n",
      "..........\n",
      "[1,  7900] loss: 122.087162\n",
      "..........\n",
      "[1,  8000] loss: 119.271611\n",
      "total average loss : 118.856\n",
      "train acc : 0.0984\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.119\n",
      "step : 1200 / 3125 acc : 0.120\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.105\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.100\n",
      "step : 2200 / 3125 acc : 0.099\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.096\n",
      "step : 2800 / 3125 acc : 0.098\n",
      "step : 3000 / 3125 acc : 0.100\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 119.279142\n",
      "..........\n",
      "[1,   200] loss: 121.713708\n",
      "..........\n",
      "[1,   300] loss: 121.132066\n",
      "..........\n",
      "[1,   400] loss: 119.280842\n",
      "..........\n",
      "[1,   500] loss: 120.096368\n",
      "..........\n",
      "[1,   600] loss: 120.866881\n",
      "..........\n",
      "[1,   700] loss: 118.503418\n",
      "..........\n",
      "[1,   800] loss: 121.707327\n",
      "..........\n",
      "[1,   900] loss: 118.999180\n",
      "..........\n",
      "[1,  1000] loss: 122.496150\n",
      "..........\n",
      "[1,  1100] loss: 121.056285\n",
      "..........\n",
      "[1,  1200] loss: 119.918202\n",
      "..........\n",
      "[1,  1300] loss: 121.306984\n",
      "..........\n",
      "[1,  1400] loss: 118.532224\n",
      "..........\n",
      "[1,  1500] loss: 122.694662\n",
      "..........\n",
      "[1,  1600] loss: 120.563784\n",
      "..........\n",
      "[1,  1700] loss: 121.247123\n",
      "..........\n",
      "[1,  1800] loss: 119.809075\n",
      "..........\n",
      "[1,  1900] loss: 121.607945\n",
      "..........\n",
      "[1,  2000] loss: 120.964474\n",
      "..........\n",
      "[1,  2100] loss: 121.571992\n",
      "..........\n",
      "[1,  2200] loss: 120.597364\n",
      "..........\n",
      "[1,  2300] loss: 119.071089\n",
      "..........\n",
      "[1,  2400] loss: 120.332707\n",
      "..........\n",
      "[1,  2500] loss: 121.744615\n",
      "..........\n",
      "[1,  2600] loss: 122.703004\n",
      "..........\n",
      "[1,  2700] loss: 120.744526\n",
      "..........\n",
      "[1,  2800] loss: 121.398196\n",
      "..........\n",
      "[1,  2900] loss: 119.167139\n",
      "..........\n",
      "[1,  3000] loss: 121.143142\n",
      "..........\n",
      "[1,  3100] loss: 119.677027\n",
      "..........\n",
      "[1,  3200] loss: 120.223405\n",
      "..........\n",
      "[1,  3300] loss: 121.415302\n",
      "..........\n",
      "[1,  3400] loss: 120.964921\n",
      "..........\n",
      "[1,  3500] loss: 122.114060\n",
      "..........\n",
      "[1,  3600] loss: 121.618417\n",
      "..........\n",
      "[1,  3700] loss: 120.560082\n",
      "..........\n",
      "[1,  3800] loss: 121.547635\n",
      "..........\n",
      "[1,  3900] loss: 121.343635\n",
      "..........\n",
      "[1,  4000] loss: 123.310898\n",
      "..........\n",
      "[1,  4100] loss: 121.298715\n",
      "..........\n",
      "[1,  4200] loss: 120.109393\n",
      "..........\n",
      "[1,  4300] loss: 121.857922\n",
      "..........\n",
      "[1,  4400] loss: 121.368036\n",
      "..........\n",
      "[1,  4500] loss: 121.695187\n",
      "..........\n",
      "[1,  4600] loss: 119.798426\n",
      "..........\n",
      "[1,  4700] loss: 121.191626\n",
      "..........\n",
      "[1,  4800] loss: 121.203053\n",
      "..........\n",
      "[1,  4900] loss: 121.724234\n",
      "..........\n",
      "[1,  5000] loss: 121.096746\n",
      "..........\n",
      "[1,  5100] loss: 121.412670\n",
      "..........\n",
      "[1,  5200] loss: 119.911647\n",
      "..........\n",
      "[1,  5300] loss: 122.085185\n",
      "..........\n",
      "[1,  5400] loss: 121.735594\n",
      "..........\n",
      "[1,  5500] loss: 121.882610\n",
      "..........\n",
      "[1,  5600] loss: 120.015680\n",
      "..........\n",
      "[1,  5700] loss: 118.959752\n",
      "..........\n",
      "[1,  5800] loss: 121.894481\n",
      "..........\n",
      "[1,  5900] loss: 120.984893\n",
      "..........\n",
      "[1,  6000] loss: 120.087495\n",
      "..........\n",
      "[1,  6100] loss: 121.679787\n",
      "..........\n",
      "[1,  6200] loss: 120.955692\n",
      "..........\n",
      "[1,  6300] loss: 120.906782\n",
      "..........\n",
      "[1,  6400] loss: 121.685910\n",
      "..........\n",
      "[1,  6500] loss: 122.131353\n",
      "..........\n",
      "[1,  6600] loss: 124.067564\n",
      "..........\n",
      "[1,  6700] loss: 122.736691\n",
      "..........\n",
      "[1,  6800] loss: 122.912178\n",
      "..........\n",
      "[1,  6900] loss: 122.345595\n",
      "..........\n",
      "[1,  7000] loss: 122.092362\n",
      "..........\n",
      "[1,  7100] loss: 122.475735\n",
      "..........\n",
      "[1,  7200] loss: 122.540220\n",
      "..........\n",
      "[1,  7300] loss: 122.071649\n",
      "..........\n",
      "[1,  7400] loss: 121.210292\n",
      "..........\n",
      "[1,  7500] loss: 121.985067\n",
      "..........\n",
      "[1,  7600] loss: 119.757189\n",
      "..........\n",
      "[1,  7700] loss: 120.586570\n",
      "..........\n",
      "[1,  7800] loss: 119.300610\n",
      "..........\n",
      "[1,  7900] loss: 122.128254\n",
      "..........\n",
      "[1,  8000] loss: 120.626514\n",
      "total average loss : 121.094\n",
      "train acc : 0.1000\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.047\n",
      "step : 600 / 3125 acc : 0.052\n",
      "step : 800 / 3125 acc : 0.070\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.083\n",
      "step : 1400 / 3125 acc : 0.094\n",
      "step : 1600 / 3125 acc : 0.098\n",
      "step : 1800 / 3125 acc : 0.094\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.091\n",
      "step : 2400 / 3125 acc : 0.094\n",
      "step : 2600 / 3125 acc : 0.091\n",
      "step : 2800 / 3125 acc : 0.100\n",
      "step : 3000 / 3125 acc : 0.098\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1020 %\n",
      "======eval  end ======\n",
      "======= epoch 10 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 122.485884\n",
      "..........\n",
      "[1,   200] loss: 122.838746\n",
      "..........\n",
      "[1,   300] loss: 122.771984\n",
      "..........\n",
      "[1,   400] loss: 121.590912\n",
      "..........\n",
      "[1,   500] loss: 121.688774\n",
      "..........\n",
      "[1,   600] loss: 122.386434\n",
      "..........\n",
      "[1,   700] loss: 123.705726\n",
      "..........\n",
      "[1,   800] loss: 122.953734\n",
      "..........\n",
      "[1,   900] loss: 122.713052\n",
      "..........\n",
      "[1,  1000] loss: 122.479244\n",
      "..........\n",
      "[1,  1100] loss: 124.037882\n",
      "..........\n",
      "[1,  1200] loss: 122.370176\n",
      "..........\n",
      "[1,  1300] loss: 124.078131\n",
      "..........\n",
      "[1,  1400] loss: 123.614144\n",
      "..........\n",
      "[1,  1500] loss: 122.488705\n",
      "..........\n",
      "[1,  1600] loss: 123.540806\n",
      "..........\n",
      "[1,  1700] loss: 120.788887\n",
      "..........\n",
      "[1,  1800] loss: 123.033154\n",
      "..........\n",
      "[1,  1900] loss: 123.813944\n",
      "..........\n",
      "[1,  2000] loss: 122.208143\n",
      "..........\n",
      "[1,  2100] loss: 124.240645\n",
      "..........\n",
      "[1,  2200] loss: 122.942313\n",
      "..........\n",
      "[1,  2300] loss: 123.074781\n",
      "..........\n",
      "[1,  2400] loss: 123.946436\n",
      "..........\n",
      "[1,  2500] loss: 120.774333\n",
      "..........\n",
      "[1,  2600] loss: 120.088004\n",
      "..........\n",
      "[1,  2700] loss: 126.105527\n",
      "..........\n",
      "[1,  2800] loss: 124.398947\n",
      "..........\n",
      "[1,  2900] loss: 121.724886\n",
      "..........\n",
      "[1,  3000] loss: 122.379839\n",
      "..........\n",
      "[1,  3100] loss: 126.675992\n",
      "..........\n",
      "[1,  3200] loss: 123.554394\n",
      "..........\n",
      "[1,  3300] loss: 121.314755\n",
      "..........\n",
      "[1,  3400] loss: 123.075060\n",
      "..........\n",
      "[1,  3500] loss: 122.598108\n",
      "..........\n",
      "[1,  3600] loss: 124.923282\n",
      "..........\n",
      "[1,  3700] loss: 125.133264\n",
      "..........\n",
      "[1,  3800] loss: 125.509404\n",
      "..........\n",
      "[1,  3900] loss: 120.715105\n",
      "..........\n",
      "[1,  4000] loss: 124.742466\n",
      "..........\n",
      "[1,  4100] loss: 122.693184\n",
      "..........\n",
      "[1,  4200] loss: 121.269430\n",
      "..........\n",
      "[1,  4300] loss: 123.681232\n",
      "..........\n",
      "[1,  4400] loss: 124.605020\n",
      "..........\n",
      "[1,  4500] loss: 122.150910\n",
      "..........\n",
      "[1,  4600] loss: 124.025415\n",
      "..........\n",
      "[1,  4700] loss: 124.440538\n",
      "..........\n",
      "[1,  4800] loss: 122.004948\n",
      "..........\n",
      "[1,  4900] loss: 123.250538\n",
      "..........\n",
      "[1,  5000] loss: 122.159565\n",
      "..........\n",
      "[1,  5100] loss: 122.050294\n",
      "..........\n",
      "[1,  5200] loss: 123.750457\n",
      "..........\n",
      "[1,  5300] loss: 123.126505\n",
      "..........\n",
      "[1,  5400] loss: 124.082502\n",
      "..........\n",
      "[1,  5500] loss: 122.915741\n",
      "..........\n",
      "[1,  5600] loss: 122.331405\n",
      "..........\n",
      "[1,  5700] loss: 123.114287\n",
      "..........\n",
      "[1,  5800] loss: 121.891313\n",
      "..........\n",
      "[1,  5900] loss: 122.739857\n",
      "..........\n",
      "[1,  6000] loss: 122.610307\n",
      "..........\n",
      "[1,  6100] loss: 124.782367\n",
      "..........\n",
      "[1,  6200] loss: 122.674193\n",
      "..........\n",
      "[1,  6300] loss: 121.764176\n",
      "..........\n",
      "[1,  6400] loss: 122.843378\n",
      "..........\n",
      "[1,  6500] loss: 122.592103\n",
      "..........\n",
      "[1,  6600] loss: 123.210130\n",
      "..........\n",
      "[1,  6700] loss: 124.025437\n",
      "..........\n",
      "[1,  6800] loss: 124.929648\n",
      "..........\n",
      "[1,  6900] loss: 125.391736\n",
      "..........\n",
      "[1,  7000] loss: 126.017549\n",
      "..........\n",
      "[1,  7100] loss: 123.326613\n",
      "..........\n",
      "[1,  7200] loss: 127.062294\n",
      "..........\n",
      "[1,  7300] loss: 123.479876\n",
      "..........\n",
      "[1,  7400] loss: 123.457878\n",
      "..........\n",
      "[1,  7500] loss: 124.151086\n",
      "..........\n",
      "[1,  7600] loss: 122.977573\n",
      "..........\n",
      "[1,  7700] loss: 124.877333\n",
      "..........\n",
      "[1,  7800] loss: 124.999846\n",
      "..........\n",
      "[1,  7900] loss: 123.571164\n",
      "..........\n",
      "[1,  8000] loss: 125.131740\n",
      "total average loss : 123.296\n",
      "train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.098\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.104\n",
      "step : 2000 / 3125 acc : 0.103\n",
      "step : 2200 / 3125 acc : 0.102\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.101\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.104\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1040 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 123.383552\n",
      "..........\n",
      "[1,   200] loss: 123.525722\n",
      "..........\n",
      "[1,   300] loss: 125.441413\n",
      "..........\n",
      "[1,   400] loss: 124.257303\n",
      "..........\n",
      "[1,   500] loss: 124.026492\n",
      "..........\n",
      "[1,   600] loss: 125.750962\n",
      "..........\n",
      "[1,   700] loss: 124.639581\n",
      "..........\n",
      "[1,   800] loss: 126.204252\n",
      "..........\n",
      "[1,   900] loss: 123.616984\n",
      "..........\n",
      "[1,  1000] loss: 126.139341\n",
      "..........\n",
      "[1,  1100] loss: 124.693190\n",
      "..........\n",
      "[1,  1200] loss: 124.134342\n",
      "..........\n",
      "[1,  1300] loss: 124.706635\n",
      "..........\n",
      "[1,  1400] loss: 124.196957\n",
      "..........\n",
      "[1,  1500] loss: 124.604024\n",
      "..........\n",
      "[1,  1600] loss: 126.537410\n",
      "..........\n",
      "[1,  1700] loss: 125.909851\n",
      "..........\n",
      "[1,  1800] loss: 123.893831\n",
      "..........\n",
      "[1,  1900] loss: 124.866657\n",
      "..........\n",
      "[1,  2000] loss: 124.341106\n",
      "..........\n",
      "[1,  2100] loss: 123.786889\n",
      "..........\n",
      "[1,  2200] loss: 124.803952\n",
      "..........\n",
      "[1,  2300] loss: 122.644711\n",
      "..........\n",
      "[1,  2400] loss: 126.214449\n",
      "..........\n",
      "[1,  2500] loss: 124.519749\n",
      "..........\n",
      "[1,  2600] loss: 125.523344\n",
      "..........\n",
      "[1,  2700] loss: 125.779391\n",
      "..........\n",
      "[1,  2800] loss: 124.998471\n",
      "..........\n",
      "[1,  2900] loss: 126.408164\n",
      "..........\n",
      "[1,  3000] loss: 124.275023\n",
      "..........\n",
      "[1,  3100] loss: 124.174342\n",
      "..........\n",
      "[1,  3200] loss: 125.231305\n",
      "..........\n",
      "[1,  3300] loss: 124.319650\n",
      "..........\n",
      "[1,  3400] loss: 126.102818\n",
      "..........\n",
      "[1,  3500] loss: 124.797010\n",
      "..........\n",
      "[1,  3600] loss: 125.646568\n",
      "..........\n",
      "[1,  3700] loss: 125.968684\n",
      "..........\n",
      "[1,  3800] loss: 123.876440\n",
      "..........\n",
      "[1,  3900] loss: 127.057660\n",
      "..........\n",
      "[1,  4000] loss: 124.868253\n",
      "..........\n",
      "[1,  4100] loss: 125.447807\n",
      "..........\n",
      "[1,  4200] loss: 126.107171\n",
      "..........\n",
      "[1,  4300] loss: 125.362668\n",
      "..........\n",
      "[1,  4400] loss: 125.319407\n",
      "..........\n",
      "[1,  4500] loss: 128.367069\n",
      "..........\n",
      "[1,  4600] loss: 126.925624\n",
      "..........\n",
      "[1,  4700] loss: 126.164007\n",
      "..........\n",
      "[1,  4800] loss: 125.129986\n",
      "..........\n",
      "[1,  4900] loss: 124.285947\n",
      "..........\n",
      "[1,  5000] loss: 125.717220\n",
      "..........\n",
      "[1,  5100] loss: 125.314212\n",
      "..........\n",
      "[1,  5200] loss: 124.432937\n",
      "..........\n",
      "[1,  5300] loss: 125.115966\n",
      "..........\n",
      "[1,  5400] loss: 125.227217\n",
      "..........\n",
      "[1,  5500] loss: 124.727959\n",
      "..........\n",
      "[1,  5600] loss: 124.899462\n",
      "..........\n",
      "[1,  5700] loss: 125.193688\n",
      "..........\n",
      "[1,  5800] loss: 125.710978\n",
      "..........\n",
      "[1,  5900] loss: 124.794872\n",
      "..........\n",
      "[1,  6000] loss: 124.619445\n",
      "..........\n",
      "[1,  6100] loss: 125.968864\n",
      "..........\n",
      "[1,  6200] loss: 125.395551\n",
      "..........\n",
      "[1,  6300] loss: 124.551506\n",
      "..........\n",
      "[1,  6400] loss: 123.938077\n",
      "..........\n",
      "[1,  6500] loss: 124.703248\n",
      "..........\n",
      "[1,  6600] loss: 127.019666\n",
      "..........\n",
      "[1,  6700] loss: 124.911982\n",
      "..........\n",
      "[1,  6800] loss: 126.634802\n",
      "..........\n",
      "[1,  6900] loss: 125.850470\n",
      "..........\n",
      "[1,  7000] loss: 126.331274\n",
      "..........\n",
      "[1,  7100] loss: 127.823805\n",
      "..........\n",
      "[1,  7200] loss: 126.190434\n",
      "..........\n",
      "[1,  7300] loss: 125.633629\n",
      "..........\n",
      "[1,  7400] loss: 127.861562\n",
      "..........\n",
      "[1,  7500] loss: 125.989251\n",
      "..........\n",
      "[1,  7600] loss: 127.145052\n",
      "..........\n",
      "[1,  7700] loss: 128.422714\n",
      "..........\n",
      "[1,  7800] loss: 126.899743\n",
      "..........\n",
      "[1,  7900] loss: 126.957591\n",
      "..........\n",
      "[1,  8000] loss: 126.094447\n",
      "total average loss : 125.363\n",
      "train acc : 0.0984\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.125\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.125\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.108\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.108\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.108\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 127.127353\n",
      "..........\n",
      "[1,   200] loss: 127.101246\n",
      "..........\n",
      "[1,   300] loss: 126.140667\n",
      "..........\n",
      "[1,   400] loss: 128.261302\n",
      "..........\n",
      "[1,   500] loss: 128.852601\n",
      "..........\n",
      "[1,   600] loss: 125.953285\n",
      "..........\n",
      "[1,   700] loss: 127.179646\n",
      "..........\n",
      "[1,   800] loss: 126.637937\n",
      "..........\n",
      "[1,   900] loss: 126.806248\n",
      "..........\n",
      "[1,  1000] loss: 126.135272\n",
      "..........\n",
      "[1,  1100] loss: 125.093679\n",
      "..........\n",
      "[1,  1200] loss: 128.475644\n",
      "..........\n",
      "[1,  1300] loss: 127.397144\n",
      "..........\n",
      "[1,  1400] loss: 125.582932\n",
      "..........\n",
      "[1,  1500] loss: 124.962251\n",
      "..........\n",
      "[1,  1600] loss: 127.601279\n",
      "..........\n",
      "[1,  1700] loss: 124.584184\n",
      "..........\n",
      "[1,  1800] loss: 128.314568\n",
      "..........\n",
      "[1,  1900] loss: 128.395542\n",
      "..........\n",
      "[1,  2000] loss: 128.655111\n",
      "..........\n",
      "[1,  2100] loss: 127.160233\n",
      "..........\n",
      "[1,  2200] loss: 124.974125\n",
      "..........\n",
      "[1,  2300] loss: 126.344347\n",
      "..........\n",
      "[1,  2400] loss: 126.749410\n",
      "..........\n",
      "[1,  2500] loss: 125.439744\n",
      "..........\n",
      "[1,  2600] loss: 125.921782\n",
      "..........\n",
      "[1,  2700] loss: 127.708761\n",
      "..........\n",
      "[1,  2800] loss: 127.178175\n",
      "..........\n",
      "[1,  2900] loss: 126.977339\n",
      "..........\n",
      "[1,  3000] loss: 126.605955\n",
      "..........\n",
      "[1,  3100] loss: 126.053480\n",
      "..........\n",
      "[1,  3200] loss: 126.480440\n",
      "..........\n",
      "[1,  3300] loss: 127.604448\n",
      "..........\n",
      "[1,  3400] loss: 127.381096\n",
      "..........\n",
      "[1,  3500] loss: 125.927995\n",
      "..........\n",
      "[1,  3600] loss: 127.606884\n",
      "..........\n",
      "[1,  3700] loss: 126.628881\n",
      "..........\n",
      "[1,  3800] loss: 126.793914\n",
      "..........\n",
      "[1,  3900] loss: 127.911410\n",
      "..........\n",
      "[1,  4000] loss: 128.468489\n",
      "..........\n",
      "[1,  4100] loss: 127.481612\n",
      "..........\n",
      "[1,  4200] loss: 128.189388\n",
      "..........\n",
      "[1,  4300] loss: 128.566561\n",
      "..........\n",
      "[1,  4400] loss: 125.891568\n",
      "..........\n",
      "[1,  4500] loss: 125.399031\n",
      "..........\n",
      "[1,  4600] loss: 125.693028\n",
      "..........\n",
      "[1,  4700] loss: 131.815555\n",
      "..........\n",
      "[1,  4800] loss: 125.090047\n",
      "..........\n",
      "[1,  4900] loss: 130.537327\n",
      "..........\n",
      "[1,  5000] loss: 128.603813\n",
      "..........\n",
      "[1,  5100] loss: 126.024289\n",
      "..........\n",
      "[1,  5200] loss: 127.802262\n",
      "..........\n",
      "[1,  5300] loss: 128.525510\n",
      "..........\n",
      "[1,  5400] loss: 126.527141\n",
      "..........\n",
      "[1,  5500] loss: 126.751469\n",
      "..........\n",
      "[1,  5600] loss: 128.148331\n",
      "..........\n",
      "[1,  5700] loss: 128.928448\n",
      "..........\n",
      "[1,  5800] loss: 127.685547\n",
      "..........\n",
      "[1,  5900] loss: 126.970008\n",
      "..........\n",
      "[1,  6000] loss: 127.705518\n",
      "..........\n",
      "[1,  6100] loss: 127.185633\n",
      "..........\n",
      "[1,  6200] loss: 126.722884\n",
      "..........\n",
      "[1,  6300] loss: 129.642718\n",
      "..........\n",
      "[1,  6400] loss: 126.586950\n",
      "..........\n",
      "[1,  6500] loss: 124.719750\n",
      "..........\n",
      "[1,  6600] loss: 128.328994\n",
      "..........\n",
      "[1,  6700] loss: 129.917001\n",
      "..........\n",
      "[1,  6800] loss: 125.295507\n",
      "..........\n",
      "[1,  6900] loss: 128.243905\n",
      "..........\n",
      "[1,  7000] loss: 128.111059\n",
      "..........\n",
      "[1,  7100] loss: 127.810755\n",
      "..........\n",
      "[1,  7200] loss: 127.674290\n",
      "..........\n",
      "[1,  7300] loss: 126.205393\n",
      "..........\n",
      "[1,  7400] loss: 128.646112\n",
      "..........\n",
      "[1,  7500] loss: 129.562527\n",
      "..........\n",
      "[1,  7600] loss: 128.326790\n",
      "..........\n",
      "[1,  7700] loss: 129.172909\n",
      "..........\n",
      "[1,  7800] loss: 129.945820\n",
      "..........\n",
      "[1,  7900] loss: 129.160764\n",
      "..........\n",
      "[1,  8000] loss: 127.530759\n",
      "total average loss : 127.329\n",
      "train acc : 0.0984\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.070\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.109\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.122\n",
      "step : 2000 / 3125 acc : 0.122\n",
      "step : 2200 / 3125 acc : 0.116\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.113\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 128.530002\n",
      "..........\n",
      "[1,   200] loss: 129.002399\n",
      "..........\n",
      "[1,   300] loss: 128.404767\n",
      "..........\n",
      "[1,   400] loss: 129.031543\n",
      "..........\n",
      "[1,   500] loss: 128.719790\n",
      "..........\n",
      "[1,   600] loss: 131.395979\n",
      "..........\n",
      "[1,   700] loss: 127.272729\n",
      "..........\n",
      "[1,   800] loss: 128.175564\n",
      "..........\n",
      "[1,   900] loss: 129.640893\n",
      "..........\n",
      "[1,  1000] loss: 126.968903\n",
      "..........\n",
      "[1,  1100] loss: 127.964025\n",
      "..........\n",
      "[1,  1200] loss: 132.136536\n",
      "..........\n",
      "[1,  1300] loss: 127.505408\n",
      "..........\n",
      "[1,  1400] loss: 127.017884\n",
      "..........\n",
      "[1,  1500] loss: 126.722071\n",
      "..........\n",
      "[1,  1600] loss: 126.748622\n",
      "..........\n",
      "[1,  1700] loss: 129.063070\n",
      "..........\n",
      "[1,  1800] loss: 129.192333\n",
      "..........\n",
      "[1,  1900] loss: 126.611042\n",
      "..........\n",
      "[1,  2000] loss: 129.661644\n",
      "..........\n",
      "[1,  2100] loss: 131.400785\n",
      "..........\n",
      "[1,  2200] loss: 129.612624\n",
      "..........\n",
      "[1,  2300] loss: 128.420561\n",
      "..........\n",
      "[1,  2400] loss: 129.559059\n",
      "..........\n",
      "[1,  2500] loss: 128.225149\n",
      "..........\n",
      "[1,  2600] loss: 127.080718\n",
      "..........\n",
      "[1,  2700] loss: 128.785513\n",
      "..........\n",
      "[1,  2800] loss: 129.957227\n",
      "..........\n",
      "[1,  2900] loss: 131.157738\n",
      "..........\n",
      "[1,  3000] loss: 129.985385\n",
      "..........\n",
      "[1,  3100] loss: 130.787079\n",
      "..........\n",
      "[1,  3200] loss: 129.701297\n",
      "..........\n",
      "[1,  3300] loss: 129.409117\n",
      "..........\n",
      "[1,  3400] loss: 130.130920\n",
      "..........\n",
      "[1,  3500] loss: 128.580926\n",
      "..........\n",
      "[1,  3600] loss: 130.949185\n",
      "..........\n",
      "[1,  3700] loss: 130.391839\n",
      "..........\n",
      "[1,  3800] loss: 132.253916\n",
      "..........\n",
      "[1,  3900] loss: 129.307397\n",
      "..........\n",
      "[1,  4000] loss: 128.258592\n",
      "..........\n",
      "[1,  4100] loss: 127.777992\n",
      "..........\n",
      "[1,  4200] loss: 129.831522\n",
      "..........\n",
      "[1,  4300] loss: 128.763203\n",
      "..........\n",
      "[1,  4400] loss: 127.660706\n",
      "..........\n",
      "[1,  4500] loss: 130.276594\n",
      "..........\n",
      "[1,  4600] loss: 128.145950\n",
      "..........\n",
      "[1,  4700] loss: 130.025435\n",
      "..........\n",
      "[1,  4800] loss: 130.245227\n",
      "..........\n",
      "[1,  4900] loss: 130.550249\n",
      "..........\n",
      "[1,  5000] loss: 131.814828\n",
      "..........\n",
      "[1,  5100] loss: 126.707930\n",
      "..........\n",
      "[1,  5200] loss: 130.935690\n",
      "..........\n",
      "[1,  5300] loss: 130.677969\n",
      "..........\n",
      "[1,  5400] loss: 127.691740\n",
      "..........\n",
      "[1,  5500] loss: 129.567137\n",
      "..........\n",
      "[1,  5600] loss: 130.631486\n",
      "..........\n",
      "[1,  5700] loss: 129.272781\n",
      "..........\n",
      "[1,  5800] loss: 131.241136\n",
      "..........\n",
      "[1,  5900] loss: 128.696571\n",
      "..........\n",
      "[1,  6000] loss: 129.989770\n",
      "..........\n",
      "[1,  6100] loss: 129.352580\n",
      "..........\n",
      "[1,  6200] loss: 129.500725\n",
      "..........\n",
      "[1,  6300] loss: 131.079497\n",
      "..........\n",
      "[1,  6400] loss: 129.050022\n",
      "..........\n",
      "[1,  6500] loss: 129.819430\n",
      "..........\n",
      "[1,  6600] loss: 127.628475\n",
      "..........\n",
      "[1,  6700] loss: 128.148352\n",
      "..........\n",
      "[1,  6800] loss: 130.656104\n",
      "..........\n",
      "[1,  6900] loss: 129.697460\n",
      "..........\n",
      "[1,  7000] loss: 129.049542\n",
      "..........\n",
      "[1,  7100] loss: 128.414816\n",
      "..........\n",
      "[1,  7200] loss: 128.664475\n",
      "..........\n",
      "[1,  7300] loss: 129.076473\n",
      "..........\n",
      "[1,  7400] loss: 128.820661\n",
      "..........\n",
      "[1,  7500] loss: 130.365201\n",
      "..........\n",
      "[1,  7600] loss: 128.810379\n",
      "..........\n",
      "[1,  7700] loss: 129.563422\n",
      "..........\n",
      "[1,  7800] loss: 129.294307\n",
      "..........\n",
      "[1,  7900] loss: 131.354835\n",
      "..........\n",
      "[1,  8000] loss: 130.198285\n",
      "total average loss : 129.285\n",
      "train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.119\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.105\n",
      "step : 1800 / 3125 acc : 0.111\n",
      "step : 2000 / 3125 acc : 0.109\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.106\n",
      "step : 2800 / 3125 acc : 0.112\n",
      "step : 3000 / 3125 acc : 0.115\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "======= epoch 11 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 130.981544\n",
      "..........\n",
      "[1,   200] loss: 129.974944\n",
      "..........\n",
      "[1,   300] loss: 132.260778\n",
      "..........\n",
      "[1,   400] loss: 128.764437\n",
      "..........\n",
      "[1,   500] loss: 130.901660\n",
      "..........\n",
      "[1,   600] loss: 131.591787\n",
      "..........\n",
      "[1,   700] loss: 131.395488\n",
      "..........\n",
      "[1,   800] loss: 130.918254\n",
      "..........\n",
      "[1,   900] loss: 131.610553\n",
      "..........\n",
      "[1,  1000] loss: 128.670375\n",
      "..........\n",
      "[1,  1100] loss: 128.818996\n",
      "..........\n",
      "[1,  1200] loss: 132.870688\n",
      "..........\n",
      "[1,  1300] loss: 131.666792\n",
      "..........\n",
      "[1,  1400] loss: 132.460720\n",
      "..........\n",
      "[1,  1500] loss: 131.415046\n",
      "..........\n",
      "[1,  1600] loss: 130.337727\n",
      "..........\n",
      "[1,  1700] loss: 131.701224\n",
      "..........\n",
      "[1,  1800] loss: 133.340525\n",
      "..........\n",
      "[1,  1900] loss: 130.621545\n",
      "..........\n",
      "[1,  2000] loss: 130.288454\n",
      "..........\n",
      "[1,  2100] loss: 129.254086\n",
      "..........\n",
      "[1,  2200] loss: 131.914082\n",
      "..........\n",
      "[1,  2300] loss: 132.234929\n",
      "..........\n",
      "[1,  2400] loss: 131.003675\n",
      "..........\n",
      "[1,  2500] loss: 128.270608\n",
      "..........\n",
      "[1,  2600] loss: 129.424316\n",
      "..........\n",
      "[1,  2700] loss: 132.346405\n",
      "..........\n",
      "[1,  2800] loss: 132.104226\n",
      "..........\n",
      "[1,  2900] loss: 131.828407\n",
      "..........\n",
      "[1,  3000] loss: 131.471201\n",
      "..........\n",
      "[1,  3100] loss: 131.711979\n",
      "..........\n",
      "[1,  3200] loss: 132.012959\n",
      "..........\n",
      "[1,  3300] loss: 131.316760\n",
      "..........\n",
      "[1,  3400] loss: 131.116765\n",
      "..........\n",
      "[1,  3500] loss: 131.445945\n",
      "..........\n",
      "[1,  3600] loss: 130.266051\n",
      "..........\n",
      "[1,  3700] loss: 131.819035\n",
      "..........\n",
      "[1,  3800] loss: 131.027187\n",
      "..........\n",
      "[1,  3900] loss: 131.365825\n",
      "..........\n",
      "[1,  4000] loss: 133.233518\n",
      "..........\n",
      "[1,  4100] loss: 132.439472\n",
      "..........\n",
      "[1,  4200] loss: 131.145114\n",
      "..........\n",
      "[1,  4300] loss: 129.713323\n",
      "..........\n",
      "[1,  4400] loss: 129.145065\n",
      "..........\n",
      "[1,  4500] loss: 132.849077\n",
      "..........\n",
      "[1,  4600] loss: 130.875450\n",
      "..........\n",
      "[1,  4700] loss: 131.893977\n",
      "..........\n",
      "[1,  4800] loss: 131.339473\n",
      "..........\n",
      "[1,  4900] loss: 130.192358\n",
      "..........\n",
      "[1,  5000] loss: 131.068474\n",
      "..........\n",
      "[1,  5100] loss: 131.260151\n",
      "..........\n",
      "[1,  5200] loss: 132.364685\n",
      "..........\n",
      "[1,  5300] loss: 132.778921\n",
      "..........\n",
      "[1,  5400] loss: 130.153948\n",
      "..........\n",
      "[1,  5500] loss: 132.819453\n",
      "..........\n",
      "[1,  5600] loss: 132.876161\n",
      "..........\n",
      "[1,  5700] loss: 134.603053\n",
      "..........\n",
      "[1,  5800] loss: 130.669965\n",
      "..........\n",
      "[1,  5900] loss: 131.628276\n",
      "..........\n",
      "[1,  6000] loss: 131.947365\n",
      "..........\n",
      "[1,  6100] loss: 131.681501\n",
      "..........\n",
      "[1,  6200] loss: 131.865871\n",
      "..........\n",
      "[1,  6300] loss: 132.593652\n",
      "..........\n",
      "[1,  6400] loss: 132.233495\n",
      "..........\n",
      "[1,  6500] loss: 131.416917\n",
      "..........\n",
      "[1,  6600] loss: 132.767680\n",
      "..........\n",
      "[1,  6700] loss: 130.202457\n",
      "..........\n",
      "[1,  6800] loss: 132.038501\n",
      "..........\n",
      "[1,  6900] loss: 129.357856\n",
      "..........\n",
      "[1,  7000] loss: 130.889132\n",
      "..........\n",
      "[1,  7100] loss: 134.536077\n",
      "..........\n",
      "[1,  7200] loss: 132.716543\n",
      "..........\n",
      "[1,  7300] loss: 131.369004\n",
      "..........\n",
      "[1,  7400] loss: 133.807541\n",
      "..........\n",
      "[1,  7500] loss: 129.283392\n",
      "..........\n",
      "[1,  7600] loss: 131.977599\n",
      "..........\n",
      "[1,  7700] loss: 132.233219\n",
      "..........\n",
      "[1,  7800] loss: 131.520173\n",
      "..........\n",
      "[1,  7900] loss: 130.527912\n",
      "..........\n",
      "[1,  8000] loss: 130.814925\n",
      "total average loss : 131.392\n",
      "train acc : 0.0977\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.120\n",
      "step : 1400 / 3125 acc : 0.134\n",
      "step : 1600 / 3125 acc : 0.129\n",
      "step : 1800 / 3125 acc : 0.128\n",
      "step : 2000 / 3125 acc : 0.128\n",
      "step : 2200 / 3125 acc : 0.131\n",
      "step : 2400 / 3125 acc : 0.130\n",
      "step : 2600 / 3125 acc : 0.125\n",
      "step : 2800 / 3125 acc : 0.121\n",
      "step : 3000 / 3125 acc : 0.119\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1200 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 131.507972\n",
      "..........\n",
      "[1,   200] loss: 131.186262\n",
      "..........\n",
      "[1,   300] loss: 135.406198\n",
      "..........\n",
      "[1,   400] loss: 132.701449\n",
      "..........\n",
      "[1,   500] loss: 132.171192\n",
      "..........\n",
      "[1,   600] loss: 133.131590\n",
      "..........\n",
      "[1,   700] loss: 130.901459\n",
      "..........\n",
      "[1,   800] loss: 130.269897\n",
      "..........\n",
      "[1,   900] loss: 133.507441\n",
      "..........\n",
      "[1,  1000] loss: 132.960231\n",
      "..........\n",
      "[1,  1100] loss: 131.540448\n",
      "..........\n",
      "[1,  1200] loss: 130.860791\n",
      "..........\n",
      "[1,  1300] loss: 132.017145\n",
      "..........\n",
      "[1,  1400] loss: 133.271418\n",
      "..........\n",
      "[1,  1500] loss: 133.018275\n",
      "..........\n",
      "[1,  1600] loss: 133.348332\n",
      "..........\n",
      "[1,  1700] loss: 135.283721\n",
      "..........\n",
      "[1,  1800] loss: 132.990474\n",
      "..........\n",
      "[1,  1900] loss: 133.604666\n",
      "..........\n",
      "[1,  2000] loss: 134.585582\n",
      "..........\n",
      "[1,  2100] loss: 135.301376\n",
      "..........\n",
      "[1,  2200] loss: 132.823550\n",
      "..........\n",
      "[1,  2300] loss: 133.914209\n",
      "..........\n",
      "[1,  2400] loss: 131.881578\n",
      "..........\n",
      "[1,  2500] loss: 135.093394\n",
      "..........\n",
      "[1,  2600] loss: 132.720571\n",
      "..........\n",
      "[1,  2700] loss: 131.138143\n",
      "..........\n",
      "[1,  2800] loss: 133.880645\n",
      "..........\n",
      "[1,  2900] loss: 132.719129\n",
      "..........\n",
      "[1,  3000] loss: 132.274660\n",
      "..........\n",
      "[1,  3100] loss: 133.846213\n",
      "..........\n",
      "[1,  3200] loss: 132.463870\n",
      "..........\n",
      "[1,  3300] loss: 132.619615\n",
      "..........\n",
      "[1,  3400] loss: 132.708062\n",
      "..........\n",
      "[1,  3500] loss: 135.977164\n",
      "..........\n",
      "[1,  3600] loss: 131.623858\n",
      "..........\n",
      "[1,  3700] loss: 134.769750\n",
      "..........\n",
      "[1,  3800] loss: 132.634944\n",
      "..........\n",
      "[1,  3900] loss: 132.691244\n",
      "..........\n",
      "[1,  4000] loss: 132.905148\n",
      "..........\n",
      "[1,  4100] loss: 134.314615\n",
      "..........\n",
      "[1,  4200] loss: 133.381022\n",
      "..........\n",
      "[1,  4300] loss: 131.993766\n",
      "..........\n",
      "[1,  4400] loss: 132.898928\n",
      "..........\n",
      "[1,  4500] loss: 132.052176\n",
      "..........\n",
      "[1,  4600] loss: 136.227337\n",
      "..........\n",
      "[1,  4700] loss: 132.427504\n",
      "..........\n",
      "[1,  4800] loss: 135.447293\n",
      "..........\n",
      "[1,  4900] loss: 133.030907\n",
      "..........\n",
      "[1,  5000] loss: 134.684827\n",
      "..........\n",
      "[1,  5100] loss: 132.044329\n",
      "..........\n",
      "[1,  5200] loss: 133.940515\n",
      "..........\n",
      "[1,  5300] loss: 134.666541\n",
      "..........\n",
      "[1,  5400] loss: 132.678601\n",
      "..........\n",
      "[1,  5500] loss: 134.852887\n",
      "..........\n",
      "[1,  5600] loss: 134.450012\n",
      "..........\n",
      "[1,  5700] loss: 134.541625\n",
      "..........\n",
      "[1,  5800] loss: 136.595173\n",
      "..........\n",
      "[1,  5900] loss: 133.746782\n",
      "..........\n",
      "[1,  6000] loss: 134.493313\n",
      "..........\n",
      "[1,  6100] loss: 131.897269\n",
      "..........\n",
      "[1,  6200] loss: 135.123341\n",
      "..........\n",
      "[1,  6300] loss: 133.220049\n",
      "..........\n",
      "[1,  6400] loss: 136.894126\n",
      "..........\n",
      "[1,  6500] loss: 133.520105\n",
      "..........\n",
      "[1,  6600] loss: 133.792975\n",
      "..........\n",
      "[1,  6700] loss: 132.825138\n",
      "..........\n",
      "[1,  6800] loss: 134.109304\n",
      "..........\n",
      "[1,  6900] loss: 133.563522\n",
      "..........\n",
      "[1,  7000] loss: 132.450626\n",
      "..........\n",
      "[1,  7100] loss: 134.664175\n",
      "..........\n",
      "[1,  7200] loss: 134.095128\n",
      "..........\n",
      "[1,  7300] loss: 134.698324\n",
      "..........\n",
      "[1,  7400] loss: 136.169763\n",
      "..........\n",
      "[1,  7500] loss: 133.634997\n",
      "..........\n",
      "[1,  7600] loss: 134.841777\n",
      "..........\n",
      "[1,  7700] loss: 133.714951\n",
      "..........\n",
      "[1,  7800] loss: 135.164397\n",
      "..........\n",
      "[1,  7900] loss: 134.338851\n",
      "..........\n",
      "[1,  8000] loss: 135.090898\n",
      "total average loss : 133.507\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.125\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.109\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.122\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.120\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.119\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1220 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 134.013990\n",
      "..........\n",
      "[1,   200] loss: 134.984671\n",
      "..........\n",
      "[1,   300] loss: 135.839744\n",
      "..........\n",
      "[1,   400] loss: 134.051737\n",
      "..........\n",
      "[1,   500] loss: 135.941110\n",
      "..........\n",
      "[1,   600] loss: 135.155378\n",
      "..........\n",
      "[1,   700] loss: 134.945828\n",
      "..........\n",
      "[1,   800] loss: 136.146949\n",
      "..........\n",
      "[1,   900] loss: 135.980530\n",
      "..........\n",
      "[1,  1000] loss: 135.760647\n",
      "..........\n",
      "[1,  1100] loss: 135.179244\n",
      "..........\n",
      "[1,  1200] loss: 137.847615\n",
      "..........\n",
      "[1,  1300] loss: 134.657516\n",
      "..........\n",
      "[1,  1400] loss: 133.736831\n",
      "..........\n",
      "[1,  1500] loss: 135.317994\n",
      "..........\n",
      "[1,  1600] loss: 135.183140\n",
      "..........\n",
      "[1,  1700] loss: 132.638602\n",
      "..........\n",
      "[1,  1800] loss: 135.224414\n",
      "..........\n",
      "[1,  1900] loss: 133.087329\n",
      "..........\n",
      "[1,  2000] loss: 134.443403\n",
      "..........\n",
      "[1,  2100] loss: 135.543233\n",
      "..........\n",
      "[1,  2200] loss: 134.324287\n",
      "..........\n",
      "[1,  2300] loss: 137.890368\n",
      "..........\n",
      "[1,  2400] loss: 136.945633\n",
      "..........\n",
      "[1,  2500] loss: 133.989576\n",
      "..........\n",
      "[1,  2600] loss: 134.852597\n",
      "..........\n",
      "[1,  2700] loss: 136.282548\n",
      "..........\n",
      "[1,  2800] loss: 133.750831\n",
      "..........\n",
      "[1,  2900] loss: 134.682617\n",
      "..........\n",
      "[1,  3000] loss: 137.082356\n",
      "..........\n",
      "[1,  3100] loss: 133.847509\n",
      "..........\n",
      "[1,  3200] loss: 136.170132\n",
      "..........\n",
      "[1,  3300] loss: 135.117329\n",
      "..........\n",
      "[1,  3400] loss: 135.782853\n",
      "..........\n",
      "[1,  3500] loss: 136.866521\n",
      "..........\n",
      "[1,  3600] loss: 134.169845\n",
      "..........\n",
      "[1,  3700] loss: 137.645080\n",
      "..........\n",
      "[1,  3800] loss: 135.470789\n",
      "..........\n",
      "[1,  3900] loss: 137.600730\n",
      "..........\n",
      "[1,  4000] loss: 134.830376\n",
      "..........\n",
      "[1,  4100] loss: 136.546248\n",
      "..........\n",
      "[1,  4200] loss: 137.699847\n",
      "..........\n",
      "[1,  4300] loss: 135.128238\n",
      "..........\n",
      "[1,  4400] loss: 136.731464\n",
      "..........\n",
      "[1,  4500] loss: 134.833818\n",
      "..........\n",
      "[1,  4600] loss: 134.387296\n",
      "..........\n",
      "[1,  4700] loss: 134.149262\n",
      "..........\n",
      "[1,  4800] loss: 135.395620\n",
      "..........\n",
      "[1,  4900] loss: 135.788918\n",
      "..........\n",
      "[1,  5000] loss: 137.599475\n",
      "..........\n",
      "[1,  5100] loss: 137.055826\n",
      "..........\n",
      "[1,  5200] loss: 135.213500\n",
      "..........\n",
      "[1,  5300] loss: 136.419608\n",
      "..........\n",
      "[1,  5400] loss: 135.979396\n",
      "..........\n",
      "[1,  5500] loss: 135.033592\n",
      "..........\n",
      "[1,  5600] loss: 135.801437\n",
      "..........\n",
      "[1,  5700] loss: 135.203276\n",
      "..........\n",
      "[1,  5800] loss: 138.441067\n",
      "..........\n",
      "[1,  5900] loss: 133.859305\n",
      "..........\n",
      "[1,  6000] loss: 134.841823\n",
      "..........\n",
      "[1,  6100] loss: 137.340661\n",
      "..........\n",
      "[1,  6200] loss: 137.098374\n",
      "..........\n",
      "[1,  6300] loss: 137.498263\n",
      "..........\n",
      "[1,  6400] loss: 137.009719\n",
      "..........\n",
      "[1,  6500] loss: 137.415727\n",
      "..........\n",
      "[1,  6600] loss: 138.798952\n",
      "..........\n",
      "[1,  6700] loss: 135.509186\n",
      "..........\n",
      "[1,  6800] loss: 135.554868\n",
      "..........\n",
      "[1,  6900] loss: 137.393877\n",
      "..........\n",
      "[1,  7000] loss: 137.810433\n",
      "..........\n",
      "[1,  7100] loss: 137.389437\n",
      "..........\n",
      "[1,  7200] loss: 134.674597\n",
      "..........\n",
      "[1,  7300] loss: 137.115117\n",
      "....."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-129e67419b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                   \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                   \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                   error_idx,num_error,1,True)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfirst_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moriginal_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-49027496e88d>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(f4f, target_model, train_dataloader, test_dataloader, loss_fn, optimizer, error_idx, num_error, max_epochs, subset)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m#print(labels.size(),target_out.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#optimizer = torch.optim.SGD(param_list,lr=0.01,weight_decay=1e-4)\n",
    "first_feature = []\n",
    "original_out = []\n",
    "f = open(log_file,\"w\")\n",
    "f.close()\n",
    "num_error = 128\n",
    "max_epoch = 50\n",
    "for epoch in range(max_epoch):\n",
    "    print(\"======= epoch %2d =======\"%(epoch))\n",
    "    hook_register(vgg16_bn,error_index,num_error)\n",
    "    target_model = Target_model(vgg16_bn).to(device)\n",
    "    \n",
    "    for error_idx in range(0,512,num_error):\n",
    "        print(\"error(%d~%d) inserted training\"%(error_idx,error_idx+num_error))\n",
    "        tmp= training(f4f,target_model,\n",
    "                  train_dataloader,test_dataloader,\n",
    "                  loss_fn,optimizer,\n",
    "                  error_idx,num_error,1,True)\n",
    "    first_feature.append(tmp[1])\n",
    "    original_out.append(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 feature 그림 보기 위한 것들입니다.\n",
    "len(first_feature),len(original_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_out[0].size())\n",
    "w = 10\n",
    "h = 10\n",
    "cols = 32\n",
    "rows = 16\n",
    "def feature_print(pic):\n",
    "    print(\"test with 'after pooling 4 feature'\")\n",
    "    fig = plt.figure(figsize=(64,32))\n",
    "    ax = []\n",
    "    for i in range(cols*rows):\n",
    "        ch = pic[i,:,:]\n",
    "        ax.append(fig.add_subplot(rows,cols,i+1))\n",
    "        ax[-1].set_title(str(i)+\"th ch (14x14)\")\n",
    "        plt.imshow(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델 (에러없이, f4f없이)을 통과한 결과\n",
    "feature_print(original_out[0][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 1\n",
    "%matplotlib inline\n",
    "feature_print(first_feature[0][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 9\n",
    "print(\"epoch 9\")\n",
    "feature_print(first_feature[0][9].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14x14 의 feature 모두 합한 결과\n",
    "tmp = first_feature[0][6][0]\n",
    "for i in range(1,512):\n",
    "    tmp += first_feature[0][6][i]\n",
    "%matplotlib inline\n",
    "plt.imshow(tmp.cpu().detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14x14 의 feature 모두 합한 결과\n",
    "print(\"original\")\n",
    "tmp1 = original_out[0][6][0]\n",
    "for i in range(1,512):\n",
    "    tmp1 += original_out[0][6][i]\n",
    "%matplotlib inline\n",
    "plt.imshow(tmp1.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 6\n",
    "print(\"epoch 6\")\n",
    "feature_print(first_feature[0][6].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "error_info = make_error_info(error_index,num_error).to(device)\n",
    "for feature,label in train_dataloader:\n",
    "    print(\"======================================\")\n",
    "    feature,label = feature.to(device),label.to(device)\n",
    "    print(\"dataloader data : \",feature.size(),label.size())\n",
    "    target_out = test_model(feature,f4f,error_info)\n",
    "    print(\"output :\",target_out.size())\n",
    "    original_out = original_model(feature)\n",
    "    if torch.equal(target_out,original_out) is False :\n",
    "        print(\"=====compare two output======\")\n",
    "        print(target_out[0][0][0][0])\n",
    "        print(original_out[0][0][0][0])\n",
    "    else :\n",
    "        print(\"same\")\n",
    "    loss = loss_fn(original_out,target_out)\n",
    "    running_loss += loss.item()\n",
    "    target_model.model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break\n",
    "#target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

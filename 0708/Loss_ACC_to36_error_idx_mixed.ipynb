{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0 0.2.2\n",
      "TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy as d_copy\n",
    "import random\n",
    "\n",
    "print(torch.__version__, torchvision.__version__)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#error_index = 0\n",
    "vgg16_bn = torchvision.models.vgg16_bn(pretrained=True)#.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_layer_number = 34 # 34 conv5_1 convolution\n",
    "Out_layer_number = 36 # 36 conv5_1 relu \n",
    "error_index=0\n",
    "max_epochs = 30\n",
    "num_error = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomness 제어 \n",
    "# https://hoya012.github.io/blog/reproducible_pytorch/\n",
    "def set_randomness(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "# func\n",
    "\n",
    "# only apply for feature part (not pooling, classfier)\n",
    "# because of layers.feature \n",
    "def split_layer(model,start,end):\n",
    "    ct = 0\n",
    "    split_model=[] # from start to Conv5_1(include ReLU)\n",
    "    for name,layers in model.named_modules():\n",
    "        #print(name,layer)\n",
    "        #print(layers.features)\n",
    "        for idx,layer in enumerate(layers.features):\n",
    "            #print(idx,layer)\n",
    "            if start <=idx and idx <=end :\n",
    "                split_model.append(layer)\n",
    "        break\n",
    "    return nn.Sequential(*split_model)\n",
    "\n",
    "def error_injection(name,num_error,start_index):\n",
    "    def hook(model,input):\n",
    "        start = start_index\n",
    "        end = start_index + num_error\n",
    "        input[0][:, start:end]=0\n",
    "        print(\"shape :\",input[0][:, start:end].size())\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_log_to34.txt   F4F_pytorch-to36_error_idx_mixed-Copy1.ipynb\n",
      "acc_log_toEnd.txt  Loss_ACC_to36_error_idx_mixed.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls /media/2/hwbae0326/F4F/0708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 3125\n"
     ]
    }
   ],
   "source": [
    "# dataset load\n",
    "batch_size = 16 # 32~ out of memory in 3080\n",
    "num_train = 128000\n",
    "#num_train = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_path = \"/media/2/Network/Imagenet_dup/\"\n",
    "retrain_model_path = \"/media/0/Network/0708_to_34models/\"\n",
    "# imagenet data load\n",
    "train_dataset = dataset.ImageFolder(root=dataset_path+\"train\",\n",
    "                                       transform=transform)\n",
    "subset_train_dataset,_ = torch.utils.data.random_split(train_dataset, [num_train,len(train_dataset)-num_train])\n",
    "\n",
    "test_dataset = dataset.ImageFolder(root=dataset_path+\"val\",\n",
    "                                       transform=transform)\n",
    "'''\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=4)\n",
    "'''\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(subset_train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4) # for using subset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4)\n",
    "print(len(train_dataloader),len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(retrain_model_path) is False:\n",
    "    os.mkdir(retrain_model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make_F4F_pytorch-to34_F4Famended.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'extracted_feature',\n",
       " 'VGG16',\n",
       " 'start.py',\n",
       " 'acc_log_to34.txt',\n",
       " 'make_F4F_pytorch-to34.ipynb',\n",
       " '0708_to_34models',\n",
       " '0624_to_34models']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(retrain_model_path+\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "set_randomness(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external variable in error_index, num_error\n",
    "\n",
    "def make_error_info(error_index, num_error):\n",
    "    data = []\n",
    "    for i in range(511,-1,-1):\n",
    "        if error_index <= i and i < error_index+num_error:\n",
    "            data.append(1)\n",
    "        else :\n",
    "            data.append(0)\n",
    "        #print(data)\n",
    "    error_info = torch.Tensor(data)\n",
    "    error_info  = error_info.unsqueeze(0).repeat(512,1)\n",
    "    #print(\"error_info :\",error_info)\n",
    "    return error_info # 512,521\n",
    "class F4F(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f4f = nn.Linear(3*3*512+512,3*3*512) # 4167,4608 filter which change feature.34 (Conv5_1)\n",
    "        # 512 x5120 사이즈로 batch 저장\n",
    "    def get_f4f_weight(self):\n",
    "        # fc.weight.size(),fc.bias.size()\n",
    "        return self.f4f.weight # torch.Size([4608, 5120])\n",
    "    def forward(self,x):\n",
    "        x = self.f4f(x)\n",
    "        y = torch.tanh(x)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_register(model,error_index,num_error):\n",
    "    for name,layer in model.named_modules():\n",
    "        #print(name,layer)\n",
    "        if \"34\" in name  and isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "            print(\"input\",name,layer) # target layer Conv5_1\n",
    "            layer.register_forward_pre_hook(error_injection(name,num_error,error_index))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target_model(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def get_layer(self,idx):\n",
    "        #print(self.model._modules['34'])\n",
    "        layer =None\n",
    "        try : # target model\n",
    "            layer = self.model._modules[str(idx)]\n",
    "        except KeyError: # test_model\n",
    "            layer = self.model.features._modules[str(idx)]\n",
    "        return layer\n",
    "    def apply_f4f(self,f4f,error_info):\n",
    "        \n",
    "        weight = torch.reshape(self.get_layer(34).weight.data,(512,512*3*3)).to(device) # flatten [512,5210] (batch 512)\n",
    "        \n",
    "        data = torch.cat( (weight,error_info), 1 )\n",
    "        offset = torch.reshape(f4f(data),(512,512,3,3))\n",
    "        self.get_layer(34).weight.data = self.get_layer(34).weight.data + offset\n",
    "    def forward(self,x,f4f,error_info):\n",
    "        # apply_f4f는 매 epoch마다 동일하므로 \n",
    "        self.apply_f4f(f4f,error_info)\n",
    "        y = self.model(x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation phasetraining\n",
    "def eval(model,dataloader,epoch,f4f,error_info):\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct =0\n",
    "    with torch.no_grad():\n",
    "        print(\"======eval start=======\")\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "            y_hat = model(inputs,f4f,error_info)\n",
    "            _, predicted = torch.max(y_hat.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if(i%200 == 199):\n",
    "                print(\"step : %d / %d acc : %.3f\"\n",
    "                      %(i + 1,int(len(dataloader)), correct*100/total))\n",
    "                #print(\".\",end=\"\")\n",
    "        print(\"\")\n",
    "    acc = 100*correct/total\n",
    "    print(\"%dth epoch acc of %s on imagenet : %.4f %%\" %(epoch, model.__class__.__name__,acc)) \n",
    "    f = open(log_file,\"a\")\n",
    "    print(\"%dth epoch acc of %s on imagenet : %.4f %%\" %(epoch, model.__class__.__name__,acc),file=f) \n",
    "    f.close()\n",
    "    print(\"======eval  end ======\")  \n",
    "    return acc\n",
    "#torch.save(vgg16_bn.state_dict(), retrain_model_path+\"test_vgg16_bn_state_dict.pt\")\n",
    "def model_copy(model):\n",
    "    return d_copy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def training(f4f,target_model,\n",
    "             train_dataloader,test_dataloader,\n",
    "             loss_fn,optimizer,\n",
    "             error_idx,num_error,\n",
    "             max_epochs=30,subset=False):\n",
    "    \n",
    "    target_model.to(device)\n",
    "    original_model.to(device)\n",
    "    error_info = make_error_info(error_index,num_error).to(device)\n",
    "    first_feature = []\n",
    "    \n",
    "    feature_num = 100\n",
    "    for epoch in range(max_epochs):\n",
    "        running_loss = 0.0\n",
    "        total_loss = []\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        f4f.train()\n",
    "        # update f4f filter\n",
    "        #target_model.apply_f4f(f4f,error_info)\n",
    "    \n",
    "        # compare\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            if i % 10 == 0:\n",
    "                print(\".\",end=\"\")\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            target_out = target_model(inputs,f4f,error_info)\n",
    "            #print(original_out[0][0][0],target_out[0][0][0])\n",
    "            #exit(0)\n",
    "            \n",
    "            first_feature.append(target_out)\n",
    "            if len(first_feature) > feature_num:\n",
    "                first_feature.pop(0)\n",
    "            _,predicted = torch.max(target_out.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            \n",
    "            #print(labels.size(),target_out.size())\n",
    "            loss = loss_fn(target_out,labels)\n",
    "            #print(loss.size())\n",
    "            running_loss += loss.item()\n",
    "            target_model.model.zero_grad()\n",
    "            f4f.f4f.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 99: \n",
    "                total_loss.append(running_loss/100)\n",
    "                print(\"\")\n",
    "                print('[%d, %5d] loss: %.6f' % (epoch+1, i+1, running_loss/100)) \n",
    "                running_loss = 0.0\n",
    "        # save weight\n",
    "        #print((len(train_dataloader)/batch_size))\n",
    "        total_avg_loss = sum(total_loss)/len(total_loss)\n",
    "        acc = 100*correct/total\n",
    "        print(\"total average loss : %.3f\" %(total_avg_loss))\n",
    "        print(\"train acc : %.4f\" %(acc))\n",
    "        acc = eval(target_model,test_dataloader,epoch,f4f,error_info)\n",
    "        \n",
    "        torch.save(f4f.get_f4f_weight(), \n",
    "               retrain_model_path+\"%s~%s_pkt_err_f4f_epoch_%s_acc_%.4f_loss_%.4f.pt\"\n",
    "               %(str(error_idx).zfill(3),str(error_idx+num_error).zfill(3),\n",
    "                str(epoch+1).zfill(2),acc,total_avg_loss))    \n",
    "    return original_out,first_feature\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"./acc_log_toEnd.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_model = split_layer(vgg16_bn,0,Out_layer_number)\n",
    "\n",
    "original_model = d_copy(vgg16_bn).to(device)\n",
    "# subset of vgg16 (whole layer) with f4f\n",
    "hook_register(vgg16_bn,error_index,num_error)\n",
    "target_model = Target_model(vgg16_bn).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4f = F4F().to(device)\n",
    "optimizer = torch.optim.SGD(f4f.parameters(),lr=0.0001,weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= epoch  0 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 7.531204\n",
      "..........\n",
      "[1,   200] loss: 7.511229\n",
      "..........\n",
      "[1,   300] loss: 7.503648\n",
      "..........\n",
      "[1,   400] loss: 7.480855\n",
      "..........\n",
      "[1,   500] loss: 7.529524\n",
      "..........\n",
      "[1,   600] loss: 7.493235\n",
      "..........\n",
      "[1,   700] loss: 7.525830\n",
      "..........\n",
      "[1,   800] loss: 7.490340\n",
      "..........\n",
      "[1,   900] loss: 7.498394\n",
      "..........\n",
      "[1,  1000] loss: 7.484054\n",
      "..........\n",
      "[1,  1100] loss: 7.451966\n",
      "..........\n",
      "[1,  1200] loss: 7.521332\n",
      "..........\n",
      "[1,  1300] loss: 7.492091\n",
      "..........\n",
      "[1,  1400] loss: 7.462381\n",
      "..........\n",
      "[1,  1500] loss: 7.446392\n",
      "..........\n",
      "[1,  1600] loss: 7.509788\n",
      "..........\n",
      "[1,  1700] loss: 7.506300\n",
      "..........\n",
      "[1,  1800] loss: 7.488217\n",
      "..........\n",
      "[1,  1900] loss: 7.461961\n",
      "..........\n",
      "[1,  2000] loss: 7.413736\n",
      "..........\n",
      "[1,  2100] loss: 7.520183\n",
      "..........\n",
      "[1,  2200] loss: 7.469573\n",
      "..........\n",
      "[1,  2300] loss: 7.434176\n",
      "..........\n",
      "[1,  2400] loss: 7.487003\n",
      "..........\n",
      "[1,  2500] loss: 7.498735\n",
      "..........\n",
      "[1,  2600] loss: 7.470558\n",
      "..........\n",
      "[1,  2700] loss: 7.507446\n",
      "..........\n",
      "[1,  2800] loss: 7.507767\n",
      "..........\n",
      "[1,  2900] loss: 7.458457\n",
      "..........\n",
      "[1,  3000] loss: 7.539994\n",
      "..........\n",
      "[1,  3100] loss: 7.490716\n",
      "..........\n",
      "[1,  3200] loss: 7.482906\n",
      "..........\n",
      "[1,  3300] loss: 7.548090\n",
      "..........\n",
      "[1,  3400] loss: 7.521131\n",
      "..........\n",
      "[1,  3500] loss: 7.480171\n",
      "..........\n",
      "[1,  3600] loss: 7.508727\n",
      "..........\n",
      "[1,  3700] loss: 7.501629\n",
      "..........\n",
      "[1,  3800] loss: 7.448936\n",
      "..........\n",
      "[1,  3900] loss: 7.493964\n",
      "..........\n",
      "[1,  4000] loss: 7.479689\n",
      "..........\n",
      "[1,  4100] loss: 7.403270\n",
      "..........\n",
      "[1,  4200] loss: 7.471487\n",
      "..........\n",
      "[1,  4300] loss: 7.460608\n",
      "..........\n",
      "[1,  4400] loss: 7.495974\n",
      "..........\n",
      "[1,  4500] loss: 7.557357\n",
      "..........\n",
      "[1,  4600] loss: 7.439688\n",
      "..........\n",
      "[1,  4700] loss: 7.493004\n",
      "..........\n",
      "[1,  4800] loss: 7.480861\n",
      "..........\n",
      "[1,  4900] loss: 7.469919\n",
      "..........\n",
      "[1,  5000] loss: 7.546834\n",
      "..........\n",
      "[1,  5100] loss: 7.484899\n",
      "..........\n",
      "[1,  5200] loss: 7.484501\n",
      "..........\n",
      "[1,  5300] loss: 7.537207\n",
      "..........\n",
      "[1,  5400] loss: 7.502079\n",
      "..........\n",
      "[1,  5500] loss: 7.467065\n",
      "..........\n",
      "[1,  5600] loss: 7.461594\n",
      "..........\n",
      "[1,  5700] loss: 7.466425\n",
      "..........\n",
      "[1,  5800] loss: 7.465784\n",
      "..........\n",
      "[1,  5900] loss: 7.482240\n",
      "..........\n",
      "[1,  6000] loss: 7.496273\n",
      "..........\n",
      "[1,  6100] loss: 7.491040\n",
      "..........\n",
      "[1,  6200] loss: 7.487992\n",
      "..........\n",
      "[1,  6300] loss: 7.464673\n",
      "..........\n",
      "[1,  6400] loss: 7.456159\n",
      "..........\n",
      "[1,  6500] loss: 7.528432\n",
      "..........\n",
      "[1,  6600] loss: 7.489500\n",
      "..........\n",
      "[1,  6700] loss: 7.502354\n",
      "..........\n",
      "[1,  6800] loss: 7.470026\n",
      "..........\n",
      "[1,  6900] loss: 7.458369\n",
      "..........\n",
      "[1,  7000] loss: 7.479698\n",
      "..........\n",
      "[1,  7100] loss: 7.472896\n",
      "..........\n",
      "[1,  7200] loss: 7.487585\n",
      "..........\n",
      "[1,  7300] loss: 7.470817\n",
      "..........\n",
      "[1,  7400] loss: 7.490050\n",
      "..........\n",
      "[1,  7500] loss: 7.482840\n",
      "..........\n",
      "[1,  7600] loss: 7.483768\n",
      "..........\n",
      "[1,  7700] loss: 7.447738\n",
      "..........\n",
      "[1,  7800] loss: 7.472675\n",
      "..........\n",
      "[1,  7900] loss: 7.509016\n",
      "..........\n",
      "[1,  8000] loss: 7.493933\n",
      "total average loss : 7.487\n",
      "train acc : 0.1086\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.172\n",
      "step : 600 / 3125 acc : 0.156\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.113\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.109\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.125\n",
      "step : 2600 / 3125 acc : 0.123\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.117\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1140 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 7.472967\n",
      "..........\n",
      "[1,   200] loss: 7.483386\n",
      "..........\n",
      "[1,   300] loss: 7.477395\n",
      "..........\n",
      "[1,   400] loss: 7.479076\n",
      "..........\n",
      "[1,   500] loss: 7.453780\n",
      "..........\n",
      "[1,   600] loss: 7.522517\n",
      "..........\n",
      "[1,   700] loss: 7.451066\n",
      "..........\n",
      "[1,   800] loss: 7.545053\n",
      "..........\n",
      "[1,   900] loss: 7.463057\n",
      "..........\n",
      "[1,  1000] loss: 7.502314\n",
      "..........\n",
      "[1,  1100] loss: 7.508730\n",
      "..........\n",
      "[1,  1200] loss: 7.488925\n",
      "..........\n",
      "[1,  1300] loss: 7.501389\n",
      "..........\n",
      "[1,  1400] loss: 7.476800\n",
      "..........\n",
      "[1,  1500] loss: 7.486649\n",
      "..........\n",
      "[1,  1600] loss: 7.526579\n",
      "..........\n",
      "[1,  1700] loss: 7.505007\n",
      "..........\n",
      "[1,  1800] loss: 7.499876\n",
      "..........\n",
      "[1,  1900] loss: 7.536274\n",
      "..........\n",
      "[1,  2000] loss: 7.559457\n",
      "..........\n",
      "[1,  2100] loss: 7.471770\n",
      "..........\n",
      "[1,  2200] loss: 7.484949\n",
      "..........\n",
      "[1,  2300] loss: 7.508360\n",
      "..........\n",
      "[1,  2400] loss: 7.539745\n",
      "..........\n",
      "[1,  2500] loss: 7.541768\n",
      "..........\n",
      "[1,  2600] loss: 7.597941\n",
      "..........\n",
      "[1,  2700] loss: 7.518548\n",
      "..........\n",
      "[1,  2800] loss: 7.575594\n",
      "..........\n",
      "[1,  2900] loss: 7.528892\n",
      "..........\n",
      "[1,  3000] loss: 7.569018\n",
      "..........\n",
      "[1,  3100] loss: 7.573328\n",
      "..........\n",
      "[1,  3200] loss: 7.585970\n",
      "..........\n",
      "[1,  3300] loss: 7.578270\n",
      "..........\n",
      "[1,  3400] loss: 7.553605\n",
      "..........\n",
      "[1,  3500] loss: 7.548710\n",
      "..........\n",
      "[1,  3600] loss: 7.571546\n",
      "..........\n",
      "[1,  3700] loss: 7.558617\n",
      "..........\n",
      "[1,  3800] loss: 7.542509\n",
      "..........\n",
      "[1,  3900] loss: 7.610083\n",
      "..........\n",
      "[1,  4000] loss: 7.532730\n",
      "..........\n",
      "[1,  4100] loss: 7.626893\n",
      "..........\n",
      "[1,  4200] loss: 7.641794\n",
      "..........\n",
      "[1,  4300] loss: 7.584882\n",
      "..........\n",
      "[1,  4400] loss: 7.617605\n",
      "..........\n",
      "[1,  4500] loss: 7.675635\n",
      "..........\n",
      "[1,  4600] loss: 7.617748\n",
      "..........\n",
      "[1,  4700] loss: 7.634802\n",
      "..........\n",
      "[1,  4800] loss: 7.618434\n",
      "..........\n",
      "[1,  4900] loss: 7.654631\n",
      "..........\n",
      "[1,  5000] loss: 7.650137\n",
      "..........\n",
      "[1,  5100] loss: 7.669812\n",
      "..........\n",
      "[1,  5200] loss: 7.638881\n",
      "..........\n",
      "[1,  5300] loss: 7.685909\n",
      "..........\n",
      "[1,  5400] loss: 7.725539\n",
      "..........\n",
      "[1,  5500] loss: 7.750200\n",
      "..........\n",
      "[1,  5600] loss: 7.677776\n",
      "..........\n",
      "[1,  5700] loss: 7.665223\n",
      "..........\n",
      "[1,  5800] loss: 7.705945\n",
      "..........\n",
      "[1,  5900] loss: 7.709957\n",
      "..........\n",
      "[1,  6000] loss: 7.767731\n",
      "..........\n",
      "[1,  6100] loss: 7.732268\n",
      "..........\n",
      "[1,  6200] loss: 7.757912\n",
      "..........\n",
      "[1,  6300] loss: 7.703903\n",
      "..........\n",
      "[1,  6400] loss: 7.793440\n",
      "..........\n",
      "[1,  6500] loss: 7.812519\n",
      "..........\n",
      "[1,  6600] loss: 7.806170\n",
      "..........\n",
      "[1,  6700] loss: 7.842015\n",
      "..........\n",
      "[1,  6800] loss: 7.896219\n",
      "..........\n",
      "[1,  6900] loss: 7.790466\n",
      "..........\n",
      "[1,  7000] loss: 7.822255\n",
      "..........\n",
      "[1,  7100] loss: 7.842852\n",
      "..........\n",
      "[1,  7200] loss: 7.846801\n",
      "..........\n",
      "[1,  7300] loss: 7.839883\n",
      "..........\n",
      "[1,  7400] loss: 7.843225\n",
      "..........\n",
      "[1,  7500] loss: 7.897854\n",
      "..........\n",
      "[1,  7600] loss: 7.888579\n",
      "..........\n",
      "[1,  7700] loss: 7.882875\n",
      "..........\n",
      "[1,  7800] loss: 7.870546\n",
      "..........\n",
      "[1,  7900] loss: 7.961472\n",
      "..........\n",
      "[1,  8000] loss: 7.913574\n",
      "total average loss : 7.637\n",
      "train acc : 0.1187\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.172\n",
      "step : 600 / 3125 acc : 0.167\n",
      "step : 800 / 3125 acc : 0.148\n",
      "step : 1000 / 3125 acc : 0.181\n",
      "step : 1200 / 3125 acc : 0.151\n",
      "step : 1400 / 3125 acc : 0.156\n",
      "step : 1600 / 3125 acc : 0.156\n",
      "step : 1800 / 3125 acc : 0.149\n",
      "step : 2000 / 3125 acc : 0.144\n",
      "step : 2200 / 3125 acc : 0.142\n",
      "step : 2400 / 3125 acc : 0.146\n",
      "step : 2600 / 3125 acc : 0.142\n",
      "step : 2800 / 3125 acc : 0.134\n",
      "step : 3000 / 3125 acc : 0.127\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1280 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 8.346282\n",
      "..........\n",
      "[1,   200] loss: 8.328264\n",
      "..........\n",
      "[1,   300] loss: 8.396405\n",
      "..........\n",
      "[1,   400] loss: 8.344729\n",
      "..........\n",
      "[1,   500] loss: 8.324757\n",
      "..........\n",
      "[1,   600] loss: 8.365411\n",
      "..........\n",
      "[1,   700] loss: 8.359528\n",
      "..........\n",
      "[1,   800] loss: 8.403755\n",
      "..........\n",
      "[1,   900] loss: 8.392744\n",
      "..........\n",
      "[1,  1000] loss: 8.440560\n",
      "..........\n",
      "[1,  1100] loss: 8.444581\n",
      "..........\n",
      "[1,  1200] loss: 8.469353\n",
      "..........\n",
      "[1,  1300] loss: 8.533211\n",
      "..........\n",
      "[1,  1400] loss: 8.426639\n",
      "..........\n",
      "[1,  1500] loss: 8.579084\n",
      "..........\n",
      "[1,  1600] loss: 8.542371\n",
      "..........\n",
      "[1,  1700] loss: 8.550144\n",
      "..........\n",
      "[1,  1800] loss: 8.526992\n",
      "..........\n",
      "[1,  1900] loss: 8.499047\n",
      "..........\n",
      "[1,  2000] loss: 8.615005\n",
      "..........\n",
      "[1,  2100] loss: 8.490759\n",
      "..........\n",
      "[1,  2200] loss: 8.673498\n",
      "..........\n",
      "[1,  2300] loss: 8.755382\n",
      "..........\n",
      "[1,  2400] loss: 8.587351\n",
      "..........\n",
      "[1,  2500] loss: 8.562362\n",
      "..........\n",
      "[1,  2600] loss: 8.686268\n",
      "..........\n",
      "[1,  2700] loss: 8.634320\n",
      "..........\n",
      "[1,  2800] loss: 8.793957\n",
      "..........\n",
      "[1,  2900] loss: 8.746191\n",
      "..........\n",
      "[1,  3000] loss: 8.853893\n",
      "..........\n",
      "[1,  3100] loss: 8.901786\n",
      "..........\n",
      "[1,  3200] loss: 8.922518\n",
      "..........\n",
      "[1,  3300] loss: 8.699887\n",
      "..........\n",
      "[1,  3400] loss: 8.826311\n",
      "..........\n",
      "[1,  3500] loss: 8.866286\n",
      "..........\n",
      "[1,  3600] loss: 9.029355\n",
      "..........\n",
      "[1,  3700] loss: 8.878443\n",
      "..........\n",
      "[1,  3800] loss: 8.835565\n",
      "..........\n",
      "[1,  3900] loss: 8.873521\n",
      "..........\n",
      "[1,  4000] loss: 8.933203\n",
      "..........\n",
      "[1,  4100] loss: 8.983527\n",
      "..........\n",
      "[1,  4200] loss: 9.046960\n",
      "..........\n",
      "[1,  4300] loss: 9.013632\n",
      "..........\n",
      "[1,  4400] loss: 8.864229\n",
      "..........\n",
      "[1,  4500] loss: 9.056463\n",
      "..........\n",
      "[1,  4600] loss: 9.020876\n",
      "..........\n",
      "[1,  4700] loss: 9.082458\n",
      "..........\n",
      "[1,  4800] loss: 9.031418\n",
      "..........\n",
      "[1,  4900] loss: 9.074716\n",
      "..........\n",
      "[1,  5000] loss: 9.106252\n",
      "..........\n",
      "[1,  5100] loss: 9.135374\n",
      "..........\n",
      "[1,  5200] loss: 9.172946\n",
      "..........\n",
      "[1,  5300] loss: 9.210866\n",
      "..........\n",
      "[1,  5400] loss: 9.322107\n",
      "..........\n",
      "[1,  5500] loss: 9.177912\n",
      "..........\n",
      "[1,  5600] loss: 9.163543\n",
      "..........\n",
      "[1,  5700] loss: 9.384047\n",
      "..........\n",
      "[1,  5800] loss: 9.178818\n",
      "..........\n",
      "[1,  5900] loss: 9.362202\n",
      "..........\n",
      "[1,  6000] loss: 9.240396\n",
      "..........\n",
      "[1,  6100] loss: 9.276088\n",
      "..........\n",
      "[1,  6200] loss: 9.356309\n",
      "..........\n",
      "[1,  6300] loss: 9.385422\n",
      "..........\n",
      "[1,  6400] loss: 9.386770\n",
      "..........\n",
      "[1,  6500] loss: 9.444496\n",
      "..........\n",
      "[1,  6600] loss: 9.410642\n",
      "..........\n",
      "[1,  6700] loss: 9.455152\n",
      "..........\n",
      "[1,  6800] loss: 9.444469\n",
      "..........\n",
      "[1,  6900] loss: 9.437173\n",
      "..........\n",
      "[1,  7000] loss: 9.411902\n",
      "..........\n",
      "[1,  7100] loss: 9.464385\n",
      "..........\n",
      "[1,  7200] loss: 9.733507\n",
      "..........\n",
      "[1,  7300] loss: 9.689586\n",
      "..........\n",
      "[1,  7400] loss: 9.504707\n",
      "..........\n",
      "[1,  7500] loss: 9.632199\n",
      "..........\n",
      "[1,  7600] loss: 9.471500\n",
      "..........\n",
      "[1,  7700] loss: 9.720551\n",
      "..........\n",
      "[1,  7800] loss: 9.601659\n",
      "..........\n",
      "[1,  7900] loss: 9.544093\n",
      "..........\n",
      "[1,  8000] loss: 9.748466\n",
      "total average loss : 8.965\n",
      "train acc : 0.1422\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.109\n",
      "step : 1400 / 3125 acc : 0.103\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.104\n",
      "step : 2000 / 3125 acc : 0.109\n",
      "step : 2200 / 3125 acc : 0.111\n",
      "step : 2400 / 3125 acc : 0.120\n",
      "step : 2600 / 3125 acc : 0.125\n",
      "step : 2800 / 3125 acc : 0.125\n",
      "step : 3000 / 3125 acc : 0.127\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1320 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 10.202188\n",
      "..........\n",
      "[1,   200] loss: 10.488190\n",
      "..........\n",
      "[1,   300] loss: 10.405028\n",
      "..........\n",
      "[1,   400] loss: 10.468237\n",
      "..........\n",
      "[1,   500] loss: 10.260088\n",
      "..........\n",
      "[1,   600] loss: 10.521446\n",
      "..........\n",
      "[1,   700] loss: 10.665913\n",
      "..........\n",
      "[1,   800] loss: 10.349647\n",
      "..........\n",
      "[1,   900] loss: 10.671873\n",
      "..........\n",
      "[1,  1000] loss: 10.570407\n",
      "..........\n",
      "[1,  1100] loss: 10.622400\n",
      "..........\n",
      "[1,  1200] loss: 10.556263\n",
      "..........\n",
      "[1,  1300] loss: 10.461884\n",
      "..........\n",
      "[1,  1400] loss: 10.638017\n",
      "..........\n",
      "[1,  1500] loss: 10.579964\n",
      "..........\n",
      "[1,  1600] loss: 10.632064\n",
      "..........\n",
      "[1,  1700] loss: 10.698789\n",
      "..........\n",
      "[1,  1800] loss: 10.461166\n",
      "..........\n",
      "[1,  1900] loss: 10.744786\n",
      "..........\n",
      "[1,  2000] loss: 10.645594\n",
      "..........\n",
      "[1,  2100] loss: 10.594065\n",
      "..........\n",
      "[1,  2200] loss: 10.907418\n",
      "..........\n",
      "[1,  2300] loss: 10.707740\n",
      "..........\n",
      "[1,  2400] loss: 10.899642\n",
      "..........\n",
      "[1,  2500] loss: 10.790489\n",
      "..........\n",
      "[1,  2600] loss: 10.851295\n",
      "..........\n",
      "[1,  2700] loss: 10.768486\n",
      "..........\n",
      "[1,  2800] loss: 10.884636\n",
      "..........\n",
      "[1,  2900] loss: 10.900839\n",
      "..........\n",
      "[1,  3000] loss: 10.734545\n",
      "..........\n",
      "[1,  3100] loss: 10.848683\n",
      "..........\n",
      "[1,  3200] loss: 11.007617\n",
      "..........\n",
      "[1,  3300] loss: 10.937530\n",
      "..........\n",
      "[1,  3400] loss: 11.027671\n",
      "..........\n",
      "[1,  3500] loss: 10.940239\n",
      "..........\n",
      "[1,  3600] loss: 11.223224\n",
      "..........\n",
      "[1,  3700] loss: 11.151052\n",
      "..........\n",
      "[1,  3800] loss: 11.115459\n",
      "..........\n",
      "[1,  3900] loss: 11.180303\n",
      "..........\n",
      "[1,  4000] loss: 11.123264\n",
      "..........\n",
      "[1,  4100] loss: 11.120379\n",
      "..........\n",
      "[1,  4200] loss: 11.199527\n",
      "..........\n",
      "[1,  4300] loss: 11.175349\n",
      "..........\n",
      "[1,  4400] loss: 11.189785\n",
      "..........\n",
      "[1,  4500] loss: 11.185385\n",
      "..........\n",
      "[1,  4600] loss: 11.339056\n",
      "..........\n",
      "[1,  4700] loss: 11.143110\n",
      "..........\n",
      "[1,  4800] loss: 11.437262\n",
      "..........\n",
      "[1,  4900] loss: 11.409309\n",
      "..........\n",
      "[1,  5000] loss: 11.283780\n",
      "..........\n",
      "[1,  5100] loss: 11.551149\n",
      "..........\n",
      "[1,  5200] loss: 11.376431\n",
      "..........\n",
      "[1,  5300] loss: 11.404094\n",
      "..........\n",
      "[1,  5400] loss: 11.721837\n",
      "..........\n",
      "[1,  5500] loss: 11.610535\n",
      "..........\n",
      "[1,  5600] loss: 11.393434\n",
      "..........\n",
      "[1,  5700] loss: 11.463567\n",
      "..........\n",
      "[1,  5800] loss: 11.539900\n",
      "..........\n",
      "[1,  5900] loss: 11.830592\n",
      "..........\n",
      "[1,  6000] loss: 11.459598\n",
      "..........\n",
      "[1,  6100] loss: 11.581379\n",
      "..........\n",
      "[1,  6200] loss: 11.792600\n",
      "..........\n",
      "[1,  6300] loss: 11.701118\n",
      "..........\n",
      "[1,  6400] loss: 11.799607\n",
      "..........\n",
      "[1,  6500] loss: 11.837365\n",
      "..........\n",
      "[1,  6600] loss: 11.624312\n",
      "..........\n",
      "[1,  6700] loss: 12.058417\n",
      "..........\n",
      "[1,  6800] loss: 11.795949\n",
      "..........\n",
      "[1,  6900] loss: 11.738560\n",
      "..........\n",
      "[1,  7000] loss: 12.056217\n",
      "..........\n",
      "[1,  7100] loss: 11.733196\n",
      "..........\n",
      "[1,  7200] loss: 11.867653\n",
      "..........\n",
      "[1,  7300] loss: 11.985711\n",
      "..........\n",
      "[1,  7400] loss: 11.947179\n",
      "..........\n",
      "[1,  7500] loss: 11.993086\n",
      "..........\n",
      "[1,  7600] loss: 11.910846\n",
      "..........\n",
      "[1,  7700] loss: 11.927878\n",
      "..........\n",
      "[1,  7800] loss: 12.143171\n",
      "..........\n",
      "[1,  7900] loss: 12.051019\n",
      "..........\n",
      "[1,  8000] loss: 12.025889\n",
      "total average loss : 11.183\n",
      "train acc : 0.1133\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.130\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.113\n",
      "step : 1800 / 3125 acc : 0.115\n",
      "step : 2000 / 3125 acc : 0.125\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.127\n",
      "step : 2800 / 3125 acc : 0.127\n",
      "step : 3000 / 3125 acc : 0.125\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1260 %\n",
      "======eval  end ======\n",
      "======= epoch  1 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 12.618555\n",
      "..........\n",
      "[1,   200] loss: 12.752393\n",
      "..........\n",
      "[1,   300] loss: 13.149260\n",
      "..........\n",
      "[1,   400] loss: 13.086407\n",
      "..........\n",
      "[1,   500] loss: 12.832180\n",
      "..........\n",
      "[1,   600] loss: 13.057296\n",
      "..........\n",
      "[1,   700] loss: 13.380255\n",
      "..........\n",
      "[1,   800] loss: 12.880552\n",
      "..........\n",
      "[1,   900] loss: 13.041424\n",
      "..........\n",
      "[1,  1000] loss: 13.049016\n",
      "..........\n",
      "[1,  1100] loss: 13.204342\n",
      "..........\n",
      "[1,  1200] loss: 13.027257\n",
      "..........\n",
      "[1,  1300] loss: 13.661809\n",
      "..........\n",
      "[1,  1400] loss: 13.284813\n",
      "..........\n",
      "[1,  1500] loss: 13.314792\n",
      "..........\n",
      "[1,  1600] loss: 13.379586\n",
      "..........\n",
      "[1,  1700] loss: 13.632548\n",
      "..........\n",
      "[1,  1800] loss: 13.708606\n",
      "..........\n",
      "[1,  1900] loss: 13.510549\n",
      "..........\n",
      "[1,  2000] loss: 13.443779\n",
      "..........\n",
      "[1,  2100] loss: 13.752711\n",
      "..........\n",
      "[1,  2200] loss: 13.425184\n",
      "..........\n",
      "[1,  2300] loss: 13.372836\n",
      "..........\n",
      "[1,  2400] loss: 13.416443\n",
      "..........\n",
      "[1,  2500] loss: 13.651451\n",
      "..........\n",
      "[1,  2600] loss: 13.485688\n",
      "..........\n",
      "[1,  2700] loss: 13.680261\n",
      "..........\n",
      "[1,  2800] loss: 13.599029\n",
      "..........\n",
      "[1,  2900] loss: 13.560327\n",
      "..........\n",
      "[1,  3000] loss: 13.497097\n",
      "..........\n",
      "[1,  3100] loss: 13.596024\n",
      "..........\n",
      "[1,  3200] loss: 13.916668\n",
      "..........\n",
      "[1,  3300] loss: 13.804565\n",
      "..........\n",
      "[1,  3400] loss: 13.942556\n",
      "..........\n",
      "[1,  3500] loss: 14.037677\n",
      "..........\n",
      "[1,  3600] loss: 14.082103\n",
      "..........\n",
      "[1,  3700] loss: 13.939275\n",
      "..........\n",
      "[1,  3800] loss: 13.906380\n",
      "..........\n",
      "[1,  3900] loss: 13.751204\n",
      "..........\n",
      "[1,  4000] loss: 13.920594\n",
      "..........\n",
      "[1,  4100] loss: 13.979801\n",
      "..........\n",
      "[1,  4200] loss: 13.974685\n",
      "..........\n",
      "[1,  4300] loss: 14.008293\n",
      "..........\n",
      "[1,  4400] loss: 13.858961\n",
      "..........\n",
      "[1,  4500] loss: 13.868050\n",
      "..........\n",
      "[1,  4600] loss: 14.099059\n",
      "..........\n",
      "[1,  4700] loss: 14.010232\n",
      "..........\n",
      "[1,  4800] loss: 14.200665\n",
      "..........\n",
      "[1,  4900] loss: 14.110177\n",
      "..........\n",
      "[1,  5000] loss: 14.374711\n",
      "..........\n",
      "[1,  5100] loss: 14.278914\n",
      "..........\n",
      "[1,  5200] loss: 14.197583\n",
      "..........\n",
      "[1,  5300] loss: 14.455692\n",
      "..........\n",
      "[1,  5400] loss: 14.316546\n",
      "..........\n",
      "[1,  5500] loss: 14.311207\n",
      "..........\n",
      "[1,  5600] loss: 14.768367\n",
      "..........\n",
      "[1,  5700] loss: 14.675243\n",
      "..........\n",
      "[1,  5800] loss: 14.393487\n",
      "..........\n",
      "[1,  5900] loss: 14.604477\n",
      "..........\n",
      "[1,  6000] loss: 14.442297\n",
      "..........\n",
      "[1,  6100] loss: 14.632815\n",
      "..........\n",
      "[1,  6200] loss: 14.627316\n",
      "..........\n",
      "[1,  6300] loss: 14.314630\n",
      "..........\n",
      "[1,  6400] loss: 14.702922\n",
      "..........\n",
      "[1,  6500] loss: 14.698769\n",
      "..........\n",
      "[1,  6600] loss: 14.567996\n",
      "..........\n",
      "[1,  6700] loss: 14.682805\n",
      "..........\n",
      "[1,  6800] loss: 14.875481\n",
      "..........\n",
      "[1,  6900] loss: 14.950259\n",
      "..........\n",
      "[1,  7000] loss: 14.820459\n",
      "..........\n",
      "[1,  7100] loss: 14.774450\n",
      "..........\n",
      "[1,  7200] loss: 14.701044\n",
      "..........\n",
      "[1,  7300] loss: 15.089275\n",
      "..........\n",
      "[1,  7400] loss: 15.119740\n",
      "..........\n",
      "[1,  7500] loss: 14.721170\n",
      "..........\n",
      "[1,  7600] loss: 14.913843\n",
      "..........\n",
      "[1,  7700] loss: 15.129614\n",
      "..........\n",
      "[1,  7800] loss: 15.008083\n",
      "..........\n",
      "[1,  7900] loss: 14.801629\n",
      "..........\n",
      "[1,  8000] loss: 15.420717\n",
      "total average loss : 13.998\n",
      "train acc : 0.1164\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.000\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.156\n",
      "step : 1200 / 3125 acc : 0.135\n",
      "step : 1400 / 3125 acc : 0.129\n",
      "step : 1600 / 3125 acc : 0.141\n",
      "step : 1800 / 3125 acc : 0.132\n",
      "step : 2000 / 3125 acc : 0.122\n",
      "step : 2200 / 3125 acc : 0.116\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.123\n",
      "step : 3000 / 3125 acc : 0.119\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1160 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 15.907573\n",
      "..........\n",
      "[1,   200] loss: 15.992847\n",
      "..........\n",
      "[1,   300] loss: 16.110579\n",
      "..........\n",
      "[1,   400] loss: 16.131457\n",
      "..........\n",
      "[1,   500] loss: 15.978744\n",
      "..........\n",
      "[1,   600] loss: 16.197803\n",
      "..........\n",
      "[1,   700] loss: 16.266335\n",
      "..........\n",
      "[1,   800] loss: 16.138690\n",
      "..........\n",
      "[1,   900] loss: 16.136996\n",
      "..........\n",
      "[1,  1000] loss: 16.339967\n",
      "..........\n",
      "[1,  1100] loss: 16.400473\n",
      "..........\n",
      "[1,  1200] loss: 16.032178\n",
      "..........\n",
      "[1,  1300] loss: 16.020861\n",
      "..........\n",
      "[1,  1400] loss: 16.241591\n",
      "..........\n",
      "[1,  1500] loss: 16.236597\n",
      "..........\n",
      "[1,  1600] loss: 16.266093\n",
      "..........\n",
      "[1,  1700] loss: 16.391424\n",
      "..........\n",
      "[1,  1800] loss: 16.602203\n",
      "..........\n",
      "[1,  1900] loss: 16.270915\n",
      "..........\n",
      "[1,  2000] loss: 16.545378\n",
      "..........\n",
      "[1,  2100] loss: 16.625333\n",
      "..........\n",
      "[1,  2200] loss: 16.294802\n",
      "..........\n",
      "[1,  2300] loss: 16.438996\n",
      "..........\n",
      "[1,  2400] loss: 16.751693\n",
      "..........\n",
      "[1,  2500] loss: 16.771307\n",
      "..........\n",
      "[1,  2600] loss: 16.632519\n",
      "..........\n",
      "[1,  2700] loss: 16.775779\n",
      "..........\n",
      "[1,  2800] loss: 17.044865\n",
      "..........\n",
      "[1,  2900] loss: 16.626063\n",
      "..........\n",
      "[1,  3000] loss: 17.111626\n",
      "..........\n",
      "[1,  3100] loss: 16.638700\n",
      "..........\n",
      "[1,  3200] loss: 16.557642\n",
      "..........\n",
      "[1,  3300] loss: 16.900534\n",
      "..........\n",
      "[1,  3400] loss: 16.757743\n",
      "..........\n",
      "[1,  3500] loss: 16.999226\n",
      "..........\n",
      "[1,  3600] loss: 17.005177\n",
      "..........\n",
      "[1,  3700] loss: 16.863653\n",
      "..........\n",
      "[1,  3800] loss: 16.720407\n",
      "..........\n",
      "[1,  3900] loss: 17.055114\n",
      "..........\n",
      "[1,  4000] loss: 17.027779\n",
      "..........\n",
      "[1,  4100] loss: 16.937463\n",
      "..........\n",
      "[1,  4200] loss: 17.002648\n",
      "..........\n",
      "[1,  4300] loss: 17.103879\n",
      "..........\n",
      "[1,  4400] loss: 16.983330\n",
      "..........\n",
      "[1,  4500] loss: 16.644513\n",
      "..........\n",
      "[1,  4600] loss: 17.534496\n",
      "..........\n",
      "[1,  4700] loss: 17.410575\n",
      "..........\n",
      "[1,  4800] loss: 17.043086\n",
      "..........\n",
      "[1,  4900] loss: 16.904261\n",
      "..........\n",
      "[1,  5000] loss: 17.374935\n",
      "..........\n",
      "[1,  5100] loss: 17.410627\n",
      "..........\n",
      "[1,  5200] loss: 17.531798\n",
      "..........\n",
      "[1,  5300] loss: 17.222299\n",
      "..........\n",
      "[1,  5400] loss: 17.161483\n",
      "..........\n",
      "[1,  5500] loss: 17.347884\n",
      "..........\n",
      "[1,  5600] loss: 17.346071\n",
      "..........\n",
      "[1,  5700] loss: 17.453549\n",
      "..........\n",
      "[1,  5800] loss: 17.399333\n",
      "..........\n",
      "[1,  5900] loss: 17.416164\n",
      "..........\n",
      "[1,  6000] loss: 17.136412\n",
      "..........\n",
      "[1,  6100] loss: 17.876194\n",
      "..........\n",
      "[1,  6200] loss: 17.452576\n",
      "..........\n",
      "[1,  6300] loss: 17.389127\n",
      "..........\n",
      "[1,  6400] loss: 17.554212\n",
      "..........\n",
      "[1,  6500] loss: 17.830654\n",
      "..........\n",
      "[1,  6600] loss: 17.692655\n",
      "..........\n",
      "[1,  6700] loss: 17.727923\n",
      "..........\n",
      "[1,  6800] loss: 17.486514\n",
      "..........\n",
      "[1,  6900] loss: 17.468172\n",
      "..........\n",
      "[1,  7000] loss: 17.704357\n",
      "..........\n",
      "[1,  7100] loss: 17.806810\n",
      "..........\n",
      "[1,  7200] loss: 17.840986\n",
      "..........\n",
      "[1,  7300] loss: 17.642413\n",
      "..........\n",
      "[1,  7400] loss: 17.907654\n",
      "..........\n",
      "[1,  7500] loss: 17.864028\n",
      "..........\n",
      "[1,  7600] loss: 17.682983\n",
      "..........\n",
      "[1,  7700] loss: 17.929747\n",
      "..........\n",
      "[1,  7800] loss: 17.844720\n",
      "..........\n",
      "[1,  7900] loss: 17.754755\n",
      "..........\n",
      "[1,  8000] loss: 18.411853\n",
      "total average loss : 16.988\n",
      "train acc : 0.1117\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.031\n",
      "step : 600 / 3125 acc : 0.042\n",
      "step : 800 / 3125 acc : 0.055\n",
      "step : 1000 / 3125 acc : 0.056\n",
      "step : 1200 / 3125 acc : 0.057\n",
      "step : 1400 / 3125 acc : 0.071\n",
      "step : 1600 / 3125 acc : 0.078\n",
      "step : 1800 / 3125 acc : 0.087\n",
      "step : 2000 / 3125 acc : 0.097\n",
      "step : 2200 / 3125 acc : 0.099\n",
      "step : 2400 / 3125 acc : 0.104\n",
      "step : 2600 / 3125 acc : 0.106\n",
      "step : 2800 / 3125 acc : 0.107\n",
      "step : 3000 / 3125 acc : 0.104\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1100 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 18.853431\n",
      "..........\n",
      "[1,   200] loss: 18.815418\n",
      "..........\n",
      "[1,   300] loss: 18.645828\n",
      "..........\n",
      "[1,   400] loss: 18.609975\n",
      "..........\n",
      "[1,   500] loss: 18.788728\n",
      "..........\n",
      "[1,   600] loss: 18.857650\n",
      "..........\n",
      "[1,   700] loss: 18.802848\n",
      "..........\n",
      "[1,   800] loss: 18.875712\n",
      "..........\n",
      "[1,   900] loss: 18.772321\n",
      "..........\n",
      "[1,  1000] loss: 18.912985\n",
      "..........\n",
      "[1,  1100] loss: 18.550766\n",
      "..........\n",
      "[1,  1200] loss: 18.527315\n",
      "..........\n",
      "[1,  1300] loss: 19.108100\n",
      "..........\n",
      "[1,  1400] loss: 18.693828\n",
      "..........\n",
      "[1,  1500] loss: 18.737129\n",
      "..........\n",
      "[1,  1600] loss: 19.230809\n",
      "..........\n",
      "[1,  1700] loss: 18.784022\n",
      "..........\n",
      "[1,  1800] loss: 18.885002\n",
      "..........\n",
      "[1,  1900] loss: 18.935186\n",
      "..........\n",
      "[1,  2000] loss: 19.162150\n",
      "..........\n",
      "[1,  2100] loss: 18.963271\n",
      "..........\n",
      "[1,  2200] loss: 18.988382\n",
      "..........\n",
      "[1,  2300] loss: 18.746524\n",
      "..........\n",
      "[1,  2400] loss: 18.912315\n",
      "..........\n",
      "[1,  2500] loss: 19.087285\n",
      "..........\n",
      "[1,  2600] loss: 19.214981\n",
      "..........\n",
      "[1,  2700] loss: 19.005031\n",
      "..........\n",
      "[1,  2800] loss: 19.036303\n",
      "..........\n",
      "[1,  2900] loss: 19.361265\n",
      "..........\n",
      "[1,  3000] loss: 19.259409\n",
      "..........\n",
      "[1,  3100] loss: 19.008745\n",
      "..........\n",
      "[1,  3200] loss: 19.129631\n",
      "..........\n",
      "[1,  3300] loss: 19.082338\n",
      "..........\n",
      "[1,  3400] loss: 19.274746\n",
      "..........\n",
      "[1,  3500] loss: 19.536462\n",
      "..........\n",
      "[1,  3600] loss: 19.387638\n",
      "..........\n",
      "[1,  3700] loss: 19.181164\n",
      "..........\n",
      "[1,  3800] loss: 19.559731\n",
      "..........\n",
      "[1,  3900] loss: 19.600180\n",
      "..........\n",
      "[1,  4000] loss: 19.277402\n",
      "..........\n",
      "[1,  4100] loss: 19.317476\n",
      "..........\n",
      "[1,  4200] loss: 19.642883\n",
      "..........\n",
      "[1,  4300] loss: 19.452512\n",
      "..........\n",
      "[1,  4400] loss: 19.668048\n",
      "..........\n",
      "[1,  4500] loss: 19.708335\n",
      "..........\n",
      "[1,  4600] loss: 19.566156\n",
      "..........\n",
      "[1,  4700] loss: 19.401230\n",
      "..........\n",
      "[1,  4800] loss: 19.886020\n",
      "..........\n",
      "[1,  4900] loss: 19.994559\n",
      "..........\n",
      "[1,  5000] loss: 19.673931\n",
      "..........\n",
      "[1,  5100] loss: 19.869652\n",
      "..........\n",
      "[1,  5200] loss: 19.840794\n",
      "..........\n",
      "[1,  5300] loss: 19.680687\n",
      "..........\n",
      "[1,  5400] loss: 20.131802\n",
      "..........\n",
      "[1,  5500] loss: 19.722678\n",
      "..........\n",
      "[1,  5600] loss: 19.370650\n",
      "..........\n",
      "[1,  5700] loss: 19.433350\n",
      "..........\n",
      "[1,  5800] loss: 19.666432\n",
      "..........\n",
      "[1,  5900] loss: 19.681247\n",
      "..........\n",
      "[1,  6000] loss: 19.517879\n",
      "..........\n",
      "[1,  6100] loss: 19.984291\n",
      "..........\n",
      "[1,  6200] loss: 19.726349\n",
      "..........\n",
      "[1,  6300] loss: 19.661025\n",
      "..........\n",
      "[1,  6400] loss: 19.808307\n",
      "..........\n",
      "[1,  6500] loss: 19.568207\n",
      "..........\n",
      "[1,  6600] loss: 19.942874\n",
      "..........\n",
      "[1,  6700] loss: 20.064552\n",
      "..........\n",
      "[1,  6800] loss: 20.121646\n",
      "..........\n",
      "[1,  6900] loss: 20.060710\n",
      "..........\n",
      "[1,  7000] loss: 20.469384\n",
      "..........\n",
      "[1,  7100] loss: 20.189135\n",
      "..........\n",
      "[1,  7200] loss: 20.476749\n",
      "..........\n",
      "[1,  7300] loss: 20.020101\n",
      "..........\n",
      "[1,  7400] loss: 19.900013\n",
      "..........\n",
      "[1,  7500] loss: 20.087865\n",
      "..........\n",
      "[1,  7600] loss: 20.102254\n",
      "..........\n",
      "[1,  7700] loss: 20.132225\n",
      "..........\n",
      "[1,  7800] loss: 20.182798\n",
      "..........\n",
      "[1,  7900] loss: 20.109653\n",
      "..........\n",
      "[1,  8000] loss: 19.898055\n",
      "total average loss : 19.424\n",
      "train acc : 0.1164\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.087\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.094\n",
      "step : 1600 / 3125 acc : 0.094\n",
      "step : 1800 / 3125 acc : 0.090\n",
      "step : 2000 / 3125 acc : 0.097\n",
      "step : 2200 / 3125 acc : 0.088\n",
      "step : 2400 / 3125 acc : 0.096\n",
      "step : 2600 / 3125 acc : 0.101\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.100\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1060 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 20.429812\n",
      "..........\n",
      "[1,   200] loss: 20.960125\n",
      "..........\n",
      "[1,   300] loss: 20.873897\n",
      "..........\n",
      "[1,   400] loss: 20.647582\n",
      "..........\n",
      "[1,   500] loss: 21.080794\n",
      "..........\n",
      "[1,   600] loss: 20.742020\n",
      "..........\n",
      "[1,   700] loss: 21.208567\n",
      "..........\n",
      "[1,   800] loss: 21.001992\n",
      "..........\n",
      "[1,   900] loss: 20.681423\n",
      "..........\n",
      "[1,  1000] loss: 21.123182\n",
      "..........\n",
      "[1,  1100] loss: 20.725907\n",
      "..........\n",
      "[1,  1200] loss: 20.931031\n",
      "..........\n",
      "[1,  1300] loss: 21.081492\n",
      "..........\n",
      "[1,  1400] loss: 21.262108\n",
      "..........\n",
      "[1,  1500] loss: 20.737895\n",
      "..........\n",
      "[1,  1600] loss: 20.993080\n",
      "..........\n",
      "[1,  1700] loss: 20.822196\n",
      "..........\n",
      "[1,  1800] loss: 20.974124\n",
      "..........\n",
      "[1,  1900] loss: 20.977754\n",
      "..........\n",
      "[1,  2000] loss: 21.164844\n",
      "..........\n",
      "[1,  2100] loss: 20.787935\n",
      "..........\n",
      "[1,  2200] loss: 21.010701\n",
      "..........\n",
      "[1,  2300] loss: 21.546153\n",
      "..........\n",
      "[1,  2400] loss: 21.086371\n",
      "..........\n",
      "[1,  2500] loss: 21.143655\n",
      "..........\n",
      "[1,  2600] loss: 21.251100\n",
      "..........\n",
      "[1,  2700] loss: 21.365926\n",
      "..........\n",
      "[1,  2800] loss: 20.978353\n",
      "..........\n",
      "[1,  2900] loss: 21.296000\n",
      "..........\n",
      "[1,  3000] loss: 21.193615\n",
      "..........\n",
      "[1,  3100] loss: 21.208682\n",
      "..........\n",
      "[1,  3200] loss: 21.161982\n",
      "..........\n",
      "[1,  3300] loss: 21.327318\n",
      "..........\n",
      "[1,  3400] loss: 21.305703\n",
      "..........\n",
      "[1,  3500] loss: 21.573300\n",
      "..........\n",
      "[1,  3600] loss: 21.661349\n",
      "..........\n",
      "[1,  3700] loss: 21.402543\n",
      "..........\n",
      "[1,  3800] loss: 21.044865\n",
      "..........\n",
      "[1,  3900] loss: 21.675431\n",
      "..........\n",
      "[1,  4000] loss: 21.493194\n",
      "..........\n",
      "[1,  4100] loss: 21.328558\n",
      "..........\n",
      "[1,  4200] loss: 21.589003\n",
      "..........\n",
      "[1,  4300] loss: 20.892086\n",
      "..........\n",
      "[1,  4400] loss: 21.658325\n",
      "..........\n",
      "[1,  4500] loss: 21.625300\n",
      "..........\n",
      "[1,  4600] loss: 21.585098\n",
      "..........\n",
      "[1,  4700] loss: 21.477732\n",
      "..........\n",
      "[1,  4800] loss: 21.464506\n",
      "..........\n",
      "[1,  4900] loss: 21.747405\n",
      "..........\n",
      "[1,  5000] loss: 21.738937\n",
      "..........\n",
      "[1,  5100] loss: 21.200806\n",
      "..........\n",
      "[1,  5200] loss: 22.036683\n",
      "..........\n",
      "[1,  5300] loss: 21.688476\n",
      "..........\n",
      "[1,  5400] loss: 22.172632\n",
      "..........\n",
      "[1,  5500] loss: 21.549808\n",
      "..........\n",
      "[1,  5600] loss: 22.115805\n",
      "..........\n",
      "[1,  5700] loss: 21.715563\n",
      "..........\n",
      "[1,  5800] loss: 21.783778\n",
      "..........\n",
      "[1,  5900] loss: 21.914523\n",
      "..........\n",
      "[1,  6000] loss: 21.353136\n",
      "..........\n",
      "[1,  6100] loss: 21.855159\n",
      "..........\n",
      "[1,  6200] loss: 21.836550\n",
      "..........\n",
      "[1,  6300] loss: 21.864654\n",
      "..........\n",
      "[1,  6400] loss: 21.857837\n",
      "..........\n",
      "[1,  6500] loss: 22.314052\n",
      "..........\n",
      "[1,  6600] loss: 21.990454\n",
      "..........\n",
      "[1,  6700] loss: 21.616660\n",
      "..........\n",
      "[1,  6800] loss: 21.967759\n",
      "..........\n",
      "[1,  6900] loss: 21.805067\n",
      "..........\n",
      "[1,  7000] loss: 21.418461\n",
      "..........\n",
      "[1,  7100] loss: 21.796649\n",
      "..........\n",
      "[1,  7200] loss: 22.125456\n",
      "..........\n",
      "[1,  7300] loss: 21.778733\n",
      "..........\n",
      "[1,  7400] loss: 21.764111\n",
      "..........\n",
      "[1,  7500] loss: 22.003485\n",
      "..........\n",
      "[1,  7600] loss: 22.009035\n",
      "..........\n",
      "[1,  7700] loss: 22.324508\n",
      "..........\n",
      "[1,  7800] loss: 21.654907\n",
      "..........\n",
      "[1,  7900] loss: 21.963086\n",
      "..........\n",
      "[1,  8000] loss: 22.264566\n",
      "total average loss : 21.435\n",
      "train acc : 0.1133\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.047\n",
      "step : 600 / 3125 acc : 0.062\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.087\n",
      "step : 1200 / 3125 acc : 0.083\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.111\n",
      "step : 2000 / 3125 acc : 0.109\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.115\n",
      "step : 2800 / 3125 acc : 0.112\n",
      "step : 3000 / 3125 acc : 0.121\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1200 %\n",
      "======eval  end ======\n",
      "======= epoch  2 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 22.690466\n",
      "..........\n",
      "[1,   200] loss: 22.452758\n",
      "..........\n",
      "[1,   300] loss: 23.004569\n",
      "..........\n",
      "[1,   400] loss: 22.543159\n",
      "..........\n",
      "[1,   500] loss: 22.729713\n",
      "..........\n",
      "[1,   600] loss: 22.906294\n",
      "..........\n",
      "[1,   700] loss: 22.692054\n",
      "..........\n",
      "[1,   800] loss: 23.228925\n",
      "..........\n",
      "[1,   900] loss: 22.667508\n",
      "..........\n",
      "[1,  1000] loss: 22.357144\n",
      "..........\n",
      "[1,  1100] loss: 22.628273\n",
      "..........\n",
      "[1,  1200] loss: 22.623879\n",
      "..........\n",
      "[1,  1300] loss: 22.848905\n",
      "..........\n",
      "[1,  1400] loss: 22.568955\n",
      "..........\n",
      "[1,  1500] loss: 23.055641\n",
      "..........\n",
      "[1,  1600] loss: 22.824887\n",
      "..........\n",
      "[1,  1700] loss: 22.587355\n",
      "..........\n",
      "[1,  1800] loss: 22.309948\n",
      "..........\n",
      "[1,  1900] loss: 22.418944\n",
      "..........\n",
      "[1,  2000] loss: 22.816544\n",
      "..........\n",
      "[1,  2100] loss: 22.324778\n",
      "..........\n",
      "[1,  2200] loss: 22.658988\n",
      "..........\n",
      "[1,  2300] loss: 22.932725\n",
      "..........\n",
      "[1,  2400] loss: 23.066667\n",
      "..........\n",
      "[1,  2500] loss: 23.231287\n",
      "..........\n",
      "[1,  2600] loss: 23.008919\n",
      "..........\n",
      "[1,  2700] loss: 23.428266\n",
      "..........\n",
      "[1,  2800] loss: 23.297052\n",
      "..........\n",
      "[1,  2900] loss: 23.299759\n",
      "..........\n",
      "[1,  3000] loss: 23.018315\n",
      "..........\n",
      "[1,  3100] loss: 22.882826\n",
      "..........\n",
      "[1,  3200] loss: 23.406268\n",
      "..........\n",
      "[1,  3300] loss: 23.282680\n",
      "..........\n",
      "[1,  3400] loss: 22.911732\n",
      "..........\n",
      "[1,  3500] loss: 23.063359\n",
      "..........\n",
      "[1,  3600] loss: 23.693777\n",
      "..........\n",
      "[1,  3700] loss: 23.153347\n",
      "..........\n",
      "[1,  3800] loss: 23.061500\n",
      "..........\n",
      "[1,  3900] loss: 23.607602\n",
      "..........\n",
      "[1,  4000] loss: 23.456077\n",
      "..........\n",
      "[1,  4100] loss: 22.946092\n",
      "..........\n",
      "[1,  4200] loss: 23.165770\n",
      "..........\n",
      "[1,  4300] loss: 23.154399\n",
      "..........\n",
      "[1,  4400] loss: 23.400770\n",
      "..........\n",
      "[1,  4500] loss: 22.865877\n",
      "..........\n",
      "[1,  4600] loss: 23.729771\n",
      "..........\n",
      "[1,  4700] loss: 23.580461\n",
      "..........\n",
      "[1,  4800] loss: 23.571608\n",
      "..........\n",
      "[1,  4900] loss: 23.465238\n",
      "..........\n",
      "[1,  5000] loss: 23.383010\n",
      "..........\n",
      "[1,  5100] loss: 23.344438\n",
      "..........\n",
      "[1,  5200] loss: 23.134809\n",
      "..........\n",
      "[1,  5300] loss: 23.790342\n",
      "..........\n",
      "[1,  5400] loss: 23.428883\n",
      "..........\n",
      "[1,  5500] loss: 23.117397\n",
      "..........\n",
      "[1,  5600] loss: 23.716919\n",
      "..........\n",
      "[1,  5700] loss: 23.832078\n",
      "..........\n",
      "[1,  5800] loss: 23.189779\n",
      "..........\n",
      "[1,  5900] loss: 23.689898\n",
      "..........\n",
      "[1,  6000] loss: 23.761823\n",
      "..........\n",
      "[1,  6100] loss: 23.243430\n",
      "..........\n",
      "[1,  6200] loss: 23.318396\n",
      "..........\n",
      "[1,  6300] loss: 23.704885\n",
      "..........\n",
      "[1,  6400] loss: 23.830574\n",
      "..........\n",
      "[1,  6500] loss: 23.862785\n",
      "..........\n",
      "[1,  6600] loss: 23.849940\n",
      "..........\n",
      "[1,  6700] loss: 23.528825\n",
      "..........\n",
      "[1,  6800] loss: 23.989108\n",
      "..........\n",
      "[1,  6900] loss: 23.841140\n",
      "..........\n",
      "[1,  7000] loss: 23.690037\n",
      "..........\n",
      "[1,  7100] loss: 24.127783\n",
      "..........\n",
      "[1,  7200] loss: 24.037178\n",
      "..........\n",
      "[1,  7300] loss: 23.927167\n",
      "..........\n",
      "[1,  7400] loss: 24.015277\n",
      "..........\n",
      "[1,  7500] loss: 23.688598\n",
      "..........\n",
      "[1,  7600] loss: 23.403460\n",
      "..........\n",
      "[1,  7700] loss: 23.476131\n",
      "..........\n",
      "[1,  7800] loss: 23.891260\n",
      "..........\n",
      "[1,  7900] loss: 24.011193\n",
      "..........\n",
      "[1,  8000] loss: 23.832241\n",
      "total average loss : 23.254\n",
      "train acc : 0.1078\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.129\n",
      "step : 1600 / 3125 acc : 0.125\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.125\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.117\n",
      "step : 2600 / 3125 acc : 0.115\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.115\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1180 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 24.236346\n",
      "..........\n",
      "[1,   200] loss: 24.757736\n",
      "..........\n",
      "[1,   300] loss: 24.139665\n",
      "..........\n",
      "[1,   400] loss: 24.362293\n",
      "..........\n",
      "[1,   500] loss: 24.525424\n",
      "..........\n",
      "[1,   600] loss: 24.378717\n",
      "..........\n",
      "[1,   700] loss: 24.554631\n",
      "..........\n",
      "[1,   800] loss: 24.428909\n",
      "..........\n",
      "[1,   900] loss: 24.647737\n",
      "..........\n",
      "[1,  1000] loss: 25.161271\n",
      "..........\n",
      "[1,  1100] loss: 24.099394\n",
      "..........\n",
      "[1,  1200] loss: 24.996417\n",
      "..........\n",
      "[1,  1300] loss: 24.336065\n",
      "..........\n",
      "[1,  1400] loss: 24.326714\n",
      "..........\n",
      "[1,  1500] loss: 24.752696\n",
      "..........\n",
      "[1,  1600] loss: 24.344373\n",
      "..........\n",
      "[1,  1700] loss: 24.620527\n",
      "..........\n",
      "[1,  1800] loss: 24.795378\n",
      "..........\n",
      "[1,  1900] loss: 24.339371\n",
      "..........\n",
      "[1,  2000] loss: 24.622689\n",
      "..........\n",
      "[1,  2100] loss: 24.952056\n",
      "..........\n",
      "[1,  2200] loss: 25.032467\n",
      "..........\n",
      "[1,  2300] loss: 24.705586\n",
      "..........\n",
      "[1,  2400] loss: 25.137953\n",
      "..........\n",
      "[1,  2500] loss: 24.864465\n",
      "..........\n",
      "[1,  2600] loss: 24.389568\n",
      "..........\n",
      "[1,  2700] loss: 25.196577\n",
      "..........\n",
      "[1,  2800] loss: 24.996360\n",
      "..........\n",
      "[1,  2900] loss: 25.339559\n",
      "..........\n",
      "[1,  3000] loss: 24.490351\n",
      "..........\n",
      "[1,  3100] loss: 24.390603\n",
      "..........\n",
      "[1,  3200] loss: 24.922190\n",
      "..........\n",
      "[1,  3300] loss: 24.938048\n",
      "..........\n",
      "[1,  3400] loss: 24.908657\n",
      "..........\n",
      "[1,  3500] loss: 24.931129\n",
      "..........\n",
      "[1,  3600] loss: 24.858727\n",
      "..........\n",
      "[1,  3700] loss: 24.892458\n",
      "..........\n",
      "[1,  3800] loss: 25.091571\n",
      "..........\n",
      "[1,  3900] loss: 25.076991\n",
      "..........\n",
      "[1,  4000] loss: 24.980395\n",
      "..........\n",
      "[1,  4100] loss: 25.012032\n",
      "..........\n",
      "[1,  4200] loss: 24.722937\n",
      "..........\n",
      "[1,  4300] loss: 25.222825\n",
      "..........\n",
      "[1,  4400] loss: 25.175070\n",
      "..........\n",
      "[1,  4500] loss: 24.913472\n",
      "..........\n",
      "[1,  4600] loss: 24.656903\n",
      "..........\n",
      "[1,  4700] loss: 25.354362\n",
      "..........\n",
      "[1,  4800] loss: 24.778459\n",
      "..........\n",
      "[1,  4900] loss: 25.411638\n",
      "..........\n",
      "[1,  5000] loss: 25.456913\n",
      "..........\n",
      "[1,  5100] loss: 25.422346\n",
      "..........\n",
      "[1,  5200] loss: 24.600257\n",
      "..........\n",
      "[1,  5300] loss: 25.610370\n",
      "..........\n",
      "[1,  5400] loss: 25.196717\n",
      "..........\n",
      "[1,  5500] loss: 25.282804\n",
      "..........\n",
      "[1,  5600] loss: 25.638983\n",
      "..........\n",
      "[1,  5700] loss: 25.298721\n",
      "..........\n",
      "[1,  5800] loss: 25.340101\n",
      "..........\n",
      "[1,  5900] loss: 24.795374\n",
      "..........\n",
      "[1,  6000] loss: 25.337539\n",
      "..........\n",
      "[1,  6100] loss: 25.666931\n",
      "..........\n",
      "[1,  6200] loss: 25.527573\n",
      "..........\n",
      "[1,  6300] loss: 25.798693\n",
      "..........\n",
      "[1,  6400] loss: 24.958480\n",
      "..........\n",
      "[1,  6500] loss: 25.206951\n",
      "..........\n",
      "[1,  6600] loss: 25.273562\n",
      "..........\n",
      "[1,  6700] loss: 25.139587\n",
      "..........\n",
      "[1,  6800] loss: 25.499849\n",
      "..........\n",
      "[1,  6900] loss: 25.542587\n",
      "..........\n",
      "[1,  7000] loss: 25.446206\n",
      "..........\n",
      "[1,  7100] loss: 25.367024\n",
      "..........\n",
      "[1,  7200] loss: 25.713130\n",
      "..........\n",
      "[1,  7300] loss: 25.733456\n",
      "..........\n",
      "[1,  7400] loss: 25.622788\n",
      "..........\n",
      "[1,  7500] loss: 25.297560\n",
      "..........\n",
      "[1,  7600] loss: 25.331695\n",
      "..........\n",
      "[1,  7700] loss: 25.580530\n",
      "..........\n",
      "[1,  7800] loss: 25.649269\n",
      "..........\n",
      "[1,  7900] loss: 25.571555\n",
      "..........\n",
      "[1,  8000] loss: 26.455483\n",
      "total average loss : 25.027\n",
      "train acc : 0.1031\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.120\n",
      "step : 1400 / 3125 acc : 0.121\n",
      "step : 1600 / 3125 acc : 0.125\n",
      "step : 1800 / 3125 acc : 0.125\n",
      "step : 2000 / 3125 acc : 0.119\n",
      "step : 2200 / 3125 acc : 0.122\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.120\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.113\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 25.855663\n",
      "..........\n",
      "[1,   200] loss: 26.473158\n",
      "..........\n",
      "[1,   300] loss: 26.191347\n",
      "..........\n",
      "[1,   400] loss: 26.268269\n",
      "..........\n",
      "[1,   500] loss: 26.097895\n",
      "..........\n",
      "[1,   600] loss: 26.241972\n",
      "..........\n",
      "[1,   700] loss: 26.609529\n",
      "..........\n",
      "[1,   800] loss: 26.407727\n",
      "..........\n",
      "[1,   900] loss: 26.290304\n",
      "..........\n",
      "[1,  1000] loss: 25.871278\n",
      "..........\n",
      "[1,  1100] loss: 26.157659\n",
      "..........\n",
      "[1,  1200] loss: 26.332044\n",
      "..........\n",
      "[1,  1300] loss: 26.031287\n",
      "..........\n",
      "[1,  1400] loss: 25.521425\n",
      "..........\n",
      "[1,  1500] loss: 25.774957\n",
      "..........\n",
      "[1,  1600] loss: 26.171781\n",
      "..........\n",
      "[1,  1700] loss: 25.986793\n",
      "..........\n",
      "[1,  1800] loss: 26.273797\n",
      "..........\n",
      "[1,  1900] loss: 26.475006\n",
      "..........\n",
      "[1,  2000] loss: 26.387311\n",
      "..........\n",
      "[1,  2100] loss: 26.693199\n",
      "..........\n",
      "[1,  2200] loss: 26.735236\n",
      "..........\n",
      "[1,  2300] loss: 26.372885\n",
      "..........\n",
      "[1,  2400] loss: 26.263225\n",
      "..........\n",
      "[1,  2500] loss: 26.683351\n",
      "..........\n",
      "[1,  2600] loss: 26.534172\n",
      "..........\n",
      "[1,  2700] loss: 26.096491\n",
      "..........\n",
      "[1,  2800] loss: 26.424034\n",
      "..........\n",
      "[1,  2900] loss: 27.161689\n",
      "..........\n",
      "[1,  3000] loss: 26.591177\n",
      "..........\n",
      "[1,  3100] loss: 26.645951\n",
      "..........\n",
      "[1,  3200] loss: 26.655177\n",
      "..........\n",
      "[1,  3300] loss: 26.712098\n",
      "..........\n",
      "[1,  3400] loss: 26.808409\n",
      "..........\n",
      "[1,  3500] loss: 26.508181\n",
      "..........\n",
      "[1,  3600] loss: 26.259194\n",
      "..........\n",
      "[1,  3700] loss: 26.434756\n",
      "..........\n",
      "[1,  3800] loss: 26.882595\n",
      "..........\n",
      "[1,  3900] loss: 27.099919\n",
      "..........\n",
      "[1,  4000] loss: 26.641769\n",
      "..........\n",
      "[1,  4100] loss: 26.526440\n",
      "..........\n",
      "[1,  4200] loss: 27.138015\n",
      "..........\n",
      "[1,  4300] loss: 26.453362\n",
      "..........\n",
      "[1,  4400] loss: 27.179630\n",
      "..........\n",
      "[1,  4500] loss: 26.734382\n",
      "..........\n",
      "[1,  4600] loss: 26.779103\n",
      "..........\n",
      "[1,  4700] loss: 26.863501\n",
      "..........\n",
      "[1,  4800] loss: 27.235690\n",
      "..........\n",
      "[1,  4900] loss: 27.545620\n",
      "..........\n",
      "[1,  5000] loss: 26.260306\n",
      "..........\n",
      "[1,  5100] loss: 27.128478\n",
      "..........\n",
      "[1,  5200] loss: 26.861280\n",
      "..........\n",
      "[1,  5300] loss: 26.950988\n",
      "..........\n",
      "[1,  5400] loss: 26.651535\n",
      "..........\n",
      "[1,  5500] loss: 26.908779\n",
      "..........\n",
      "[1,  5600] loss: 27.235130\n",
      "..........\n",
      "[1,  5700] loss: 27.218463\n",
      "..........\n",
      "[1,  5800] loss: 27.343088\n",
      "..........\n",
      "[1,  5900] loss: 26.816638\n",
      "..........\n",
      "[1,  6000] loss: 27.647055\n",
      "..........\n",
      "[1,  6100] loss: 27.511304\n",
      "..........\n",
      "[1,  6200] loss: 27.689193\n",
      "..........\n",
      "[1,  6300] loss: 27.829545\n",
      "..........\n",
      "[1,  6400] loss: 27.372979\n",
      "..........\n",
      "[1,  6500] loss: 26.764186\n",
      "..........\n",
      "[1,  6600] loss: 27.215997\n",
      "..........\n",
      "[1,  6700] loss: 27.265663\n",
      "..........\n",
      "[1,  6800] loss: 26.742253\n",
      "..........\n",
      "[1,  6900] loss: 27.184548\n",
      "..........\n",
      "[1,  7000] loss: 27.120971\n",
      "..........\n",
      "[1,  7100] loss: 27.597970\n",
      "..........\n",
      "[1,  7200] loss: 27.542599\n",
      "..........\n",
      "[1,  7300] loss: 27.777051\n",
      "..........\n",
      "[1,  7400] loss: 27.450242\n",
      "..........\n",
      "[1,  7500] loss: 26.893305\n",
      "..........\n",
      "[1,  7600] loss: 27.726121\n",
      "..........\n",
      "[1,  7700] loss: 28.060910\n",
      "..........\n",
      "[1,  7800] loss: 27.330993\n",
      "..........\n",
      "[1,  7900] loss: 27.940143\n",
      "..........\n",
      "[1,  8000] loss: 27.481617\n",
      "total average loss : 26.795\n",
      "train acc : 0.0938\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.146\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.131\n",
      "step : 1200 / 3125 acc : 0.146\n",
      "step : 1400 / 3125 acc : 0.147\n",
      "step : 1600 / 3125 acc : 0.137\n",
      "step : 1800 / 3125 acc : 0.132\n",
      "step : 2000 / 3125 acc : 0.125\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.112\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.118\n",
      "step : 3000 / 3125 acc : 0.115\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1160 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 27.700912\n",
      "..........\n",
      "[1,   200] loss: 28.122771\n",
      "..........\n",
      "[1,   300] loss: 27.750372\n",
      "..........\n",
      "[1,   400] loss: 27.843327\n",
      "..........\n",
      "[1,   500] loss: 27.983733\n",
      "..........\n",
      "[1,   600] loss: 28.067327\n",
      "..........\n",
      "[1,   700] loss: 28.553895\n",
      "..........\n",
      "[1,   800] loss: 27.966494\n",
      "..........\n",
      "[1,   900] loss: 28.154312\n",
      "..........\n",
      "[1,  1000] loss: 28.058941\n",
      "..........\n",
      "[1,  1100] loss: 28.176982\n",
      "..........\n",
      "[1,  1200] loss: 28.067648\n",
      "..........\n",
      "[1,  1300] loss: 28.060192\n",
      "..........\n",
      "[1,  1400] loss: 28.659232\n",
      "..........\n",
      "[1,  1500] loss: 28.043404\n",
      "..........\n",
      "[1,  1600] loss: 28.044602\n",
      "..........\n",
      "[1,  1700] loss: 28.326055\n",
      "..........\n",
      "[1,  1800] loss: 28.406205\n",
      "..........\n",
      "[1,  1900] loss: 27.699636\n",
      "..........\n",
      "[1,  2000] loss: 28.687571\n",
      "..........\n",
      "[1,  2100] loss: 28.460447\n",
      "..........\n",
      "[1,  2200] loss: 28.007059\n",
      "..........\n",
      "[1,  2300] loss: 28.607598\n",
      "..........\n",
      "[1,  2400] loss: 28.633321\n",
      "..........\n",
      "[1,  2500] loss: 28.556085\n",
      "..........\n",
      "[1,  2600] loss: 28.194170\n",
      "..........\n",
      "[1,  2700] loss: 28.238286\n",
      "..........\n",
      "[1,  2800] loss: 28.996357\n",
      "..........\n",
      "[1,  2900] loss: 28.743715\n",
      "..........\n",
      "[1,  3000] loss: 28.198754\n",
      "..........\n",
      "[1,  3100] loss: 28.461940\n",
      "..........\n",
      "[1,  3200] loss: 29.009290\n",
      "..........\n",
      "[1,  3300] loss: 28.261833\n",
      "..........\n",
      "[1,  3400] loss: 28.701890\n",
      "..........\n",
      "[1,  3500] loss: 28.857323\n",
      "..........\n",
      "[1,  3600] loss: 28.360261\n",
      "..........\n",
      "[1,  3700] loss: 28.671938\n",
      "..........\n",
      "[1,  3800] loss: 28.261430\n",
      "..........\n",
      "[1,  3900] loss: 28.912288\n",
      "..........\n",
      "[1,  4000] loss: 28.917049\n",
      "..........\n",
      "[1,  4100] loss: 29.146469\n",
      "..........\n",
      "[1,  4200] loss: 29.033269\n",
      "..........\n",
      "[1,  4300] loss: 28.426996\n",
      "..........\n",
      "[1,  4400] loss: 28.774700\n",
      "..........\n",
      "[1,  4500] loss: 29.116116\n",
      "..........\n",
      "[1,  4600] loss: 28.354290\n",
      "..........\n",
      "[1,  4700] loss: 29.107140\n",
      "..........\n",
      "[1,  4800] loss: 28.792180\n",
      "..........\n",
      "[1,  4900] loss: 29.258823\n",
      "..........\n",
      "[1,  5000] loss: 29.144776\n",
      "..........\n",
      "[1,  5100] loss: 28.746753\n",
      "..........\n",
      "[1,  5200] loss: 29.204938\n",
      "..........\n",
      "[1,  5300] loss: 29.244848\n",
      "..........\n",
      "[1,  5400] loss: 29.051399\n",
      "..........\n",
      "[1,  5500] loss: 28.856177\n",
      "..........\n",
      "[1,  5600] loss: 28.984973\n",
      "..........\n",
      "[1,  5700] loss: 28.692096\n",
      "..........\n",
      "[1,  5800] loss: 29.190688\n",
      "..........\n",
      "[1,  5900] loss: 29.513598\n",
      "..........\n",
      "[1,  6000] loss: 29.861455\n",
      "..........\n",
      "[1,  6100] loss: 29.383801\n",
      "..........\n",
      "[1,  6200] loss: 29.439762\n",
      "..........\n",
      "[1,  6300] loss: 28.877911\n",
      "..........\n",
      "[1,  6400] loss: 29.878510\n",
      "..........\n",
      "[1,  6500] loss: 28.487834\n",
      "..........\n",
      "[1,  6600] loss: 29.023493\n",
      "..........\n",
      "[1,  6700] loss: 29.305162\n",
      "..........\n",
      "[1,  6800] loss: 29.348128\n",
      "..........\n",
      "[1,  6900] loss: 29.322795\n",
      "..........\n",
      "[1,  7000] loss: 29.406433\n",
      "..........\n",
      "[1,  7100] loss: 29.151329\n",
      "..........\n",
      "[1,  7200] loss: 29.584978\n",
      "..........\n",
      "[1,  7300] loss: 28.930679\n",
      "..........\n",
      "[1,  7400] loss: 28.658386\n",
      "..........\n",
      "[1,  7500] loss: 29.308240\n",
      "..........\n",
      "[1,  7600] loss: 29.134466\n",
      "..........\n",
      "[1,  7700] loss: 29.407378\n",
      "..........\n",
      "[1,  7800] loss: 29.605021\n",
      "..........\n",
      "[1,  7900] loss: 29.502012\n",
      "..........\n",
      "[1,  8000] loss: 29.876433\n",
      "total average loss : 28.744\n",
      "train acc : 0.0922\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.135\n",
      "step : 800 / 3125 acc : 0.148\n",
      "step : 1000 / 3125 acc : 0.163\n",
      "step : 1200 / 3125 acc : 0.172\n",
      "step : 1400 / 3125 acc : 0.152\n",
      "step : 1600 / 3125 acc : 0.145\n",
      "step : 1800 / 3125 acc : 0.132\n",
      "step : 2000 / 3125 acc : 0.125\n",
      "step : 2200 / 3125 acc : 0.128\n",
      "step : 2400 / 3125 acc : 0.125\n",
      "step : 2600 / 3125 acc : 0.115\n",
      "step : 2800 / 3125 acc : 0.112\n",
      "step : 3000 / 3125 acc : 0.104\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1040 %\n",
      "======eval  end ======\n",
      "======= epoch  3 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 30.622711\n",
      "..........\n",
      "[1,   200] loss: 30.308375\n",
      "..........\n",
      "[1,   300] loss: 30.418849\n",
      "..........\n",
      "[1,   400] loss: 29.855911\n",
      "..........\n",
      "[1,   500] loss: 30.423577\n",
      "..........\n",
      "[1,   600] loss: 30.438039\n",
      "..........\n",
      "[1,   700] loss: 29.757388\n",
      "..........\n",
      "[1,   800] loss: 30.544204\n",
      "..........\n",
      "[1,   900] loss: 30.825320\n",
      "..........\n",
      "[1,  1000] loss: 30.348131\n",
      "..........\n",
      "[1,  1100] loss: 31.192742\n",
      "..........\n",
      "[1,  1200] loss: 30.296102\n",
      "..........\n",
      "[1,  1300] loss: 30.207966\n",
      "..........\n",
      "[1,  1400] loss: 30.921582\n",
      "..........\n",
      "[1,  1500] loss: 30.218827\n",
      "..........\n",
      "[1,  1600] loss: 30.385353\n",
      "..........\n",
      "[1,  1700] loss: 30.758127\n",
      "..........\n",
      "[1,  1800] loss: 30.659430\n",
      "..........\n",
      "[1,  1900] loss: 30.589044\n",
      "..........\n",
      "[1,  2000] loss: 30.975783\n",
      "..........\n",
      "[1,  2100] loss: 30.379218\n",
      "..........\n",
      "[1,  2200] loss: 30.651682\n",
      "..........\n",
      "[1,  2300] loss: 30.472951\n",
      "..........\n",
      "[1,  2400] loss: 30.889087\n",
      "..........\n",
      "[1,  2500] loss: 30.597176\n",
      "..........\n",
      "[1,  2600] loss: 31.238404\n",
      "..........\n",
      "[1,  2700] loss: 31.165836\n",
      "..........\n",
      "[1,  2800] loss: 31.359769\n",
      "..........\n",
      "[1,  2900] loss: 30.612210\n",
      "..........\n",
      "[1,  3000] loss: 30.567452\n",
      "..........\n",
      "[1,  3100] loss: 30.550580\n",
      "..........\n",
      "[1,  3200] loss: 30.387187\n",
      "..........\n",
      "[1,  3300] loss: 30.675336\n",
      "..........\n",
      "[1,  3400] loss: 30.561317\n",
      "..........\n",
      "[1,  3500] loss: 30.219101\n",
      "..........\n",
      "[1,  3600] loss: 30.971870\n",
      "..........\n",
      "[1,  3700] loss: 30.737005\n",
      "..........\n",
      "[1,  3800] loss: 30.948885\n",
      "..........\n",
      "[1,  3900] loss: 30.446130\n",
      "..........\n",
      "[1,  4000] loss: 30.505567\n",
      "..........\n",
      "[1,  4100] loss: 30.944763\n",
      "..........\n",
      "[1,  4200] loss: 30.875532\n",
      "..........\n",
      "[1,  4300] loss: 30.995728\n",
      "..........\n",
      "[1,  4400] loss: 31.239100\n",
      "..........\n",
      "[1,  4500] loss: 30.179107\n",
      "..........\n",
      "[1,  4600] loss: 31.221065\n",
      "..........\n",
      "[1,  4700] loss: 31.096286\n",
      "..........\n",
      "[1,  4800] loss: 30.813287\n",
      "..........\n",
      "[1,  4900] loss: 31.375033\n",
      "..........\n",
      "[1,  5000] loss: 31.313430\n",
      "..........\n",
      "[1,  5100] loss: 31.147702\n",
      "..........\n",
      "[1,  5200] loss: 31.233278\n",
      "..........\n",
      "[1,  5300] loss: 31.726922\n",
      "..........\n",
      "[1,  5400] loss: 31.389717\n",
      "..........\n",
      "[1,  5500] loss: 31.063960\n",
      "..........\n",
      "[1,  5600] loss: 31.684229\n",
      "..........\n",
      "[1,  5700] loss: 31.347549\n",
      "..........\n",
      "[1,  5800] loss: 31.454927\n",
      "..........\n",
      "[1,  5900] loss: 30.844035\n",
      "..........\n",
      "[1,  6000] loss: 32.008644\n",
      "..........\n",
      "[1,  6100] loss: 31.550017\n",
      "..........\n",
      "[1,  6200] loss: 31.973665\n",
      "..........\n",
      "[1,  6300] loss: 31.986933\n",
      "..........\n",
      "[1,  6400] loss: 31.279214\n",
      "..........\n",
      "[1,  6500] loss: 31.154970\n",
      "..........\n",
      "[1,  6600] loss: 31.131592\n",
      "..........\n",
      "[1,  6700] loss: 31.300912\n",
      "..........\n",
      "[1,  6800] loss: 31.742774\n",
      "..........\n",
      "[1,  6900] loss: 31.521920\n",
      "..........\n",
      "[1,  7000] loss: 31.690029\n",
      "..........\n",
      "[1,  7100] loss: 32.065277\n",
      "..........\n",
      "[1,  7200] loss: 31.196101\n",
      "..........\n",
      "[1,  7300] loss: 32.359910\n",
      "..........\n",
      "[1,  7400] loss: 31.260655\n",
      "..........\n",
      "[1,  7500] loss: 31.821901\n",
      "..........\n",
      "[1,  7600] loss: 31.928057\n",
      "..........\n",
      "[1,  7700] loss: 31.832178\n",
      "..........\n",
      "[1,  7800] loss: 31.544037\n",
      "..........\n",
      "[1,  7900] loss: 31.859156\n",
      "..........\n",
      "[1,  8000] loss: 32.156599\n",
      "total average loss : 31.012\n",
      "train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.062\n",
      "step : 1000 / 3125 acc : 0.069\n",
      "step : 1200 / 3125 acc : 0.078\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.094\n",
      "step : 1800 / 3125 acc : 0.090\n",
      "step : 2000 / 3125 acc : 0.091\n",
      "step : 2200 / 3125 acc : 0.088\n",
      "step : 2400 / 3125 acc : 0.081\n",
      "step : 2600 / 3125 acc : 0.096\n",
      "step : 2800 / 3125 acc : 0.107\n",
      "step : 3000 / 3125 acc : 0.102\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0980 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 32.578921\n",
      "..........\n",
      "[1,   200] loss: 32.696467\n",
      "..........\n",
      "[1,   300] loss: 32.384624\n",
      "..........\n",
      "[1,   400] loss: 33.100164\n",
      "..........\n",
      "[1,   500] loss: 32.636368\n",
      "..........\n",
      "[1,   600] loss: 32.779550\n",
      "..........\n",
      "[1,   700] loss: 32.541989\n",
      "..........\n",
      "[1,   800] loss: 32.943247\n",
      "..........\n",
      "[1,   900] loss: 32.729581\n",
      "..........\n",
      "[1,  1000] loss: 33.055525\n",
      "..........\n",
      "[1,  1100] loss: 32.942075\n",
      "..........\n",
      "[1,  1200] loss: 32.611465\n",
      "..........\n",
      "[1,  1300] loss: 32.950509\n",
      "..........\n",
      "[1,  1400] loss: 33.105865\n",
      "..........\n",
      "[1,  1500] loss: 32.185789\n",
      "..........\n",
      "[1,  1600] loss: 33.195359\n",
      "..........\n",
      "[1,  1700] loss: 32.630860\n",
      "..........\n",
      "[1,  1800] loss: 32.891770\n",
      "..........\n",
      "[1,  1900] loss: 33.237832\n",
      "..........\n",
      "[1,  2000] loss: 32.452692\n",
      "..........\n",
      "[1,  2100] loss: 32.688200\n",
      "..........\n",
      "[1,  2200] loss: 32.643445\n",
      "..........\n",
      "[1,  2300] loss: 33.359099\n",
      "..........\n",
      "[1,  2400] loss: 33.306976\n",
      "..........\n",
      "[1,  2500] loss: 32.955801\n",
      "..........\n",
      "[1,  2600] loss: 32.804595\n",
      "..........\n",
      "[1,  2700] loss: 32.852504\n",
      "..........\n",
      "[1,  2800] loss: 33.220963\n",
      "..........\n",
      "[1,  2900] loss: 32.795397\n",
      "..........\n",
      "[1,  3000] loss: 32.890253\n",
      "..........\n",
      "[1,  3100] loss: 32.623003\n",
      "..........\n",
      "[1,  3200] loss: 33.110346\n",
      "..........\n",
      "[1,  3300] loss: 33.604575\n",
      "..........\n",
      "[1,  3400] loss: 33.379345\n",
      "..........\n",
      "[1,  3500] loss: 33.431634\n",
      "..........\n",
      "[1,  3600] loss: 33.558101\n",
      "..........\n",
      "[1,  3700] loss: 33.352909\n",
      "..........\n",
      "[1,  3800] loss: 33.684847\n",
      "..........\n",
      "[1,  3900] loss: 34.019612\n",
      "..........\n",
      "[1,  4000] loss: 33.944784\n",
      "..........\n",
      "[1,  4100] loss: 33.638401\n",
      "..........\n",
      "[1,  4200] loss: 33.548499\n",
      "..........\n",
      "[1,  4300] loss: 33.395821\n",
      "..........\n",
      "[1,  4400] loss: 32.915092\n",
      "..........\n",
      "[1,  4500] loss: 33.375733\n",
      "..........\n",
      "[1,  4600] loss: 32.865740\n",
      "..........\n",
      "[1,  4700] loss: 33.070998\n",
      "..........\n",
      "[1,  4800] loss: 33.815986\n",
      "..........\n",
      "[1,  4900] loss: 34.057094\n",
      "..........\n",
      "[1,  5000] loss: 33.194641\n",
      "..........\n",
      "[1,  5100] loss: 33.710023\n",
      "..........\n",
      "[1,  5200] loss: 33.003622\n",
      "..........\n",
      "[1,  5300] loss: 33.445846\n",
      "..........\n",
      "[1,  5400] loss: 33.532419\n",
      "..........\n",
      "[1,  5500] loss: 33.255692\n",
      "..........\n",
      "[1,  5600] loss: 33.937232\n",
      "..........\n",
      "[1,  5700] loss: 34.317361\n",
      "..........\n",
      "[1,  5800] loss: 33.741000\n",
      "..........\n",
      "[1,  5900] loss: 34.272873\n",
      "..........\n",
      "[1,  6000] loss: 33.439822\n",
      "..........\n",
      "[1,  6100] loss: 33.728483\n",
      "..........\n",
      "[1,  6200] loss: 33.800974\n",
      "..........\n",
      "[1,  6300] loss: 34.403228\n",
      "..........\n",
      "[1,  6400] loss: 34.079540\n",
      "..........\n",
      "[1,  6500] loss: 33.523826\n",
      "..........\n",
      "[1,  6600] loss: 34.349861\n",
      "..........\n",
      "[1,  6700] loss: 33.909249\n",
      "..........\n",
      "[1,  6800] loss: 34.380163\n",
      "..........\n",
      "[1,  6900] loss: 34.291465\n",
      "..........\n",
      "[1,  7000] loss: 33.917807\n",
      "..........\n",
      "[1,  7100] loss: 33.284179\n",
      "..........\n",
      "[1,  7200] loss: 33.943408\n",
      "..........\n",
      "[1,  7300] loss: 34.176223\n",
      "..........\n",
      "[1,  7400] loss: 34.251830\n",
      "..........\n",
      "[1,  7500] loss: 34.293062\n",
      "..........\n",
      "[1,  7600] loss: 33.588651\n",
      "..........\n",
      "[1,  7700] loss: 33.988077\n",
      "..........\n",
      "[1,  7800] loss: 34.671607\n",
      "..........\n",
      "[1,  7900] loss: 34.302021\n",
      "..........\n",
      "[1,  8000] loss: 34.560123\n",
      "total average loss : 33.398\n",
      "train acc : 0.0945\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.188\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.089\n",
      "step : 1400 / 3125 acc : 0.089\n",
      "step : 1600 / 3125 acc : 0.086\n",
      "step : 1800 / 3125 acc : 0.076\n",
      "step : 2000 / 3125 acc : 0.078\n",
      "step : 2200 / 3125 acc : 0.080\n",
      "step : 2400 / 3125 acc : 0.083\n",
      "step : 2600 / 3125 acc : 0.091\n",
      "step : 2800 / 3125 acc : 0.094\n",
      "step : 3000 / 3125 acc : 0.094\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0940 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 34.948937\n",
      "..........\n",
      "[1,   200] loss: 35.152022\n",
      "..........\n",
      "[1,   300] loss: 34.597703\n",
      "..........\n",
      "[1,   400] loss: 35.071235\n",
      "..........\n",
      "[1,   500] loss: 35.081143\n",
      "..........\n",
      "[1,   600] loss: 34.855471\n",
      "..........\n",
      "[1,   700] loss: 35.358216\n",
      "..........\n",
      "[1,   800] loss: 35.108684\n",
      "..........\n",
      "[1,   900] loss: 34.914263\n",
      "..........\n",
      "[1,  1000] loss: 34.636572\n",
      "..........\n",
      "[1,  1100] loss: 34.393053\n",
      "..........\n",
      "[1,  1200] loss: 35.635864\n",
      "..........\n",
      "[1,  1300] loss: 35.888658\n",
      "..........\n",
      "[1,  1400] loss: 35.587061\n",
      "..........\n",
      "[1,  1500] loss: 34.895885\n",
      "..........\n",
      "[1,  1600] loss: 35.240406\n",
      "..........\n",
      "[1,  1700] loss: 35.034999\n",
      "..........\n",
      "[1,  1800] loss: 35.107033\n",
      "..........\n",
      "[1,  1900] loss: 35.499383\n",
      "..........\n",
      "[1,  2000] loss: 35.352836\n",
      "..........\n",
      "[1,  2100] loss: 35.143072\n",
      "..........\n",
      "[1,  2200] loss: 35.468149\n",
      "..........\n",
      "[1,  2300] loss: 34.447589\n",
      "..........\n",
      "[1,  2400] loss: 36.235846\n",
      "..........\n",
      "[1,  2500] loss: 36.104834\n",
      "..........\n",
      "[1,  2600] loss: 35.903093\n",
      "..........\n",
      "[1,  2700] loss: 35.608493\n",
      "..........\n",
      "[1,  2800] loss: 35.843008\n",
      "..........\n",
      "[1,  2900] loss: 35.017540\n",
      "..........\n",
      "[1,  3000] loss: 35.236882\n",
      "..........\n",
      "[1,  3100] loss: 35.708022\n",
      "..........\n",
      "[1,  3200] loss: 35.250906\n",
      "..........\n",
      "[1,  3300] loss: 35.412787\n",
      "..........\n",
      "[1,  3400] loss: 36.148745\n",
      "..........\n",
      "[1,  3500] loss: 36.254456\n",
      "..........\n",
      "[1,  3600] loss: 35.344811\n",
      "..........\n",
      "[1,  3700] loss: 36.507168\n",
      "..........\n",
      "[1,  3800] loss: 36.450583\n",
      "..........\n",
      "[1,  3900] loss: 35.632789\n",
      "..........\n",
      "[1,  4000] loss: 35.873219\n",
      "..........\n",
      "[1,  4100] loss: 36.292567\n",
      "..........\n",
      "[1,  4200] loss: 35.792768\n",
      "..........\n",
      "[1,  4300] loss: 35.370406\n",
      "..........\n",
      "[1,  4400] loss: 35.833530\n",
      "..........\n",
      "[1,  4500] loss: 36.224757\n",
      "..........\n",
      "[1,  4600] loss: 36.579607\n",
      "..........\n",
      "[1,  4700] loss: 36.035391\n",
      "..........\n",
      "[1,  4800] loss: 36.385921\n",
      "..........\n",
      "[1,  4900] loss: 36.016387\n",
      "..........\n",
      "[1,  5000] loss: 35.478458\n",
      "..........\n",
      "[1,  5100] loss: 36.166739\n",
      "..........\n",
      "[1,  5200] loss: 36.506096\n",
      "..........\n",
      "[1,  5300] loss: 35.991000\n",
      "..........\n",
      "[1,  5400] loss: 35.932307\n",
      "..........\n",
      "[1,  5500] loss: 36.290270\n",
      "..........\n",
      "[1,  5600] loss: 36.671153\n",
      "..........\n",
      "[1,  5700] loss: 36.751266\n",
      "..........\n",
      "[1,  5800] loss: 35.858510\n",
      "..........\n",
      "[1,  5900] loss: 36.591193\n",
      "..........\n",
      "[1,  6000] loss: 36.525542\n",
      "..........\n",
      "[1,  6100] loss: 36.060588\n",
      "..........\n",
      "[1,  6200] loss: 36.850081\n",
      "..........\n",
      "[1,  6300] loss: 36.402970\n",
      "..........\n",
      "[1,  6400] loss: 36.273209\n",
      "..........\n",
      "[1,  6500] loss: 36.606265\n",
      "..........\n",
      "[1,  6600] loss: 36.736116\n",
      "..........\n",
      "[1,  6700] loss: 36.806337\n",
      "..........\n",
      "[1,  6800] loss: 36.946908\n",
      "..........\n",
      "[1,  6900] loss: 36.457998\n",
      "..........\n",
      "[1,  7000] loss: 36.445107\n",
      "..........\n",
      "[1,  7100] loss: 36.323986\n",
      "..........\n",
      "[1,  7200] loss: 36.777013\n",
      "..........\n",
      "[1,  7300] loss: 35.875308\n",
      "..........\n",
      "[1,  7400] loss: 36.792784\n",
      "..........\n",
      "[1,  7500] loss: 35.877700\n",
      "..........\n",
      "[1,  7600] loss: 36.823792\n",
      "..........\n",
      "[1,  7700] loss: 36.481499\n",
      "..........\n",
      "[1,  7800] loss: 36.812397\n",
      "..........\n",
      "[1,  7900] loss: 36.722077\n",
      "..........\n",
      "[1,  8000] loss: 36.831829\n",
      "total average loss : 35.877\n",
      "train acc : 0.0992\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.070\n",
      "step : 1000 / 3125 acc : 0.069\n",
      "step : 1200 / 3125 acc : 0.062\n",
      "step : 1400 / 3125 acc : 0.067\n",
      "step : 1600 / 3125 acc : 0.066\n",
      "step : 1800 / 3125 acc : 0.076\n",
      "step : 2000 / 3125 acc : 0.075\n",
      "step : 2200 / 3125 acc : 0.077\n",
      "step : 2400 / 3125 acc : 0.073\n",
      "step : 2600 / 3125 acc : 0.075\n",
      "step : 2800 / 3125 acc : 0.080\n",
      "step : 3000 / 3125 acc : 0.079\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0860 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 37.867326\n",
      "..........\n",
      "[1,   200] loss: 37.708078\n",
      "..........\n",
      "[1,   300] loss: 36.985156\n",
      "..........\n",
      "[1,   400] loss: 37.454636\n",
      "..........\n",
      "[1,   500] loss: 37.523958\n",
      "..........\n",
      "[1,   600] loss: 37.466654\n",
      "..........\n",
      "[1,   700] loss: 37.580941\n",
      "..........\n",
      "[1,   800] loss: 37.430469\n",
      "..........\n",
      "[1,   900] loss: 38.303571\n",
      "..........\n",
      "[1,  1000] loss: 38.191469\n",
      "..........\n",
      "[1,  1100] loss: 38.168409\n",
      "..........\n",
      "[1,  1200] loss: 37.585105\n",
      "..........\n",
      "[1,  1300] loss: 37.545413\n",
      "..........\n",
      "[1,  1400] loss: 37.725605\n",
      "..........\n",
      "[1,  1500] loss: 38.162544\n",
      "..........\n",
      "[1,  1600] loss: 38.420964\n",
      "..........\n",
      "[1,  1700] loss: 37.720587\n",
      "..........\n",
      "[1,  1800] loss: 37.317945\n",
      "..........\n",
      "[1,  1900] loss: 37.317731\n",
      "..........\n",
      "[1,  2000] loss: 38.087893\n",
      "..........\n",
      "[1,  2100] loss: 37.954921\n",
      "..........\n",
      "[1,  2200] loss: 38.069831\n",
      "..........\n",
      "[1,  2300] loss: 38.668299\n",
      "..........\n",
      "[1,  2400] loss: 38.668876\n",
      "..........\n",
      "[1,  2500] loss: 37.967445\n",
      "..........\n",
      "[1,  2600] loss: 38.079703\n",
      "..........\n",
      "[1,  2700] loss: 38.123452\n",
      "..........\n",
      "[1,  2800] loss: 37.683131\n",
      "..........\n",
      "[1,  2900] loss: 37.459242\n",
      "..........\n",
      "[1,  3000] loss: 38.105049\n",
      "..........\n",
      "[1,  3100] loss: 38.550190\n",
      "..........\n",
      "[1,  3200] loss: 38.127648\n",
      "..........\n",
      "[1,  3300] loss: 37.663658\n",
      "..........\n",
      "[1,  3400] loss: 38.555973\n",
      "..........\n",
      "[1,  3500] loss: 38.203885\n",
      "..........\n",
      "[1,  3600] loss: 38.180941\n",
      "..........\n",
      "[1,  3700] loss: 38.505192\n",
      "..........\n",
      "[1,  3800] loss: 38.276829\n",
      "..........\n",
      "[1,  3900] loss: 38.458069\n",
      "..........\n",
      "[1,  4000] loss: 39.164043\n",
      "..........\n",
      "[1,  4100] loss: 38.664197\n",
      "..........\n",
      "[1,  4200] loss: 38.179193\n",
      "..........\n",
      "[1,  4300] loss: 38.031199\n",
      "..........\n",
      "[1,  4400] loss: 38.934104\n",
      "..........\n",
      "[1,  4500] loss: 38.550219\n",
      "..........\n",
      "[1,  4600] loss: 38.891688\n",
      "..........\n",
      "[1,  4700] loss: 38.481815\n",
      "..........\n",
      "[1,  4800] loss: 38.531813\n",
      "..........\n",
      "[1,  4900] loss: 38.726250\n",
      "..........\n",
      "[1,  5000] loss: 38.728140\n",
      "..........\n",
      "[1,  5100] loss: 38.125704\n",
      "..........\n",
      "[1,  5200] loss: 38.975344\n",
      "..........\n",
      "[1,  5300] loss: 38.600855\n",
      "..........\n",
      "[1,  5400] loss: 38.934673\n",
      "..........\n",
      "[1,  5500] loss: 39.038390\n",
      "..........\n",
      "[1,  5600] loss: 39.598816\n",
      "..........\n",
      "[1,  5700] loss: 38.386482\n",
      "..........\n",
      "[1,  5800] loss: 39.115993\n",
      "..........\n",
      "[1,  5900] loss: 39.534602\n",
      "..........\n",
      "[1,  6000] loss: 38.756616\n",
      "..........\n",
      "[1,  6100] loss: 38.821320\n",
      "..........\n",
      "[1,  6200] loss: 39.049589\n",
      "..........\n",
      "[1,  6300] loss: 39.461584\n",
      "..........\n",
      "[1,  6400] loss: 39.332957\n",
      "..........\n",
      "[1,  6500] loss: 38.557845\n",
      "..........\n",
      "[1,  6600] loss: 39.471812\n",
      "..........\n",
      "[1,  6700] loss: 38.926044\n",
      "..........\n",
      "[1,  6800] loss: 39.523718\n",
      "..........\n",
      "[1,  6900] loss: 39.357600\n",
      "..........\n",
      "[1,  7000] loss: 39.108297\n",
      "..........\n",
      "[1,  7100] loss: 39.013867\n",
      "..........\n",
      "[1,  7200] loss: 39.405829\n",
      "..........\n",
      "[1,  7300] loss: 39.226858\n",
      "..........\n",
      "[1,  7400] loss: 39.005562\n",
      "..........\n",
      "[1,  7500] loss: 39.855741\n",
      "..........\n",
      "[1,  7600] loss: 39.633937\n",
      "..........\n",
      "[1,  7700] loss: 39.319651\n",
      "..........\n",
      "[1,  7800] loss: 39.824943\n",
      "..........\n",
      "[1,  7900] loss: 38.868305\n",
      "..........\n",
      "[1,  8000] loss: 39.779564\n",
      "total average loss : 38.492\n",
      "train acc : 0.0914\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.135\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.094\n",
      "step : 1600 / 3125 acc : 0.090\n",
      "step : 1800 / 3125 acc : 0.094\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.094\n",
      "step : 2400 / 3125 acc : 0.094\n",
      "step : 2600 / 3125 acc : 0.089\n",
      "step : 2800 / 3125 acc : 0.087\n",
      "step : 3000 / 3125 acc : 0.094\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0920 %\n",
      "======eval  end ======\n",
      "======= epoch  4 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 40.065777\n",
      "..........\n",
      "[1,   200] loss: 40.411384\n",
      "..........\n",
      "[1,   300] loss: 39.882264\n",
      "..........\n",
      "[1,   400] loss: 40.030099\n",
      "..........\n",
      "[1,   500] loss: 39.529354\n",
      "..........\n",
      "[1,   600] loss: 40.472854\n",
      "..........\n",
      "[1,   700] loss: 40.422468\n",
      "..........\n",
      "[1,   800] loss: 40.636066\n",
      "..........\n",
      "[1,   900] loss: 39.915881\n",
      "..........\n",
      "[1,  1000] loss: 40.381967\n",
      "..........\n",
      "[1,  1100] loss: 40.684827\n",
      "..........\n",
      "[1,  1200] loss: 40.673124\n",
      "..........\n",
      "[1,  1300] loss: 40.158726\n",
      "..........\n",
      "[1,  1400] loss: 41.541398\n",
      "..........\n",
      "[1,  1500] loss: 40.644950\n",
      "..........\n",
      "[1,  1600] loss: 40.553505\n",
      "..........\n",
      "[1,  1700] loss: 40.539435\n",
      "..........\n",
      "[1,  1800] loss: 40.855977\n",
      "..........\n",
      "[1,  1900] loss: 40.926936\n",
      "..........\n",
      "[1,  2000] loss: 40.063698\n",
      "..........\n",
      "[1,  2100] loss: 40.714533\n",
      "..........\n",
      "[1,  2200] loss: 40.078830\n",
      "..........\n",
      "[1,  2300] loss: 40.989832\n",
      "..........\n",
      "[1,  2400] loss: 40.392390\n",
      "..........\n",
      "[1,  2500] loss: 40.432256\n",
      "..........\n",
      "[1,  2600] loss: 41.490405\n",
      "..........\n",
      "[1,  2700] loss: 40.664286\n",
      "..........\n",
      "[1,  2800] loss: 40.477850\n",
      "..........\n",
      "[1,  2900] loss: 40.623436\n",
      "..........\n",
      "[1,  3000] loss: 41.028312\n",
      "..........\n",
      "[1,  3100] loss: 40.659983\n",
      "..........\n",
      "[1,  3200] loss: 40.793304\n",
      "..........\n",
      "[1,  3300] loss: 41.006013\n",
      "..........\n",
      "[1,  3400] loss: 41.050023\n",
      "..........\n",
      "[1,  3500] loss: 41.419266\n",
      "..........\n",
      "[1,  3600] loss: 40.409624\n",
      "..........\n",
      "[1,  3700] loss: 40.519654\n",
      "..........\n",
      "[1,  3800] loss: 40.913316\n",
      "..........\n",
      "[1,  3900] loss: 41.001297\n",
      "..........\n",
      "[1,  4000] loss: 40.965206\n",
      "..........\n",
      "[1,  4100] loss: 40.955005\n",
      "..........\n",
      "[1,  4200] loss: 41.389143\n",
      "..........\n",
      "[1,  4300] loss: 41.386090\n",
      "..........\n",
      "[1,  4400] loss: 41.168645\n",
      "..........\n",
      "[1,  4500] loss: 41.639999\n",
      "..........\n",
      "[1,  4600] loss: 41.111990\n",
      "..........\n",
      "[1,  4700] loss: 40.931270\n",
      "..........\n",
      "[1,  4800] loss: 40.946456\n",
      "..........\n",
      "[1,  4900] loss: 41.679691\n",
      "..........\n",
      "[1,  5000] loss: 40.995184\n",
      "..........\n",
      "[1,  5100] loss: 41.619094\n",
      "..........\n",
      "[1,  5200] loss: 41.706422\n",
      "..........\n",
      "[1,  5300] loss: 42.279059\n",
      "..........\n",
      "[1,  5400] loss: 41.524898\n",
      "..........\n",
      "[1,  5500] loss: 41.439293\n",
      "..........\n",
      "[1,  5600] loss: 41.662744\n",
      "..........\n",
      "[1,  5700] loss: 41.951265\n",
      "..........\n",
      "[1,  5800] loss: 42.320875\n",
      "..........\n",
      "[1,  5900] loss: 42.190540\n",
      "..........\n",
      "[1,  6000] loss: 41.993250\n",
      "..........\n",
      "[1,  6100] loss: 41.473260\n",
      "..........\n",
      "[1,  6200] loss: 41.810602\n",
      "..........\n",
      "[1,  6300] loss: 41.606831\n",
      "..........\n",
      "[1,  6400] loss: 42.009662\n",
      "..........\n",
      "[1,  6500] loss: 42.012576\n",
      "..........\n",
      "[1,  6600] loss: 41.369945\n",
      "..........\n",
      "[1,  6700] loss: 41.693090\n",
      "..........\n",
      "[1,  6800] loss: 41.431607\n",
      "..........\n",
      "[1,  6900] loss: 41.336445\n",
      "..........\n",
      "[1,  7000] loss: 41.677420\n",
      "..........\n",
      "[1,  7100] loss: 41.149242\n",
      "..........\n",
      "[1,  7200] loss: 41.996696\n",
      "..........\n",
      "[1,  7300] loss: 41.650231\n",
      "..........\n",
      "[1,  7400] loss: 42.373370\n",
      "..........\n",
      "[1,  7500] loss: 41.582595\n",
      "..........\n",
      "[1,  7600] loss: 41.289390\n",
      "..........\n",
      "[1,  7700] loss: 42.870087\n",
      "..........\n",
      "[1,  7800] loss: 41.581552\n",
      "..........\n",
      "[1,  7900] loss: 41.937113\n",
      "..........\n",
      "[1,  8000] loss: 42.286532\n",
      "total average loss : 41.126\n",
      "train acc : 0.0836\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.000\n",
      "step : 400 / 3125 acc : 0.031\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.103\n",
      "step : 1600 / 3125 acc : 0.098\n",
      "step : 1800 / 3125 acc : 0.094\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.094\n",
      "step : 2400 / 3125 acc : 0.091\n",
      "step : 2600 / 3125 acc : 0.091\n",
      "step : 2800 / 3125 acc : 0.094\n",
      "step : 3000 / 3125 acc : 0.096\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0940 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 43.127966\n",
      "..........\n",
      "[1,   200] loss: 42.960399\n",
      "..........\n",
      "[1,   300] loss: 42.825512\n",
      "..........\n",
      "[1,   400] loss: 42.852168\n",
      "..........\n",
      "[1,   500] loss: 43.057567\n",
      "..........\n",
      "[1,   600] loss: 43.039617\n",
      "..........\n",
      "[1,   700] loss: 43.782672\n",
      "..........\n",
      "[1,   800] loss: 43.153613\n",
      "..........\n",
      "[1,   900] loss: 42.831144\n",
      "..........\n",
      "[1,  1000] loss: 43.040127\n",
      "..........\n",
      "[1,  1100] loss: 43.436603\n",
      "..........\n",
      "[1,  1200] loss: 43.032492\n",
      "..........\n",
      "[1,  1300] loss: 43.285803\n",
      "..........\n",
      "[1,  1400] loss: 43.313151\n",
      "..........\n",
      "[1,  1500] loss: 42.675202\n",
      "..........\n",
      "[1,  1600] loss: 43.573399\n",
      "..........\n",
      "[1,  1700] loss: 43.858001\n",
      "..........\n",
      "[1,  1800] loss: 43.377131\n",
      "..........\n",
      "[1,  1900] loss: 43.099033\n",
      "..........\n",
      "[1,  2000] loss: 44.067055\n",
      "..........\n",
      "[1,  2100] loss: 43.303263\n",
      "..........\n",
      "[1,  2200] loss: 43.325666\n",
      "..........\n",
      "[1,  2300] loss: 43.553978\n",
      "..........\n",
      "[1,  2400] loss: 43.395814\n",
      "..........\n",
      "[1,  2500] loss: 43.500403\n",
      "..........\n",
      "[1,  2600] loss: 42.411339\n",
      "..........\n",
      "[1,  2700] loss: 42.903025\n",
      "..........\n",
      "[1,  2800] loss: 43.316486\n",
      "..........\n",
      "[1,  2900] loss: 43.097556\n",
      "..........\n",
      "[1,  3000] loss: 43.676832\n",
      "..........\n",
      "[1,  3100] loss: 43.602436\n",
      "..........\n",
      "[1,  3200] loss: 43.064783\n",
      "..........\n",
      "[1,  3300] loss: 44.089087\n",
      "..........\n",
      "[1,  3400] loss: 43.928116\n",
      "..........\n",
      "[1,  3500] loss: 43.864904\n",
      "..........\n",
      "[1,  3600] loss: 44.165473\n",
      "..........\n",
      "[1,  3700] loss: 43.926655\n",
      "..........\n",
      "[1,  3800] loss: 43.810899\n",
      "..........\n",
      "[1,  3900] loss: 43.949014\n",
      "..........\n",
      "[1,  4000] loss: 42.987583\n",
      "..........\n",
      "[1,  4100] loss: 44.005763\n",
      "..........\n",
      "[1,  4200] loss: 43.526968\n",
      "..........\n",
      "[1,  4300] loss: 44.828715\n",
      "..........\n",
      "[1,  4400] loss: 44.451443\n",
      "..........\n",
      "[1,  4500] loss: 44.608569\n",
      "..........\n",
      "[1,  4600] loss: 44.127651\n",
      "..........\n",
      "[1,  4700] loss: 44.147385\n",
      "..........\n",
      "[1,  4800] loss: 44.137554\n",
      "..........\n",
      "[1,  4900] loss: 45.158716\n",
      "..........\n",
      "[1,  5000] loss: 43.872131\n",
      "..........\n",
      "[1,  5100] loss: 44.126366\n",
      "..........\n",
      "[1,  5200] loss: 44.236423\n",
      "..........\n",
      "[1,  5300] loss: 44.290478\n",
      "..........\n",
      "[1,  5400] loss: 44.028130\n",
      "..........\n",
      "[1,  5500] loss: 44.150788\n",
      "..........\n",
      "[1,  5600] loss: 44.314421\n",
      "..........\n",
      "[1,  5700] loss: 43.941333\n",
      "..........\n",
      "[1,  5800] loss: 43.417204\n",
      "..........\n",
      "[1,  5900] loss: 44.365422\n",
      "..........\n",
      "[1,  6000] loss: 44.067149\n",
      "..........\n",
      "[1,  6100] loss: 44.151436\n",
      "..........\n",
      "[1,  6200] loss: 44.111590\n",
      "..........\n",
      "[1,  6300] loss: 43.275248\n",
      "..........\n",
      "[1,  6400] loss: 44.629008\n",
      "..........\n",
      "[1,  6500] loss: 44.634650\n",
      "..........\n",
      "[1,  6600] loss: 44.585655\n",
      "..........\n",
      "[1,  6700] loss: 43.899815\n",
      "..........\n",
      "[1,  6800] loss: 45.091898\n",
      "..........\n",
      "[1,  6900] loss: 44.782794\n",
      "..........\n",
      "[1,  7000] loss: 44.612048\n",
      "..........\n",
      "[1,  7100] loss: 44.182837\n",
      "..........\n",
      "[1,  7200] loss: 44.707788\n",
      "..........\n",
      "[1,  7300] loss: 44.579779\n",
      "..........\n",
      "[1,  7400] loss: 45.489033\n",
      "..........\n",
      "[1,  7500] loss: 45.004881\n",
      "..........\n",
      "[1,  7600] loss: 44.742776\n",
      "..........\n",
      "[1,  7700] loss: 44.678738\n",
      "..........\n",
      "[1,  7800] loss: 44.871844\n",
      "..........\n",
      "[1,  7900] loss: 45.794745\n",
      "..........\n",
      "[1,  8000] loss: 45.491405\n",
      "total average loss : 43.892\n",
      "train acc : 0.0836\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.083\n",
      "step : 1400 / 3125 acc : 0.076\n",
      "step : 1600 / 3125 acc : 0.082\n",
      "step : 1800 / 3125 acc : 0.090\n",
      "step : 2000 / 3125 acc : 0.100\n",
      "step : 2200 / 3125 acc : 0.099\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.111\n",
      "step : 2800 / 3125 acc : 0.105\n",
      "step : 3000 / 3125 acc : 0.100\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0980 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 45.678576\n",
      "..........\n",
      "[1,   200] loss: 45.120354\n",
      "..........\n",
      "[1,   300] loss: 46.490312\n",
      "..........\n",
      "[1,   400] loss: 45.750237\n",
      "..........\n",
      "[1,   500] loss: 44.475186\n",
      "..........\n",
      "[1,   600] loss: 46.181170\n",
      "..........\n",
      "[1,   700] loss: 45.899554\n",
      "..........\n",
      "[1,   800] loss: 46.263863\n",
      "..........\n",
      "[1,   900] loss: 45.016256\n",
      "..........\n",
      "[1,  1000] loss: 45.934405\n",
      "..........\n",
      "[1,  1100] loss: 45.788239\n",
      "..........\n",
      "[1,  1200] loss: 44.781928\n",
      "..........\n",
      "[1,  1300] loss: 46.217712\n",
      "..........\n",
      "[1,  1400] loss: 46.943277\n",
      "..........\n",
      "[1,  1500] loss: 46.383675\n",
      "..........\n",
      "[1,  1600] loss: 46.766257\n",
      "..........\n",
      "[1,  1700] loss: 45.933778\n",
      "..........\n",
      "[1,  1800] loss: 46.086410\n",
      "..........\n",
      "[1,  1900] loss: 46.685668\n",
      "..........\n",
      "[1,  2000] loss: 46.132098\n",
      "..........\n",
      "[1,  2100] loss: 47.020740\n",
      "..........\n",
      "[1,  2200] loss: 46.689059\n",
      "..........\n",
      "[1,  2300] loss: 46.707435\n",
      "..........\n",
      "[1,  2400] loss: 46.979844\n",
      "..........\n",
      "[1,  2500] loss: 46.042442\n",
      "..........\n",
      "[1,  2600] loss: 46.096025\n",
      "..........\n",
      "[1,  2700] loss: 46.891849\n",
      "..........\n",
      "[1,  2800] loss: 46.139110\n",
      "..........\n",
      "[1,  2900] loss: 47.108605\n",
      "..........\n",
      "[1,  3000] loss: 46.717302\n",
      "..........\n",
      "[1,  3100] loss: 46.458581\n",
      "..........\n",
      "[1,  3200] loss: 45.833006\n",
      "..........\n",
      "[1,  3300] loss: 46.829946\n",
      "..........\n",
      "[1,  3400] loss: 46.482525\n",
      "..........\n",
      "[1,  3500] loss: 46.997439\n",
      "..........\n",
      "[1,  3600] loss: 46.223135\n",
      "..........\n",
      "[1,  3700] loss: 45.619094\n",
      "..........\n",
      "[1,  3800] loss: 46.277796\n",
      "..........\n",
      "[1,  3900] loss: 45.747912\n",
      "..........\n",
      "[1,  4000] loss: 46.665840\n",
      "..........\n",
      "[1,  4100] loss: 46.533733\n",
      "..........\n",
      "[1,  4200] loss: 45.816380\n",
      "..........\n",
      "[1,  4300] loss: 46.975459\n",
      "..........\n",
      "[1,  4400] loss: 46.845270\n",
      "..........\n",
      "[1,  4500] loss: 47.183404\n",
      "..........\n",
      "[1,  4600] loss: 46.859344\n",
      "..........\n",
      "[1,  4700] loss: 46.620305\n",
      "..........\n",
      "[1,  4800] loss: 47.794395\n",
      "..........\n",
      "[1,  4900] loss: 47.319107\n",
      "..........\n",
      "[1,  5000] loss: 47.125395\n",
      "..........\n",
      "[1,  5100] loss: 46.497773\n",
      "..........\n",
      "[1,  5200] loss: 46.823316\n",
      "..........\n",
      "[1,  5300] loss: 47.553082\n",
      "..........\n",
      "[1,  5400] loss: 47.389488\n",
      "..........\n",
      "[1,  5500] loss: 47.734596\n",
      "..........\n",
      "[1,  5600] loss: 46.874227\n",
      "..........\n",
      "[1,  5700] loss: 48.422666\n",
      "..........\n",
      "[1,  5800] loss: 47.641511\n",
      "..........\n",
      "[1,  5900] loss: 47.711322\n",
      "..........\n",
      "[1,  6000] loss: 46.699337\n",
      "..........\n",
      "[1,  6100] loss: 46.711434\n",
      "..........\n",
      "[1,  6200] loss: 47.044390\n",
      "..........\n",
      "[1,  6300] loss: 47.658541\n",
      "..........\n",
      "[1,  6400] loss: 46.597731\n",
      "..........\n",
      "[1,  6500] loss: 48.024536\n",
      "..........\n",
      "[1,  6600] loss: 47.508920\n",
      "..........\n",
      "[1,  6700] loss: 47.512411\n",
      "..........\n",
      "[1,  6800] loss: 48.244769\n",
      "..........\n",
      "[1,  6900] loss: 46.885775\n",
      "..........\n",
      "[1,  7000] loss: 47.514644\n",
      "..........\n",
      "[1,  7100] loss: 47.469998\n",
      "..........\n",
      "[1,  7200] loss: 47.076075\n",
      "..........\n",
      "[1,  7300] loss: 47.529687\n",
      "..........\n",
      "[1,  7400] loss: 48.198391\n",
      "..........\n",
      "[1,  7500] loss: 47.605749\n",
      "..........\n",
      "[1,  7600] loss: 46.940701\n",
      "..........\n",
      "[1,  7700] loss: 48.377257\n",
      "..........\n",
      "[1,  7800] loss: 47.703685\n",
      "..........\n",
      "[1,  7900] loss: 47.900342\n",
      "..........\n",
      "[1,  8000] loss: 47.591321\n",
      "total average loss : 46.757\n",
      "train acc : 0.0805\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.000\n",
      "step : 400 / 3125 acc : 0.047\n",
      "step : 600 / 3125 acc : 0.042\n",
      "step : 800 / 3125 acc : 0.055\n",
      "step : 1000 / 3125 acc : 0.062\n",
      "step : 1200 / 3125 acc : 0.068\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.074\n",
      "step : 1800 / 3125 acc : 0.073\n",
      "step : 2000 / 3125 acc : 0.075\n",
      "step : 2200 / 3125 acc : 0.077\n",
      "step : 2400 / 3125 acc : 0.089\n",
      "step : 2600 / 3125 acc : 0.096\n",
      "step : 2800 / 3125 acc : 0.098\n",
      "step : 3000 / 3125 acc : 0.098\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1020 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 48.882771\n",
      "..........\n",
      "[1,   200] loss: 48.162672\n",
      "..........\n",
      "[1,   300] loss: 48.853709\n",
      "..........\n",
      "[1,   400] loss: 48.459295\n",
      "..........\n",
      "[1,   500] loss: 48.984478\n",
      "..........\n",
      "[1,   600] loss: 49.106944\n",
      "..........\n",
      "[1,   700] loss: 47.702208\n",
      "..........\n",
      "[1,   800] loss: 49.276676\n",
      "..........\n",
      "[1,   900] loss: 48.554274\n",
      "..........\n",
      "[1,  1000] loss: 48.761252\n",
      "..........\n",
      "[1,  1100] loss: 48.381340\n",
      "..........\n",
      "[1,  1200] loss: 48.429279\n",
      "..........\n",
      "[1,  1300] loss: 49.814253\n",
      "..........\n",
      "[1,  1400] loss: 48.128399\n",
      "..........\n",
      "[1,  1500] loss: 49.169627\n",
      "..........\n",
      "[1,  1600] loss: 49.181542\n",
      "..........\n",
      "[1,  1700] loss: 49.069775\n",
      "..........\n",
      "[1,  1800] loss: 49.490916\n",
      "..........\n",
      "[1,  1900] loss: 49.173439\n",
      "..........\n",
      "[1,  2000] loss: 48.889004\n",
      "..........\n",
      "[1,  2100] loss: 49.294497\n",
      "..........\n",
      "[1,  2200] loss: 49.104801\n",
      "..........\n",
      "[1,  2300] loss: 49.132385\n",
      "..........\n",
      "[1,  2400] loss: 49.096999\n",
      "..........\n",
      "[1,  2500] loss: 49.233837\n",
      "..........\n",
      "[1,  2600] loss: 49.092533\n",
      "..........\n",
      "[1,  2700] loss: 49.345601\n",
      "..........\n",
      "[1,  2800] loss: 48.900598\n",
      "..........\n",
      "[1,  2900] loss: 50.155961\n",
      "..........\n",
      "[1,  3000] loss: 48.933175\n",
      "..........\n",
      "[1,  3100] loss: 50.344380\n",
      "..........\n",
      "[1,  3200] loss: 50.277514\n",
      "..........\n",
      "[1,  3300] loss: 49.985258\n",
      "..........\n",
      "[1,  3400] loss: 49.463292\n",
      "..........\n",
      "[1,  3500] loss: 49.852962\n",
      "..........\n",
      "[1,  3600] loss: 48.807543\n",
      "..........\n",
      "[1,  3700] loss: 49.780389\n",
      "..........\n",
      "[1,  3800] loss: 50.381364\n",
      "..........\n",
      "[1,  3900] loss: 49.798698\n",
      "..........\n",
      "[1,  4000] loss: 49.520376\n",
      "..........\n",
      "[1,  4100] loss: 49.992420\n",
      "..........\n",
      "[1,  4200] loss: 49.623286\n",
      "..........\n",
      "[1,  4300] loss: 49.149837\n",
      "..........\n",
      "[1,  4400] loss: 50.209074\n",
      "..........\n",
      "[1,  4500] loss: 49.705684\n",
      "..........\n",
      "[1,  4600] loss: 49.984633\n",
      "..........\n",
      "[1,  4700] loss: 48.851749\n",
      "..........\n",
      "[1,  4800] loss: 50.447068\n",
      "..........\n",
      "[1,  4900] loss: 50.257980\n",
      "..........\n",
      "[1,  5000] loss: 50.600192\n",
      "..........\n",
      "[1,  5100] loss: 49.758946\n",
      "..........\n",
      "[1,  5200] loss: 49.734756\n",
      "..........\n",
      "[1,  5300] loss: 50.481340\n",
      "..........\n",
      "[1,  5400] loss: 51.491102\n",
      "..........\n",
      "[1,  5500] loss: 50.834480\n",
      "..........\n",
      "[1,  5600] loss: 50.577344\n",
      "..........\n",
      "[1,  5700] loss: 49.579427\n",
      "..........\n",
      "[1,  5800] loss: 50.526099\n",
      "..........\n",
      "[1,  5900] loss: 49.401353\n",
      "..........\n",
      "[1,  6000] loss: 49.865533\n",
      "..........\n",
      "[1,  6100] loss: 50.260249\n",
      "..........\n",
      "[1,  6200] loss: 49.951055\n",
      "..........\n",
      "[1,  6300] loss: 51.406874\n",
      "..........\n",
      "[1,  6400] loss: 51.498465\n",
      "..........\n",
      "[1,  6500] loss: 50.716149\n",
      "..........\n",
      "[1,  6600] loss: 51.216974\n",
      "..........\n",
      "[1,  6700] loss: 50.334107\n",
      "..........\n",
      "[1,  6800] loss: 50.110054\n",
      "..........\n",
      "[1,  6900] loss: 50.134923\n",
      "..........\n",
      "[1,  7000] loss: 50.284806\n",
      "..........\n",
      "[1,  7100] loss: 51.534190\n",
      "..........\n",
      "[1,  7200] loss: 50.249674\n",
      "..........\n",
      "[1,  7300] loss: 50.323728\n",
      "..........\n",
      "[1,  7400] loss: 51.025996\n",
      "..........\n",
      "[1,  7500] loss: 50.589339\n",
      "..........\n",
      "[1,  7600] loss: 50.604686\n",
      "..........\n",
      "[1,  7700] loss: 50.420455\n",
      "..........\n",
      "[1,  7800] loss: 51.190244\n",
      "..........\n",
      "[1,  7900] loss: 50.354309\n",
      "..........\n",
      "[1,  8000] loss: 50.591678\n",
      "total average loss : 49.761\n",
      "train acc : 0.0820\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.078\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.083\n",
      "step : 1400 / 3125 acc : 0.103\n",
      "step : 1600 / 3125 acc : 0.105\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.109\n",
      "step : 2200 / 3125 acc : 0.102\n",
      "step : 2400 / 3125 acc : 0.107\n",
      "step : 2600 / 3125 acc : 0.113\n",
      "step : 2800 / 3125 acc : 0.114\n",
      "step : 3000 / 3125 acc : 0.110\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1140 %\n",
      "======eval  end ======\n",
      "======= epoch  5 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 52.509022\n",
      "..........\n",
      "[1,   200] loss: 50.607787\n",
      "..........\n",
      "[1,   300] loss: 52.333497\n",
      "..........\n",
      "[1,   400] loss: 52.750546\n",
      "..........\n",
      "[1,   500] loss: 51.032695\n",
      "..........\n",
      "[1,   600] loss: 51.519654\n",
      "..........\n",
      "[1,   700] loss: 52.753734\n",
      "..........\n",
      "[1,   800] loss: 52.516398\n",
      "..........\n",
      "[1,   900] loss: 51.551211\n",
      "..........\n",
      "[1,  1000] loss: 51.019103\n",
      "..........\n",
      "[1,  1100] loss: 52.616993\n",
      "..........\n",
      "[1,  1200] loss: 52.004724\n",
      "..........\n",
      "[1,  1300] loss: 51.839359\n",
      "..........\n",
      "[1,  1400] loss: 50.505544\n",
      "..........\n",
      "[1,  1500] loss: 51.958737\n",
      "..........\n",
      "[1,  1600] loss: 51.638725\n",
      "..........\n",
      "[1,  1700] loss: 51.628323\n",
      "..........\n",
      "[1,  1800] loss: 51.600515\n",
      "..........\n",
      "[1,  1900] loss: 51.668596\n",
      "..........\n",
      "[1,  2000] loss: 52.167305\n",
      "..........\n",
      "[1,  2100] loss: 53.099207\n",
      "..........\n",
      "[1,  2200] loss: 52.475960\n",
      "..........\n",
      "[1,  2300] loss: 52.159800\n",
      "..........\n",
      "[1,  2400] loss: 51.384278\n",
      "..........\n",
      "[1,  2500] loss: 51.928306\n",
      "..........\n",
      "[1,  2600] loss: 51.516543\n",
      "..........\n",
      "[1,  2700] loss: 52.884003\n",
      "..........\n",
      "[1,  2800] loss: 52.191057\n",
      "..........\n",
      "[1,  2900] loss: 52.026111\n",
      "..........\n",
      "[1,  3000] loss: 51.539779\n",
      "..........\n",
      "[1,  3100] loss: 52.627524\n",
      "..........\n",
      "[1,  3200] loss: 52.148397\n",
      "..........\n",
      "[1,  3300] loss: 52.751120\n",
      "..........\n",
      "[1,  3400] loss: 52.735203\n",
      "..........\n",
      "[1,  3500] loss: 53.248665\n",
      "..........\n",
      "[1,  3600] loss: 52.775123\n",
      "..........\n",
      "[1,  3700] loss: 52.314462\n",
      "..........\n",
      "[1,  3800] loss: 52.331594\n",
      "..........\n",
      "[1,  3900] loss: 52.513815\n",
      "..........\n",
      "[1,  4000] loss: 52.322943\n",
      "..........\n",
      "[1,  4100] loss: 52.482859\n",
      "..........\n",
      "[1,  4200] loss: 53.447480\n",
      "..........\n",
      "[1,  4300] loss: 51.782997\n",
      "..........\n",
      "[1,  4400] loss: 52.841254\n",
      "..........\n",
      "[1,  4500] loss: 53.359855\n",
      "..........\n",
      "[1,  4600] loss: 52.794743\n",
      "..........\n",
      "[1,  4700] loss: 52.572316\n",
      "..........\n",
      "[1,  4800] loss: 52.395546\n",
      "..........\n",
      "[1,  4900] loss: 52.868443\n",
      "..........\n",
      "[1,  5000] loss: 53.088164\n",
      "..........\n",
      "[1,  5100] loss: 52.154658\n",
      "..........\n",
      "[1,  5200] loss: 53.347910\n",
      "..........\n",
      "[1,  5300] loss: 52.669876\n",
      "..........\n",
      "[1,  5400] loss: 51.747970\n",
      "..........\n",
      "[1,  5500] loss: 53.800316\n",
      "..........\n",
      "[1,  5600] loss: 53.026450\n",
      "..........\n",
      "[1,  5700] loss: 54.298488\n",
      "..........\n",
      "[1,  5800] loss: 53.448594\n",
      "..........\n",
      "[1,  5900] loss: 52.332394\n",
      "..........\n",
      "[1,  6000] loss: 53.482540\n",
      "..........\n",
      "[1,  6100] loss: 53.239937\n",
      "..........\n",
      "[1,  6200] loss: 53.591362\n",
      "..........\n",
      "[1,  6300] loss: 53.441928\n",
      "..........\n",
      "[1,  6400] loss: 53.309979\n",
      "..........\n",
      "[1,  6500] loss: 53.240501\n",
      "..........\n",
      "[1,  6600] loss: 53.828516\n",
      "..........\n",
      "[1,  6700] loss: 53.164428\n",
      "..........\n",
      "[1,  6800] loss: 53.893038\n",
      "..........\n",
      "[1,  6900] loss: 53.386481\n",
      "..........\n",
      "[1,  7000] loss: 53.046883\n",
      "..........\n",
      "[1,  7100] loss: 53.117161\n",
      "..........\n",
      "[1,  7200] loss: 53.213683\n",
      "..........\n",
      "[1,  7300] loss: 53.955863\n",
      "..........\n",
      "[1,  7400] loss: 53.359922\n",
      "..........\n",
      "[1,  7500] loss: 53.681251\n",
      "..........\n",
      "[1,  7600] loss: 54.315451\n",
      "..........\n",
      "[1,  7700] loss: 52.503003\n",
      "..........\n",
      "[1,  7800] loss: 53.964866\n",
      "..........\n",
      "[1,  7900] loss: 55.248344\n",
      "..........\n",
      "[1,  8000] loss: 53.880043\n",
      "total average loss : 52.657\n",
      "train acc : 0.0859\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.121\n",
      "step : 1600 / 3125 acc : 0.113\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.100\n",
      "step : 2200 / 3125 acc : 0.102\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.121\n",
      "step : 3000 / 3125 acc : 0.119\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1160 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 54.616994\n",
      "..........\n",
      "[1,   200] loss: 54.668606\n",
      "..........\n",
      "[1,   300] loss: 54.527489\n",
      "..........\n",
      "[1,   400] loss: 53.987878\n",
      "..........\n",
      "[1,   500] loss: 54.297472\n",
      "..........\n",
      "[1,   600] loss: 53.837784\n",
      "..........\n",
      "[1,   700] loss: 54.011401\n",
      "..........\n",
      "[1,   800] loss: 53.546596\n",
      "..........\n",
      "[1,   900] loss: 54.790720\n",
      "..........\n",
      "[1,  1000] loss: 55.270869\n",
      "..........\n",
      "[1,  1100] loss: 54.764128\n",
      "..........\n",
      "[1,  1200] loss: 54.433765\n",
      "..........\n",
      "[1,  1300] loss: 54.352142\n",
      "..........\n",
      "[1,  1400] loss: 54.323802\n",
      "..........\n",
      "[1,  1500] loss: 55.006140\n",
      "..........\n",
      "[1,  1600] loss: 54.510334\n",
      "..........\n",
      "[1,  1700] loss: 54.325594\n",
      "..........\n",
      "[1,  1800] loss: 54.524986\n",
      "..........\n",
      "[1,  1900] loss: 54.492419\n",
      "..........\n",
      "[1,  2000] loss: 54.338209\n",
      "..........\n",
      "[1,  2100] loss: 54.740435\n",
      "..........\n",
      "[1,  2200] loss: 54.871271\n",
      "..........\n",
      "[1,  2300] loss: 55.014117\n",
      "..........\n",
      "[1,  2400] loss: 54.382633\n",
      "..........\n",
      "[1,  2500] loss: 55.536093\n",
      "..........\n",
      "[1,  2600] loss: 55.090097\n",
      "..........\n",
      "[1,  2700] loss: 55.047453\n",
      "..........\n",
      "[1,  2800] loss: 54.839705\n",
      "..........\n",
      "[1,  2900] loss: 55.563508\n",
      "..........\n",
      "[1,  3000] loss: 55.324888\n",
      "..........\n",
      "[1,  3100] loss: 55.510869\n",
      "..........\n",
      "[1,  3200] loss: 54.938136\n",
      "..........\n",
      "[1,  3300] loss: 55.526108\n",
      "..........\n",
      "[1,  3400] loss: 55.394191\n",
      "..........\n",
      "[1,  3500] loss: 55.489452\n",
      "..........\n",
      "[1,  3600] loss: 56.013736\n",
      "..........\n",
      "[1,  3700] loss: 54.900670\n",
      "..........\n",
      "[1,  3800] loss: 55.510478\n",
      "..........\n",
      "[1,  3900] loss: 56.086763\n",
      "..........\n",
      "[1,  4000] loss: 55.553075\n",
      "..........\n",
      "[1,  4100] loss: 55.422543\n",
      "..........\n",
      "[1,  4200] loss: 54.764763\n",
      "..........\n",
      "[1,  4300] loss: 55.537270\n",
      "..........\n",
      "[1,  4400] loss: 55.746697\n",
      "..........\n",
      "[1,  4500] loss: 54.101438\n",
      "..........\n",
      "[1,  4600] loss: 54.567215\n",
      "..........\n",
      "[1,  4700] loss: 54.754730\n",
      "..........\n",
      "[1,  4800] loss: 55.023472\n",
      "..........\n",
      "[1,  4900] loss: 56.160771\n",
      "..........\n",
      "[1,  5000] loss: 56.195871\n",
      "..........\n",
      "[1,  5100] loss: 57.106688\n",
      "..........\n",
      "[1,  5200] loss: 55.858035\n",
      "..........\n",
      "[1,  5300] loss: 55.005320\n",
      "..........\n",
      "[1,  5400] loss: 56.208176\n",
      "..........\n",
      "[1,  5500] loss: 56.133322\n",
      "..........\n",
      "[1,  5600] loss: 55.696936\n",
      "..........\n",
      "[1,  5700] loss: 54.789227\n",
      "..........\n",
      "[1,  5800] loss: 55.567576\n",
      "..........\n",
      "[1,  5900] loss: 55.692449\n",
      "..........\n",
      "[1,  6000] loss: 55.577267\n",
      "..........\n",
      "[1,  6100] loss: 56.204633\n",
      "..........\n",
      "[1,  6200] loss: 55.547049\n",
      "..........\n",
      "[1,  6300] loss: 56.126276\n",
      "..........\n",
      "[1,  6400] loss: 56.298317\n",
      "..........\n",
      "[1,  6500] loss: 56.035638\n",
      "..........\n",
      "[1,  6600] loss: 57.093182\n",
      "..........\n",
      "[1,  6700] loss: 56.337851\n",
      "..........\n",
      "[1,  6800] loss: 56.636224\n",
      "..........\n",
      "[1,  6900] loss: 55.450870\n",
      "..........\n",
      "[1,  7000] loss: 56.706512\n",
      "..........\n",
      "[1,  7100] loss: 56.110724\n",
      "..........\n",
      "[1,  7200] loss: 56.255589\n",
      "..........\n",
      "[1,  7300] loss: 56.773116\n",
      "..........\n",
      "[1,  7400] loss: 56.335101\n",
      "..........\n",
      "[1,  7500] loss: 55.832037\n",
      "..........\n",
      "[1,  7600] loss: 55.923950\n",
      "..........\n",
      "[1,  7700] loss: 57.205916\n",
      "..........\n",
      "[1,  7800] loss: 56.452828\n",
      "..........\n",
      "[1,  7900] loss: 54.707556\n",
      "..........\n",
      "[1,  8000] loss: 56.995191\n",
      "total average loss : 55.361\n",
      "train acc : 0.0867\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.105\n",
      "step : 1800 / 3125 acc : 0.111\n",
      "step : 2000 / 3125 acc : 0.100\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.102\n",
      "step : 2600 / 3125 acc : 0.108\n",
      "step : 2800 / 3125 acc : 0.112\n",
      "step : 3000 / 3125 acc : 0.108\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1100 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 57.565678\n",
      "..........\n",
      "[1,   200] loss: 56.861769\n",
      "..........\n",
      "[1,   300] loss: 58.021785\n",
      "..........\n",
      "[1,   400] loss: 57.180741\n",
      "..........\n",
      "[1,   500] loss: 56.841003\n",
      "..........\n",
      "[1,   600] loss: 58.681995\n",
      "..........\n",
      "[1,   700] loss: 56.561853\n",
      "..........\n",
      "[1,   800] loss: 57.372269\n",
      "..........\n",
      "[1,   900] loss: 56.854564\n",
      "..........\n",
      "[1,  1000] loss: 58.894230\n",
      "..........\n",
      "[1,  1100] loss: 56.932144\n",
      "..........\n",
      "[1,  1200] loss: 57.372888\n",
      "..........\n",
      "[1,  1300] loss: 57.037791\n",
      "..........\n",
      "[1,  1400] loss: 58.210256\n",
      "..........\n",
      "[1,  1500] loss: 56.573729\n",
      "..........\n",
      "[1,  1600] loss: 57.141782\n",
      "..........\n",
      "[1,  1700] loss: 58.473427\n",
      "..........\n",
      "[1,  1800] loss: 56.401593\n",
      "..........\n",
      "[1,  1900] loss: 57.375181\n",
      "..........\n",
      "[1,  2000] loss: 57.706280\n",
      "..........\n",
      "[1,  2100] loss: 57.377921\n",
      "..........\n",
      "[1,  2200] loss: 57.845995\n",
      "..........\n",
      "[1,  2300] loss: 57.163833\n",
      "..........\n",
      "[1,  2400] loss: 57.622799\n",
      "..........\n",
      "[1,  2500] loss: 57.907252\n",
      "..........\n",
      "[1,  2600] loss: 57.991807\n",
      "..........\n",
      "[1,  2700] loss: 58.780543\n",
      "..........\n",
      "[1,  2800] loss: 58.742406\n",
      "..........\n",
      "[1,  2900] loss: 57.135637\n",
      "..........\n",
      "[1,  3000] loss: 57.958167\n",
      "..........\n",
      "[1,  3100] loss: 57.398809\n",
      "..........\n",
      "[1,  3200] loss: 57.275864\n",
      "..........\n",
      "[1,  3300] loss: 57.662965\n",
      "..........\n",
      "[1,  3400] loss: 57.563909\n",
      "..........\n",
      "[1,  3500] loss: 58.256517\n",
      "..........\n",
      "[1,  3600] loss: 58.067497\n",
      "..........\n",
      "[1,  3700] loss: 58.307576\n",
      "..........\n",
      "[1,  3800] loss: 58.222464\n",
      "..........\n",
      "[1,  3900] loss: 58.130983\n",
      "..........\n",
      "[1,  4000] loss: 57.010993\n",
      "..........\n",
      "[1,  4100] loss: 58.650865\n",
      "..........\n",
      "[1,  4200] loss: 57.843873\n",
      "..........\n",
      "[1,  4300] loss: 57.584860\n",
      "..........\n",
      "[1,  4400] loss: 57.278589\n",
      "..........\n",
      "[1,  4500] loss: 59.968716\n",
      "..........\n",
      "[1,  4600] loss: 57.575579\n",
      "..........\n",
      "[1,  4700] loss: 57.995255\n",
      "..........\n",
      "[1,  4800] loss: 58.232115\n",
      "..........\n",
      "[1,  4900] loss: 58.168741\n",
      "..........\n",
      "[1,  5000] loss: 57.807360\n",
      "..........\n",
      "[1,  5100] loss: 58.486001\n",
      "..........\n",
      "[1,  5200] loss: 56.879890\n",
      "..........\n",
      "[1,  5300] loss: 57.827523\n",
      "..........\n",
      "[1,  5400] loss: 59.520899\n",
      "..........\n",
      "[1,  5500] loss: 57.438803\n",
      "..........\n",
      "[1,  5600] loss: 58.404158\n",
      "..........\n",
      "[1,  5700] loss: 58.097138\n",
      "..........\n",
      "[1,  5800] loss: 58.029389\n",
      "..........\n",
      "[1,  5900] loss: 58.350214\n",
      "..........\n",
      "[1,  6000] loss: 58.221722\n",
      "..........\n",
      "[1,  6100] loss: 58.567488\n",
      "..........\n",
      "[1,  6200] loss: 58.403117\n",
      "..........\n",
      "[1,  6300] loss: 58.363627\n",
      "..........\n",
      "[1,  6400] loss: 59.071299\n",
      "..........\n",
      "[1,  6500] loss: 58.110740\n",
      "..........\n",
      "[1,  6600] loss: 58.420592\n",
      "..........\n",
      "[1,  6700] loss: 59.410737\n",
      "..........\n",
      "[1,  6800] loss: 57.941013\n",
      "..........\n",
      "[1,  6900] loss: 58.556109\n",
      "..........\n",
      "[1,  7000] loss: 58.458277\n",
      "..........\n",
      "[1,  7100] loss: 58.912633\n",
      "..........\n",
      "[1,  7200] loss: 57.485957\n",
      "..........\n",
      "[1,  7300] loss: 58.598248\n",
      "..........\n",
      "[1,  7400] loss: 58.933226\n",
      "..........\n",
      "[1,  7500] loss: 58.247944\n",
      "..........\n",
      "[1,  7600] loss: 58.125033\n",
      "..........\n",
      "[1,  7700] loss: 58.018553\n",
      "..........\n",
      "[1,  7800] loss: 59.026754\n",
      "..........\n",
      "[1,  7900] loss: 57.824663\n",
      "..........\n",
      "[1,  8000] loss: 58.112318\n",
      "total average loss : 57.943\n",
      "train acc : 0.0875\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.250\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.083\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.104\n",
      "step : 2000 / 3125 acc : 0.097\n",
      "step : 2200 / 3125 acc : 0.088\n",
      "step : 2400 / 3125 acc : 0.094\n",
      "step : 2600 / 3125 acc : 0.099\n",
      "step : 2800 / 3125 acc : 0.105\n",
      "step : 3000 / 3125 acc : 0.102\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 59.369029\n",
      "..........\n",
      "[1,   200] loss: 59.688018\n",
      "..........\n",
      "[1,   300] loss: 59.985144\n",
      "..........\n",
      "[1,   400] loss: 59.330878\n",
      "..........\n",
      "[1,   500] loss: 60.426125\n",
      "..........\n",
      "[1,   600] loss: 59.497788\n",
      "..........\n",
      "[1,   700] loss: 59.853435\n",
      "..........\n",
      "[1,   800] loss: 59.384701\n",
      "..........\n",
      "[1,   900] loss: 60.272016\n",
      "..........\n",
      "[1,  1000] loss: 58.998234\n",
      "..........\n",
      "[1,  1100] loss: 60.041952\n",
      "..........\n",
      "[1,  1200] loss: 59.647933\n",
      "..........\n",
      "[1,  1300] loss: 59.616090\n",
      "..........\n",
      "[1,  1400] loss: 59.680560\n",
      "..........\n",
      "[1,  1500] loss: 59.681165\n",
      "..........\n",
      "[1,  1600] loss: 60.040313\n",
      "..........\n",
      "[1,  1700] loss: 60.036796\n",
      "..........\n",
      "[1,  1800] loss: 59.839866\n",
      "..........\n",
      "[1,  1900] loss: 59.390645\n",
      "..........\n",
      "[1,  2000] loss: 59.522241\n",
      "..........\n",
      "[1,  2100] loss: 60.218413\n",
      "..........\n",
      "[1,  2200] loss: 61.218950\n",
      "..........\n",
      "[1,  2300] loss: 59.965256\n",
      "..........\n",
      "[1,  2400] loss: 59.007913\n",
      "..........\n",
      "[1,  2500] loss: 60.141442\n",
      "..........\n",
      "[1,  2600] loss: 59.017153\n",
      "..........\n",
      "[1,  2700] loss: 61.005045\n",
      "..........\n",
      "[1,  2800] loss: 59.754788\n",
      "..........\n",
      "[1,  2900] loss: 60.241748\n",
      "..........\n",
      "[1,  3000] loss: 59.739936\n",
      "..........\n",
      "[1,  3100] loss: 60.617899\n",
      "..........\n",
      "[1,  3200] loss: 60.381230\n",
      "..........\n",
      "[1,  3300] loss: 59.465027\n",
      "..........\n",
      "[1,  3400] loss: 59.961752\n",
      "..........\n",
      "[1,  3500] loss: 60.458227\n",
      "..........\n",
      "[1,  3600] loss: 59.839076\n",
      "..........\n",
      "[1,  3700] loss: 61.261119\n",
      "..........\n",
      "[1,  3800] loss: 59.671762\n",
      "..........\n",
      "[1,  3900] loss: 60.036078\n",
      "..........\n",
      "[1,  4000] loss: 60.678983\n",
      "..........\n",
      "[1,  4100] loss: 61.077461\n",
      "..........\n",
      "[1,  4200] loss: 60.157553\n",
      "..........\n",
      "[1,  4300] loss: 60.400495\n",
      "..........\n",
      "[1,  4400] loss: 60.601143\n",
      "..........\n",
      "[1,  4500] loss: 59.872118\n",
      "..........\n",
      "[1,  4600] loss: 61.255723\n",
      "..........\n",
      "[1,  4700] loss: 60.961484\n",
      "..........\n",
      "[1,  4800] loss: 61.415428\n",
      "..........\n",
      "[1,  4900] loss: 60.614043\n",
      "..........\n",
      "[1,  5000] loss: 59.246350\n",
      "..........\n",
      "[1,  5100] loss: 60.943527\n",
      "..........\n",
      "[1,  5200] loss: 60.473610\n",
      "..........\n",
      "[1,  5300] loss: 60.294109\n",
      "..........\n",
      "[1,  5400] loss: 60.265939\n",
      "..........\n",
      "[1,  5500] loss: 60.548688\n",
      "..........\n",
      "[1,  5600] loss: 60.551713\n",
      "..........\n",
      "[1,  5700] loss: 60.516192\n",
      "..........\n",
      "[1,  5800] loss: 59.873060\n",
      "..........\n",
      "[1,  5900] loss: 60.887997\n",
      "..........\n",
      "[1,  6000] loss: 60.907399\n",
      "..........\n",
      "[1,  6100] loss: 60.770787\n",
      "..........\n",
      "[1,  6200] loss: 59.962209\n",
      "..........\n",
      "[1,  6300] loss: 61.999916\n",
      "..........\n",
      "[1,  6400] loss: 60.146105\n",
      "..........\n",
      "[1,  6500] loss: 59.865234\n",
      "..........\n",
      "[1,  6600] loss: 61.050824\n",
      "..........\n",
      "[1,  6700] loss: 59.479620\n",
      "..........\n",
      "[1,  6800] loss: 60.625065\n",
      "..........\n",
      "[1,  6900] loss: 60.103075\n",
      "..........\n",
      "[1,  7000] loss: 61.409253\n",
      "..........\n",
      "[1,  7100] loss: 60.491477\n",
      "..........\n",
      "[1,  7200] loss: 60.592095\n",
      "..........\n",
      "[1,  7300] loss: 60.734777\n",
      "..........\n",
      "[1,  7400] loss: 60.069200\n",
      "..........\n",
      "[1,  7500] loss: 60.939239\n",
      "..........\n",
      "[1,  7600] loss: 60.888332\n",
      "..........\n",
      "[1,  7700] loss: 61.597445\n",
      "..........\n",
      "[1,  7800] loss: 60.289159\n",
      "..........\n",
      "[1,  7900] loss: 61.234091\n",
      "..........\n",
      "[1,  8000] loss: 60.189514\n",
      "total average loss : 60.254\n",
      "train acc : 0.0859\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.047\n",
      "step : 600 / 3125 acc : 0.052\n",
      "step : 800 / 3125 acc : 0.070\n",
      "step : 1000 / 3125 acc : 0.056\n",
      "step : 1200 / 3125 acc : 0.073\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.090\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.102\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.099\n",
      "step : 2800 / 3125 acc : 0.100\n",
      "step : 3000 / 3125 acc : 0.098\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "======= epoch  6 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 62.454039\n",
      "..........\n",
      "[1,   200] loss: 61.138613\n",
      "..........\n",
      "[1,   300] loss: 61.390985\n",
      "..........\n",
      "[1,   400] loss: 62.093123\n",
      "..........\n",
      "[1,   500] loss: 62.276431\n",
      "..........\n",
      "[1,   600] loss: 61.289937\n",
      "..........\n",
      "[1,   700] loss: 60.328811\n",
      "..........\n",
      "[1,   800] loss: 61.214104\n",
      "..........\n",
      "[1,   900] loss: 62.438298\n",
      "..........\n",
      "[1,  1000] loss: 62.482433\n",
      "..........\n",
      "[1,  1100] loss: 60.895650\n",
      "..........\n",
      "[1,  1200] loss: 62.576510\n",
      "..........\n",
      "[1,  1300] loss: 61.907804\n",
      "..........\n",
      "[1,  1400] loss: 62.630644\n",
      "..........\n",
      "[1,  1500] loss: 61.634805\n",
      "..........\n",
      "[1,  1600] loss: 62.270156\n",
      "..........\n",
      "[1,  1700] loss: 62.461376\n",
      "..........\n",
      "[1,  1800] loss: 62.724815\n",
      "..........\n",
      "[1,  1900] loss: 62.024539\n",
      "..........\n",
      "[1,  2000] loss: 63.120395\n",
      "..........\n",
      "[1,  2100] loss: 61.058733\n",
      "..........\n",
      "[1,  2200] loss: 62.516765\n",
      "..........\n",
      "[1,  2300] loss: 62.180810\n",
      "..........\n",
      "[1,  2400] loss: 62.215142\n",
      "..........\n",
      "[1,  2500] loss: 63.103643\n",
      "..........\n",
      "[1,  2600] loss: 62.582102\n",
      "..........\n",
      "[1,  2700] loss: 62.556444\n",
      "..........\n",
      "[1,  2800] loss: 61.490079\n",
      "..........\n",
      "[1,  2900] loss: 62.537333\n",
      "..........\n",
      "[1,  3000] loss: 63.259726\n",
      "..........\n",
      "[1,  3100] loss: 61.928044\n",
      "..........\n",
      "[1,  3200] loss: 63.068282\n",
      "..........\n",
      "[1,  3300] loss: 62.369653\n",
      "..........\n",
      "[1,  3400] loss: 62.757003\n",
      "..........\n",
      "[1,  3500] loss: 62.957186\n",
      "..........\n",
      "[1,  3600] loss: 61.857844\n",
      "..........\n",
      "[1,  3700] loss: 62.176001\n",
      "..........\n",
      "[1,  3800] loss: 62.699760\n",
      "..........\n",
      "[1,  3900] loss: 61.689245\n",
      "..........\n",
      "[1,  4000] loss: 61.248612\n",
      "..........\n",
      "[1,  4100] loss: 62.220451\n",
      "..........\n",
      "[1,  4200] loss: 62.624558\n",
      "..........\n",
      "[1,  4300] loss: 61.848613\n",
      "..........\n",
      "[1,  4400] loss: 62.759637\n",
      "..........\n",
      "[1,  4500] loss: 62.089544\n",
      "..........\n",
      "[1,  4600] loss: 62.690802\n",
      "..........\n",
      "[1,  4700] loss: 62.668236\n",
      "..........\n",
      "[1,  4800] loss: 63.454304\n",
      "..........\n",
      "[1,  4900] loss: 62.153026\n",
      "..........\n",
      "[1,  5000] loss: 63.038911\n",
      "..........\n",
      "[1,  5100] loss: 61.558881\n",
      "..........\n",
      "[1,  5200] loss: 62.676211\n",
      "..........\n",
      "[1,  5300] loss: 63.750001\n",
      "..........\n",
      "[1,  5400] loss: 62.693959\n",
      "..........\n",
      "[1,  5500] loss: 62.918724\n",
      "..........\n",
      "[1,  5600] loss: 62.178014\n",
      "..........\n",
      "[1,  5700] loss: 63.537043\n",
      "..........\n",
      "[1,  5800] loss: 63.034494\n",
      "..........\n",
      "[1,  5900] loss: 62.500453\n",
      "..........\n",
      "[1,  6000] loss: 62.437081\n",
      "..........\n",
      "[1,  6100] loss: 61.864447\n",
      "..........\n",
      "[1,  6200] loss: 62.510831\n",
      "..........\n",
      "[1,  6300] loss: 62.926185\n",
      "..........\n",
      "[1,  6400] loss: 62.832148\n",
      "..........\n",
      "[1,  6500] loss: 63.441947\n",
      "..........\n",
      "[1,  6600] loss: 62.077087\n",
      "..........\n",
      "[1,  6700] loss: 62.915674\n",
      "..........\n",
      "[1,  6800] loss: 64.073406\n",
      "..........\n",
      "[1,  6900] loss: 64.296517\n",
      "..........\n",
      "[1,  7000] loss: 64.033633\n",
      "..........\n",
      "[1,  7100] loss: 64.156915\n",
      "..........\n",
      "[1,  7200] loss: 63.869732\n",
      "..........\n",
      "[1,  7300] loss: 62.618633\n",
      "..........\n",
      "[1,  7400] loss: 64.168197\n",
      "..........\n",
      "[1,  7500] loss: 62.726568\n",
      "..........\n",
      "[1,  7600] loss: 62.818347\n",
      "..........\n",
      "[1,  7700] loss: 63.049311\n",
      "..........\n",
      "[1,  7800] loss: 63.436408\n",
      "..........\n",
      "[1,  7900] loss: 63.024575\n",
      "..........\n",
      "[1,  8000] loss: 63.969847\n",
      "total average loss : 62.541\n",
      "train acc : 0.0836\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.089\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.082\n",
      "step : 1800 / 3125 acc : 0.090\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.102\n",
      "step : 2400 / 3125 acc : 0.096\n",
      "step : 2600 / 3125 acc : 0.094\n",
      "step : 2800 / 3125 acc : 0.089\n",
      "step : 3000 / 3125 acc : 0.090\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0960 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 64.010316\n",
      "..........\n",
      "[1,   200] loss: 64.746189\n",
      "..........\n",
      "[1,   300] loss: 64.552216\n",
      "..........\n",
      "[1,   400] loss: 64.276879\n",
      "..........\n",
      "[1,   500] loss: 64.322862\n",
      "..........\n",
      "[1,   600] loss: 63.499757\n",
      "..........\n",
      "[1,   700] loss: 64.394339\n",
      "..........\n",
      "[1,   800] loss: 64.215810\n",
      "..........\n",
      "[1,   900] loss: 64.208428\n",
      "..........\n",
      "[1,  1000] loss: 64.928627\n",
      "..........\n",
      "[1,  1100] loss: 64.428590\n",
      "..........\n",
      "[1,  1200] loss: 63.577402\n",
      "..........\n",
      "[1,  1300] loss: 64.298928\n",
      "..........\n",
      "[1,  1400] loss: 64.872295\n",
      "..........\n",
      "[1,  1500] loss: 63.951503\n",
      "..........\n",
      "[1,  1600] loss: 63.771186\n",
      "..........\n",
      "[1,  1700] loss: 65.005149\n",
      "..........\n",
      "[1,  1800] loss: 64.173083\n",
      "..........\n",
      "[1,  1900] loss: 64.492626\n",
      "..........\n",
      "[1,  2000] loss: 64.559440\n",
      "..........\n",
      "[1,  2100] loss: 64.829424\n",
      "..........\n",
      "[1,  2200] loss: 65.716625\n",
      "..........\n",
      "[1,  2300] loss: 64.528263\n",
      "..........\n",
      "[1,  2400] loss: 63.659420\n",
      "..........\n",
      "[1,  2500] loss: 63.888376\n",
      "..........\n",
      "[1,  2600] loss: 64.684337\n",
      "..........\n",
      "[1,  2700] loss: 64.209931\n",
      "..........\n",
      "[1,  2800] loss: 63.867930\n",
      "..........\n",
      "[1,  2900] loss: 65.158234\n",
      "..........\n",
      "[1,  3000] loss: 63.378935\n",
      "..........\n",
      "[1,  3100] loss: 64.968020\n",
      "..........\n",
      "[1,  3200] loss: 64.723880\n",
      "..........\n",
      "[1,  3300] loss: 63.702926\n",
      "..........\n",
      "[1,  3400] loss: 64.283015\n",
      "..........\n",
      "[1,  3500] loss: 65.444000\n",
      "..........\n",
      "[1,  3600] loss: 66.124191\n",
      "..........\n",
      "[1,  3700] loss: 64.213322\n",
      "..........\n",
      "[1,  3800] loss: 64.845485\n",
      "..........\n",
      "[1,  3900] loss: 63.733939\n",
      "..........\n",
      "[1,  4000] loss: 64.032328\n",
      "..........\n",
      "[1,  4100] loss: 65.158394\n",
      "..........\n",
      "[1,  4200] loss: 65.235962\n",
      "..........\n",
      "[1,  4300] loss: 64.422761\n",
      "..........\n",
      "[1,  4400] loss: 64.749169\n",
      "..........\n",
      "[1,  4500] loss: 64.034133\n",
      "..........\n",
      "[1,  4600] loss: 64.454452\n",
      "..........\n",
      "[1,  4700] loss: 65.172505\n",
      "..........\n",
      "[1,  4800] loss: 65.569029\n",
      "..........\n",
      "[1,  4900] loss: 64.861385\n",
      "..........\n",
      "[1,  5000] loss: 65.091621\n",
      "..........\n",
      "[1,  5100] loss: 65.440058\n",
      "..........\n",
      "[1,  5200] loss: 64.745561\n",
      "..........\n",
      "[1,  5300] loss: 63.535583\n",
      "..........\n",
      "[1,  5400] loss: 63.954508\n",
      "..........\n",
      "[1,  5500] loss: 65.345023\n",
      "..........\n",
      "[1,  5600] loss: 64.332713\n",
      "..........\n",
      "[1,  5700] loss: 65.109814\n",
      "..........\n",
      "[1,  5800] loss: 64.645481\n",
      "..........\n",
      "[1,  5900] loss: 65.575646\n",
      "..........\n",
      "[1,  6000] loss: 66.122731\n",
      "..........\n",
      "[1,  6100] loss: 65.007054\n",
      "..........\n",
      "[1,  6200] loss: 65.291285\n",
      "..........\n",
      "[1,  6300] loss: 64.819752\n",
      "..........\n",
      "[1,  6400] loss: 65.104179\n",
      "..........\n",
      "[1,  6500] loss: 65.965689\n",
      "..........\n",
      "[1,  6600] loss: 64.962043\n",
      "..........\n",
      "[1,  6700] loss: 64.562998\n",
      "..........\n",
      "[1,  6800] loss: 65.819062\n",
      "..........\n",
      "[1,  6900] loss: 64.854932\n",
      "..........\n",
      "[1,  7000] loss: 65.720620\n",
      "..........\n",
      "[1,  7100] loss: 65.541679\n",
      "..........\n",
      "[1,  7200] loss: 66.455957\n",
      "..........\n",
      "[1,  7300] loss: 65.475430\n",
      "..........\n",
      "[1,  7400] loss: 66.813943\n",
      "..........\n",
      "[1,  7500] loss: 67.125558\n",
      "..........\n",
      "[1,  7600] loss: 66.853374\n",
      "..........\n",
      "[1,  7700] loss: 66.294565\n",
      "..........\n",
      "[1,  7800] loss: 65.561143\n",
      "..........\n",
      "[1,  7900] loss: 65.802004\n",
      "..........\n",
      "[1,  8000] loss: 65.479006\n",
      "total average loss : 64.842\n",
      "train acc : 0.0875\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.062\n",
      "step : 800 / 3125 acc : 0.055\n",
      "step : 1000 / 3125 acc : 0.056\n",
      "step : 1200 / 3125 acc : 0.068\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.082\n",
      "step : 1800 / 3125 acc : 0.090\n",
      "step : 2000 / 3125 acc : 0.087\n",
      "step : 2200 / 3125 acc : 0.088\n",
      "step : 2400 / 3125 acc : 0.091\n",
      "step : 2600 / 3125 acc : 0.089\n",
      "step : 2800 / 3125 acc : 0.089\n",
      "step : 3000 / 3125 acc : 0.092\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0900 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 67.299962\n",
      "..........\n",
      "[1,   200] loss: 67.158396\n",
      "..........\n",
      "[1,   300] loss: 65.764716\n",
      "..........\n",
      "[1,   400] loss: 66.819323\n",
      "..........\n",
      "[1,   500] loss: 66.956700\n",
      "..........\n",
      "[1,   600] loss: 66.479291\n",
      "..........\n",
      "[1,   700] loss: 66.704182\n",
      "..........\n",
      "[1,   800] loss: 67.585456\n",
      "..........\n",
      "[1,   900] loss: 67.019035\n",
      "..........\n",
      "[1,  1000] loss: 66.855965\n",
      "..........\n",
      "[1,  1100] loss: 65.193900\n",
      "..........\n",
      "[1,  1200] loss: 67.552482\n",
      "..........\n",
      "[1,  1300] loss: 66.310036\n",
      "..........\n",
      "[1,  1400] loss: 67.086921\n",
      "..........\n",
      "[1,  1500] loss: 66.683431\n",
      "..........\n",
      "[1,  1600] loss: 66.288087\n",
      "..........\n",
      "[1,  1700] loss: 67.114382\n",
      "..........\n",
      "[1,  1800] loss: 66.368873\n",
      "..........\n",
      "[1,  1900] loss: 67.119600\n",
      "..........\n",
      "[1,  2000] loss: 66.901697\n",
      "..........\n",
      "[1,  2100] loss: 67.763976\n",
      "..........\n",
      "[1,  2200] loss: 66.477272\n",
      "..........\n",
      "[1,  2300] loss: 68.113292\n",
      "..........\n",
      "[1,  2400] loss: 65.406526\n",
      "..........\n",
      "[1,  2500] loss: 66.666798\n",
      "..........\n",
      "[1,  2600] loss: 67.335172\n",
      "..........\n",
      "[1,  2700] loss: 66.150727\n",
      "..........\n",
      "[1,  2800] loss: 66.975920\n",
      "..........\n",
      "[1,  2900] loss: 68.252149\n",
      "..........\n",
      "[1,  3000] loss: 67.901272\n",
      "..........\n",
      "[1,  3100] loss: 68.299414\n",
      "..........\n",
      "[1,  3200] loss: 67.710284\n",
      "..........\n",
      "[1,  3300] loss: 67.654612\n",
      "..........\n",
      "[1,  3400] loss: 67.027240\n",
      "..........\n",
      "[1,  3500] loss: 67.122662\n",
      "..........\n",
      "[1,  3600] loss: 65.370998\n",
      "..........\n",
      "[1,  3700] loss: 66.487292\n",
      "..........\n",
      "[1,  3800] loss: 67.800088\n",
      "..........\n",
      "[1,  3900] loss: 67.130077\n",
      "..........\n",
      "[1,  4000] loss: 67.723644\n",
      "..........\n",
      "[1,  4100] loss: 68.174667\n",
      "..........\n",
      "[1,  4200] loss: 67.149301\n",
      "..........\n",
      "[1,  4300] loss: 67.179152\n",
      "..........\n",
      "[1,  4400] loss: 66.253475\n",
      "..........\n",
      "[1,  4500] loss: 67.318809\n",
      "..........\n",
      "[1,  4600] loss: 67.524461\n",
      "..........\n",
      "[1,  4700] loss: 66.096036\n",
      "..........\n",
      "[1,  4800] loss: 68.296345\n",
      "..........\n",
      "[1,  4900] loss: 67.539984\n",
      "..........\n",
      "[1,  5000] loss: 66.964459\n",
      "..........\n",
      "[1,  5100] loss: 66.957505\n",
      "..........\n",
      "[1,  5200] loss: 66.915401\n",
      "..........\n",
      "[1,  5300] loss: 67.523077\n",
      "..........\n",
      "[1,  5400] loss: 66.739259\n",
      "..........\n",
      "[1,  5500] loss: 67.818687\n",
      "..........\n",
      "[1,  5600] loss: 67.925619\n",
      "..........\n",
      "[1,  5700] loss: 67.512342\n",
      "..........\n",
      "[1,  5800] loss: 66.806108\n",
      "..........\n",
      "[1,  5900] loss: 67.871145\n",
      "..........\n",
      "[1,  6000] loss: 67.337856\n",
      "..........\n",
      "[1,  6100] loss: 67.626570\n",
      "..........\n",
      "[1,  6200] loss: 66.480206\n",
      "..........\n",
      "[1,  6300] loss: 68.396692\n",
      "..........\n",
      "[1,  6400] loss: 67.997215\n",
      "..........\n",
      "[1,  6500] loss: 67.900811\n",
      "..........\n",
      "[1,  6600] loss: 68.766194\n",
      "..........\n",
      "[1,  6700] loss: 67.249659\n",
      "..........\n",
      "[1,  6800] loss: 68.198473\n",
      "..........\n",
      "[1,  6900] loss: 67.537979\n",
      "..........\n",
      "[1,  7000] loss: 68.016586\n",
      "..........\n",
      "[1,  7100] loss: 67.291131\n",
      "..........\n",
      "[1,  7200] loss: 67.789100\n",
      "..........\n",
      "[1,  7300] loss: 67.341510\n",
      "..........\n",
      "[1,  7400] loss: 67.800613\n",
      "..........\n",
      "[1,  7500] loss: 67.397236\n",
      "..........\n",
      "[1,  7600] loss: 68.659867\n",
      "..........\n",
      "[1,  7700] loss: 68.852201\n",
      "..........\n",
      "[1,  7800] loss: 68.598040\n",
      "..........\n",
      "[1,  7900] loss: 68.087743\n",
      "..........\n",
      "[1,  8000] loss: 67.675939\n",
      "total average loss : 67.277\n",
      "train acc : 0.0875\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.121\n",
      "step : 1800 / 3125 acc : 0.111\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.111\n",
      "step : 2400 / 3125 acc : 0.104\n",
      "step : 2600 / 3125 acc : 0.096\n",
      "step : 2800 / 3125 acc : 0.096\n",
      "step : 3000 / 3125 acc : 0.092\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.0900 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 70.152868\n",
      "..........\n",
      "[1,   200] loss: 69.084842\n",
      "..........\n",
      "[1,   300] loss: 68.361839\n",
      "..........\n",
      "[1,   400] loss: 69.040494\n",
      "..........\n",
      "[1,   500] loss: 67.609502\n",
      "..........\n",
      "[1,   600] loss: 68.567021\n",
      "..........\n",
      "[1,   700] loss: 68.701851\n",
      "..........\n",
      "[1,   800] loss: 69.209019\n",
      "..........\n",
      "[1,   900] loss: 69.993656\n",
      "..........\n",
      "[1,  1000] loss: 68.927719\n",
      "..........\n",
      "[1,  1100] loss: 70.410969\n",
      "..........\n",
      "[1,  1200] loss: 69.249735\n",
      "..........\n",
      "[1,  1300] loss: 69.642912\n",
      "..........\n",
      "[1,  1400] loss: 69.276646\n",
      "..........\n",
      "[1,  1500] loss: 69.279108\n",
      "..........\n",
      "[1,  1600] loss: 69.075234\n",
      "..........\n",
      "[1,  1700] loss: 67.968740\n",
      "..........\n",
      "[1,  1800] loss: 70.143665\n",
      "..........\n",
      "[1,  1900] loss: 69.179231\n",
      "..........\n",
      "[1,  2000] loss: 68.366895\n",
      "..........\n",
      "[1,  2100] loss: 69.201742\n",
      "..........\n",
      "[1,  2200] loss: 69.805919\n",
      "..........\n",
      "[1,  2300] loss: 69.483071\n",
      "..........\n",
      "[1,  2400] loss: 70.250650\n",
      "..........\n",
      "[1,  2500] loss: 69.252686\n",
      "..........\n",
      "[1,  2600] loss: 69.117978\n",
      "..........\n",
      "[1,  2700] loss: 69.557706\n",
      "..........\n",
      "[1,  2800] loss: 70.074823\n",
      "..........\n",
      "[1,  2900] loss: 69.396304\n",
      "..........\n",
      "[1,  3000] loss: 68.949817\n",
      "..........\n",
      "[1,  3100] loss: 69.035393\n",
      "..........\n",
      "[1,  3200] loss: 68.917561\n",
      "..........\n",
      "[1,  3300] loss: 70.802486\n",
      "..........\n",
      "[1,  3400] loss: 70.582501\n",
      "..........\n",
      "[1,  3500] loss: 68.756438\n",
      "..........\n",
      "[1,  3600] loss: 69.801983\n",
      "..........\n",
      "[1,  3700] loss: 69.411409\n",
      "..........\n",
      "[1,  3800] loss: 69.103648\n",
      "..........\n",
      "[1,  3900] loss: 69.564081\n",
      "..........\n",
      "[1,  4000] loss: 70.805230\n",
      "..........\n",
      "[1,  4100] loss: 69.712047\n",
      "..........\n",
      "[1,  4200] loss: 70.591262\n",
      "..........\n",
      "[1,  4300] loss: 69.300166\n",
      "..........\n",
      "[1,  4400] loss: 70.245702\n",
      "..........\n",
      "[1,  4500] loss: 71.945248\n",
      "..........\n",
      "[1,  4600] loss: 69.411124\n",
      "..........\n",
      "[1,  4700] loss: 70.192694\n",
      "..........\n",
      "[1,  4800] loss: 69.957818\n",
      "..........\n",
      "[1,  4900] loss: 69.878013\n",
      "..........\n",
      "[1,  5000] loss: 70.431941\n",
      "..........\n",
      "[1,  5100] loss: 70.921197\n",
      "..........\n",
      "[1,  5200] loss: 71.026239\n",
      "..........\n",
      "[1,  5300] loss: 70.577090\n",
      "..........\n",
      "[1,  5400] loss: 71.902650\n",
      "..........\n",
      "[1,  5500] loss: 70.176706\n",
      "..........\n",
      "[1,  5600] loss: 69.192905\n",
      "..........\n",
      "[1,  5700] loss: 70.506283\n",
      "..........\n",
      "[1,  5800] loss: 70.489391\n",
      "..........\n",
      "[1,  5900] loss: 69.604644\n",
      "..........\n",
      "[1,  6000] loss: 70.161237\n",
      "..........\n",
      "[1,  6100] loss: 70.454537\n",
      "..........\n",
      "[1,  6200] loss: 69.908562\n",
      "..........\n",
      "[1,  6300] loss: 69.758440\n",
      "..........\n",
      "[1,  6400] loss: 70.224315\n",
      "..........\n",
      "[1,  6500] loss: 69.845302\n",
      "..........\n",
      "[1,  6600] loss: 69.763205\n",
      "..........\n",
      "[1,  6700] loss: 71.080589\n",
      "..........\n",
      "[1,  6800] loss: 70.572520\n",
      "..........\n",
      "[1,  6900] loss: 71.360623\n",
      "..........\n",
      "[1,  7000] loss: 71.244737\n",
      "..........\n",
      "[1,  7100] loss: 70.196436\n",
      "..........\n",
      "[1,  7200] loss: 70.283335\n",
      "..........\n",
      "[1,  7300] loss: 70.234101\n",
      "..........\n",
      "[1,  7400] loss: 70.857818\n",
      "..........\n",
      "[1,  7500] loss: 71.634071\n",
      "..........\n",
      "[1,  7600] loss: 69.598648\n",
      "..........\n",
      "[1,  7700] loss: 69.558324\n",
      "..........\n",
      "[1,  7800] loss: 70.344994\n",
      "..........\n",
      "[1,  7900] loss: 72.078314\n",
      "..........\n",
      "[1,  8000] loss: 70.960378\n",
      "total average loss : 69.879\n",
      "train acc : 0.0883\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.098\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.100\n",
      "step : 2200 / 3125 acc : 0.097\n",
      "step : 2400 / 3125 acc : 0.107\n",
      "step : 2600 / 3125 acc : 0.106\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.102\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "======= epoch  7 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 70.983143\n",
      "..........\n",
      "[1,   200] loss: 72.339389\n",
      "..........\n",
      "[1,   300] loss: 71.206187\n",
      "..........\n",
      "[1,   400] loss: 71.355618\n",
      "..........\n",
      "[1,   500] loss: 70.143587\n",
      "..........\n",
      "[1,   600] loss: 72.925541\n",
      "..........\n",
      "[1,   700] loss: 71.984804\n",
      "..........\n",
      "[1,   800] loss: 72.536848\n",
      "..........\n",
      "[1,   900] loss: 72.304928\n",
      "..........\n",
      "[1,  1000] loss: 71.866666\n",
      "..........\n",
      "[1,  1100] loss: 72.189800\n",
      "..........\n",
      "[1,  1200] loss: 70.632761\n",
      "..........\n",
      "[1,  1300] loss: 71.877638\n",
      "..........\n",
      "[1,  1400] loss: 70.806922\n",
      "..........\n",
      "[1,  1500] loss: 72.229703\n",
      "..........\n",
      "[1,  1600] loss: 71.305240\n",
      "..........\n",
      "[1,  1700] loss: 73.515546\n",
      "..........\n",
      "[1,  1800] loss: 72.626753\n",
      "..........\n",
      "[1,  1900] loss: 72.424019\n",
      "..........\n",
      "[1,  2000] loss: 72.485995\n",
      "..........\n",
      "[1,  2100] loss: 71.767543\n",
      "..........\n",
      "[1,  2200] loss: 72.801947\n",
      "..........\n",
      "[1,  2300] loss: 72.469743\n",
      "..........\n",
      "[1,  2400] loss: 73.715364\n",
      "..........\n",
      "[1,  2500] loss: 72.902331\n",
      "..........\n",
      "[1,  2600] loss: 71.333847\n",
      "..........\n",
      "[1,  2700] loss: 72.256123\n",
      "..........\n",
      "[1,  2800] loss: 73.414435\n",
      "..........\n",
      "[1,  2900] loss: 72.993542\n",
      "..........\n",
      "[1,  3000] loss: 71.996246\n",
      "..........\n",
      "[1,  3100] loss: 72.949804\n",
      "..........\n",
      "[1,  3200] loss: 73.353413\n",
      "..........\n",
      "[1,  3300] loss: 73.092271\n",
      "..........\n",
      "[1,  3400] loss: 72.486168\n",
      "..........\n",
      "[1,  3500] loss: 69.914308\n",
      "..........\n",
      "[1,  3600] loss: 71.894146\n",
      "..........\n",
      "[1,  3700] loss: 72.623249\n",
      "..........\n",
      "[1,  3800] loss: 72.098006\n",
      "..........\n",
      "[1,  3900] loss: 72.781753\n",
      "..........\n",
      "[1,  4000] loss: 73.169720\n",
      "..........\n",
      "[1,  4100] loss: 71.821941\n",
      "..........\n",
      "[1,  4200] loss: 71.737828\n",
      "..........\n",
      "[1,  4300] loss: 73.570912\n",
      "..........\n",
      "[1,  4400] loss: 73.632949\n",
      "..........\n",
      "[1,  4500] loss: 72.253360\n",
      "..........\n",
      "[1,  4600] loss: 72.767928\n",
      "..........\n",
      "[1,  4700] loss: 72.698065\n",
      "..........\n",
      "[1,  4800] loss: 72.437243\n",
      "..........\n",
      "[1,  4900] loss: 71.792640\n",
      "..........\n",
      "[1,  5000] loss: 73.511542\n",
      "..........\n",
      "[1,  5100] loss: 72.305986\n",
      "..........\n",
      "[1,  5200] loss: 73.756471\n",
      "..........\n",
      "[1,  5300] loss: 73.596085\n",
      "..........\n",
      "[1,  5400] loss: 71.783406\n",
      "..........\n",
      "[1,  5500] loss: 73.998807\n",
      "..........\n",
      "[1,  5600] loss: 71.726852\n",
      "..........\n",
      "[1,  5700] loss: 72.779101\n",
      "..........\n",
      "[1,  5800] loss: 72.308850\n",
      "..........\n",
      "[1,  5900] loss: 73.281689\n",
      "..........\n",
      "[1,  6000] loss: 73.078049\n",
      "..........\n",
      "[1,  6100] loss: 73.067810\n",
      "..........\n",
      "[1,  6200] loss: 73.129143\n",
      "..........\n",
      "[1,  6300] loss: 72.097811\n",
      "..........\n",
      "[1,  6400] loss: 72.860596\n",
      "..........\n",
      "[1,  6500] loss: 74.504659\n",
      "..........\n",
      "[1,  6600] loss: 71.173314\n",
      "..........\n",
      "[1,  6700] loss: 74.415989\n",
      "..........\n",
      "[1,  6800] loss: 72.940900\n",
      "..........\n",
      "[1,  6900] loss: 72.873563\n",
      "..........\n",
      "[1,  7000] loss: 74.077165\n",
      "..........\n",
      "[1,  7100] loss: 71.914415\n",
      "..........\n",
      "[1,  7200] loss: 72.080052\n",
      "..........\n",
      "[1,  7300] loss: 73.814388\n",
      "..........\n",
      "[1,  7400] loss: 73.199345\n",
      "..........\n",
      "[1,  7500] loss: 73.817678\n",
      "..........\n",
      "[1,  7600] loss: 72.947087\n",
      "..........\n",
      "[1,  7700] loss: 73.626684\n",
      "..........\n",
      "[1,  7800] loss: 74.475009\n",
      "..........\n",
      "[1,  7900] loss: 73.256019\n",
      "..........\n",
      "[1,  8000] loss: 73.657295\n",
      "total average loss : 72.582\n",
      "train acc : 0.0898\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.075\n",
      "step : 1200 / 3125 acc : 0.089\n",
      "step : 1400 / 3125 acc : 0.089\n",
      "step : 1600 / 3125 acc : 0.090\n",
      "step : 1800 / 3125 acc : 0.090\n",
      "step : 2000 / 3125 acc : 0.091\n",
      "step : 2200 / 3125 acc : 0.082\n",
      "step : 2400 / 3125 acc : 0.083\n",
      "step : 2600 / 3125 acc : 0.084\n",
      "step : 2800 / 3125 acc : 0.083\n",
      "step : 3000 / 3125 acc : 0.098\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 74.003213\n",
      "..........\n",
      "[1,   200] loss: 73.516732\n",
      "..........\n",
      "[1,   300] loss: 73.835408\n",
      "..........\n",
      "[1,   400] loss: 74.472180\n",
      "..........\n",
      "[1,   500] loss: 74.196318\n",
      "..........\n",
      "[1,   600] loss: 74.471232\n",
      "..........\n",
      "[1,   700] loss: 73.866573\n",
      "..........\n",
      "[1,   800] loss: 73.811446\n",
      "..........\n",
      "[1,   900] loss: 75.096112\n",
      "..........\n",
      "[1,  1000] loss: 74.752265\n",
      "..........\n",
      "[1,  1100] loss: 75.082873\n",
      "..........\n",
      "[1,  1200] loss: 74.633654\n",
      "..........\n",
      "[1,  1300] loss: 74.548411\n",
      "..........\n",
      "[1,  1400] loss: 74.309914\n",
      "..........\n",
      "[1,  1500] loss: 73.999574\n",
      "..........\n",
      "[1,  1600] loss: 74.938933\n",
      "..........\n",
      "[1,  1700] loss: 75.559105\n",
      "..........\n",
      "[1,  1800] loss: 74.841084\n",
      "..........\n",
      "[1,  1900] loss: 74.642796\n",
      "..........\n",
      "[1,  2000] loss: 74.510268\n",
      "..........\n",
      "[1,  2100] loss: 74.214083\n",
      "..........\n",
      "[1,  2200] loss: 76.508071\n",
      "..........\n",
      "[1,  2300] loss: 74.487263\n",
      "..........\n",
      "[1,  2400] loss: 75.696684\n",
      "..........\n",
      "[1,  2500] loss: 77.406120\n",
      "..........\n",
      "[1,  2600] loss: 74.682838\n",
      "..........\n",
      "[1,  2700] loss: 75.391109\n",
      "..........\n",
      "[1,  2800] loss: 74.346007\n",
      "..........\n",
      "[1,  2900] loss: 75.051068\n",
      "..........\n",
      "[1,  3000] loss: 74.997027\n",
      "..........\n",
      "[1,  3100] loss: 74.818320\n",
      "..........\n",
      "[1,  3200] loss: 74.542289\n",
      "..........\n",
      "[1,  3300] loss: 74.378110\n",
      "..........\n",
      "[1,  3400] loss: 75.310891\n",
      "..........\n",
      "[1,  3500] loss: 74.166966\n",
      "..........\n",
      "[1,  3600] loss: 74.818049\n",
      "..........\n",
      "[1,  3700] loss: 74.498036\n",
      "..........\n",
      "[1,  3800] loss: 74.710016\n",
      "..........\n",
      "[1,  3900] loss: 74.763377\n",
      "..........\n",
      "[1,  4000] loss: 74.282541\n",
      "..........\n",
      "[1,  4100] loss: 75.118332\n",
      "..........\n",
      "[1,  4200] loss: 75.633344\n",
      "..........\n",
      "[1,  4300] loss: 75.236358\n",
      "..........\n",
      "[1,  4400] loss: 74.251004\n",
      "..........\n",
      "[1,  4500] loss: 73.967827\n",
      "..........\n",
      "[1,  4600] loss: 75.668421\n",
      "..........\n",
      "[1,  4700] loss: 75.381250\n",
      "..........\n",
      "[1,  4800] loss: 74.138563\n",
      "..........\n",
      "[1,  4900] loss: 75.349691\n",
      "..........\n",
      "[1,  5000] loss: 76.320523\n",
      "..........\n",
      "[1,  5100] loss: 75.174875\n",
      "..........\n",
      "[1,  5200] loss: 75.542816\n",
      "..........\n",
      "[1,  5300] loss: 75.165911\n",
      "..........\n",
      "[1,  5400] loss: 74.505228\n",
      "..........\n",
      "[1,  5500] loss: 75.212830\n",
      "..........\n",
      "[1,  5600] loss: 76.175474\n",
      "..........\n",
      "[1,  5700] loss: 75.747648\n",
      "..........\n",
      "[1,  5800] loss: 75.742678\n",
      "..........\n",
      "[1,  5900] loss: 75.579321\n",
      "..........\n",
      "[1,  6000] loss: 75.968200\n",
      "..........\n",
      "[1,  6100] loss: 75.391857\n",
      "..........\n",
      "[1,  6200] loss: 74.319553\n",
      "..........\n",
      "[1,  6300] loss: 76.113398\n",
      "..........\n",
      "[1,  6400] loss: 76.934686\n",
      "..........\n",
      "[1,  6500] loss: 76.401133\n",
      "..........\n",
      "[1,  6600] loss: 74.643407\n",
      "..........\n",
      "[1,  6700] loss: 77.135976\n",
      "..........\n",
      "[1,  6800] loss: 75.316295\n",
      "..........\n",
      "[1,  6900] loss: 75.662464\n",
      "..........\n",
      "[1,  7000] loss: 74.619436\n",
      "..........\n",
      "[1,  7100] loss: 77.622753\n",
      "..........\n",
      "[1,  7200] loss: 76.404184\n",
      "..........\n",
      "[1,  7300] loss: 75.564005\n",
      "..........\n",
      "[1,  7400] loss: 75.263890\n",
      "..........\n",
      "[1,  7500] loss: 75.624617\n",
      "..........\n",
      "[1,  7600] loss: 75.936698\n",
      "..........\n",
      "[1,  7700] loss: 76.376257\n",
      "..........\n",
      "[1,  7800] loss: 75.235223\n",
      "..........\n",
      "[1,  7900] loss: 77.114425\n",
      "..........\n",
      "[1,  8000] loss: 75.344834\n",
      "total average loss : 75.138\n",
      "train acc : 0.0867\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.146\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.119\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.129\n",
      "step : 1800 / 3125 acc : 0.125\n",
      "step : 2000 / 3125 acc : 0.119\n",
      "step : 2200 / 3125 acc : 0.116\n",
      "step : 2400 / 3125 acc : 0.112\n",
      "step : 2600 / 3125 acc : 0.111\n",
      "step : 2800 / 3125 acc : 0.107\n",
      "step : 3000 / 3125 acc : 0.104\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1020 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 77.608475\n",
      "..........\n",
      "[1,   200] loss: 76.354592\n",
      "..........\n",
      "[1,   300] loss: 77.062268\n",
      "..........\n",
      "[1,   400] loss: 76.990729\n",
      "..........\n",
      "[1,   500] loss: 76.646319\n",
      "..........\n",
      "[1,   600] loss: 77.138828\n",
      "..........\n",
      "[1,   700] loss: 76.494825\n",
      "..........\n",
      "[1,   800] loss: 76.732792\n",
      "..........\n",
      "[1,   900] loss: 77.172761\n",
      "..........\n",
      "[1,  1000] loss: 77.100939\n",
      "..........\n",
      "[1,  1100] loss: 77.022047\n",
      "..........\n",
      "[1,  1200] loss: 77.066605\n",
      "..........\n",
      "[1,  1300] loss: 77.195305\n",
      "..........\n",
      "[1,  1400] loss: 76.695899\n",
      "..........\n",
      "[1,  1500] loss: 77.334654\n",
      "..........\n",
      "[1,  1600] loss: 77.671988\n",
      "..........\n",
      "[1,  1700] loss: 79.400071\n",
      "..........\n",
      "[1,  1800] loss: 77.684224\n",
      "..........\n",
      "[1,  1900] loss: 77.211522\n",
      "..........\n",
      "[1,  2000] loss: 76.003906\n",
      "..........\n",
      "[1,  2100] loss: 75.889696\n",
      "..........\n",
      "[1,  2200] loss: 76.458911\n",
      "..........\n",
      "[1,  2300] loss: 77.479664\n",
      "..........\n",
      "[1,  2400] loss: 78.299384\n",
      "..........\n",
      "[1,  2500] loss: 76.657087\n",
      "..........\n",
      "[1,  2600] loss: 78.060090\n",
      "..........\n",
      "[1,  2700] loss: 77.038394\n",
      "..........\n",
      "[1,  2800] loss: 76.730762\n",
      "..........\n",
      "[1,  2900] loss: 77.203221\n",
      "..........\n",
      "[1,  3000] loss: 77.448518\n",
      "..........\n",
      "[1,  3100] loss: 77.006430\n",
      "..........\n",
      "[1,  3200] loss: 78.115986\n",
      "..........\n",
      "[1,  3300] loss: 77.146103\n",
      "..........\n",
      "[1,  3400] loss: 77.578370\n",
      "..........\n",
      "[1,  3500] loss: 78.651176\n",
      "..........\n",
      "[1,  3600] loss: 78.408743\n",
      "..........\n",
      "[1,  3700] loss: 78.643636\n",
      "..........\n",
      "[1,  3800] loss: 77.144371\n",
      "..........\n",
      "[1,  3900] loss: 77.945763\n",
      "..........\n",
      "[1,  4000] loss: 79.427269\n",
      "..........\n",
      "[1,  4100] loss: 76.633264\n",
      "..........\n",
      "[1,  4200] loss: 78.631644\n",
      "..........\n",
      "[1,  4300] loss: 76.919060\n",
      "..........\n",
      "[1,  4400] loss: 77.989286\n",
      "..........\n",
      "[1,  4500] loss: 78.089447\n",
      "..........\n",
      "[1,  4600] loss: 76.854556\n",
      "..........\n",
      "[1,  4700] loss: 78.109848\n",
      "..........\n",
      "[1,  4800] loss: 76.802319\n",
      "..........\n",
      "[1,  4900] loss: 79.916982\n",
      "..........\n",
      "[1,  5000] loss: 78.536619\n",
      "..........\n",
      "[1,  5100] loss: 79.337038\n",
      "..........\n",
      "[1,  5200] loss: 77.771719\n",
      "..........\n",
      "[1,  5300] loss: 78.462014\n",
      "..........\n",
      "[1,  5400] loss: 78.415150\n",
      "..........\n",
      "[1,  5500] loss: 78.191871\n",
      "..........\n",
      "[1,  5600] loss: 77.131595\n",
      "..........\n",
      "[1,  5700] loss: 78.845233\n",
      "..........\n",
      "[1,  5800] loss: 77.239480\n",
      "..........\n",
      "[1,  5900] loss: 77.543177\n",
      "..........\n",
      "[1,  6000] loss: 77.589891\n",
      "..........\n",
      "[1,  6100] loss: 78.913020\n",
      "..........\n",
      "[1,  6200] loss: 78.303876\n",
      "..........\n",
      "[1,  6300] loss: 77.990084\n",
      "..........\n",
      "[1,  6400] loss: 79.342842\n",
      "..........\n",
      "[1,  6500] loss: 77.383534\n",
      "..........\n",
      "[1,  6600] loss: 78.965050\n",
      "..........\n",
      "[1,  6700] loss: 78.352837\n",
      "..........\n",
      "[1,  6800] loss: 77.825045\n",
      "..........\n",
      "[1,  6900] loss: 78.436745\n",
      "..........\n",
      "[1,  7000] loss: 78.330945\n",
      "..........\n",
      "[1,  7100] loss: 78.929145\n",
      "..........\n",
      "[1,  7200] loss: 79.688797\n",
      "..........\n",
      "[1,  7300] loss: 78.787887\n",
      "..........\n",
      "[1,  7400] loss: 79.264813\n",
      "..........\n",
      "[1,  7500] loss: 77.580603\n",
      "..........\n",
      "[1,  7600] loss: 77.891140\n",
      "..........\n",
      "[1,  7700] loss: 77.613135\n",
      "..........\n",
      "[1,  7800] loss: 79.951336\n",
      "..........\n",
      "[1,  7900] loss: 78.197682\n",
      "..........\n",
      "[1,  8000] loss: 77.597272\n",
      "total average loss : 77.778\n",
      "train acc : 0.0898\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.119\n",
      "step : 1200 / 3125 acc : 0.120\n",
      "step : 1400 / 3125 acc : 0.103\n",
      "step : 1600 / 3125 acc : 0.094\n",
      "step : 1800 / 3125 acc : 0.097\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.108\n",
      "step : 2400 / 3125 acc : 0.104\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.096\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 80.065951\n",
      "..........\n",
      "[1,   200] loss: 79.951746\n",
      "..........\n",
      "[1,   300] loss: 79.642808\n",
      "..........\n",
      "[1,   400] loss: 79.905719\n",
      "..........\n",
      "[1,   500] loss: 78.998255\n",
      "..........\n",
      "[1,   600] loss: 81.004036\n",
      "..........\n",
      "[1,   700] loss: 78.878618\n",
      "..........\n",
      "[1,   800] loss: 79.565158\n",
      "..........\n",
      "[1,   900] loss: 77.621710\n",
      "..........\n",
      "[1,  1000] loss: 80.412768\n",
      "..........\n",
      "[1,  1100] loss: 79.409238\n",
      "..........\n",
      "[1,  1200] loss: 78.094881\n",
      "..........\n",
      "[1,  1300] loss: 79.375307\n",
      "..........\n",
      "[1,  1400] loss: 80.084716\n",
      "..........\n",
      "[1,  1500] loss: 79.178936\n",
      "..........\n",
      "[1,  1600] loss: 79.244463\n",
      "..........\n",
      "[1,  1700] loss: 80.307229\n",
      "..........\n",
      "[1,  1800] loss: 80.065350\n",
      "..........\n",
      "[1,  1900] loss: 79.667792\n",
      "..........\n",
      "[1,  2000] loss: 79.584993\n",
      "..........\n",
      "[1,  2100] loss: 81.660834\n",
      "..........\n",
      "[1,  2200] loss: 78.577368\n",
      "..........\n",
      "[1,  2300] loss: 79.348820\n",
      "..........\n",
      "[1,  2400] loss: 79.010459\n",
      "..........\n",
      "[1,  2500] loss: 80.369638\n",
      "..........\n",
      "[1,  2600] loss: 79.244710\n",
      "..........\n",
      "[1,  2700] loss: 79.683934\n",
      "..........\n",
      "[1,  2800] loss: 80.172213\n",
      "..........\n",
      "[1,  2900] loss: 81.337559\n",
      "..........\n",
      "[1,  3000] loss: 80.599712\n",
      "..........\n",
      "[1,  3100] loss: 79.272978\n",
      "..........\n",
      "[1,  3200] loss: 81.437723\n",
      "..........\n",
      "[1,  3300] loss: 82.117836\n",
      "..........\n",
      "[1,  3400] loss: 80.436577\n",
      "..........\n",
      "[1,  3500] loss: 79.838728\n",
      "..........\n",
      "[1,  3600] loss: 79.449115\n",
      "..........\n",
      "[1,  3700] loss: 79.836901\n",
      "..........\n",
      "[1,  3800] loss: 79.954934\n",
      "..........\n",
      "[1,  3900] loss: 80.660434\n",
      "..........\n",
      "[1,  4000] loss: 81.203901\n",
      "..........\n",
      "[1,  4100] loss: 79.971292\n",
      "..........\n",
      "[1,  4200] loss: 80.477082\n",
      "..........\n",
      "[1,  4300] loss: 80.997456\n",
      "..........\n",
      "[1,  4400] loss: 81.734760\n",
      "..........\n",
      "[1,  4500] loss: 80.347732\n",
      "..........\n",
      "[1,  4600] loss: 80.705109\n",
      "..........\n",
      "[1,  4700] loss: 80.417276\n",
      "..........\n",
      "[1,  4800] loss: 81.161678\n",
      "..........\n",
      "[1,  4900] loss: 81.941950\n",
      "..........\n",
      "[1,  5000] loss: 80.888348\n",
      "..........\n",
      "[1,  5100] loss: 81.521015\n",
      "..........\n",
      "[1,  5200] loss: 81.399213\n",
      "..........\n",
      "[1,  5300] loss: 80.182063\n",
      "..........\n",
      "[1,  5400] loss: 81.623302\n",
      "..........\n",
      "[1,  5500] loss: 81.273977\n",
      "..........\n",
      "[1,  5600] loss: 79.752944\n",
      "..........\n",
      "[1,  5700] loss: 80.568156\n",
      "..........\n",
      "[1,  5800] loss: 79.174886\n",
      "..........\n",
      "[1,  5900] loss: 80.434583\n",
      "..........\n",
      "[1,  6000] loss: 79.640648\n",
      "..........\n",
      "[1,  6100] loss: 81.204077\n",
      "..........\n",
      "[1,  6200] loss: 80.114972\n",
      "..........\n",
      "[1,  6300] loss: 80.178637\n",
      "..........\n",
      "[1,  6400] loss: 82.409432\n",
      "..........\n",
      "[1,  6500] loss: 80.514303\n",
      "..........\n",
      "[1,  6600] loss: 80.999315\n",
      "..........\n",
      "[1,  6700] loss: 81.417011\n",
      "..........\n",
      "[1,  6800] loss: 81.619817\n",
      "..........\n",
      "[1,  6900] loss: 82.738680\n",
      "..........\n",
      "[1,  7000] loss: 79.679628\n",
      "..........\n",
      "[1,  7100] loss: 80.246675\n",
      "..........\n",
      "[1,  7200] loss: 81.003547\n",
      "..........\n",
      "[1,  7300] loss: 80.963669\n",
      "..........\n",
      "[1,  7400] loss: 81.304702\n",
      "..........\n",
      "[1,  7500] loss: 82.322306\n",
      "..........\n",
      "[1,  7600] loss: 81.641963\n",
      "..........\n",
      "[1,  7700] loss: 81.532222\n",
      "..........\n",
      "[1,  7800] loss: 81.794314\n",
      "..........\n",
      "[1,  7900] loss: 80.249049\n",
      "..........\n",
      "[1,  8000] loss: 81.967241\n",
      "total average loss : 80.417\n",
      "train acc : 0.0891\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.083\n",
      "step : 1400 / 3125 acc : 0.089\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.094\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.104\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.104\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1020 %\n",
      "======eval  end ======\n",
      "======= epoch  8 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 83.030281\n",
      "..........\n",
      "[1,   200] loss: 81.385645\n",
      "..........\n",
      "[1,   300] loss: 82.612342\n",
      "..........\n",
      "[1,   400] loss: 82.079908\n",
      "..........\n",
      "[1,   500] loss: 82.080191\n",
      "..........\n",
      "[1,   600] loss: 81.810289\n",
      "..........\n",
      "[1,   700] loss: 81.556905\n",
      "..........\n",
      "[1,   800] loss: 82.667458\n",
      "..........\n",
      "[1,   900] loss: 81.872523\n",
      "..........\n",
      "[1,  1000] loss: 80.274598\n",
      "..........\n",
      "[1,  1100] loss: 82.608459\n",
      "..........\n",
      "[1,  1200] loss: 83.112543\n",
      "..........\n",
      "[1,  1300] loss: 83.105425\n",
      "..........\n",
      "[1,  1400] loss: 82.586814\n",
      "..........\n",
      "[1,  1500] loss: 83.552647\n",
      "..........\n",
      "[1,  1600] loss: 82.331478\n",
      "..........\n",
      "[1,  1700] loss: 81.725036\n",
      "..........\n",
      "[1,  1800] loss: 83.231709\n",
      "..........\n",
      "[1,  1900] loss: 82.656219\n",
      "..........\n",
      "[1,  2000] loss: 82.662000\n",
      "..........\n",
      "[1,  2100] loss: 84.405530\n",
      "..........\n",
      "[1,  2200] loss: 83.106556\n",
      "..........\n",
      "[1,  2300] loss: 82.531087\n",
      "..........\n",
      "[1,  2400] loss: 82.953602\n",
      "..........\n",
      "[1,  2500] loss: 82.306078\n",
      "..........\n",
      "[1,  2600] loss: 82.866755\n",
      "..........\n",
      "[1,  2700] loss: 83.716583\n",
      "..........\n",
      "[1,  2800] loss: 81.172079\n",
      "..........\n",
      "[1,  2900] loss: 81.679803\n",
      "..........\n",
      "[1,  3000] loss: 82.910360\n",
      "..........\n",
      "[1,  3100] loss: 84.552254\n",
      "..........\n",
      "[1,  3200] loss: 81.273538\n",
      "..........\n",
      "[1,  3300] loss: 82.077443\n",
      "..........\n",
      "[1,  3400] loss: 82.059822\n",
      "..........\n",
      "[1,  3500] loss: 81.951205\n",
      "..........\n",
      "[1,  3600] loss: 84.252105\n",
      "..........\n",
      "[1,  3700] loss: 80.279357\n",
      "..........\n",
      "[1,  3800] loss: 83.896045\n",
      "..........\n",
      "[1,  3900] loss: 81.222143\n",
      "..........\n",
      "[1,  4000] loss: 83.701105\n",
      "..........\n",
      "[1,  4100] loss: 83.323401\n",
      "..........\n",
      "[1,  4200] loss: 83.304183\n",
      "..........\n",
      "[1,  4300] loss: 84.311012\n",
      "..........\n",
      "[1,  4400] loss: 83.927215\n",
      "..........\n",
      "[1,  4500] loss: 82.674192\n",
      "..........\n",
      "[1,  4600] loss: 82.660460\n",
      "..........\n",
      "[1,  4700] loss: 82.790175\n",
      "..........\n",
      "[1,  4800] loss: 82.133889\n",
      "..........\n",
      "[1,  4900] loss: 83.874912\n",
      "..........\n",
      "[1,  5000] loss: 81.858842\n",
      "..........\n",
      "[1,  5100] loss: 83.576757\n",
      "..........\n",
      "[1,  5200] loss: 84.797052\n",
      "..........\n",
      "[1,  5300] loss: 83.166089\n",
      "..........\n",
      "[1,  5400] loss: 84.861060\n",
      "..........\n",
      "[1,  5500] loss: 84.031826\n",
      "..........\n",
      "[1,  5600] loss: 83.301666\n",
      "..........\n",
      "[1,  5700] loss: 83.510774\n",
      "..........\n",
      "[1,  5800] loss: 83.485884\n",
      "..........\n",
      "[1,  5900] loss: 83.991370\n",
      "..........\n",
      "[1,  6000] loss: 84.552741\n",
      "..........\n",
      "[1,  6100] loss: 83.144193\n",
      "..........\n",
      "[1,  6200] loss: 82.663616\n",
      "..........\n",
      "[1,  6300] loss: 81.974079\n",
      "..........\n",
      "[1,  6400] loss: 82.887804\n",
      "..........\n",
      "[1,  6500] loss: 83.833779\n",
      "..........\n",
      "[1,  6600] loss: 82.876018\n",
      "..........\n",
      "[1,  6700] loss: 82.853373\n",
      "..........\n",
      "[1,  6800] loss: 84.305278\n",
      "..........\n",
      "[1,  6900] loss: 82.806016\n",
      "..........\n",
      "[1,  7000] loss: 84.824067\n",
      "..........\n",
      "[1,  7100] loss: 82.379888\n",
      "..........\n",
      "[1,  7200] loss: 84.201732\n",
      "..........\n",
      "[1,  7300] loss: 85.305627\n",
      "..........\n",
      "[1,  7400] loss: 83.480170\n",
      "..........\n",
      "[1,  7500] loss: 85.848547\n",
      "..........\n",
      "[1,  7600] loss: 82.949833\n",
      "..........\n",
      "[1,  7700] loss: 84.186282\n",
      "..........\n",
      "[1,  7800] loss: 86.566277\n",
      "..........\n",
      "[1,  7900] loss: 84.315960\n",
      "..........\n",
      "[1,  8000] loss: 83.819370\n",
      "total average loss : 83.065\n",
      "train acc : 0.0883\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.109\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.088\n",
      "step : 2400 / 3125 acc : 0.089\n",
      "step : 2600 / 3125 acc : 0.091\n",
      "step : 2800 / 3125 acc : 0.096\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1040 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 83.091378\n",
      "..........\n",
      "[1,   200] loss: 84.549064\n",
      "..........\n",
      "[1,   300] loss: 83.892924\n",
      "..........\n",
      "[1,   400] loss: 86.137205\n",
      "..........\n",
      "[1,   500] loss: 84.047199\n",
      "..........\n",
      "[1,   600] loss: 85.128769\n",
      "..........\n",
      "[1,   700] loss: 83.948317\n",
      "..........\n",
      "[1,   800] loss: 83.496171\n",
      "..........\n",
      "[1,   900] loss: 85.371925\n",
      "..........\n",
      "[1,  1000] loss: 83.915282\n",
      "..........\n",
      "[1,  1100] loss: 84.563840\n",
      "..........\n",
      "[1,  1200] loss: 84.735207\n",
      "..........\n",
      "[1,  1300] loss: 85.964744\n",
      "..........\n",
      "[1,  1400] loss: 85.368965\n",
      "..........\n",
      "[1,  1500] loss: 86.046060\n",
      "..........\n",
      "[1,  1600] loss: 85.573570\n",
      "..........\n",
      "[1,  1700] loss: 85.440314\n",
      "..........\n",
      "[1,  1800] loss: 83.562180\n",
      "..........\n",
      "[1,  1900] loss: 85.761716\n",
      "..........\n",
      "[1,  2000] loss: 85.350127\n",
      "..........\n",
      "[1,  2100] loss: 84.401553\n",
      "..........\n",
      "[1,  2200] loss: 84.413871\n",
      "..........\n",
      "[1,  2300] loss: 85.579320\n",
      "..........\n",
      "[1,  2400] loss: 85.417031\n",
      "..........\n",
      "[1,  2500] loss: 84.452431\n",
      "..........\n",
      "[1,  2600] loss: 84.428107\n",
      "..........\n",
      "[1,  2700] loss: 85.448271\n",
      "..........\n",
      "[1,  2800] loss: 86.569376\n",
      "..........\n",
      "[1,  2900] loss: 85.852453\n",
      "..........\n",
      "[1,  3000] loss: 84.678119\n",
      "..........\n",
      "[1,  3100] loss: 86.505162\n",
      "..........\n",
      "[1,  3200] loss: 86.116153\n",
      "..........\n",
      "[1,  3300] loss: 85.432061\n",
      "..........\n",
      "[1,  3400] loss: 84.696392\n",
      "..........\n",
      "[1,  3500] loss: 87.583705\n",
      "..........\n",
      "[1,  3600] loss: 87.017570\n",
      "..........\n",
      "[1,  3700] loss: 84.869513\n",
      "..........\n",
      "[1,  3800] loss: 84.459266\n",
      "..........\n",
      "[1,  3900] loss: 84.943038\n",
      "..........\n",
      "[1,  4000] loss: 85.437208\n",
      "..........\n",
      "[1,  4100] loss: 88.482435\n",
      "..........\n",
      "[1,  4200] loss: 85.671750\n",
      "..........\n",
      "[1,  4300] loss: 85.475517\n",
      "..........\n",
      "[1,  4400] loss: 86.809987\n",
      "..........\n",
      "[1,  4500] loss: 86.407634\n",
      "..........\n",
      "[1,  4600] loss: 85.102775\n",
      "..........\n",
      "[1,  4700] loss: 84.533408\n",
      "..........\n",
      "[1,  4800] loss: 85.428902\n",
      "..........\n",
      "[1,  4900] loss: 86.001643\n",
      "..........\n",
      "[1,  5000] loss: 85.435599\n",
      "..........\n",
      "[1,  5100] loss: 86.992666\n",
      "..........\n",
      "[1,  5200] loss: 87.000215\n",
      "..........\n",
      "[1,  5300] loss: 86.709081\n",
      "..........\n",
      "[1,  5400] loss: 85.446750\n",
      "..........\n",
      "[1,  5500] loss: 85.643965\n",
      "..........\n",
      "[1,  5600] loss: 86.126966\n",
      "..........\n",
      "[1,  5700] loss: 86.101718\n",
      "..........\n",
      "[1,  5800] loss: 85.766533\n",
      "..........\n",
      "[1,  5900] loss: 86.570551\n",
      "..........\n",
      "[1,  6000] loss: 87.671662\n",
      "..........\n",
      "[1,  6100] loss: 86.783919\n",
      "..........\n",
      "[1,  6200] loss: 86.593604\n",
      "..........\n",
      "[1,  6300] loss: 86.409380\n",
      "..........\n",
      "[1,  6400] loss: 86.176540\n",
      "..........\n",
      "[1,  6500] loss: 85.714756\n",
      "..........\n",
      "[1,  6600] loss: 87.401590\n",
      "..........\n",
      "[1,  6700] loss: 86.456383\n",
      "..........\n",
      "[1,  6800] loss: 84.757088\n",
      "..........\n",
      "[1,  6900] loss: 85.798822\n",
      "..........\n",
      "[1,  7000] loss: 86.461084\n",
      "..........\n",
      "[1,  7100] loss: 86.510397\n",
      "..........\n",
      "[1,  7200] loss: 86.537546\n",
      "..........\n",
      "[1,  7300] loss: 87.073025\n",
      "..........\n",
      "[1,  7400] loss: 86.648940\n",
      "..........\n",
      "[1,  7500] loss: 85.373286\n",
      "..........\n",
      "[1,  7600] loss: 87.635903\n",
      "..........\n",
      "[1,  7700] loss: 88.473666\n",
      "..........\n",
      "[1,  7800] loss: 87.409047\n",
      "..........\n",
      "[1,  7900] loss: 87.633420\n",
      "..........\n",
      "[1,  8000] loss: 86.324433\n",
      "total average loss : 85.747\n",
      "train acc : 0.0922\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.062\n",
      "step : 800 / 3125 acc : 0.086\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.078\n",
      "step : 1400 / 3125 acc : 0.080\n",
      "step : 1600 / 3125 acc : 0.082\n",
      "step : 1800 / 3125 acc : 0.080\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.102\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.094\n",
      "step : 2800 / 3125 acc : 0.100\n",
      "step : 3000 / 3125 acc : 0.100\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1000 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 86.813519\n",
      "..........\n",
      "[1,   200] loss: 88.387015\n",
      "..........\n",
      "[1,   300] loss: 86.673849\n",
      "..........\n",
      "[1,   400] loss: 87.149256\n",
      "..........\n",
      "[1,   500] loss: 88.091365\n",
      "..........\n",
      "[1,   600] loss: 87.603761\n",
      "..........\n",
      "[1,   700] loss: 88.022409\n",
      "..........\n",
      "[1,   800] loss: 87.153123\n",
      "..........\n",
      "[1,   900] loss: 87.696856\n",
      "..........\n",
      "[1,  1000] loss: 88.110748\n",
      "..........\n",
      "[1,  1100] loss: 87.250592\n",
      "..........\n",
      "[1,  1200] loss: 86.593249\n",
      "..........\n",
      "[1,  1300] loss: 88.109718\n",
      "..........\n",
      "[1,  1400] loss: 88.542731\n",
      "..........\n",
      "[1,  1500] loss: 89.017481\n",
      "..........\n",
      "[1,  1600] loss: 88.877803\n",
      "..........\n",
      "[1,  1700] loss: 88.257493\n",
      "..........\n",
      "[1,  1800] loss: 87.916412\n",
      "..........\n",
      "[1,  1900] loss: 89.182388\n",
      "..........\n",
      "[1,  2000] loss: 87.574861\n",
      "..........\n",
      "[1,  2100] loss: 86.649202\n",
      "..........\n",
      "[1,  2200] loss: 86.676617\n",
      "..........\n",
      "[1,  2300] loss: 87.952120\n",
      "..........\n",
      "[1,  2400] loss: 86.765754\n",
      "..........\n",
      "[1,  2500] loss: 88.962115\n",
      "..........\n",
      "[1,  2600] loss: 87.596069\n",
      "..........\n",
      "[1,  2700] loss: 89.043995\n",
      "..........\n",
      "[1,  2800] loss: 87.207657\n",
      "..........\n",
      "[1,  2900] loss: 88.240949\n",
      "..........\n",
      "[1,  3000] loss: 87.145961\n",
      "..........\n",
      "[1,  3100] loss: 89.276571\n",
      "..........\n",
      "[1,  3200] loss: 88.459243\n",
      "..........\n",
      "[1,  3300] loss: 87.702387\n",
      "..........\n",
      "[1,  3400] loss: 87.891628\n",
      "..........\n",
      "[1,  3500] loss: 88.789447\n",
      "..........\n",
      "[1,  3600] loss: 86.916505\n",
      "..........\n",
      "[1,  3700] loss: 88.908107\n",
      "..........\n",
      "[1,  3800] loss: 88.066714\n",
      "..........\n",
      "[1,  3900] loss: 88.608658\n",
      "..........\n",
      "[1,  4000] loss: 87.361805\n",
      "..........\n",
      "[1,  4100] loss: 88.079831\n",
      "..........\n",
      "[1,  4200] loss: 88.797098\n",
      "..........\n",
      "[1,  4300] loss: 88.764180\n",
      "..........\n",
      "[1,  4400] loss: 88.017526\n",
      "..........\n",
      "[1,  4500] loss: 90.255387\n",
      "..........\n",
      "[1,  4600] loss: 87.687567\n",
      "..........\n",
      "[1,  4700] loss: 87.722584\n",
      "..........\n",
      "[1,  4800] loss: 89.371774\n",
      "..........\n",
      "[1,  4900] loss: 87.028461\n",
      "..........\n",
      "[1,  5000] loss: 88.500613\n",
      "..........\n",
      "[1,  5100] loss: 88.218282\n",
      "..........\n",
      "[1,  5200] loss: 87.649421\n",
      "..........\n",
      "[1,  5300] loss: 86.461334\n",
      "..........\n",
      "[1,  5400] loss: 89.155388\n",
      "..........\n",
      "[1,  5500] loss: 86.893367\n",
      "..........\n",
      "[1,  5600] loss: 86.564264\n",
      "..........\n",
      "[1,  5700] loss: 89.760830\n",
      "..........\n",
      "[1,  5800] loss: 89.771152\n",
      "..........\n",
      "[1,  5900] loss: 88.783932\n",
      "..........\n",
      "[1,  6000] loss: 88.891128\n",
      "..........\n",
      "[1,  6100] loss: 88.450672\n",
      "..........\n",
      "[1,  6200] loss: 88.313943\n",
      "..........\n",
      "[1,  6300] loss: 89.085492\n",
      "..........\n",
      "[1,  6400] loss: 90.341225\n",
      "..........\n",
      "[1,  6500] loss: 89.241104\n",
      "..........\n",
      "[1,  6600] loss: 89.853638\n",
      "..........\n",
      "[1,  6700] loss: 88.759731\n",
      "..........\n",
      "[1,  6800] loss: 89.020102\n",
      "..........\n",
      "[1,  6900] loss: 88.400476\n",
      "..........\n",
      "[1,  7000] loss: 89.185085\n",
      "..........\n",
      "[1,  7100] loss: 88.162777\n",
      "..........\n",
      "[1,  7200] loss: 87.243409\n",
      "..........\n",
      "[1,  7300] loss: 89.633464\n",
      "..........\n",
      "[1,  7400] loss: 89.728569\n",
      "..........\n",
      "[1,  7500] loss: 88.544419\n",
      "..........\n",
      "[1,  7600] loss: 89.190731\n",
      "..........\n",
      "[1,  7700] loss: 87.223238\n",
      "..........\n",
      "[1,  7800] loss: 88.485760\n",
      "..........\n",
      "[1,  7900] loss: 90.398406\n",
      "..........\n",
      "[1,  8000] loss: 88.852151\n",
      "total average loss : 88.247\n",
      "train acc : 0.0906\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.070\n",
      "step : 1000 / 3125 acc : 0.075\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.111\n",
      "step : 2000 / 3125 acc : 0.113\n",
      "step : 2200 / 3125 acc : 0.111\n",
      "step : 2400 / 3125 acc : 0.112\n",
      "step : 2600 / 3125 acc : 0.111\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1040 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 89.654491\n",
      "..........\n",
      "[1,   200] loss: 87.273335\n",
      "..........\n",
      "[1,   300] loss: 91.011190\n",
      "..........\n",
      "[1,   400] loss: 90.009568\n",
      "..........\n",
      "[1,   500] loss: 89.897219\n",
      "..........\n",
      "[1,   600] loss: 91.151758\n",
      "..........\n",
      "[1,   700] loss: 88.097389\n",
      "..........\n",
      "[1,   800] loss: 88.783234\n",
      "..........\n",
      "[1,   900] loss: 89.098849\n",
      "..........\n",
      "[1,  1000] loss: 90.555798\n",
      "..........\n",
      "[1,  1100] loss: 91.213299\n",
      "..........\n",
      "[1,  1200] loss: 89.922743\n",
      "..........\n",
      "[1,  1300] loss: 90.385440\n",
      "..........\n",
      "[1,  1400] loss: 90.232787\n",
      "..........\n",
      "[1,  1500] loss: 90.903179\n",
      "..........\n",
      "[1,  1600] loss: 90.578982\n",
      "..........\n",
      "[1,  1700] loss: 88.902234\n",
      "..........\n",
      "[1,  1800] loss: 90.107099\n",
      "..........\n",
      "[1,  1900] loss: 89.822943\n",
      "..........\n",
      "[1,  2000] loss: 89.631941\n",
      "..........\n",
      "[1,  2100] loss: 91.699546\n",
      "..........\n",
      "[1,  2200] loss: 89.661424\n",
      "..........\n",
      "[1,  2300] loss: 91.570528\n",
      "..........\n",
      "[1,  2400] loss: 91.205436\n",
      "..........\n",
      "[1,  2500] loss: 88.932940\n",
      "..........\n",
      "[1,  2600] loss: 90.878876\n",
      "..........\n",
      "[1,  2700] loss: 91.175847\n",
      "..........\n",
      "[1,  2800] loss: 88.989137\n",
      "..........\n",
      "[1,  2900] loss: 91.041977\n",
      "..........\n",
      "[1,  3000] loss: 91.026749\n",
      "..........\n",
      "[1,  3100] loss: 88.607598\n",
      "..........\n",
      "[1,  3200] loss: 91.360644\n",
      "..........\n",
      "[1,  3300] loss: 91.069178\n",
      "..........\n",
      "[1,  3400] loss: 90.918669\n",
      "..........\n",
      "[1,  3500] loss: 90.674754\n",
      "..........\n",
      "[1,  3600] loss: 91.135213\n",
      "..........\n",
      "[1,  3700] loss: 90.175093\n",
      "..........\n",
      "[1,  3800] loss: 91.371230\n",
      "..........\n",
      "[1,  3900] loss: 91.876237\n",
      "..........\n",
      "[1,  4000] loss: 90.061565\n",
      "..........\n",
      "[1,  4100] loss: 91.113677\n",
      "..........\n",
      "[1,  4200] loss: 91.584200\n",
      "..........\n",
      "[1,  4300] loss: 90.780404\n",
      "..........\n",
      "[1,  4400] loss: 91.559035\n",
      "..........\n",
      "[1,  4500] loss: 91.413875\n",
      "..........\n",
      "[1,  4600] loss: 92.152827\n",
      "..........\n",
      "[1,  4700] loss: 90.851051\n",
      "..........\n",
      "[1,  4800] loss: 90.214155\n",
      "..........\n",
      "[1,  4900] loss: 91.845862\n",
      "..........\n",
      "[1,  5000] loss: 91.110039\n",
      "..........\n",
      "[1,  5100] loss: 91.191290\n",
      "..........\n",
      "[1,  5200] loss: 91.394043\n",
      "..........\n",
      "[1,  5300] loss: 91.033593\n",
      "..........\n",
      "[1,  5400] loss: 90.447327\n",
      "..........\n",
      "[1,  5500] loss: 91.643783\n",
      "..........\n",
      "[1,  5600] loss: 90.012741\n",
      "..........\n",
      "[1,  5700] loss: 89.502670\n",
      "..........\n",
      "[1,  5800] loss: 91.534597\n",
      "..........\n",
      "[1,  5900] loss: 91.253382\n",
      "..........\n",
      "[1,  6000] loss: 89.166387\n",
      "..........\n",
      "[1,  6100] loss: 92.232656\n",
      "..........\n",
      "[1,  6200] loss: 90.508911\n",
      "..........\n",
      "[1,  6300] loss: 90.221554\n",
      "..........\n",
      "[1,  6400] loss: 91.064940\n",
      "..........\n",
      "[1,  6500] loss: 90.739530\n",
      "..........\n",
      "[1,  6600] loss: 90.741197\n",
      "..........\n",
      "[1,  6700] loss: 90.643361\n",
      "..........\n",
      "[1,  6800] loss: 90.811879\n",
      "..........\n",
      "[1,  6900] loss: 91.171492\n",
      "..........\n",
      "[1,  7000] loss: 90.157075\n",
      "..........\n",
      "[1,  7100] loss: 90.158328\n",
      "..........\n",
      "[1,  7200] loss: 91.435341\n",
      "..........\n",
      "[1,  7300] loss: 90.715611\n",
      "..........\n",
      "[1,  7400] loss: 93.195737\n",
      "..........\n",
      "[1,  7500] loss: 91.367093\n",
      "..........\n",
      "[1,  7600] loss: 92.537872\n",
      "..........\n",
      "[1,  7700] loss: 89.726816\n",
      "..........\n",
      "[1,  7800] loss: 91.030597\n",
      "..........\n",
      "[1,  7900] loss: 90.995215\n",
      "..........\n",
      "[1,  8000] loss: 91.605760\n",
      "total average loss : 90.644\n",
      "train acc : 0.0945\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.188\n",
      "step : 400 / 3125 acc : 0.188\n",
      "step : 600 / 3125 acc : 0.177\n",
      "step : 800 / 3125 acc : 0.148\n",
      "step : 1000 / 3125 acc : 0.163\n",
      "step : 1200 / 3125 acc : 0.151\n",
      "step : 1400 / 3125 acc : 0.134\n",
      "step : 1600 / 3125 acc : 0.129\n",
      "step : 1800 / 3125 acc : 0.122\n",
      "step : 2000 / 3125 acc : 0.113\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.099\n",
      "step : 2600 / 3125 acc : 0.099\n",
      "step : 2800 / 3125 acc : 0.105\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1040 %\n",
      "======eval  end ======\n",
      "======= epoch  9 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 91.424175\n",
      "..........\n",
      "[1,   200] loss: 93.264797\n",
      "..........\n",
      "[1,   300] loss: 92.422991\n",
      "..........\n",
      "[1,   400] loss: 93.055187\n",
      "..........\n",
      "[1,   500] loss: 90.978269\n",
      "..........\n",
      "[1,   600] loss: 93.081935\n",
      "..........\n",
      "[1,   700] loss: 92.902426\n",
      "..........\n",
      "[1,   800] loss: 92.172981\n",
      "..........\n",
      "[1,   900] loss: 92.607984\n",
      "..........\n",
      "[1,  1000] loss: 91.878621\n",
      "..........\n",
      "[1,  1100] loss: 91.362377\n",
      "..........\n",
      "[1,  1200] loss: 91.505822\n",
      "..........\n",
      "[1,  1300] loss: 92.161058\n",
      "..........\n",
      "[1,  1400] loss: 94.063596\n",
      "..........\n",
      "[1,  1500] loss: 92.098301\n",
      "..........\n",
      "[1,  1600] loss: 92.081032\n",
      "..........\n",
      "[1,  1700] loss: 93.446071\n",
      "..........\n",
      "[1,  1800] loss: 91.295104\n",
      "..........\n",
      "[1,  1900] loss: 94.682848\n",
      "..........\n",
      "[1,  2000] loss: 93.029935\n",
      "..........\n",
      "[1,  2100] loss: 92.198416\n",
      "..........\n",
      "[1,  2200] loss: 92.493447\n",
      "..........\n",
      "[1,  2300] loss: 94.028560\n",
      "..........\n",
      "[1,  2400] loss: 91.700794\n",
      "..........\n",
      "[1,  2500] loss: 92.538230\n",
      "..........\n",
      "[1,  2600] loss: 91.524530\n",
      "..........\n",
      "[1,  2700] loss: 91.558000\n",
      "..........\n",
      "[1,  2800] loss: 92.192108\n",
      "..........\n",
      "[1,  2900] loss: 90.556611\n",
      "..........\n",
      "[1,  3000] loss: 91.954973\n",
      "..........\n",
      "[1,  3100] loss: 91.943275\n",
      "..........\n",
      "[1,  3200] loss: 94.218719\n",
      "..........\n",
      "[1,  3300] loss: 92.291591\n",
      "..........\n",
      "[1,  3400] loss: 92.886450\n",
      "..........\n",
      "[1,  3500] loss: 91.612185\n",
      "..........\n",
      "[1,  3600] loss: 92.590227\n",
      "..........\n",
      "[1,  3700] loss: 92.743661\n",
      "..........\n",
      "[1,  3800] loss: 93.465244\n",
      "..........\n",
      "[1,  3900] loss: 92.441443\n",
      "..........\n",
      "[1,  4000] loss: 94.708872\n",
      "..........\n",
      "[1,  4100] loss: 92.808421\n",
      "..........\n",
      "[1,  4200] loss: 94.100861\n",
      "..........\n",
      "[1,  4300] loss: 92.943888\n",
      "..........\n",
      "[1,  4400] loss: 92.681739\n",
      "..........\n",
      "[1,  4500] loss: 92.665568\n",
      "..........\n",
      "[1,  4600] loss: 91.651378\n",
      "..........\n",
      "[1,  4700] loss: 92.399911\n",
      "..........\n",
      "[1,  4800] loss: 92.806064\n",
      "..........\n",
      "[1,  4900] loss: 92.129251\n",
      "..........\n",
      "[1,  5000] loss: 94.271382\n",
      "..........\n",
      "[1,  5100] loss: 93.850634\n",
      "..........\n",
      "[1,  5200] loss: 93.034008\n",
      "..........\n",
      "[1,  5300] loss: 93.999530\n",
      "..........\n",
      "[1,  5400] loss: 94.321839\n",
      "..........\n",
      "[1,  5500] loss: 92.084045\n",
      "..........\n",
      "[1,  5600] loss: 94.481571\n",
      "..........\n",
      "[1,  5700] loss: 93.106873\n",
      "..........\n",
      "[1,  5800] loss: 93.320789\n",
      "..........\n",
      "[1,  5900] loss: 93.608440\n",
      "..........\n",
      "[1,  6000] loss: 93.324145\n",
      "..........\n",
      "[1,  6100] loss: 92.362274\n",
      "..........\n",
      "[1,  6200] loss: 92.459856\n",
      "..........\n",
      "[1,  6300] loss: 92.938809\n",
      "..........\n",
      "[1,  6400] loss: 93.386183\n",
      "..........\n",
      "[1,  6500] loss: 93.358119\n",
      "..........\n",
      "[1,  6600] loss: 93.488306\n",
      "..........\n",
      "[1,  6700] loss: 92.226636\n",
      "..........\n",
      "[1,  6800] loss: 94.585034\n",
      "..........\n",
      "[1,  6900] loss: 94.431414\n",
      "..........\n",
      "[1,  7000] loss: 93.322020\n",
      "..........\n",
      "[1,  7100] loss: 93.896685\n",
      "..........\n",
      "[1,  7200] loss: 94.011775\n",
      "..........\n",
      "[1,  7300] loss: 94.259706\n",
      "..........\n",
      "[1,  7400] loss: 94.226216\n",
      "..........\n",
      "[1,  7500] loss: 93.167565\n",
      "..........\n",
      "[1,  7600] loss: 93.964838\n",
      "..........\n",
      "[1,  7700] loss: 93.184417\n",
      "..........\n",
      "[1,  7800] loss: 94.793486\n",
      "..........\n",
      "[1,  7900] loss: 94.443037\n",
      "..........\n",
      "[1,  8000] loss: 92.751718\n",
      "total average loss : 92.925\n",
      "train acc : 0.0938\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.103\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.104\n",
      "step : 2000 / 3125 acc : 0.113\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.112\n",
      "step : 2600 / 3125 acc : 0.108\n",
      "step : 2800 / 3125 acc : 0.107\n",
      "step : 3000 / 3125 acc : 0.102\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1060 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 94.777176\n",
      "..........\n",
      "[1,   200] loss: 96.019883\n",
      "..........\n",
      "[1,   300] loss: 94.159681\n",
      "..........\n",
      "[1,   400] loss: 93.554831\n",
      "..........\n",
      "[1,   500] loss: 93.444698\n",
      "..........\n",
      "[1,   600] loss: 96.639419\n",
      "..........\n",
      "[1,   700] loss: 93.954501\n",
      "..........\n",
      "[1,   800] loss: 94.032187\n",
      "..........\n",
      "[1,   900] loss: 94.824728\n",
      "..........\n",
      "[1,  1000] loss: 93.177366\n",
      "..........\n",
      "[1,  1100] loss: 95.130224\n",
      "..........\n",
      "[1,  1200] loss: 93.742256\n",
      "..........\n",
      "[1,  1300] loss: 93.975193\n",
      "..........\n",
      "[1,  1400] loss: 95.307784\n",
      "..........\n",
      "[1,  1500] loss: 95.299512\n",
      "..........\n",
      "[1,  1600] loss: 94.703525\n",
      "..........\n",
      "[1,  1700] loss: 94.947667\n",
      "..........\n",
      "[1,  1800] loss: 94.650707\n",
      "..........\n",
      "[1,  1900] loss: 95.184137\n",
      "..........\n",
      "[1,  2000] loss: 93.401950\n",
      "..........\n",
      "[1,  2100] loss: 94.569387\n",
      "..........\n",
      "[1,  2200] loss: 96.725209\n",
      "..........\n",
      "[1,  2300] loss: 94.223112\n",
      "..........\n",
      "[1,  2400] loss: 95.746087\n",
      "..........\n",
      "[1,  2500] loss: 95.349965\n",
      "..........\n",
      "[1,  2600] loss: 92.753032\n",
      "..........\n",
      "[1,  2700] loss: 94.704712\n",
      "..........\n",
      "[1,  2800] loss: 95.693545\n",
      "..........\n",
      "[1,  2900] loss: 95.452142\n",
      "..........\n",
      "[1,  3000] loss: 95.021689\n",
      "..........\n",
      "[1,  3100] loss: 95.593022\n",
      "..........\n",
      "[1,  3200] loss: 95.914154\n",
      "..........\n",
      "[1,  3300] loss: 96.030717\n",
      "..........\n",
      "[1,  3400] loss: 95.060155\n",
      "..........\n",
      "[1,  3500] loss: 93.574543\n",
      "..........\n",
      "[1,  3600] loss: 94.953453\n",
      "..........\n",
      "[1,  3700] loss: 95.353630\n",
      "..........\n",
      "[1,  3800] loss: 93.329121\n",
      "..........\n",
      "[1,  3900] loss: 94.449770\n",
      "..........\n",
      "[1,  4000] loss: 93.636062\n",
      "..........\n",
      "[1,  4100] loss: 94.211099\n",
      "..........\n",
      "[1,  4200] loss: 95.005504\n",
      "..........\n",
      "[1,  4300] loss: 94.745293\n",
      "..........\n",
      "[1,  4400] loss: 95.008789\n",
      "..........\n",
      "[1,  4500] loss: 96.045695\n",
      "..........\n",
      "[1,  4600] loss: 95.737607\n",
      "..........\n",
      "[1,  4700] loss: 93.290516\n",
      "..........\n",
      "[1,  4800] loss: 96.011196\n",
      "..........\n",
      "[1,  4900] loss: 96.287993\n",
      "..........\n",
      "[1,  5000] loss: 94.799478\n",
      "..........\n",
      "[1,  5100] loss: 93.201886\n",
      "..........\n",
      "[1,  5200] loss: 94.495266\n",
      "..........\n",
      "[1,  5300] loss: 94.609331\n",
      "..........\n",
      "[1,  5400] loss: 94.221634\n",
      "..........\n",
      "[1,  5500] loss: 95.678920\n",
      "..........\n",
      "[1,  5600] loss: 95.500995\n",
      "..........\n",
      "[1,  5700] loss: 95.404684\n",
      "..........\n",
      "[1,  5800] loss: 94.264510\n",
      "..........\n",
      "[1,  5900] loss: 95.505877\n",
      "..........\n",
      "[1,  6000] loss: 95.247477\n",
      "..........\n",
      "[1,  6100] loss: 95.191072\n",
      "..........\n",
      "[1,  6200] loss: 95.823280\n",
      "..........\n",
      "[1,  6300] loss: 97.463151\n",
      "..........\n",
      "[1,  6400] loss: 96.234161\n",
      "..........\n",
      "[1,  6500] loss: 97.397509\n",
      "..........\n",
      "[1,  6600] loss: 95.802752\n",
      "..........\n",
      "[1,  6700] loss: 96.665973\n",
      "..........\n",
      "[1,  6800] loss: 94.762253\n",
      "..........\n",
      "[1,  6900] loss: 96.461028\n",
      "..........\n",
      "[1,  7000] loss: 96.627722\n",
      "..........\n",
      "[1,  7100] loss: 94.609223\n",
      "..........\n",
      "[1,  7200] loss: 94.540476\n",
      "..........\n",
      "[1,  7300] loss: 95.815128\n",
      "..........\n",
      "[1,  7400] loss: 94.645971\n",
      "..........\n",
      "[1,  7500] loss: 95.396864\n",
      "..........\n",
      "[1,  7600] loss: 96.381009\n",
      "..........\n",
      "[1,  7700] loss: 95.651627\n",
      "..........\n",
      "[1,  7800] loss: 94.272043\n",
      "..........\n",
      "[1,  7900] loss: 94.503278\n",
      "..........\n",
      "[1,  8000] loss: 94.348073\n",
      "total average loss : 95.012\n",
      "train acc : 0.0922\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.188\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.156\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.144\n",
      "step : 1200 / 3125 acc : 0.141\n",
      "step : 1400 / 3125 acc : 0.129\n",
      "step : 1600 / 3125 acc : 0.125\n",
      "step : 1800 / 3125 acc : 0.122\n",
      "step : 2000 / 3125 acc : 0.119\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.111\n",
      "step : 2800 / 3125 acc : 0.105\n",
      "step : 3000 / 3125 acc : 0.110\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1060 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 96.053296\n",
      "..........\n",
      "[1,   200] loss: 96.078409\n",
      "..........\n",
      "[1,   300] loss: 96.314707\n",
      "..........\n",
      "[1,   400] loss: 95.805573\n",
      "..........\n",
      "[1,   500] loss: 97.017889\n",
      "..........\n",
      "[1,   600] loss: 94.811979\n",
      "..........\n",
      "[1,   700] loss: 96.266307\n",
      "..........\n",
      "[1,   800] loss: 96.766469\n",
      "..........\n",
      "[1,   900] loss: 96.290817\n",
      "..........\n",
      "[1,  1000] loss: 97.106364\n",
      "..........\n",
      "[1,  1100] loss: 93.905963\n",
      "..........\n",
      "[1,  1200] loss: 96.325513\n",
      "..........\n",
      "[1,  1300] loss: 96.358044\n",
      "..........\n",
      "[1,  1400] loss: 96.226866\n",
      "..........\n",
      "[1,  1500] loss: 97.402234\n",
      "..........\n",
      "[1,  1600] loss: 96.299716\n",
      "..........\n",
      "[1,  1700] loss: 97.310431\n",
      "..........\n",
      "[1,  1800] loss: 95.750328\n",
      "..........\n",
      "[1,  1900] loss: 96.238617\n",
      "..........\n",
      "[1,  2000] loss: 96.485851\n",
      "..........\n",
      "[1,  2100] loss: 98.371848\n",
      "..........\n",
      "[1,  2200] loss: 96.943610\n",
      "..........\n",
      "[1,  2300] loss: 96.450731\n",
      "..........\n",
      "[1,  2400] loss: 96.345182\n",
      "..........\n",
      "[1,  2500] loss: 95.458612\n",
      "..........\n",
      "[1,  2600] loss: 96.139185\n",
      "..........\n",
      "[1,  2700] loss: 96.187879\n",
      "..........\n",
      "[1,  2800] loss: 98.755338\n",
      "..........\n",
      "[1,  2900] loss: 96.326579\n",
      "..........\n",
      "[1,  3000] loss: 96.808586\n",
      "..........\n",
      "[1,  3100] loss: 96.819162\n",
      "..........\n",
      "[1,  3200] loss: 96.929057\n",
      "..........\n",
      "[1,  3300] loss: 97.967963\n",
      "..........\n",
      "[1,  3400] loss: 96.369004\n",
      "..........\n",
      "[1,  3500] loss: 98.095740\n",
      "..........\n",
      "[1,  3600] loss: 97.615221\n",
      "..........\n",
      "[1,  3700] loss: 94.935195\n",
      "..........\n",
      "[1,  3800] loss: 97.744805\n",
      "..........\n",
      "[1,  3900] loss: 97.829833\n",
      "..........\n",
      "[1,  4000] loss: 96.185004\n",
      "..........\n",
      "[1,  4100] loss: 96.322803\n",
      "..........\n",
      "[1,  4200] loss: 98.415505\n",
      "..........\n",
      "[1,  4300] loss: 97.701597\n",
      "..........\n",
      "[1,  4400] loss: 97.847533\n",
      "..........\n",
      "[1,  4500] loss: 97.167932\n",
      "..........\n",
      "[1,  4600] loss: 96.134545\n",
      "..........\n",
      "[1,  4700] loss: 96.181392\n",
      "..........\n",
      "[1,  4800] loss: 97.133811\n",
      "..........\n",
      "[1,  4900] loss: 97.588751\n",
      "..........\n",
      "[1,  5000] loss: 98.614471\n",
      "..........\n",
      "[1,  5100] loss: 96.447564\n",
      "..........\n",
      "[1,  5200] loss: 96.689769\n",
      "..........\n",
      "[1,  5300] loss: 96.938946\n",
      "..........\n",
      "[1,  5400] loss: 97.281455\n",
      "..........\n",
      "[1,  5500] loss: 97.631311\n",
      "..........\n",
      "[1,  5600] loss: 98.632872\n",
      "..........\n",
      "[1,  5700] loss: 96.568202\n",
      "..........\n",
      "[1,  5800] loss: 97.086542\n",
      "..........\n",
      "[1,  5900] loss: 97.192273\n",
      "..........\n",
      "[1,  6000] loss: 98.152535\n",
      "..........\n",
      "[1,  6100] loss: 97.071440\n",
      "..........\n",
      "[1,  6200] loss: 96.229496\n",
      "..........\n",
      "[1,  6300] loss: 96.698373\n",
      "..........\n",
      "[1,  6400] loss: 97.111037\n",
      "..........\n",
      "[1,  6500] loss: 97.471573\n",
      "..........\n",
      "[1,  6600] loss: 95.263349\n",
      "..........\n",
      "[1,  6700] loss: 97.570634\n",
      "..........\n",
      "[1,  6800] loss: 98.151325\n",
      "..........\n",
      "[1,  6900] loss: 98.319505\n",
      "..........\n",
      "[1,  7000] loss: 97.628580\n",
      "..........\n",
      "[1,  7100] loss: 97.525879\n",
      "..........\n",
      "[1,  7200] loss: 96.068714\n",
      "..........\n",
      "[1,  7300] loss: 98.652454\n",
      "..........\n",
      "[1,  7400] loss: 98.185430\n",
      "..........\n",
      "[1,  7500] loss: 98.111447\n",
      "..........\n",
      "[1,  7600] loss: 99.279565\n",
      "..........\n",
      "[1,  7700] loss: 96.229246\n",
      "..........\n",
      "[1,  7800] loss: 97.874224\n",
      "..........\n",
      "[1,  7900] loss: 97.229904\n",
      "..........\n",
      "[1,  8000] loss: 96.687014\n",
      "total average loss : 96.952\n",
      "train acc : 0.0891\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.105\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.103\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.104\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.105\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1060 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 97.375929\n",
      "..........\n",
      "[1,   200] loss: 100.790340\n",
      "..........\n",
      "[1,   300] loss: 98.069522\n",
      "..........\n",
      "[1,   400] loss: 99.437958\n",
      "..........\n",
      "[1,   500] loss: 99.880420\n",
      "..........\n",
      "[1,   600] loss: 98.497428\n",
      "..........\n",
      "[1,   700] loss: 100.723315\n",
      "..........\n",
      "[1,   800] loss: 98.105936\n",
      "..........\n",
      "[1,   900] loss: 99.131108\n",
      "..........\n",
      "[1,  1000] loss: 98.966983\n",
      "..........\n",
      "[1,  1100] loss: 98.350567\n",
      "..........\n",
      "[1,  1200] loss: 97.691067\n",
      "..........\n",
      "[1,  1300] loss: 99.516944\n",
      "..........\n",
      "[1,  1400] loss: 100.190133\n",
      "..........\n",
      "[1,  1500] loss: 97.740426\n",
      "..........\n",
      "[1,  1600] loss: 99.561932\n",
      "..........\n",
      "[1,  1700] loss: 98.325631\n",
      "..........\n",
      "[1,  1800] loss: 99.457301\n",
      "..........\n",
      "[1,  1900] loss: 99.108646\n",
      "..........\n",
      "[1,  2000] loss: 97.031966\n",
      "..........\n",
      "[1,  2100] loss: 100.118348\n",
      "..........\n",
      "[1,  2200] loss: 99.365101\n",
      "..........\n",
      "[1,  2300] loss: 97.644812\n",
      "..........\n",
      "[1,  2400] loss: 97.592326\n",
      "..........\n",
      "[1,  2500] loss: 97.448724\n",
      "..........\n",
      "[1,  2600] loss: 97.368010\n",
      "..........\n",
      "[1,  2700] loss: 99.526539\n",
      "..........\n",
      "[1,  2800] loss: 98.917149\n",
      "..........\n",
      "[1,  2900] loss: 99.716622\n",
      "..........\n",
      "[1,  3000] loss: 98.699398\n",
      "..........\n",
      "[1,  3100] loss: 98.614819\n",
      "..........\n",
      "[1,  3200] loss: 99.594387\n",
      "..........\n",
      "[1,  3300] loss: 97.701940\n",
      "..........\n",
      "[1,  3400] loss: 100.499199\n",
      "..........\n",
      "[1,  3500] loss: 98.689004\n",
      "..........\n",
      "[1,  3600] loss: 97.575466\n",
      "..........\n",
      "[1,  3700] loss: 99.748716\n",
      "..........\n",
      "[1,  3800] loss: 98.130384\n",
      "..........\n",
      "[1,  3900] loss: 97.828716\n",
      "..........\n",
      "[1,  4000] loss: 98.390985\n",
      "..........\n",
      "[1,  4100] loss: 99.799424\n",
      "..........\n",
      "[1,  4200] loss: 97.970070\n",
      "..........\n",
      "[1,  4300] loss: 98.223438\n",
      "..........\n",
      "[1,  4400] loss: 99.984250\n",
      "..........\n",
      "[1,  4500] loss: 98.373153\n",
      "..........\n",
      "[1,  4600] loss: 98.663549\n",
      "..........\n",
      "[1,  4700] loss: 98.582478\n",
      "..........\n",
      "[1,  4800] loss: 99.623573\n",
      "..........\n",
      "[1,  4900] loss: 97.590050\n",
      "..........\n",
      "[1,  5000] loss: 98.759549\n",
      "..........\n",
      "[1,  5100] loss: 98.688926\n",
      "..........\n",
      "[1,  5200] loss: 97.623355\n",
      "..........\n",
      "[1,  5300] loss: 100.292121\n",
      "..........\n",
      "[1,  5400] loss: 100.990302\n",
      "..........\n",
      "[1,  5500] loss: 99.254354\n",
      "..........\n",
      "[1,  5600] loss: 97.671067\n",
      "..........\n",
      "[1,  5700] loss: 97.763835\n",
      "..........\n",
      "[1,  5800] loss: 97.744216\n",
      "..........\n",
      "[1,  5900] loss: 99.910564\n",
      "..........\n",
      "[1,  6000] loss: 98.073585\n",
      "..........\n",
      "[1,  6100] loss: 99.322324\n",
      "..........\n",
      "[1,  6200] loss: 98.903614\n",
      "..........\n",
      "[1,  6300] loss: 99.484519\n",
      "..........\n",
      "[1,  6400] loss: 99.536666\n",
      "..........\n",
      "[1,  6500] loss: 97.785710\n",
      "..........\n",
      "[1,  6600] loss: 98.365663\n",
      "..........\n",
      "[1,  6700] loss: 98.965747\n",
      "..........\n",
      "[1,  6800] loss: 99.589812\n",
      "..........\n",
      "[1,  6900] loss: 99.073259\n",
      "..........\n",
      "[1,  7000] loss: 99.034453\n",
      "..........\n",
      "[1,  7100] loss: 100.028523\n",
      "..........\n",
      "[1,  7200] loss: 98.569080\n",
      "..........\n",
      "[1,  7300] loss: 97.603967\n",
      "..........\n",
      "[1,  7400] loss: 99.006485\n",
      "..........\n",
      "[1,  7500] loss: 99.084498\n",
      "..........\n",
      "[1,  7600] loss: 99.897038\n",
      "..........\n",
      "[1,  7700] loss: 99.113479\n",
      "..........\n",
      "[1,  7800] loss: 99.115405\n",
      "..........\n",
      "[1,  7900] loss: 97.763315\n",
      "..........\n",
      "[1,  8000] loss: 98.851701\n",
      "total average loss : 98.823\n",
      "train acc : 0.0906\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.062\n",
      "step : 600 / 3125 acc : 0.052\n",
      "step : 800 / 3125 acc : 0.070\n",
      "step : 1000 / 3125 acc : 0.081\n",
      "step : 1200 / 3125 acc : 0.078\n",
      "step : 1400 / 3125 acc : 0.089\n",
      "step : 1600 / 3125 acc : 0.094\n",
      "step : 1800 / 3125 acc : 0.094\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.102\n",
      "step : 2400 / 3125 acc : 0.107\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1080 %\n",
      "======eval  end ======\n",
      "======= epoch 10 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 100.573853\n",
      "..........\n",
      "[1,   200] loss: 100.663134\n",
      "..........\n",
      "[1,   300] loss: 99.648860\n",
      "..........\n",
      "[1,   400] loss: 100.312384\n",
      "..........\n",
      "[1,   500] loss: 98.318745\n",
      "..........\n",
      "[1,   600] loss: 101.208179\n",
      "..........\n",
      "[1,   700] loss: 99.739482\n",
      "..........\n",
      "[1,   800] loss: 100.261197\n",
      "..........\n",
      "[1,   900] loss: 99.521137\n",
      "..........\n",
      "[1,  1000] loss: 99.328239\n",
      "..........\n",
      "[1,  1100] loss: 99.366712\n",
      "..........\n",
      "[1,  1200] loss: 100.575214\n",
      "..........\n",
      "[1,  1300] loss: 98.405383\n",
      "..........\n",
      "[1,  1400] loss: 101.170592\n",
      "..........\n",
      "[1,  1500] loss: 101.725815\n",
      "..........\n",
      "[1,  1600] loss: 99.972336\n",
      "..........\n",
      "[1,  1700] loss: 100.975630\n",
      "..........\n",
      "[1,  1800] loss: 99.592400\n",
      "..........\n",
      "[1,  1900] loss: 101.267294\n",
      "..........\n",
      "[1,  2000] loss: 100.062055\n",
      "..........\n",
      "[1,  2100] loss: 100.553126\n",
      "..........\n",
      "[1,  2200] loss: 99.764420\n",
      "..........\n",
      "[1,  2300] loss: 100.631973\n",
      "..........\n",
      "[1,  2400] loss: 100.696899\n",
      "..........\n",
      "[1,  2500] loss: 99.289167\n",
      "..........\n",
      "[1,  2600] loss: 99.112085\n",
      "..........\n",
      "[1,  2700] loss: 102.865951\n",
      "..........\n",
      "[1,  2800] loss: 100.984215\n",
      "..........\n",
      "[1,  2900] loss: 100.366175\n",
      "..........\n",
      "[1,  3000] loss: 99.514362\n",
      "..........\n",
      "[1,  3100] loss: 101.958235\n",
      "..........\n",
      "[1,  3200] loss: 97.246812\n",
      "..........\n",
      "[1,  3300] loss: 99.663280\n",
      "..........\n",
      "[1,  3400] loss: 99.996775\n",
      "..........\n",
      "[1,  3500] loss: 99.939984\n",
      "..........\n",
      "[1,  3600] loss: 101.171123\n",
      "..........\n",
      "[1,  3700] loss: 99.457319\n",
      "..........\n",
      "[1,  3800] loss: 101.491164\n",
      "..........\n",
      "[1,  3900] loss: 99.721487\n",
      "..........\n",
      "[1,  4000] loss: 100.014693\n",
      "..........\n",
      "[1,  4100] loss: 98.880909\n",
      "..........\n",
      "[1,  4200] loss: 99.936441\n",
      "..........\n",
      "[1,  4300] loss: 101.416547\n",
      "..........\n",
      "[1,  4400] loss: 100.414524\n",
      "..........\n",
      "[1,  4500] loss: 102.757071\n",
      "..........\n",
      "[1,  4600] loss: 100.189799\n",
      "..........\n",
      "[1,  4700] loss: 100.475083\n",
      "..........\n",
      "[1,  4800] loss: 101.175551\n",
      "..........\n",
      "[1,  4900] loss: 101.262505\n",
      "..........\n",
      "[1,  5000] loss: 99.470015\n",
      "..........\n",
      "[1,  5100] loss: 100.023171\n",
      "..........\n",
      "[1,  5200] loss: 100.793527\n",
      "..........\n",
      "[1,  5300] loss: 101.394784\n",
      "..........\n",
      "[1,  5400] loss: 101.282424\n",
      "..........\n",
      "[1,  5500] loss: 101.254525\n",
      "..........\n",
      "[1,  5600] loss: 101.258459\n",
      "..........\n",
      "[1,  5700] loss: 100.810380\n",
      "..........\n",
      "[1,  5800] loss: 101.318686\n",
      "..........\n",
      "[1,  5900] loss: 101.246131\n",
      "..........\n",
      "[1,  6000] loss: 101.911295\n",
      "..........\n",
      "[1,  6100] loss: 101.745690\n",
      "..........\n",
      "[1,  6200] loss: 100.692372\n",
      "..........\n",
      "[1,  6300] loss: 100.907972\n",
      "..........\n",
      "[1,  6400] loss: 99.752840\n",
      "..........\n",
      "[1,  6500] loss: 100.182310\n",
      "..........\n",
      "[1,  6600] loss: 100.783838\n",
      "..........\n",
      "[1,  6700] loss: 101.787510\n",
      "..........\n",
      "[1,  6800] loss: 102.301209\n",
      "..........\n",
      "[1,  6900] loss: 100.194062\n",
      "..........\n",
      "[1,  7000] loss: 102.681987\n",
      "..........\n",
      "[1,  7100] loss: 102.594684\n",
      "..........\n",
      "[1,  7200] loss: 101.894886\n",
      "..........\n",
      "[1,  7300] loss: 101.936498\n",
      "..........\n",
      "[1,  7400] loss: 101.243078\n",
      "..........\n",
      "[1,  7500] loss: 100.734949\n",
      "..........\n",
      "[1,  7600] loss: 102.391166\n",
      "..........\n",
      "[1,  7700] loss: 100.885546\n",
      "..........\n",
      "[1,  7800] loss: 101.300827\n",
      "..........\n",
      "[1,  7900] loss: 102.138557\n",
      "..........\n",
      "[1,  8000] loss: 102.240760\n",
      "total average loss : 100.660\n",
      "train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.188\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.125\n",
      "step : 1000 / 3125 acc : 0.119\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.103\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.104\n",
      "step : 2000 / 3125 acc : 0.100\n",
      "step : 2200 / 3125 acc : 0.099\n",
      "step : 2400 / 3125 acc : 0.096\n",
      "step : 2600 / 3125 acc : 0.099\n",
      "step : 2800 / 3125 acc : 0.100\n",
      "step : 3000 / 3125 acc : 0.106\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1060 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 102.659612\n",
      "..........\n",
      "[1,   200] loss: 102.940979\n",
      "..........\n",
      "[1,   300] loss: 103.130032\n",
      "..........\n",
      "[1,   400] loss: 103.041019\n",
      "..........\n",
      "[1,   500] loss: 101.594310\n",
      "..........\n",
      "[1,   600] loss: 100.442850\n",
      "..........\n",
      "[1,   700] loss: 102.135185\n",
      "..........\n",
      "[1,   800] loss: 103.200681\n",
      "..........\n",
      "[1,   900] loss: 101.507748\n",
      "..........\n",
      "[1,  1000] loss: 102.666181\n",
      "..........\n",
      "[1,  1100] loss: 102.977422\n",
      "..........\n",
      "[1,  1200] loss: 102.112254\n",
      "..........\n",
      "[1,  1300] loss: 102.751693\n",
      "..........\n",
      "[1,  1400] loss: 102.695640\n",
      "..........\n",
      "[1,  1500] loss: 101.585558\n",
      "..........\n",
      "[1,  1600] loss: 102.504632\n",
      "..........\n",
      "[1,  1700] loss: 103.297155\n",
      "..........\n",
      "[1,  1800] loss: 102.421101\n",
      "..........\n",
      "[1,  1900] loss: 102.582072\n",
      "..........\n",
      "[1,  2000] loss: 101.188818\n",
      "..........\n",
      "[1,  2100] loss: 101.456937\n",
      "..........\n",
      "[1,  2200] loss: 102.914829\n",
      "..........\n",
      "[1,  2300] loss: 101.106519\n",
      "..........\n",
      "[1,  2400] loss: 100.304036\n",
      "..........\n",
      "[1,  2500] loss: 101.673040\n",
      "..........\n",
      "[1,  2600] loss: 100.774671\n",
      "..........\n",
      "[1,  2700] loss: 102.033101\n",
      "..........\n",
      "[1,  2800] loss: 101.997784\n",
      "..........\n",
      "[1,  2900] loss: 101.959194\n",
      "..........\n",
      "[1,  3000] loss: 101.796165\n",
      "..........\n",
      "[1,  3100] loss: 102.642929\n",
      "..........\n",
      "[1,  3200] loss: 102.094422\n",
      "..........\n",
      "[1,  3300] loss: 101.487017\n",
      "..........\n",
      "[1,  3400] loss: 101.620728\n",
      "..........\n",
      "[1,  3500] loss: 102.420392\n",
      "..........\n",
      "[1,  3600] loss: 101.404483\n",
      "..........\n",
      "[1,  3700] loss: 103.903505\n",
      "..........\n",
      "[1,  3800] loss: 102.466846\n",
      "..........\n",
      "[1,  3900] loss: 102.715049\n",
      "..........\n",
      "[1,  4000] loss: 101.044508\n",
      "..........\n",
      "[1,  4100] loss: 102.531862\n",
      "..........\n",
      "[1,  4200] loss: 101.835714\n",
      "..........\n",
      "[1,  4300] loss: 102.417302\n",
      "..........\n",
      "[1,  4400] loss: 102.416173\n",
      "..........\n",
      "[1,  4500] loss: 102.453572\n",
      "..........\n",
      "[1,  4600] loss: 102.632005\n",
      "..........\n",
      "[1,  4700] loss: 102.797047\n",
      "..........\n",
      "[1,  4800] loss: 101.913622\n",
      "..........\n",
      "[1,  4900] loss: 102.818133\n",
      "..........\n",
      "[1,  5000] loss: 103.759278\n",
      "..........\n",
      "[1,  5100] loss: 103.148368\n",
      "..........\n",
      "[1,  5200] loss: 102.966722\n",
      "..........\n",
      "[1,  5300] loss: 103.502176\n",
      "..........\n",
      "[1,  5400] loss: 104.105452\n",
      "..........\n",
      "[1,  5500] loss: 102.645993\n",
      "..........\n",
      "[1,  5600] loss: 100.628500\n",
      "..........\n",
      "[1,  5700] loss: 103.305051\n",
      "..........\n",
      "[1,  5800] loss: 103.295910\n",
      "..........\n",
      "[1,  5900] loss: 104.138449\n",
      "..........\n",
      "[1,  6000] loss: 102.776793\n",
      "..........\n",
      "[1,  6100] loss: 102.751129\n",
      "..........\n",
      "[1,  6200] loss: 101.647287\n",
      "..........\n",
      "[1,  6300] loss: 101.803777\n",
      "..........\n",
      "[1,  6400] loss: 102.604727\n",
      "..........\n",
      "[1,  6500] loss: 102.224917\n",
      "..........\n",
      "[1,  6600] loss: 103.014309\n",
      "..........\n",
      "[1,  6700] loss: 103.289571\n",
      "..........\n",
      "[1,  6800] loss: 102.233085\n",
      "..........\n",
      "[1,  6900] loss: 103.498487\n",
      "..........\n",
      "[1,  7000] loss: 102.481661\n",
      "..........\n",
      "[1,  7100] loss: 102.205491\n",
      "..........\n",
      "[1,  7200] loss: 102.507009\n",
      "..........\n",
      "[1,  7300] loss: 102.548505\n",
      "..........\n",
      "[1,  7400] loss: 101.464226\n",
      "..........\n",
      "[1,  7500] loss: 103.611547\n",
      "..........\n",
      "[1,  7600] loss: 103.544606\n",
      "..........\n",
      "[1,  7700] loss: 101.879652\n",
      "..........\n",
      "[1,  7800] loss: 101.597356\n",
      "..........\n",
      "[1,  7900] loss: 102.022508\n",
      "..........\n",
      "[1,  8000] loss: 102.737976\n",
      "total average loss : 102.388\n",
      "train acc : 0.0938\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.125\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.120\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.094\n",
      "step : 2000 / 3125 acc : 0.094\n",
      "step : 2200 / 3125 acc : 0.094\n",
      "step : 2400 / 3125 acc : 0.096\n",
      "step : 2600 / 3125 acc : 0.096\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.102\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1080 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 103.231624\n",
      "..........\n",
      "[1,   200] loss: 104.689222\n",
      "..........\n",
      "[1,   300] loss: 103.592638\n",
      "..........\n",
      "[1,   400] loss: 103.555524\n",
      "..........\n",
      "[1,   500] loss: 103.408616\n",
      "..........\n",
      "[1,   600] loss: 103.830732\n",
      "..........\n",
      "[1,   700] loss: 105.278397\n",
      "..........\n",
      "[1,   800] loss: 104.001068\n",
      "..........\n",
      "[1,   900] loss: 103.495663\n",
      "..........\n",
      "[1,  1000] loss: 104.314689\n",
      "..........\n",
      "[1,  1100] loss: 104.689653\n",
      "..........\n",
      "[1,  1200] loss: 102.922257\n",
      "..........\n",
      "[1,  1300] loss: 103.236053\n",
      "..........\n",
      "[1,  1400] loss: 101.319272\n",
      "..........\n",
      "[1,  1500] loss: 104.571113\n",
      "..........\n",
      "[1,  1600] loss: 102.678731\n",
      "..........\n",
      "[1,  1700] loss: 101.514795\n",
      "..........\n",
      "[1,  1800] loss: 102.947551\n",
      "..........\n",
      "[1,  1900] loss: 105.658047\n",
      "..........\n",
      "[1,  2000] loss: 103.255219\n",
      "..........\n",
      "[1,  2100] loss: 104.130121\n",
      "..........\n",
      "[1,  2200] loss: 104.254201\n",
      "..........\n",
      "[1,  2300] loss: 104.793455\n",
      "..........\n",
      "[1,  2400] loss: 103.903354\n",
      "..........\n",
      "[1,  2500] loss: 102.775975\n",
      "..........\n",
      "[1,  2600] loss: 103.257351\n",
      "..........\n",
      "[1,  2700] loss: 102.030628\n",
      "..........\n",
      "[1,  2800] loss: 105.929929\n",
      "..........\n",
      "[1,  2900] loss: 104.275255\n",
      "..........\n",
      "[1,  3000] loss: 103.386224\n",
      "..........\n",
      "[1,  3100] loss: 103.155489\n",
      "..........\n",
      "[1,  3200] loss: 104.393067\n",
      "..........\n",
      "[1,  3300] loss: 104.182298\n",
      "..........\n",
      "[1,  3400] loss: 103.139377\n",
      "..........\n",
      "[1,  3500] loss: 104.325630\n",
      "..........\n",
      "[1,  3600] loss: 104.907440\n",
      "..........\n",
      "[1,  3700] loss: 105.259948\n",
      "..........\n",
      "[1,  3800] loss: 104.308763\n",
      "..........\n",
      "[1,  3900] loss: 102.050845\n",
      "..........\n",
      "[1,  4000] loss: 104.014394\n",
      "..........\n",
      "[1,  4100] loss: 104.622478\n",
      "..........\n",
      "[1,  4200] loss: 104.058547\n",
      "..........\n",
      "[1,  4300] loss: 103.659070\n",
      "..........\n",
      "[1,  4400] loss: 104.424462\n",
      "..........\n",
      "[1,  4500] loss: 105.870585\n",
      "..........\n",
      "[1,  4600] loss: 105.896467\n",
      "..........\n",
      "[1,  4700] loss: 103.402857\n",
      "..........\n",
      "[1,  4800] loss: 102.493464\n",
      "..........\n",
      "[1,  4900] loss: 103.003021\n",
      "..........\n",
      "[1,  5000] loss: 104.582916\n",
      "..........\n",
      "[1,  5100] loss: 106.210040\n",
      "..........\n",
      "[1,  5200] loss: 102.882785\n",
      "..........\n",
      "[1,  5300] loss: 105.666359\n",
      "..........\n",
      "[1,  5400] loss: 103.961346\n",
      "..........\n",
      "[1,  5500] loss: 105.239685\n",
      "..........\n",
      "[1,  5600] loss: 102.658069\n",
      "..........\n",
      "[1,  5700] loss: 104.992540\n",
      "..........\n",
      "[1,  5800] loss: 105.133222\n",
      "..........\n",
      "[1,  5900] loss: 104.427524\n",
      "..........\n",
      "[1,  6000] loss: 104.175496\n",
      "..........\n",
      "[1,  6100] loss: 103.253249\n",
      "..........\n",
      "[1,  6200] loss: 104.667919\n",
      "..........\n",
      "[1,  6300] loss: 103.532169\n",
      "..........\n",
      "[1,  6400] loss: 104.912710\n",
      "..........\n",
      "[1,  6500] loss: 105.627849\n",
      "..........\n",
      "[1,  6600] loss: 105.700897\n",
      "..........\n",
      "[1,  6700] loss: 103.481826\n",
      "..........\n",
      "[1,  6800] loss: 103.708818\n",
      "..........\n",
      "[1,  6900] loss: 102.394704\n",
      "..........\n",
      "[1,  7000] loss: 103.519157\n",
      "..........\n",
      "[1,  7100] loss: 104.624537\n",
      "..........\n",
      "[1,  7200] loss: 102.875907\n",
      "..........\n",
      "[1,  7300] loss: 104.554436\n",
      "..........\n",
      "[1,  7400] loss: 105.995996\n",
      "..........\n",
      "[1,  7500] loss: 104.370023\n",
      "..........\n",
      "[1,  7600] loss: 104.804294\n",
      "..........\n",
      "[1,  7700] loss: 105.008418\n",
      "..........\n",
      "[1,  7800] loss: 104.294184\n",
      "..........\n",
      "[1,  7900] loss: 104.453831\n",
      "..........\n",
      "[1,  8000] loss: 104.200678\n",
      "total average loss : 104.050\n",
      "train acc : 0.0938\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.125\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.125\n",
      "step : 1400 / 3125 acc : 0.125\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.128\n",
      "step : 2000 / 3125 acc : 0.125\n",
      "step : 2200 / 3125 acc : 0.122\n",
      "step : 2400 / 3125 acc : 0.120\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.112\n",
      "step : 3000 / 3125 acc : 0.110\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 103.312357\n",
      "..........\n",
      "[1,   200] loss: 105.705104\n",
      "..........\n",
      "[1,   300] loss: 104.075063\n",
      "..........\n",
      "[1,   400] loss: 103.729312\n",
      "..........\n",
      "[1,   500] loss: 104.693685\n",
      "..........\n",
      "[1,   600] loss: 105.295759\n",
      "..........\n",
      "[1,   700] loss: 106.565639\n",
      "..........\n",
      "[1,   800] loss: 104.399638\n",
      "..........\n",
      "[1,   900] loss: 103.591253\n",
      "..........\n",
      "[1,  1000] loss: 104.228297\n",
      "..........\n",
      "[1,  1100] loss: 104.392206\n",
      "..........\n",
      "[1,  1200] loss: 105.934731\n",
      "..........\n",
      "[1,  1300] loss: 105.428403\n",
      "..........\n",
      "[1,  1400] loss: 104.053437\n",
      "..........\n",
      "[1,  1500] loss: 104.574883\n",
      "..........\n",
      "[1,  1600] loss: 104.128823\n",
      "..........\n",
      "[1,  1700] loss: 106.311714\n",
      "..........\n",
      "[1,  1800] loss: 104.864391\n",
      "..........\n",
      "[1,  1900] loss: 103.981450\n",
      "..........\n",
      "[1,  2000] loss: 103.420582\n",
      "..........\n",
      "[1,  2100] loss: 105.669714\n",
      "..........\n",
      "[1,  2200] loss: 104.964076\n",
      "..........\n",
      "[1,  2300] loss: 103.669519\n",
      "..........\n",
      "[1,  2400] loss: 104.670478\n",
      "..........\n",
      "[1,  2500] loss: 106.352823\n",
      "..........\n",
      "[1,  2600] loss: 106.168115\n",
      "..........\n",
      "[1,  2700] loss: 107.114108\n",
      "..........\n",
      "[1,  2800] loss: 105.027604\n",
      "..........\n",
      "[1,  2900] loss: 105.940852\n",
      "..........\n",
      "[1,  3000] loss: 105.440785\n",
      "..........\n",
      "[1,  3100] loss: 106.173994\n",
      "..........\n",
      "[1,  3200] loss: 106.132010\n",
      "..........\n",
      "[1,  3300] loss: 107.021084\n",
      "..........\n",
      "[1,  3400] loss: 104.582515\n",
      "..........\n",
      "[1,  3500] loss: 105.910966\n",
      "..........\n",
      "[1,  3600] loss: 104.833706\n",
      "..........\n",
      "[1,  3700] loss: 104.565200\n",
      "..........\n",
      "[1,  3800] loss: 105.316302\n",
      "..........\n",
      "[1,  3900] loss: 106.467295\n",
      "..........\n",
      "[1,  4000] loss: 105.824143\n",
      "..........\n",
      "[1,  4100] loss: 104.501553\n",
      "..........\n",
      "[1,  4200] loss: 105.822239\n",
      "..........\n",
      "[1,  4300] loss: 107.484703\n",
      "..........\n",
      "[1,  4400] loss: 107.881791\n",
      "..........\n",
      "[1,  4500] loss: 104.043054\n",
      "..........\n",
      "[1,  4600] loss: 105.516108\n",
      "..........\n",
      "[1,  4700] loss: 108.415460\n",
      "..........\n",
      "[1,  4800] loss: 106.345081\n",
      "..........\n",
      "[1,  4900] loss: 104.808852\n",
      "..........\n",
      "[1,  5000] loss: 106.209851\n",
      "..........\n",
      "[1,  5100] loss: 105.685470\n",
      "..........\n",
      "[1,  5200] loss: 107.669051\n",
      "..........\n",
      "[1,  5300] loss: 105.354097\n",
      "..........\n",
      "[1,  5400] loss: 106.783919\n",
      "..........\n",
      "[1,  5500] loss: 107.173471\n",
      "..........\n",
      "[1,  5600] loss: 105.418038\n",
      "..........\n",
      "[1,  5700] loss: 105.167419\n",
      "..........\n",
      "[1,  5800] loss: 104.024314\n",
      "..........\n",
      "[1,  5900] loss: 105.266208\n",
      "..........\n",
      "[1,  6000] loss: 106.637211\n",
      "..........\n",
      "[1,  6100] loss: 104.257931\n",
      "..........\n",
      "[1,  6200] loss: 105.643587\n",
      "..........\n",
      "[1,  6300] loss: 107.085064\n",
      "..........\n",
      "[1,  6400] loss: 107.196612\n",
      "..........\n",
      "[1,  6500] loss: 106.363373\n",
      "..........\n",
      "[1,  6600] loss: 106.079204\n",
      "..........\n",
      "[1,  6700] loss: 106.299639\n",
      "..........\n",
      "[1,  6800] loss: 105.652938\n",
      "..........\n",
      "[1,  6900] loss: 107.529368\n",
      "..........\n",
      "[1,  7000] loss: 107.611279\n",
      "..........\n",
      "[1,  7100] loss: 106.465982\n",
      "..........\n",
      "[1,  7200] loss: 105.770326\n",
      "..........\n",
      "[1,  7300] loss: 107.732775\n",
      "..........\n",
      "[1,  7400] loss: 105.731460\n",
      "..........\n",
      "[1,  7500] loss: 107.125822\n",
      "..........\n",
      "[1,  7600] loss: 104.810417\n",
      "..........\n",
      "[1,  7700] loss: 106.356091\n",
      "..........\n",
      "[1,  7800] loss: 107.691909\n",
      "..........\n",
      "[1,  7900] loss: 107.905025\n",
      "..........\n",
      "[1,  8000] loss: 108.031400\n",
      "total average loss : 105.701\n",
      "train acc : 0.0938\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.109\n",
      "step : 1000 / 3125 acc : 0.119\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.108\n",
      "step : 2800 / 3125 acc : 0.114\n",
      "step : 3000 / 3125 acc : 0.117\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1140 %\n",
      "======eval  end ======\n",
      "======= epoch 11 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 105.857361\n",
      "..........\n",
      "[1,   200] loss: 107.981856\n",
      "..........\n",
      "[1,   300] loss: 106.732471\n",
      "..........\n",
      "[1,   400] loss: 107.270697\n",
      "..........\n",
      "[1,   500] loss: 107.037868\n",
      "..........\n",
      "[1,   600] loss: 107.058134\n",
      "..........\n",
      "[1,   700] loss: 106.866318\n",
      "..........\n",
      "[1,   800] loss: 105.322771\n",
      "..........\n",
      "[1,   900] loss: 105.942077\n",
      "..........\n",
      "[1,  1000] loss: 107.998590\n",
      "..........\n",
      "[1,  1100] loss: 106.612985\n",
      "..........\n",
      "[1,  1200] loss: 107.359569\n",
      "..........\n",
      "[1,  1300] loss: 108.653398\n",
      "..........\n",
      "[1,  1400] loss: 106.953985\n",
      "..........\n",
      "[1,  1500] loss: 105.364412\n",
      "..........\n",
      "[1,  1600] loss: 108.039457\n",
      "..........\n",
      "[1,  1700] loss: 106.579094\n",
      "..........\n",
      "[1,  1800] loss: 109.629528\n",
      "..........\n",
      "[1,  1900] loss: 107.252075\n",
      "..........\n",
      "[1,  2000] loss: 105.497582\n",
      "..........\n",
      "[1,  2100] loss: 107.966257\n",
      "..........\n",
      "[1,  2200] loss: 110.381125\n",
      "..........\n",
      "[1,  2300] loss: 107.500986\n",
      "..........\n",
      "[1,  2400] loss: 106.345883\n",
      "..........\n",
      "[1,  2500] loss: 108.180301\n",
      "..........\n",
      "[1,  2600] loss: 107.868320\n",
      "..........\n",
      "[1,  2700] loss: 107.336917\n",
      "..........\n",
      "[1,  2800] loss: 106.280297\n",
      "..........\n",
      "[1,  2900] loss: 109.306964\n",
      "..........\n",
      "[1,  3000] loss: 105.283491\n",
      "..........\n",
      "[1,  3100] loss: 105.912233\n",
      "..........\n",
      "[1,  3200] loss: 106.915057\n",
      "..........\n",
      "[1,  3300] loss: 108.255873\n",
      "..........\n",
      "[1,  3400] loss: 106.595545\n",
      "..........\n",
      "[1,  3500] loss: 108.826654\n",
      "..........\n",
      "[1,  3600] loss: 106.015892\n",
      "..........\n",
      "[1,  3700] loss: 106.633807\n",
      "..........\n",
      "[1,  3800] loss: 108.040361\n",
      "..........\n",
      "[1,  3900] loss: 105.594959\n",
      "..........\n",
      "[1,  4000] loss: 107.805130\n",
      "..........\n",
      "[1,  4100] loss: 107.067440\n",
      "..........\n",
      "[1,  4200] loss: 107.102352\n",
      "..........\n",
      "[1,  4300] loss: 107.660356\n",
      "..........\n",
      "[1,  4400] loss: 105.468337\n",
      "..........\n",
      "[1,  4500] loss: 107.585251\n",
      "..........\n",
      "[1,  4600] loss: 105.640020\n",
      "..........\n",
      "[1,  4700] loss: 109.522477\n",
      "..........\n",
      "[1,  4800] loss: 108.682187\n",
      "..........\n",
      "[1,  4900] loss: 108.011480\n",
      "..........\n",
      "[1,  5000] loss: 107.108375\n",
      "..........\n",
      "[1,  5100] loss: 108.032544\n",
      "..........\n",
      "[1,  5200] loss: 106.274540\n",
      "..........\n",
      "[1,  5300] loss: 110.255436\n",
      "..........\n",
      "[1,  5400] loss: 108.267008\n",
      "..........\n",
      "[1,  5500] loss: 108.232609\n",
      "..........\n",
      "[1,  5600] loss: 107.957295\n",
      "..........\n",
      "[1,  5700] loss: 107.415727\n",
      "..........\n",
      "[1,  5800] loss: 107.379818\n",
      "..........\n",
      "[1,  5900] loss: 105.224587\n",
      "..........\n",
      "[1,  6000] loss: 108.435219\n",
      "..........\n",
      "[1,  6100] loss: 107.438236\n",
      "..........\n",
      "[1,  6200] loss: 107.787575\n",
      "..........\n",
      "[1,  6300] loss: 108.116078\n",
      "..........\n",
      "[1,  6400] loss: 107.111941\n",
      "..........\n",
      "[1,  6500] loss: 108.015396\n",
      "..........\n",
      "[1,  6600] loss: 108.089899\n",
      "..........\n",
      "[1,  6700] loss: 108.546113\n",
      "..........\n",
      "[1,  6800] loss: 106.660761\n",
      "..........\n",
      "[1,  6900] loss: 108.963732\n",
      "..........\n",
      "[1,  7000] loss: 109.017912\n",
      "..........\n",
      "[1,  7100] loss: 108.351165\n",
      "..........\n",
      "[1,  7200] loss: 110.096660\n",
      "..........\n",
      "[1,  7300] loss: 107.664558\n",
      "..........\n",
      "[1,  7400] loss: 108.559742\n",
      "..........\n",
      "[1,  7500] loss: 108.080966\n",
      "..........\n",
      "[1,  7600] loss: 107.139257\n",
      "..........\n",
      "[1,  7700] loss: 106.415561\n",
      "..........\n",
      "[1,  7800] loss: 107.699021\n",
      "..........\n",
      "[1,  7900] loss: 105.712249\n",
      "..........\n",
      "[1,  8000] loss: 107.921059\n",
      "total average loss : 107.447\n",
      "train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.094\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.106\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.121\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.122\n",
      "step : 2000 / 3125 acc : 0.119\n",
      "step : 2200 / 3125 acc : 0.128\n",
      "step : 2400 / 3125 acc : 0.128\n",
      "step : 2600 / 3125 acc : 0.123\n",
      "step : 2800 / 3125 acc : 0.114\n",
      "step : 3000 / 3125 acc : 0.113\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1120 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 108.801876\n",
      "..........\n",
      "[1,   200] loss: 110.247644\n",
      "..........\n",
      "[1,   300] loss: 107.108384\n",
      "..........\n",
      "[1,   400] loss: 109.936803\n",
      "..........\n",
      "[1,   500] loss: 107.978255\n",
      "..........\n",
      "[1,   600] loss: 107.962471\n",
      "..........\n",
      "[1,   700] loss: 107.301857\n",
      "..........\n",
      "[1,   800] loss: 107.626879\n",
      "..........\n",
      "[1,   900] loss: 108.826610\n",
      "..........\n",
      "[1,  1000] loss: 108.465236\n",
      "..........\n",
      "[1,  1100] loss: 108.598177\n",
      "..........\n",
      "[1,  1200] loss: 107.790759\n",
      "..........\n",
      "[1,  1300] loss: 108.390707\n",
      "..........\n",
      "[1,  1400] loss: 109.106293\n",
      "..........\n",
      "[1,  1500] loss: 107.594395\n",
      "..........\n",
      "[1,  1600] loss: 106.638977\n",
      "..........\n",
      "[1,  1700] loss: 108.772206\n",
      "..........\n",
      "[1,  1800] loss: 108.808106\n",
      "..........\n",
      "[1,  1900] loss: 109.707163\n",
      "..........\n",
      "[1,  2000] loss: 108.159624\n",
      "..........\n",
      "[1,  2100] loss: 109.763757\n",
      "..........\n",
      "[1,  2200] loss: 108.341417\n",
      "..........\n",
      "[1,  2300] loss: 109.843072\n",
      "..........\n",
      "[1,  2400] loss: 109.992950\n",
      "..........\n",
      "[1,  2500] loss: 109.406640\n",
      "..........\n",
      "[1,  2600] loss: 108.495886\n",
      "..........\n",
      "[1,  2700] loss: 109.392754\n",
      "..........\n",
      "[1,  2800] loss: 108.138339\n",
      "..........\n",
      "[1,  2900] loss: 109.105513\n",
      "..........\n",
      "[1,  3000] loss: 109.023114\n",
      "..........\n",
      "[1,  3100] loss: 109.440344\n",
      "..........\n",
      "[1,  3200] loss: 108.880389\n",
      "..........\n",
      "[1,  3300] loss: 111.254726\n",
      "..........\n",
      "[1,  3400] loss: 108.936616\n",
      "..........\n",
      "[1,  3500] loss: 109.515195\n",
      "..........\n",
      "[1,  3600] loss: 109.509093\n",
      "..........\n",
      "[1,  3700] loss: 110.148162\n",
      "..........\n",
      "[1,  3800] loss: 109.189622\n",
      "..........\n",
      "[1,  3900] loss: 111.120079\n",
      "..........\n",
      "[1,  4000] loss: 107.748159\n",
      "..........\n",
      "[1,  4100] loss: 107.526343\n",
      "..........\n",
      "[1,  4200] loss: 107.648390\n",
      "..........\n",
      "[1,  4300] loss: 108.908143\n",
      "..........\n",
      "[1,  4400] loss: 110.435943\n",
      "..........\n",
      "[1,  4500] loss: 108.914447\n",
      "..........\n",
      "[1,  4600] loss: 109.555992\n",
      "..........\n",
      "[1,  4700] loss: 109.231592\n",
      "..........\n",
      "[1,  4800] loss: 107.536967\n",
      "..........\n",
      "[1,  4900] loss: 110.695253\n",
      "..........\n",
      "[1,  5000] loss: 109.378900\n",
      "..........\n",
      "[1,  5100] loss: 111.977047\n",
      "..........\n",
      "[1,  5200] loss: 108.666114\n",
      "..........\n",
      "[1,  5300] loss: 111.782861\n",
      "..........\n",
      "[1,  5400] loss: 110.872439\n",
      "..........\n",
      "[1,  5500] loss: 106.508460\n",
      "..........\n",
      "[1,  5600] loss: 108.475119\n",
      "..........\n",
      "[1,  5700] loss: 110.028291\n",
      "..........\n",
      "[1,  5800] loss: 109.432439\n",
      "..........\n",
      "[1,  5900] loss: 110.910596\n",
      "..........\n",
      "[1,  6000] loss: 110.790061\n",
      "..........\n",
      "[1,  6100] loss: 110.881584\n",
      "..........\n",
      "[1,  6200] loss: 109.433036\n",
      "..........\n",
      "[1,  6300] loss: 108.776027\n",
      "..........\n",
      "[1,  6400] loss: 108.676456\n",
      "..........\n",
      "[1,  6500] loss: 109.021779\n",
      "..........\n",
      "[1,  6600] loss: 110.621549\n",
      "..........\n",
      "[1,  6700] loss: 107.041849\n",
      "..........\n",
      "[1,  6800] loss: 109.798689\n",
      "..........\n",
      "[1,  6900] loss: 110.121871\n",
      "..........\n",
      "[1,  7000] loss: 111.402613\n",
      "..........\n",
      "[1,  7100] loss: 108.868449\n",
      "..........\n",
      "[1,  7200] loss: 109.963811\n",
      "..........\n",
      "[1,  7300] loss: 110.787712\n",
      "..........\n",
      "[1,  7400] loss: 109.240772\n",
      "..........\n",
      "[1,  7500] loss: 108.295383\n",
      "..........\n",
      "[1,  7600] loss: 108.860197\n",
      "..........\n",
      "[1,  7700] loss: 110.955605\n",
      "..........\n",
      "[1,  7800] loss: 108.659715\n",
      "..........\n",
      "[1,  7900] loss: 108.715294\n",
      "..........\n",
      "[1,  8000] loss: 111.337070\n",
      "total average loss : 109.198\n",
      "train acc : 0.0938\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.109\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.105\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.113\n",
      "step : 2200 / 3125 acc : 0.108\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.108\n",
      "step : 2800 / 3125 acc : 0.107\n",
      "step : 3000 / 3125 acc : 0.113\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1140 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 110.890402\n",
      "..........\n",
      "[1,   200] loss: 112.680387\n",
      "..........\n",
      "[1,   300] loss: 109.931546\n",
      "..........\n",
      "[1,   400] loss: 108.970156\n",
      "..........\n",
      "[1,   500] loss: 111.025568\n",
      "..........\n",
      "[1,   600] loss: 110.091484\n",
      "..........\n",
      "[1,   700] loss: 110.542560\n",
      "..........\n",
      "[1,   800] loss: 109.812647\n",
      "..........\n",
      "[1,   900] loss: 110.150597\n",
      "..........\n",
      "[1,  1000] loss: 111.922702\n",
      "..........\n",
      "[1,  1100] loss: 111.610628\n",
      "..........\n",
      "[1,  1200] loss: 111.353484\n",
      "..........\n",
      "[1,  1300] loss: 109.847374\n",
      "..........\n",
      "[1,  1400] loss: 111.072932\n",
      "..........\n",
      "[1,  1500] loss: 110.107711\n",
      "..........\n",
      "[1,  1600] loss: 110.296774\n",
      "..........\n",
      "[1,  1700] loss: 107.893070\n",
      "..........\n",
      "[1,  1800] loss: 110.782804\n",
      "..........\n",
      "[1,  1900] loss: 110.963613\n",
      "..........\n",
      "[1,  2000] loss: 111.734422\n",
      "..........\n",
      "[1,  2100] loss: 111.545827\n",
      "..........\n",
      "[1,  2200] loss: 110.697484\n",
      "..........\n",
      "[1,  2300] loss: 109.649181\n",
      "..........\n",
      "[1,  2400] loss: 110.390702\n",
      "..........\n",
      "[1,  2500] loss: 112.535682\n",
      "..........\n",
      "[1,  2600] loss: 110.877258\n",
      "..........\n",
      "[1,  2700] loss: 113.453104\n",
      "..........\n",
      "[1,  2800] loss: 112.370327\n",
      "..........\n",
      "[1,  2900] loss: 109.470868\n",
      "..........\n",
      "[1,  3000] loss: 110.400840\n",
      "..........\n",
      "[1,  3100] loss: 111.824517\n",
      "..........\n",
      "[1,  3200] loss: 110.850747\n",
      "..........\n",
      "[1,  3300] loss: 111.374832\n",
      "..........\n",
      "[1,  3400] loss: 110.450731\n",
      "..........\n",
      "[1,  3500] loss: 110.263244\n",
      "..........\n",
      "[1,  3600] loss: 112.923409\n",
      "..........\n",
      "[1,  3700] loss: 110.289345\n",
      "..........\n",
      "[1,  3800] loss: 111.216947\n",
      "..........\n",
      "[1,  3900] loss: 109.371934\n",
      "..........\n",
      "[1,  4000] loss: 109.800391\n",
      "..........\n",
      "[1,  4100] loss: 109.528161\n",
      "..........\n",
      "[1,  4200] loss: 111.773111\n",
      "..........\n",
      "[1,  4300] loss: 110.980973\n",
      "..........\n",
      "[1,  4400] loss: 111.141689\n",
      "..........\n",
      "[1,  4500] loss: 110.410331\n",
      "..........\n",
      "[1,  4600] loss: 110.998126\n",
      "..........\n",
      "[1,  4700] loss: 112.729760\n",
      "..........\n",
      "[1,  4800] loss: 110.707164\n",
      "..........\n",
      "[1,  4900] loss: 110.269807\n",
      "..........\n",
      "[1,  5000] loss: 111.341450\n",
      "..........\n",
      "[1,  5100] loss: 109.130761\n",
      "..........\n",
      "[1,  5200] loss: 111.426769\n",
      "..........\n",
      "[1,  5300] loss: 110.303135\n",
      "..........\n",
      "[1,  5400] loss: 110.617138\n",
      "..........\n",
      "[1,  5500] loss: 115.231668\n",
      "..........\n",
      "[1,  5600] loss: 111.663894\n",
      "..........\n",
      "[1,  5700] loss: 113.317179\n",
      "..........\n",
      "[1,  5800] loss: 112.212324\n",
      "..........\n",
      "[1,  5900] loss: 110.245078\n",
      "..........\n",
      "[1,  6000] loss: 112.292323\n",
      "..........\n",
      "[1,  6100] loss: 113.460067\n",
      "..........\n",
      "[1,  6200] loss: 112.210936\n",
      "..........\n",
      "[1,  6300] loss: 110.710798\n",
      "..........\n",
      "[1,  6400] loss: 111.082225\n",
      "..........\n",
      "[1,  6500] loss: 111.479508\n",
      "..........\n",
      "[1,  6600] loss: 110.839235\n",
      "..........\n",
      "[1,  6700] loss: 110.905451\n",
      "..........\n",
      "[1,  6800] loss: 111.842867\n",
      "..........\n",
      "[1,  6900] loss: 112.922393\n",
      "..........\n",
      "[1,  7000] loss: 110.398344\n",
      "..........\n",
      "[1,  7100] loss: 111.051139\n",
      "..........\n",
      "[1,  7200] loss: 110.492882\n",
      "..........\n",
      "[1,  7300] loss: 111.898001\n",
      "..........\n",
      "[1,  7400] loss: 112.471873\n",
      "..........\n",
      "[1,  7500] loss: 112.167234\n",
      "..........\n",
      "[1,  7600] loss: 111.726661\n",
      "..........\n",
      "[1,  7700] loss: 111.844937\n",
      "..........\n",
      "[1,  7800] loss: 112.360675\n",
      "..........\n",
      "[1,  7900] loss: 113.487909\n",
      "..........\n",
      "[1,  8000] loss: 110.578951\n",
      "total average loss : 111.146\n",
      "train acc : 0.0938\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.125\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.104\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.106\n",
      "step : 2200 / 3125 acc : 0.105\n",
      "step : 2400 / 3125 acc : 0.109\n",
      "step : 2600 / 3125 acc : 0.106\n",
      "step : 2800 / 3125 acc : 0.103\n",
      "step : 3000 / 3125 acc : 0.108\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1100 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 112.550433\n",
      "..........\n",
      "[1,   200] loss: 112.822592\n",
      "..........\n",
      "[1,   300] loss: 113.635792\n",
      "..........\n",
      "[1,   400] loss: 113.233937\n",
      "..........\n",
      "[1,   500] loss: 112.685185\n",
      "..........\n",
      "[1,   600] loss: 113.342411\n",
      "..........\n",
      "[1,   700] loss: 111.609147\n",
      "..........\n",
      "[1,   800] loss: 113.012608\n",
      "..........\n",
      "[1,   900] loss: 111.132106\n",
      "..........\n",
      "[1,  1000] loss: 113.808160\n",
      "..........\n",
      "[1,  1100] loss: 110.827390\n",
      "..........\n",
      "[1,  1200] loss: 110.584464\n",
      "..........\n",
      "[1,  1300] loss: 111.328301\n",
      "..........\n",
      "[1,  1400] loss: 113.165684\n",
      "..........\n",
      "[1,  1500] loss: 112.693104\n",
      "..........\n",
      "[1,  1600] loss: 112.013506\n",
      "..........\n",
      "[1,  1700] loss: 113.796753\n",
      "..........\n",
      "[1,  1800] loss: 113.678921\n",
      "..........\n",
      "[1,  1900] loss: 114.393475\n",
      "..........\n",
      "[1,  2000] loss: 112.343009\n",
      "..........\n",
      "[1,  2100] loss: 112.158405\n",
      "..........\n",
      "[1,  2200] loss: 111.963447\n",
      "..........\n",
      "[1,  2300] loss: 112.864813\n",
      "..........\n",
      "[1,  2400] loss: 111.091205\n",
      "..........\n",
      "[1,  2500] loss: 114.778383\n",
      "..........\n",
      "[1,  2600] loss: 112.606654\n",
      "..........\n",
      "[1,  2700] loss: 113.661097\n",
      "..........\n",
      "[1,  2800] loss: 113.415698\n",
      "..........\n",
      "[1,  2900] loss: 112.966382\n",
      "..........\n",
      "[1,  3000] loss: 113.573232\n",
      "..........\n",
      "[1,  3100] loss: 113.695819\n",
      "..........\n",
      "[1,  3200] loss: 111.703364\n",
      "..........\n",
      "[1,  3300] loss: 115.202044\n",
      "..........\n",
      "[1,  3400] loss: 113.902679\n",
      "..........\n",
      "[1,  3500] loss: 114.668505\n",
      "..........\n",
      "[1,  3600] loss: 112.909238\n",
      "..........\n",
      "[1,  3700] loss: 112.525139\n",
      "..........\n",
      "[1,  3800] loss: 112.503017\n",
      "..........\n",
      "[1,  3900] loss: 113.241038\n",
      "..........\n",
      "[1,  4000] loss: 112.988316\n",
      "..........\n",
      "[1,  4100] loss: 113.650455\n",
      "..........\n",
      "[1,  4200] loss: 112.914969\n",
      "..........\n",
      "[1,  4300] loss: 114.762471\n",
      "..........\n",
      "[1,  4400] loss: 113.081298\n",
      "..........\n",
      "[1,  4500] loss: 113.409666\n",
      "..........\n",
      "[1,  4600] loss: 113.537306\n",
      "..........\n",
      "[1,  4700] loss: 113.094649\n",
      "..........\n",
      "[1,  4800] loss: 112.014439\n",
      "..........\n",
      "[1,  4900] loss: 113.262985\n",
      "..........\n",
      "[1,  5000] loss: 115.164462\n",
      "..........\n",
      "[1,  5100] loss: 113.875714\n",
      "..........\n",
      "[1,  5200] loss: 112.248665\n",
      "..........\n",
      "[1,  5300] loss: 112.907799\n",
      "..........\n",
      "[1,  5400] loss: 112.424755\n",
      "..........\n",
      "[1,  5500] loss: 114.286139\n",
      "..........\n",
      "[1,  5600] loss: 112.805953\n",
      "..........\n",
      "[1,  5700] loss: 113.537136\n",
      "..........\n",
      "[1,  5800] loss: 113.942084\n",
      "..........\n",
      "[1,  5900] loss: 113.322125\n",
      "..........\n",
      "[1,  6000] loss: 113.388348\n",
      "..........\n",
      "[1,  6100] loss: 112.390151\n",
      "..........\n",
      "[1,  6200] loss: 112.801274\n",
      "..........\n",
      "[1,  6300] loss: 113.079055\n",
      "..........\n",
      "[1,  6400] loss: 114.152893\n",
      "..........\n",
      "[1,  6500] loss: 114.977366\n",
      "..........\n",
      "[1,  6600] loss: 113.309065\n",
      "..........\n",
      "[1,  6700] loss: 113.945600\n",
      "..........\n",
      "[1,  6800] loss: 114.600729\n",
      "..........\n",
      "[1,  6900] loss: 113.561522\n",
      "..........\n",
      "[1,  7000] loss: 112.409154\n",
      "..........\n",
      "[1,  7100] loss: 112.778310\n",
      "..........\n",
      "[1,  7200] loss: 112.577832\n",
      "..........\n",
      "[1,  7300] loss: 113.668886\n",
      "..........\n",
      "[1,  7400] loss: 114.919874\n",
      "..........\n",
      "[1,  7500] loss: 113.984558\n",
      "..........\n",
      "[1,  7600] loss: 116.508728\n",
      "..........\n",
      "[1,  7700] loss: 113.769743\n",
      "..........\n",
      "[1,  7800] loss: 113.745918\n",
      "..........\n",
      "[1,  7900] loss: 111.902740\n",
      "..........\n",
      "[1,  8000] loss: 114.523650\n",
      "total average loss : 113.204\n",
      "train acc : 0.0922\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.078\n",
      "step : 1000 / 3125 acc : 0.094\n",
      "step : 1200 / 3125 acc : 0.099\n",
      "step : 1400 / 3125 acc : 0.121\n",
      "step : 1600 / 3125 acc : 0.113\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.122\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.117\n",
      "step : 2600 / 3125 acc : 0.108\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.115\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1140 %\n",
      "======eval  end ======\n",
      "======= epoch 12 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 113.661682\n",
      "..........\n",
      "[1,   200] loss: 115.316520\n",
      "..........\n",
      "[1,   300] loss: 116.232387\n",
      "..........\n",
      "[1,   400] loss: 115.420126\n",
      "..........\n",
      "[1,   500] loss: 113.864381\n",
      "..........\n",
      "[1,   600] loss: 113.701415\n",
      "..........\n",
      "[1,   700] loss: 114.579701\n",
      "..........\n",
      "[1,   800] loss: 114.761133\n",
      "..........\n",
      "[1,   900] loss: 112.865563\n",
      "..........\n",
      "[1,  1000] loss: 115.292608\n",
      "..........\n",
      "[1,  1100] loss: 115.247253\n",
      "..........\n",
      "[1,  1200] loss: 114.848990\n",
      "..........\n",
      "[1,  1300] loss: 113.011181\n",
      "..........\n",
      "[1,  1400] loss: 113.940577\n",
      "..........\n",
      "[1,  1500] loss: 116.358044\n",
      "..........\n",
      "[1,  1600] loss: 114.436125\n",
      "..........\n",
      "[1,  1700] loss: 114.472761\n",
      "..........\n",
      "[1,  1800] loss: 114.628590\n",
      "..........\n",
      "[1,  1900] loss: 114.368335\n",
      "..........\n",
      "[1,  2000] loss: 116.719783\n",
      "..........\n",
      "[1,  2100] loss: 114.543887\n",
      "..........\n",
      "[1,  2200] loss: 114.195989\n",
      "..........\n",
      "[1,  2300] loss: 117.116037\n",
      "..........\n",
      "[1,  2400] loss: 115.867320\n",
      "..........\n",
      "[1,  2500] loss: 115.458761\n",
      "..........\n",
      "[1,  2600] loss: 114.709487\n",
      "..........\n",
      "[1,  2700] loss: 116.375743\n",
      "..........\n",
      "[1,  2800] loss: 116.398712\n",
      "..........\n",
      "[1,  2900] loss: 113.872203\n",
      "..........\n",
      "[1,  3000] loss: 114.173254\n",
      "..........\n",
      "[1,  3100] loss: 116.354154\n",
      "..........\n",
      "[1,  3200] loss: 114.006517\n",
      "..........\n",
      "[1,  3300] loss: 114.387546\n",
      "..........\n",
      "[1,  3400] loss: 112.888176\n",
      "..........\n",
      "[1,  3500] loss: 116.471494\n",
      "..........\n",
      "[1,  3600] loss: 114.923575\n",
      "..........\n",
      "[1,  3700] loss: 114.543418\n",
      "..........\n",
      "[1,  3800] loss: 113.830739\n",
      "..........\n",
      "[1,  3900] loss: 113.411804\n",
      "..........\n",
      "[1,  4000] loss: 115.563932\n",
      "..........\n",
      "[1,  4100] loss: 116.356812\n",
      "..........\n",
      "[1,  4200] loss: 113.447515\n",
      "..........\n",
      "[1,  4300] loss: 116.963546\n",
      "..........\n",
      "[1,  4400] loss: 113.447455\n",
      "..........\n",
      "[1,  4500] loss: 115.478976\n",
      "..........\n",
      "[1,  4600] loss: 115.349777\n",
      "..........\n",
      "[1,  4700] loss: 115.268283\n",
      "..........\n",
      "[1,  4800] loss: 115.598769\n",
      "..........\n",
      "[1,  4900] loss: 116.646536\n",
      "..........\n",
      "[1,  5000] loss: 113.391023\n",
      "..........\n",
      "[1,  5100] loss: 114.939667\n",
      "..........\n",
      "[1,  5200] loss: 116.185860\n",
      "..........\n",
      "[1,  5300] loss: 115.316440\n",
      "..........\n",
      "[1,  5400] loss: 114.086872\n",
      "..........\n",
      "[1,  5500] loss: 116.475078\n",
      "..........\n",
      "[1,  5600] loss: 116.317682\n",
      "..........\n",
      "[1,  5700] loss: 117.317894\n",
      "..........\n",
      "[1,  5800] loss: 116.034213\n",
      "..........\n",
      "[1,  5900] loss: 114.535624\n",
      "..........\n",
      "[1,  6000] loss: 116.683660\n",
      "..........\n",
      "[1,  6100] loss: 117.831511\n",
      "..........\n",
      "[1,  6200] loss: 115.189778\n",
      "..........\n",
      "[1,  6300] loss: 115.873393\n",
      "..........\n",
      "[1,  6400] loss: 116.947849\n",
      "..........\n",
      "[1,  6500] loss: 115.036400\n",
      "..........\n",
      "[1,  6600] loss: 116.252067\n",
      "..........\n",
      "[1,  6700] loss: 114.964768\n",
      "..........\n",
      "[1,  6800] loss: 116.149042\n",
      "..........\n",
      "[1,  6900] loss: 115.833596\n",
      "..........\n",
      "[1,  7000] loss: 116.718605\n",
      "..........\n",
      "[1,  7100] loss: 116.886160\n",
      "..........\n",
      "[1,  7200] loss: 113.871171\n",
      "..........\n",
      "[1,  7300] loss: 116.642037\n",
      "..........\n",
      "[1,  7400] loss: 115.334659\n",
      "..........\n",
      "[1,  7500] loss: 117.450101\n",
      "..........\n",
      "[1,  7600] loss: 116.852806\n",
      "..........\n",
      "[1,  7700] loss: 114.033292\n",
      "..........\n",
      "[1,  7800] loss: 114.824372\n",
      "..........\n",
      "[1,  7900] loss: 117.179893\n",
      "..........\n",
      "[1,  8000] loss: 116.406186\n",
      "total average loss : 115.287\n",
      "train acc : 0.0945\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.188\n",
      "step : 400 / 3125 acc : 0.172\n",
      "step : 600 / 3125 acc : 0.156\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.094\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.115\n",
      "step : 2000 / 3125 acc : 0.134\n",
      "step : 2200 / 3125 acc : 0.128\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.123\n",
      "step : 2800 / 3125 acc : 0.125\n",
      "step : 3000 / 3125 acc : 0.121\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1200 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 116.485079\n",
      "..........\n",
      "[1,   200] loss: 116.372997\n",
      "..........\n",
      "[1,   300] loss: 118.303826\n",
      "..........\n",
      "[1,   400] loss: 118.582866\n",
      "..........\n",
      "[1,   500] loss: 116.211234\n",
      "..........\n",
      "[1,   600] loss: 116.228251\n",
      "..........\n",
      "[1,   700] loss: 117.709341\n",
      "..........\n",
      "[1,   800] loss: 117.290032\n",
      "..........\n",
      "[1,   900] loss: 116.866895\n",
      "..........\n",
      "[1,  1000] loss: 117.725208\n",
      "..........\n",
      "[1,  1100] loss: 116.174381\n",
      "..........\n",
      "[1,  1200] loss: 116.183763\n",
      "..........\n",
      "[1,  1300] loss: 116.331480\n",
      "..........\n",
      "[1,  1400] loss: 114.718983\n",
      "..........\n",
      "[1,  1500] loss: 116.443434\n",
      "..........\n",
      "[1,  1600] loss: 117.459730\n",
      "..........\n",
      "[1,  1700] loss: 118.271099\n",
      "..........\n",
      "[1,  1800] loss: 117.446325\n",
      "..........\n",
      "[1,  1900] loss: 116.440863\n",
      "..........\n",
      "[1,  2000] loss: 117.389885\n",
      "..........\n",
      "[1,  2100] loss: 116.148514\n",
      "..........\n",
      "[1,  2200] loss: 116.290298\n",
      "..........\n",
      "[1,  2300] loss: 118.213810\n",
      "..........\n",
      "[1,  2400] loss: 117.085600\n",
      "..........\n",
      "[1,  2500] loss: 117.362702\n",
      "..........\n",
      "[1,  2600] loss: 118.178581\n",
      "..........\n",
      "[1,  2700] loss: 115.269873\n",
      "..........\n",
      "[1,  2800] loss: 117.920799\n",
      "..........\n",
      "[1,  2900] loss: 116.732149\n",
      "..........\n",
      "[1,  3000] loss: 115.911660\n",
      "..........\n",
      "[1,  3100] loss: 116.695126\n",
      "..........\n",
      "[1,  3200] loss: 117.832922\n",
      "..........\n",
      "[1,  3300] loss: 117.577669\n",
      "..........\n",
      "[1,  3400] loss: 117.156842\n",
      "..........\n",
      "[1,  3500] loss: 115.898767\n",
      "..........\n",
      "[1,  3600] loss: 118.751511\n",
      "..........\n",
      "[1,  3700] loss: 117.583134\n",
      "..........\n",
      "[1,  3800] loss: 118.093683\n",
      "..........\n",
      "[1,  3900] loss: 117.631134\n",
      "..........\n",
      "[1,  4000] loss: 116.840249\n",
      "..........\n",
      "[1,  4100] loss: 116.028539\n",
      "..........\n",
      "[1,  4200] loss: 115.678795\n",
      "..........\n",
      "[1,  4300] loss: 117.425179\n",
      "..........\n",
      "[1,  4400] loss: 116.546464\n",
      "..........\n",
      "[1,  4500] loss: 115.447433\n",
      "..........\n",
      "[1,  4600] loss: 117.854509\n",
      "..........\n",
      "[1,  4700] loss: 117.222070\n",
      "..........\n",
      "[1,  4800] loss: 116.883477\n",
      "..........\n",
      "[1,  4900] loss: 118.199145\n",
      "..........\n",
      "[1,  5000] loss: 119.925308\n",
      "..........\n",
      "[1,  5100] loss: 118.743712\n",
      "..........\n",
      "[1,  5200] loss: 117.767044\n",
      "..........\n",
      "[1,  5300] loss: 117.736388\n",
      "..........\n",
      "[1,  5400] loss: 117.264611\n",
      "..........\n",
      "[1,  5500] loss: 119.105398\n",
      "..........\n",
      "[1,  5600] loss: 118.834536\n",
      "..........\n",
      "[1,  5700] loss: 117.607132\n",
      "..........\n",
      "[1,  5800] loss: 118.320330\n",
      "..........\n",
      "[1,  5900] loss: 117.671208\n",
      "..........\n",
      "[1,  6000] loss: 117.373563\n",
      "..........\n",
      "[1,  6100] loss: 120.843596\n",
      "..........\n",
      "[1,  6200] loss: 116.764339\n",
      "..........\n",
      "[1,  6300] loss: 117.295886\n",
      "..........\n",
      "[1,  6400] loss: 118.098366\n",
      "..........\n",
      "[1,  6500] loss: 116.685072\n",
      "..........\n",
      "[1,  6600] loss: 117.295612\n",
      "..........\n",
      "[1,  6700] loss: 117.034501\n",
      "..........\n",
      "[1,  6800] loss: 118.938857\n",
      "..........\n",
      "[1,  6900] loss: 117.797363\n",
      "..........\n",
      "[1,  7000] loss: 118.204342\n",
      "..........\n",
      "[1,  7100] loss: 116.782103\n",
      "..........\n",
      "[1,  7200] loss: 118.223798\n",
      "..........\n",
      "[1,  7300] loss: 116.815626\n",
      "..........\n",
      "[1,  7400] loss: 118.782263\n",
      "..........\n",
      "[1,  7500] loss: 118.192402\n",
      "..........\n",
      "[1,  7600] loss: 117.309384\n",
      "..........\n",
      "[1,  7700] loss: 118.052868\n",
      "..........\n",
      "[1,  7800] loss: 119.278429\n",
      "..........\n",
      "[1,  7900] loss: 117.469219\n",
      "..........\n",
      "[1,  8000] loss: 118.375753\n",
      "total average loss : 117.396\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.117\n",
      "step : 1000 / 3125 acc : 0.119\n",
      "step : 1200 / 3125 acc : 0.109\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.101\n",
      "step : 2000 / 3125 acc : 0.103\n",
      "step : 2200 / 3125 acc : 0.108\n",
      "step : 2400 / 3125 acc : 0.102\n",
      "step : 2600 / 3125 acc : 0.103\n",
      "step : 2800 / 3125 acc : 0.109\n",
      "step : 3000 / 3125 acc : 0.117\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1200 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 120.697036\n",
      "..........\n",
      "[1,   200] loss: 117.584691\n",
      "..........\n",
      "[1,   300] loss: 117.598132\n",
      "..........\n",
      "[1,   400] loss: 118.276305\n",
      "..........\n",
      "[1,   500] loss: 118.272110\n",
      "..........\n",
      "[1,   600] loss: 118.488408\n",
      "..........\n",
      "[1,   700] loss: 118.555618\n",
      "..........\n",
      "[1,   800] loss: 117.380009\n",
      "..........\n",
      "[1,   900] loss: 117.318838\n",
      "..........\n",
      "[1,  1000] loss: 118.278317\n",
      "..........\n",
      "[1,  1100] loss: 118.270240\n",
      "..........\n",
      "[1,  1200] loss: 121.445190\n",
      "..........\n",
      "[1,  1300] loss: 118.942696\n",
      "..........\n",
      "[1,  1400] loss: 120.258102\n",
      "..........\n",
      "[1,  1500] loss: 118.125539\n",
      "..........\n",
      "[1,  1600] loss: 117.789644\n",
      "..........\n",
      "[1,  1700] loss: 118.665979\n",
      "..........\n",
      "[1,  1800] loss: 119.077842\n",
      "..........\n",
      "[1,  1900] loss: 119.713351\n",
      "..........\n",
      "[1,  2000] loss: 120.476507\n",
      "..........\n",
      "[1,  2100] loss: 121.093599\n",
      "..........\n",
      "[1,  2200] loss: 120.172880\n",
      "..........\n",
      "[1,  2300] loss: 119.290759\n",
      "..........\n",
      "[1,  2400] loss: 118.716325\n",
      "..........\n",
      "[1,  2500] loss: 122.033664\n",
      "..........\n",
      "[1,  2600] loss: 117.072064\n",
      "..........\n",
      "[1,  2700] loss: 118.784428\n",
      "..........\n",
      "[1,  2800] loss: 118.858133\n",
      "..........\n",
      "[1,  2900] loss: 121.020045\n",
      "..........\n",
      "[1,  3000] loss: 120.317398\n",
      "..........\n",
      "[1,  3100] loss: 120.576439\n",
      "..........\n",
      "[1,  3200] loss: 116.640492\n",
      "..........\n",
      "[1,  3300] loss: 118.641234\n",
      "..........\n",
      "[1,  3400] loss: 118.490661\n",
      "..........\n",
      "[1,  3500] loss: 119.415124\n",
      "..........\n",
      "[1,  3600] loss: 119.978929\n",
      "..........\n",
      "[1,  3700] loss: 120.463949\n",
      "..........\n",
      "[1,  3800] loss: 120.910420\n",
      "..........\n",
      "[1,  3900] loss: 120.774064\n",
      "..........\n",
      "[1,  4000] loss: 117.816954\n",
      "..........\n",
      "[1,  4100] loss: 119.510209\n",
      "..........\n",
      "[1,  4200] loss: 118.833478\n",
      "..........\n",
      "[1,  4300] loss: 119.639287\n",
      "..........\n",
      "[1,  4400] loss: 121.885216\n",
      "..........\n",
      "[1,  4500] loss: 118.606139\n",
      "..........\n",
      "[1,  4600] loss: 118.589828\n",
      "..........\n",
      "[1,  4700] loss: 119.165822\n",
      "..........\n",
      "[1,  4800] loss: 119.773629\n",
      "..........\n",
      "[1,  4900] loss: 118.314528\n",
      "..........\n",
      "[1,  5000] loss: 120.397992\n",
      "..........\n",
      "[1,  5100] loss: 120.139723\n",
      "..........\n",
      "[1,  5200] loss: 121.610698\n",
      "..........\n",
      "[1,  5300] loss: 118.833449\n",
      "..........\n",
      "[1,  5400] loss: 119.464414\n",
      "..........\n",
      "[1,  5500] loss: 118.973592\n",
      "..........\n",
      "[1,  5600] loss: 119.164569\n",
      "..........\n",
      "[1,  5700] loss: 119.822223\n",
      "..........\n",
      "[1,  5800] loss: 119.528461\n",
      "..........\n",
      "[1,  5900] loss: 120.605155\n",
      "..........\n",
      "[1,  6000] loss: 118.221040\n",
      "..........\n",
      "[1,  6100] loss: 119.201238\n",
      "..........\n",
      "[1,  6200] loss: 119.231186\n",
      "..........\n",
      "[1,  6300] loss: 118.044525\n",
      "..........\n",
      "[1,  6400] loss: 119.061576\n",
      "..........\n",
      "[1,  6500] loss: 121.754649\n",
      "..........\n",
      "[1,  6600] loss: 119.446366\n",
      "..........\n",
      "[1,  6700] loss: 119.504091\n",
      "..........\n",
      "[1,  6800] loss: 121.004044\n",
      "..........\n",
      "[1,  6900] loss: 120.417126\n",
      "..........\n",
      "[1,  7000] loss: 121.668225\n",
      "..........\n",
      "[1,  7100] loss: 119.389412\n",
      "..........\n",
      "[1,  7200] loss: 120.895571\n",
      "..........\n",
      "[1,  7300] loss: 119.663779\n",
      "..........\n",
      "[1,  7400] loss: 119.778118\n",
      "..........\n",
      "[1,  7500] loss: 119.975529\n",
      "..........\n",
      "[1,  7600] loss: 120.855992\n",
      "..........\n",
      "[1,  7700] loss: 120.022317\n",
      "..........\n",
      "[1,  7800] loss: 122.341577\n",
      "..........\n",
      "[1,  7900] loss: 119.627001\n",
      "..........\n",
      "[1,  8000] loss: 120.638737\n",
      "total average loss : 119.499\n",
      "train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.000\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.073\n",
      "step : 800 / 3125 acc : 0.062\n",
      "step : 1000 / 3125 acc : 0.075\n",
      "step : 1200 / 3125 acc : 0.083\n",
      "step : 1400 / 3125 acc : 0.107\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.111\n",
      "step : 2000 / 3125 acc : 0.131\n",
      "step : 2200 / 3125 acc : 0.131\n",
      "step : 2400 / 3125 acc : 0.135\n",
      "step : 2600 / 3125 acc : 0.130\n",
      "step : 2800 / 3125 acc : 0.125\n",
      "step : 3000 / 3125 acc : 0.119\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1180 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 120.432158\n",
      "..........\n",
      "[1,   200] loss: 119.306985\n",
      "..........\n",
      "[1,   300] loss: 119.991069\n",
      "..........\n",
      "[1,   400] loss: 120.755577\n",
      "..........\n",
      "[1,   500] loss: 120.223756\n",
      "..........\n",
      "[1,   600] loss: 119.892899\n",
      "..........\n",
      "[1,   700] loss: 121.321245\n",
      "..........\n",
      "[1,   800] loss: 122.564269\n",
      "..........\n",
      "[1,   900] loss: 122.429189\n",
      "..........\n",
      "[1,  1000] loss: 121.002885\n",
      "..........\n",
      "[1,  1100] loss: 120.732332\n",
      "..........\n",
      "[1,  1200] loss: 120.477971\n",
      "..........\n",
      "[1,  1300] loss: 122.935658\n",
      "..........\n",
      "[1,  1400] loss: 121.677298\n",
      "..........\n",
      "[1,  1500] loss: 121.690079\n",
      "..........\n",
      "[1,  1600] loss: 123.069300\n",
      "..........\n",
      "[1,  1700] loss: 120.654884\n",
      "..........\n",
      "[1,  1800] loss: 121.777550\n",
      "..........\n",
      "[1,  1900] loss: 121.329313\n",
      "..........\n",
      "[1,  2000] loss: 122.357472\n",
      "..........\n",
      "[1,  2100] loss: 120.529319\n",
      "..........\n",
      "[1,  2200] loss: 120.986684\n",
      "..........\n",
      "[1,  2300] loss: 120.821990\n",
      "..........\n",
      "[1,  2400] loss: 121.062052\n",
      "..........\n",
      "[1,  2500] loss: 121.309680\n",
      "..........\n",
      "[1,  2600] loss: 120.724743\n",
      "..........\n",
      "[1,  2700] loss: 122.132240\n",
      "..........\n",
      "[1,  2800] loss: 124.527978\n",
      "..........\n",
      "[1,  2900] loss: 120.038083\n",
      "..........\n",
      "[1,  3000] loss: 122.290476\n",
      "..........\n",
      "[1,  3100] loss: 119.763781\n",
      "..........\n",
      "[1,  3200] loss: 121.303620\n",
      "..........\n",
      "[1,  3300] loss: 119.456506\n",
      "..........\n",
      "[1,  3400] loss: 123.127015\n",
      "..........\n",
      "[1,  3500] loss: 122.084929\n",
      "..........\n",
      "[1,  3600] loss: 121.932943\n",
      "..........\n",
      "[1,  3700] loss: 121.430931\n",
      "..........\n",
      "[1,  3800] loss: 121.031201\n",
      "..........\n",
      "[1,  3900] loss: 122.068689\n",
      "..........\n",
      "[1,  4000] loss: 121.705355\n",
      "..........\n",
      "[1,  4100] loss: 123.575801\n",
      "..........\n",
      "[1,  4200] loss: 121.595314\n",
      "..........\n",
      "[1,  4300] loss: 121.961582\n",
      "..........\n",
      "[1,  4400] loss: 122.618392\n",
      "..........\n",
      "[1,  4500] loss: 124.045482\n",
      "..........\n",
      "[1,  4600] loss: 120.724537\n",
      "..........\n",
      "[1,  4700] loss: 119.797097\n",
      "..........\n",
      "[1,  4800] loss: 120.014394\n",
      "..........\n",
      "[1,  4900] loss: 120.414052\n",
      "..........\n",
      "[1,  5000] loss: 121.764453\n",
      "..........\n",
      "[1,  5100] loss: 121.248797\n",
      "..........\n",
      "[1,  5200] loss: 122.063097\n",
      "..........\n",
      "[1,  5300] loss: 123.003404\n",
      "..........\n",
      "[1,  5400] loss: 123.400871\n",
      "..........\n",
      "[1,  5500] loss: 122.455626\n",
      "..........\n",
      "[1,  5600] loss: 124.102199\n",
      "..........\n",
      "[1,  5700] loss: 120.969986\n",
      "..........\n",
      "[1,  5800] loss: 120.659357\n",
      "..........\n",
      "[1,  5900] loss: 121.796254\n",
      "..........\n",
      "[1,  6000] loss: 123.484457\n",
      "..........\n",
      "[1,  6100] loss: 120.249553\n",
      "..........\n",
      "[1,  6200] loss: 120.912031\n",
      "..........\n",
      "[1,  6300] loss: 122.691229\n",
      "..........\n",
      "[1,  6400] loss: 121.758856\n",
      "..........\n",
      "[1,  6500] loss: 122.019202\n",
      "..........\n",
      "[1,  6600] loss: 120.473459\n",
      "..........\n",
      "[1,  6700] loss: 122.890050\n",
      "..........\n",
      "[1,  6800] loss: 123.397528\n",
      "..........\n",
      "[1,  6900] loss: 120.558072\n",
      "..........\n",
      "[1,  7000] loss: 121.547595\n",
      "..........\n",
      "[1,  7100] loss: 122.625903\n",
      "..........\n",
      "[1,  7200] loss: 125.005873\n",
      "..........\n",
      "[1,  7300] loss: 122.168162\n",
      "..........\n",
      "[1,  7400] loss: 121.925608\n",
      "..........\n",
      "[1,  7500] loss: 122.218456\n",
      "..........\n",
      "[1,  7600] loss: 123.971737\n",
      "..........\n",
      "[1,  7700] loss: 123.200938\n",
      "..........\n",
      "[1,  7800] loss: 122.422012\n",
      "..........\n",
      "[1,  7900] loss: 121.138925\n",
      "..........\n",
      "[1,  8000] loss: 122.820867\n",
      "total average loss : 121.708\n",
      "train acc : 0.0977\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.135\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.121\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.128\n",
      "step : 2200 / 3125 acc : 0.125\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.125\n",
      "step : 2800 / 3125 acc : 0.127\n",
      "step : 3000 / 3125 acc : 0.125\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1220 %\n",
      "======eval  end ======\n",
      "======= epoch 13 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 122.528924\n",
      "..........\n",
      "[1,   200] loss: 124.259066\n",
      "..........\n",
      "[1,   300] loss: 123.746109\n",
      "..........\n",
      "[1,   400] loss: 124.182908\n",
      "..........\n",
      "[1,   500] loss: 126.556524\n",
      "..........\n",
      "[1,   600] loss: 123.466529\n",
      "..........\n",
      "[1,   700] loss: 125.648185\n",
      "..........\n",
      "[1,   800] loss: 125.033263\n",
      "..........\n",
      "[1,   900] loss: 122.804413\n",
      "..........\n",
      "[1,  1000] loss: 123.772292\n",
      "..........\n",
      "[1,  1100] loss: 122.786796\n",
      "..........\n",
      "[1,  1200] loss: 122.998738\n",
      "..........\n",
      "[1,  1300] loss: 124.281182\n",
      "..........\n",
      "[1,  1400] loss: 123.597499\n",
      "..........\n",
      "[1,  1500] loss: 124.171334\n",
      "..........\n",
      "[1,  1600] loss: 124.338363\n",
      "..........\n",
      "[1,  1700] loss: 122.177814\n",
      "..........\n",
      "[1,  1800] loss: 123.857421\n",
      "..........\n",
      "[1,  1900] loss: 122.558948\n",
      "..........\n",
      "[1,  2000] loss: 124.187631\n",
      "..........\n",
      "[1,  2100] loss: 122.072442\n",
      "..........\n",
      "[1,  2200] loss: 121.139545\n",
      "..........\n",
      "[1,  2300] loss: 121.729227\n",
      "..........\n",
      "[1,  2400] loss: 124.714621\n",
      "..........\n",
      "[1,  2500] loss: 123.965437\n",
      "..........\n",
      "[1,  2600] loss: 123.887079\n",
      "..........\n",
      "[1,  2700] loss: 122.885828\n",
      "..........\n",
      "[1,  2800] loss: 124.780816\n",
      "..........\n",
      "[1,  2900] loss: 125.374888\n",
      "..........\n",
      "[1,  3000] loss: 121.733235\n",
      "..........\n",
      "[1,  3100] loss: 122.110796\n",
      "..........\n",
      "[1,  3200] loss: 125.427730\n",
      "..........\n",
      "[1,  3300] loss: 123.213922\n",
      "..........\n",
      "[1,  3400] loss: 123.826892\n",
      "..........\n",
      "[1,  3500] loss: 122.726838\n",
      "..........\n",
      "[1,  3600] loss: 124.243032\n",
      "..........\n",
      "[1,  3700] loss: 122.801815\n",
      "..........\n",
      "[1,  3800] loss: 124.631527\n",
      "..........\n",
      "[1,  3900] loss: 123.369660\n",
      "..........\n",
      "[1,  4000] loss: 121.933910\n",
      "..........\n",
      "[1,  4100] loss: 124.573259\n",
      "..........\n",
      "[1,  4200] loss: 123.122407\n",
      "..........\n",
      "[1,  4300] loss: 124.983452\n",
      "..........\n",
      "[1,  4400] loss: 124.805721\n",
      "..........\n",
      "[1,  4500] loss: 123.171724\n",
      "..........\n",
      "[1,  4600] loss: 124.197867\n",
      "..........\n",
      "[1,  4700] loss: 124.168128\n",
      "..........\n",
      "[1,  4800] loss: 122.960129\n",
      "..........\n",
      "[1,  4900] loss: 123.786806\n",
      "..........\n",
      "[1,  5000] loss: 124.767446\n",
      "..........\n",
      "[1,  5100] loss: 123.092705\n",
      "..........\n",
      "[1,  5200] loss: 125.040704\n",
      "..........\n",
      "[1,  5300] loss: 123.485518\n",
      "..........\n",
      "[1,  5400] loss: 121.708542\n",
      "..........\n",
      "[1,  5500] loss: 123.359108\n",
      "..........\n",
      "[1,  5600] loss: 125.295162\n",
      "..........\n",
      "[1,  5700] loss: 125.888589\n",
      "..........\n",
      "[1,  5800] loss: 124.539878\n",
      "..........\n",
      "[1,  5900] loss: 124.511562\n",
      "..........\n",
      "[1,  6000] loss: 124.925091\n",
      "..........\n",
      "[1,  6100] loss: 125.043350\n",
      "..........\n",
      "[1,  6200] loss: 126.672181\n",
      "..........\n",
      "[1,  6300] loss: 124.560561\n",
      "..........\n",
      "[1,  6400] loss: 125.804108\n",
      "..........\n",
      "[1,  6500] loss: 123.596376\n",
      "..........\n",
      "[1,  6600] loss: 122.891758\n",
      "..........\n",
      "[1,  6700] loss: 123.016923\n",
      "..........\n",
      "[1,  6800] loss: 124.879021\n",
      "..........\n",
      "[1,  6900] loss: 124.400678\n",
      "..........\n",
      "[1,  7000] loss: 122.780755\n",
      "..........\n",
      "[1,  7100] loss: 122.899256\n",
      "..........\n",
      "[1,  7200] loss: 123.512001\n",
      "..........\n",
      "[1,  7300] loss: 125.773084\n",
      "..........\n",
      "[1,  7400] loss: 125.742915\n",
      "..........\n",
      "[1,  7500] loss: 125.777997\n",
      "..........\n",
      "[1,  7600] loss: 123.309548\n",
      "..........\n",
      "[1,  7700] loss: 124.276322\n",
      "..........\n",
      "[1,  7800] loss: 123.929204\n",
      "..........\n",
      "[1,  7900] loss: 124.502199\n",
      "..........\n",
      "[1,  8000] loss: 124.876807\n",
      "total average loss : 123.927\n",
      "train acc : 0.0977\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.094\n",
      "step : 600 / 3125 acc : 0.135\n",
      "step : 800 / 3125 acc : 0.125\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.135\n",
      "step : 1400 / 3125 acc : 0.129\n",
      "step : 1600 / 3125 acc : 0.125\n",
      "step : 1800 / 3125 acc : 0.135\n",
      "step : 2000 / 3125 acc : 0.131\n",
      "step : 2200 / 3125 acc : 0.122\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.123\n",
      "step : 2800 / 3125 acc : 0.121\n",
      "step : 3000 / 3125 acc : 0.121\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1220 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 125.868131\n",
      "..........\n",
      "[1,   200] loss: 124.205618\n",
      "..........\n",
      "[1,   300] loss: 124.206973\n",
      "..........\n",
      "[1,   400] loss: 127.051121\n",
      "..........\n",
      "[1,   500] loss: 124.951545\n",
      "..........\n",
      "[1,   600] loss: 125.392546\n",
      "..........\n",
      "[1,   700] loss: 126.781379\n",
      "..........\n",
      "[1,   800] loss: 125.574294\n",
      "..........\n",
      "[1,   900] loss: 124.507478\n",
      "..........\n",
      "[1,  1000] loss: 127.339678\n",
      "..........\n",
      "[1,  1100] loss: 124.409613\n",
      "..........\n",
      "[1,  1200] loss: 124.968398\n",
      "..........\n",
      "[1,  1300] loss: 127.190649\n",
      "..........\n",
      "[1,  1400] loss: 124.463495\n",
      "..........\n",
      "[1,  1500] loss: 125.758880\n",
      "..........\n",
      "[1,  1600] loss: 124.791371\n",
      "..........\n",
      "[1,  1700] loss: 124.958516\n",
      "..........\n",
      "[1,  1800] loss: 127.092963\n",
      "..........\n",
      "[1,  1900] loss: 124.920084\n",
      "..........\n",
      "[1,  2000] loss: 126.060497\n",
      "..........\n",
      "[1,  2100] loss: 128.398243\n",
      "..........\n",
      "[1,  2200] loss: 126.914810\n",
      "..........\n",
      "[1,  2300] loss: 124.500928\n",
      "..........\n",
      "[1,  2400] loss: 125.752512\n",
      "..........\n",
      "[1,  2500] loss: 126.467477\n",
      "..........\n",
      "[1,  2600] loss: 124.256632\n",
      "..........\n",
      "[1,  2700] loss: 124.554591\n",
      "..........\n",
      "[1,  2800] loss: 125.752074\n",
      "..........\n",
      "[1,  2900] loss: 128.124906\n",
      "..........\n",
      "[1,  3000] loss: 126.351236\n",
      "..........\n",
      "[1,  3100] loss: 125.919501\n",
      "..........\n",
      "[1,  3200] loss: 127.431587\n",
      "..........\n",
      "[1,  3300] loss: 126.288882\n",
      "..........\n",
      "[1,  3400] loss: 125.767666\n",
      "..........\n",
      "[1,  3500] loss: 124.715407\n",
      "..........\n",
      "[1,  3600] loss: 126.357174\n",
      "..........\n",
      "[1,  3700] loss: 124.693324\n",
      "..........\n",
      "[1,  3800] loss: 125.071123\n",
      "..........\n",
      "[1,  3900] loss: 127.955294\n",
      "..........\n",
      "[1,  4000] loss: 126.204929\n",
      "..........\n",
      "[1,  4100] loss: 123.408467\n",
      "..........\n",
      "[1,  4200] loss: 127.303125\n",
      "..........\n",
      "[1,  4300] loss: 123.786080\n",
      "..........\n",
      "[1,  4400] loss: 127.049956\n",
      "..........\n",
      "[1,  4500] loss: 126.221077\n",
      "..........\n",
      "[1,  4600] loss: 124.439436\n",
      "..........\n",
      "[1,  4700] loss: 124.989655\n",
      "..........\n",
      "[1,  4800] loss: 126.606442\n",
      "..........\n",
      "[1,  4900] loss: 124.762355\n",
      "..........\n",
      "[1,  5000] loss: 126.895818\n",
      "..........\n",
      "[1,  5100] loss: 126.470239\n",
      "..........\n",
      "[1,  5200] loss: 125.611661\n",
      "..........\n",
      "[1,  5300] loss: 126.736759\n",
      "..........\n",
      "[1,  5400] loss: 127.569208\n",
      "..........\n",
      "[1,  5500] loss: 126.561989\n",
      "..........\n",
      "[1,  5600] loss: 125.476949\n",
      "..........\n",
      "[1,  5700] loss: 125.885534\n",
      "..........\n",
      "[1,  5800] loss: 126.803772\n",
      "..........\n",
      "[1,  5900] loss: 126.131591\n",
      "..........\n",
      "[1,  6000] loss: 126.992314\n",
      "..........\n",
      "[1,  6100] loss: 127.515497\n",
      "..........\n",
      "[1,  6200] loss: 125.951709\n",
      "..........\n",
      "[1,  6300] loss: 128.973222\n",
      "..........\n",
      "[1,  6400] loss: 127.699183\n",
      "..........\n",
      "[1,  6500] loss: 127.229445\n",
      "..........\n",
      "[1,  6600] loss: 126.941381\n",
      "..........\n",
      "[1,  6700] loss: 127.574097\n",
      "..........\n",
      "[1,  6800] loss: 125.185060\n",
      "..........\n",
      "[1,  6900] loss: 128.716355\n",
      "..........\n",
      "[1,  7000] loss: 127.477449\n",
      "..........\n",
      "[1,  7100] loss: 125.663265\n",
      "..........\n",
      "[1,  7200] loss: 126.021677\n",
      "..........\n",
      "[1,  7300] loss: 126.456617\n",
      "..........\n",
      "[1,  7400] loss: 124.972567\n",
      "..........\n",
      "[1,  7500] loss: 126.084989\n",
      "..........\n",
      "[1,  7600] loss: 125.976446\n",
      "..........\n",
      "[1,  7700] loss: 128.352533\n",
      "..........\n",
      "[1,  7800] loss: 127.538194\n",
      "..........\n",
      "[1,  7900] loss: 126.063459\n",
      "..........\n",
      "[1,  8000] loss: 126.731332\n",
      "total average loss : 126.110\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.219\n",
      "step : 400 / 3125 acc : 0.156\n",
      "step : 600 / 3125 acc : 0.146\n",
      "step : 800 / 3125 acc : 0.141\n",
      "step : 1000 / 3125 acc : 0.144\n",
      "step : 1200 / 3125 acc : 0.135\n",
      "step : 1400 / 3125 acc : 0.143\n",
      "step : 1600 / 3125 acc : 0.141\n",
      "step : 1800 / 3125 acc : 0.142\n",
      "step : 2000 / 3125 acc : 0.144\n",
      "step : 2200 / 3125 acc : 0.136\n",
      "step : 2400 / 3125 acc : 0.130\n",
      "step : 2600 / 3125 acc : 0.120\n",
      "step : 2800 / 3125 acc : 0.121\n",
      "step : 3000 / 3125 acc : 0.115\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1220 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 128.125081\n",
      "..........\n",
      "[1,   200] loss: 125.802087\n",
      "..........\n",
      "[1,   300] loss: 129.097090\n",
      "..........\n",
      "[1,   400] loss: 126.782801\n",
      "..........\n",
      "[1,   500] loss: 127.610154\n",
      "..........\n",
      "[1,   600] loss: 126.474030\n",
      "..........\n",
      "[1,   700] loss: 127.768888\n",
      "..........\n",
      "[1,   800] loss: 126.366218\n",
      "..........\n",
      "[1,   900] loss: 128.135277\n",
      "..........\n",
      "[1,  1000] loss: 129.707945\n",
      "..........\n",
      "[1,  1100] loss: 128.459564\n",
      "..........\n",
      "[1,  1200] loss: 127.652179\n",
      "..........\n",
      "[1,  1300] loss: 129.045034\n",
      "..........\n",
      "[1,  1400] loss: 128.083539\n",
      "..........\n",
      "[1,  1500] loss: 129.213063\n",
      "..........\n",
      "[1,  1600] loss: 128.616338\n",
      "..........\n",
      "[1,  1700] loss: 129.233641\n",
      "..........\n",
      "[1,  1800] loss: 129.326153\n",
      "..........\n",
      "[1,  1900] loss: 128.071237\n",
      "..........\n",
      "[1,  2000] loss: 126.734445\n",
      "..........\n",
      "[1,  2100] loss: 128.230508\n",
      "..........\n",
      "[1,  2200] loss: 127.615761\n",
      "..........\n",
      "[1,  2300] loss: 128.831013\n",
      "..........\n",
      "[1,  2400] loss: 127.110636\n",
      "..........\n",
      "[1,  2500] loss: 128.014557\n",
      "..........\n",
      "[1,  2600] loss: 129.167396\n",
      "..........\n",
      "[1,  2700] loss: 125.978028\n",
      "..........\n",
      "[1,  2800] loss: 128.948260\n",
      "..........\n",
      "[1,  2900] loss: 128.546184\n",
      "..........\n",
      "[1,  3000] loss: 128.434203\n",
      "..........\n",
      "[1,  3100] loss: 127.594816\n",
      "..........\n",
      "[1,  3200] loss: 127.395479\n",
      "..........\n",
      "[1,  3300] loss: 126.723712\n",
      "..........\n",
      "[1,  3400] loss: 128.477794\n",
      "..........\n",
      "[1,  3500] loss: 127.956658\n",
      "..........\n",
      "[1,  3600] loss: 128.231941\n",
      "..........\n",
      "[1,  3700] loss: 128.969044\n",
      "..........\n",
      "[1,  3800] loss: 129.030651\n",
      "..........\n",
      "[1,  3900] loss: 127.747259\n",
      "..........\n",
      "[1,  4000] loss: 128.639478\n",
      "..........\n",
      "[1,  4100] loss: 129.166113\n",
      "..........\n",
      "[1,  4200] loss: 124.876451\n",
      "..........\n",
      "[1,  4300] loss: 127.797365\n",
      "..........\n",
      "[1,  4400] loss: 128.115196\n",
      "..........\n",
      "[1,  4500] loss: 129.886572\n",
      "..........\n",
      "[1,  4600] loss: 130.492284\n",
      "..........\n",
      "[1,  4700] loss: 128.564410\n",
      "..........\n",
      "[1,  4800] loss: 127.499920\n",
      "..........\n",
      "[1,  4900] loss: 128.813418\n",
      "..........\n",
      "[1,  5000] loss: 131.025337\n",
      "..........\n",
      "[1,  5100] loss: 129.037089\n",
      "..........\n",
      "[1,  5200] loss: 128.888773\n",
      "..........\n",
      "[1,  5300] loss: 131.171614\n",
      "..........\n",
      "[1,  5400] loss: 131.272804\n",
      "..........\n",
      "[1,  5500] loss: 126.646535\n",
      "..........\n",
      "[1,  5600] loss: 129.737787\n",
      "..........\n",
      "[1,  5700] loss: 127.361135\n",
      "..........\n",
      "[1,  5800] loss: 128.209802\n",
      "..........\n",
      "[1,  5900] loss: 127.094394\n",
      "..........\n",
      "[1,  6000] loss: 129.058504\n",
      "..........\n",
      "[1,  6100] loss: 126.890336\n",
      "..........\n",
      "[1,  6200] loss: 129.179723\n",
      "..........\n",
      "[1,  6300] loss: 127.897631\n",
      "..........\n",
      "[1,  6400] loss: 127.839331\n",
      "..........\n",
      "[1,  6500] loss: 129.393531\n",
      "..........\n",
      "[1,  6600] loss: 128.468339\n",
      "..........\n",
      "[1,  6700] loss: 127.886459\n",
      "..........\n",
      "[1,  6800] loss: 127.572330\n",
      "..........\n",
      "[1,  6900] loss: 129.523759\n",
      "..........\n",
      "[1,  7000] loss: 128.075845\n",
      "..........\n",
      "[1,  7100] loss: 127.276453\n",
      "..........\n",
      "[1,  7200] loss: 127.166443\n",
      "..........\n",
      "[1,  7300] loss: 128.703674\n",
      "..........\n",
      "[1,  7400] loss: 126.214518\n",
      "..........\n",
      "[1,  7500] loss: 129.136221\n",
      "..........\n",
      "[1,  7600] loss: 130.180833\n",
      "..........\n",
      "[1,  7700] loss: 129.900403\n",
      "..........\n",
      "[1,  7800] loss: 127.838075\n",
      "..........\n",
      "[1,  7900] loss: 127.198011\n",
      "..........\n",
      "[1,  8000] loss: 128.379084\n",
      "total average loss : 128.267\n",
      "train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.125\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.115\n",
      "step : 800 / 3125 acc : 0.133\n",
      "step : 1000 / 3125 acc : 0.144\n",
      "step : 1200 / 3125 acc : 0.130\n",
      "step : 1400 / 3125 acc : 0.134\n",
      "step : 1600 / 3125 acc : 0.129\n",
      "step : 1800 / 3125 acc : 0.125\n",
      "step : 2000 / 3125 acc : 0.131\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.123\n",
      "step : 2800 / 3125 acc : 0.116\n",
      "step : 3000 / 3125 acc : 0.117\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1200 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 129.414927\n",
      "..........\n",
      "[1,   200] loss: 130.615332\n",
      "..........\n",
      "[1,   300] loss: 129.022532\n",
      "..........\n",
      "[1,   400] loss: 131.211535\n",
      "..........\n",
      "[1,   500] loss: 131.355566\n",
      "..........\n",
      "[1,   600] loss: 132.188065\n",
      "..........\n",
      "[1,   700] loss: 130.963294\n",
      "..........\n",
      "[1,   800] loss: 128.830385\n",
      "..........\n",
      "[1,   900] loss: 129.541615\n",
      "..........\n",
      "[1,  1000] loss: 130.010263\n",
      "..........\n",
      "[1,  1100] loss: 129.306176\n",
      "..........\n",
      "[1,  1200] loss: 129.470426\n",
      "..........\n",
      "[1,  1300] loss: 130.646398\n",
      "..........\n",
      "[1,  1400] loss: 126.918936\n",
      "..........\n",
      "[1,  1500] loss: 129.505898\n",
      "..........\n",
      "[1,  1600] loss: 130.572608\n",
      "..........\n",
      "[1,  1700] loss: 129.878623\n",
      "..........\n",
      "[1,  1800] loss: 131.080283\n",
      "..........\n",
      "[1,  1900] loss: 131.724999\n",
      "..........\n",
      "[1,  2000] loss: 131.240366\n",
      "..........\n",
      "[1,  2100] loss: 129.979889\n",
      "..........\n",
      "[1,  2200] loss: 130.404417\n",
      "..........\n",
      "[1,  2300] loss: 130.573077\n",
      "..........\n",
      "[1,  2400] loss: 128.892337\n",
      "..........\n",
      "[1,  2500] loss: 132.775874\n",
      "..........\n",
      "[1,  2600] loss: 130.250241\n",
      "..........\n",
      "[1,  2700] loss: 132.590239\n",
      "..........\n",
      "[1,  2800] loss: 129.854128\n",
      "..........\n",
      "[1,  2900] loss: 130.229686\n",
      "..........\n",
      "[1,  3000] loss: 131.556206\n",
      "..........\n",
      "[1,  3100] loss: 130.277462\n",
      "..........\n",
      "[1,  3200] loss: 130.356610\n",
      "..........\n",
      "[1,  3300] loss: 131.833938\n",
      "..........\n",
      "[1,  3400] loss: 130.096122\n",
      "..........\n",
      "[1,  3500] loss: 129.675007\n",
      "..........\n",
      "[1,  3600] loss: 129.386634\n",
      "..........\n",
      "[1,  3700] loss: 131.592783\n",
      "..........\n",
      "[1,  3800] loss: 130.607984\n",
      "..........\n",
      "[1,  3900] loss: 131.027485\n",
      "..........\n",
      "[1,  4000] loss: 129.998113\n",
      "..........\n",
      "[1,  4100] loss: 131.102885\n",
      "..........\n",
      "[1,  4200] loss: 130.953106\n",
      "..........\n",
      "[1,  4300] loss: 129.399119\n",
      "..........\n",
      "[1,  4400] loss: 129.472315\n",
      "..........\n",
      "[1,  4500] loss: 129.726734\n",
      "..........\n",
      "[1,  4600] loss: 131.033265\n",
      "..........\n",
      "[1,  4700] loss: 132.203690\n",
      "..........\n",
      "[1,  4800] loss: 131.489513\n",
      "..........\n",
      "[1,  4900] loss: 130.966788\n",
      "..........\n",
      "[1,  5000] loss: 131.938906\n",
      "..........\n",
      "[1,  5100] loss: 126.637598\n",
      "..........\n",
      "[1,  5200] loss: 132.876863\n",
      "..........\n",
      "[1,  5300] loss: 130.604962\n",
      "..........\n",
      "[1,  5400] loss: 129.598323\n",
      "..........\n",
      "[1,  5500] loss: 130.999498\n",
      "..........\n",
      "[1,  5600] loss: 129.715037\n",
      "..........\n",
      "[1,  5700] loss: 132.819675\n",
      "..........\n",
      "[1,  5800] loss: 131.508958\n",
      "..........\n",
      "[1,  5900] loss: 131.356445\n",
      "..........\n",
      "[1,  6000] loss: 132.957656\n",
      "..........\n",
      "[1,  6100] loss: 130.437464\n",
      "..........\n",
      "[1,  6200] loss: 130.353857\n",
      "..........\n",
      "[1,  6300] loss: 130.625257\n",
      "..........\n",
      "[1,  6400] loss: 130.748603\n",
      "..........\n",
      "[1,  6500] loss: 131.115336\n",
      "..........\n",
      "[1,  6600] loss: 130.234199\n",
      "..........\n",
      "[1,  6700] loss: 132.333736\n",
      "..........\n",
      "[1,  6800] loss: 131.804197\n",
      "..........\n",
      "[1,  6900] loss: 127.999395\n",
      "..........\n",
      "[1,  7000] loss: 131.733071\n",
      "..........\n",
      "[1,  7100] loss: 129.861511\n",
      "..........\n",
      "[1,  7200] loss: 130.378609\n",
      "..........\n",
      "[1,  7300] loss: 131.517219\n",
      "..........\n",
      "[1,  7400] loss: 128.119735\n",
      "..........\n",
      "[1,  7500] loss: 130.489209\n",
      "..........\n",
      "[1,  7600] loss: 130.539133\n",
      "..........\n",
      "[1,  7700] loss: 133.023890\n",
      "..........\n",
      "[1,  7800] loss: 132.328018\n",
      "..........\n",
      "[1,  7900] loss: 129.578151\n",
      "..........\n",
      "[1,  8000] loss: 129.402895\n",
      "total average loss : 130.568\n",
      "train acc : 0.0977\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.102\n",
      "step : 1000 / 3125 acc : 0.113\n",
      "step : 1200 / 3125 acc : 0.109\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.121\n",
      "step : 1800 / 3125 acc : 0.118\n",
      "step : 2000 / 3125 acc : 0.116\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.120\n",
      "step : 2600 / 3125 acc : 0.115\n",
      "step : 2800 / 3125 acc : 0.118\n",
      "step : 3000 / 3125 acc : 0.119\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1180 %\n",
      "======eval  end ======\n",
      "======= epoch 14 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 130.701648\n",
      "..........\n",
      "[1,   200] loss: 131.093586\n",
      "..........\n",
      "[1,   300] loss: 133.547169\n",
      "..........\n",
      "[1,   400] loss: 131.799216\n",
      "..........\n",
      "[1,   500] loss: 131.715362\n",
      "..........\n",
      "[1,   600] loss: 131.983171\n",
      "..........\n",
      "[1,   700] loss: 133.076836\n",
      "..........\n",
      "[1,   800] loss: 132.476729\n",
      "..........\n",
      "[1,   900] loss: 132.556503\n",
      "..........\n",
      "[1,  1000] loss: 131.639777\n",
      "..........\n",
      "[1,  1100] loss: 133.683877\n",
      "..........\n",
      "[1,  1200] loss: 132.537896\n",
      "..........\n",
      "[1,  1300] loss: 132.854492\n",
      "..........\n",
      "[1,  1400] loss: 133.598646\n",
      "..........\n",
      "[1,  1500] loss: 130.156197\n",
      "..........\n",
      "[1,  1600] loss: 130.265947\n",
      "..........\n",
      "[1,  1700] loss: 133.287997\n",
      "..........\n",
      "[1,  1800] loss: 130.612565\n",
      "..........\n",
      "[1,  1900] loss: 133.678282\n",
      "..........\n",
      "[1,  2000] loss: 131.496144\n",
      "..........\n",
      "[1,  2100] loss: 131.244180\n",
      "..........\n",
      "[1,  2200] loss: 132.871022\n",
      "..........\n",
      "[1,  2300] loss: 132.199567\n",
      "..........\n",
      "[1,  2400] loss: 130.150842\n",
      "..........\n",
      "[1,  2500] loss: 130.214917\n",
      "..........\n",
      "[1,  2600] loss: 132.908367\n",
      "..........\n",
      "[1,  2700] loss: 133.137553\n",
      "..........\n",
      "[1,  2800] loss: 133.562455\n",
      "..........\n",
      "[1,  2900] loss: 131.542126\n",
      "..........\n",
      "[1,  3000] loss: 132.808003\n",
      "..........\n",
      "[1,  3100] loss: 131.904932\n",
      "..........\n",
      "[1,  3200] loss: 132.033937\n",
      "..........\n",
      "[1,  3300] loss: 131.810429\n",
      "..........\n",
      "[1,  3400] loss: 132.836131\n",
      "..........\n",
      "[1,  3500] loss: 133.160265\n",
      "..........\n",
      "[1,  3600] loss: 132.488325\n",
      "..........\n",
      "[1,  3700] loss: 131.635370\n",
      "..........\n",
      "[1,  3800] loss: 130.762335\n",
      "..........\n",
      "[1,  3900] loss: 131.662896\n",
      "..........\n",
      "[1,  4000] loss: 131.406049\n",
      "..........\n",
      "[1,  4100] loss: 134.412660\n",
      "..........\n",
      "[1,  4200] loss: 134.578952\n",
      "..........\n",
      "[1,  4300] loss: 131.390372\n",
      "..........\n",
      "[1,  4400] loss: 133.881078\n",
      "..........\n",
      "[1,  4500] loss: 135.548591\n",
      "..........\n",
      "[1,  4600] loss: 134.414155\n",
      "..........\n",
      "[1,  4700] loss: 135.261391\n",
      "..........\n",
      "[1,  4800] loss: 133.649894\n",
      "..........\n",
      "[1,  4900] loss: 131.928287\n",
      "..........\n",
      "[1,  5000] loss: 134.659128\n",
      "..........\n",
      "[1,  5100] loss: 134.552606\n",
      "..........\n",
      "[1,  5200] loss: 135.428583\n",
      "..........\n",
      "[1,  5300] loss: 134.766392\n",
      "..........\n",
      "[1,  5400] loss: 130.828108\n",
      "..........\n",
      "[1,  5500] loss: 131.961946\n",
      "..........\n",
      "[1,  5600] loss: 133.409826\n",
      "..........\n",
      "[1,  5700] loss: 131.655828\n",
      "..........\n",
      "[1,  5800] loss: 133.795590\n",
      "..........\n",
      "[1,  5900] loss: 134.805336\n",
      "..........\n",
      "[1,  6000] loss: 132.381890\n",
      "..........\n",
      "[1,  6100] loss: 133.079149\n",
      "..........\n",
      "[1,  6200] loss: 134.721571\n",
      "..........\n",
      "[1,  6300] loss: 133.663783\n",
      "..........\n",
      "[1,  6400] loss: 131.721586\n",
      "..........\n",
      "[1,  6500] loss: 131.336786\n",
      "..........\n",
      "[1,  6600] loss: 134.107625\n",
      "..........\n",
      "[1,  6700] loss: 133.243053\n",
      "..........\n",
      "[1,  6800] loss: 134.039418\n",
      "..........\n",
      "[1,  6900] loss: 134.745032\n",
      "..........\n",
      "[1,  7000] loss: 135.719903\n",
      "..........\n",
      "[1,  7100] loss: 133.113551\n",
      "..........\n",
      "[1,  7200] loss: 134.697043\n",
      "..........\n",
      "[1,  7300] loss: 133.750909\n",
      "..........\n",
      "[1,  7400] loss: 132.623910\n",
      "..........\n",
      "[1,  7500] loss: 132.507679\n",
      "..........\n",
      "[1,  7600] loss: 131.698669\n",
      "..........\n",
      "[1,  7700] loss: 132.346946\n",
      "..........\n",
      "[1,  7800] loss: 134.661455\n",
      "..........\n",
      "[1,  7900] loss: 133.089055\n",
      "..........\n",
      "[1,  8000] loss: 135.020407\n",
      "total average loss : 132.829\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.062\n",
      "step : 400 / 3125 acc : 0.141\n",
      "step : 600 / 3125 acc : 0.146\n",
      "step : 800 / 3125 acc : 0.148\n",
      "step : 1000 / 3125 acc : 0.144\n",
      "step : 1200 / 3125 acc : 0.146\n",
      "step : 1400 / 3125 acc : 0.129\n",
      "step : 1600 / 3125 acc : 0.137\n",
      "step : 1800 / 3125 acc : 0.135\n",
      "step : 2000 / 3125 acc : 0.128\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.120\n",
      "step : 2600 / 3125 acc : 0.118\n",
      "step : 2800 / 3125 acc : 0.114\n",
      "step : 3000 / 3125 acc : 0.117\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1200 %\n",
      "======eval  end ======\n",
      "error(128~256) inserted training\n",
      "..........\n",
      "[1,   100] loss: 135.951196\n",
      "..........\n",
      "[1,   200] loss: 133.247986\n",
      "..........\n",
      "[1,   300] loss: 133.488871\n",
      "..........\n",
      "[1,   400] loss: 134.333726\n",
      "..........\n",
      "[1,   500] loss: 135.550199\n",
      "..........\n",
      "[1,   600] loss: 134.785143\n",
      "..........\n",
      "[1,   700] loss: 135.064070\n",
      "..........\n",
      "[1,   800] loss: 133.026357\n",
      "..........\n",
      "[1,   900] loss: 132.820245\n",
      "..........\n",
      "[1,  1000] loss: 133.379084\n",
      "..........\n",
      "[1,  1100] loss: 134.652974\n",
      "..........\n",
      "[1,  1200] loss: 135.703503\n",
      "..........\n",
      "[1,  1300] loss: 136.593711\n",
      "..........\n",
      "[1,  1400] loss: 134.530046\n",
      "..........\n",
      "[1,  1500] loss: 132.003290\n",
      "..........\n",
      "[1,  1600] loss: 135.379045\n",
      "..........\n",
      "[1,  1700] loss: 133.683515\n",
      "..........\n",
      "[1,  1800] loss: 136.266088\n",
      "..........\n",
      "[1,  1900] loss: 132.181864\n",
      "..........\n",
      "[1,  2000] loss: 134.512583\n",
      "..........\n",
      "[1,  2100] loss: 133.628540\n",
      "..........\n",
      "[1,  2200] loss: 134.958879\n",
      "..........\n",
      "[1,  2300] loss: 133.329911\n",
      "..........\n",
      "[1,  2400] loss: 134.200511\n",
      "..........\n",
      "[1,  2500] loss: 136.343367\n",
      "..........\n",
      "[1,  2600] loss: 134.264649\n",
      "..........\n",
      "[1,  2700] loss: 135.057379\n",
      "..........\n",
      "[1,  2800] loss: 136.194680\n",
      "..........\n",
      "[1,  2900] loss: 134.282559\n",
      "..........\n",
      "[1,  3000] loss: 135.247876\n",
      "..........\n",
      "[1,  3100] loss: 135.299808\n",
      "..........\n",
      "[1,  3200] loss: 134.997699\n",
      "..........\n",
      "[1,  3300] loss: 135.242812\n",
      "..........\n",
      "[1,  3400] loss: 135.701843\n",
      "..........\n",
      "[1,  3500] loss: 132.694391\n",
      "..........\n",
      "[1,  3600] loss: 137.500157\n",
      "..........\n",
      "[1,  3700] loss: 134.064483\n",
      "..........\n",
      "[1,  3800] loss: 135.797457\n",
      "..........\n",
      "[1,  3900] loss: 134.337116\n",
      "..........\n",
      "[1,  4000] loss: 134.908753\n",
      "..........\n",
      "[1,  4100] loss: 134.554182\n",
      "..........\n",
      "[1,  4200] loss: 135.687620\n",
      "..........\n",
      "[1,  4300] loss: 136.056898\n",
      "..........\n",
      "[1,  4400] loss: 135.719632\n",
      "..........\n",
      "[1,  4500] loss: 134.992201\n",
      "..........\n",
      "[1,  4600] loss: 137.010581\n",
      "..........\n",
      "[1,  4700] loss: 135.276417\n",
      "..........\n",
      "[1,  4800] loss: 134.707339\n",
      "..........\n",
      "[1,  4900] loss: 137.938129\n",
      "..........\n",
      "[1,  5000] loss: 135.460792\n",
      "..........\n",
      "[1,  5100] loss: 135.838949\n",
      "..........\n",
      "[1,  5200] loss: 133.234367\n",
      "..........\n",
      "[1,  5300] loss: 136.915701\n",
      "..........\n",
      "[1,  5400] loss: 135.285103\n",
      "..........\n",
      "[1,  5500] loss: 134.498557\n",
      "..........\n",
      "[1,  5600] loss: 134.901627\n",
      "..........\n",
      "[1,  5700] loss: 133.414984\n",
      "..........\n",
      "[1,  5800] loss: 138.552932\n",
      "..........\n",
      "[1,  5900] loss: 136.585813\n",
      "..........\n",
      "[1,  6000] loss: 134.102753\n",
      "..........\n",
      "[1,  6100] loss: 133.769395\n",
      "..........\n",
      "[1,  6200] loss: 135.957515\n",
      "..........\n",
      "[1,  6300] loss: 133.080375\n",
      "..........\n",
      "[1,  6400] loss: 137.099377\n",
      "..........\n",
      "[1,  6500] loss: 135.334397\n",
      "..........\n",
      "[1,  6600] loss: 135.284605\n",
      "..........\n",
      "[1,  6700] loss: 136.182008\n",
      "..........\n",
      "[1,  6800] loss: 136.817594\n",
      "..........\n",
      "[1,  6900] loss: 136.889655\n",
      "..........\n",
      "[1,  7000] loss: 135.805076\n",
      "..........\n",
      "[1,  7100] loss: 134.919706\n",
      "..........\n",
      "[1,  7200] loss: 134.548767\n",
      "..........\n",
      "[1,  7300] loss: 136.216790\n",
      "..........\n",
      "[1,  7400] loss: 132.256269\n",
      "..........\n",
      "[1,  7500] loss: 136.264456\n",
      "..........\n",
      "[1,  7600] loss: 136.685093\n",
      "..........\n",
      "[1,  7700] loss: 135.263708\n",
      "..........\n",
      "[1,  7800] loss: 135.949989\n",
      "..........\n",
      "[1,  7900] loss: 134.689421\n",
      "..........\n",
      "[1,  8000] loss: 135.626660\n",
      "total average loss : 135.057\n",
      "train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.094\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.125\n",
      "step : 1000 / 3125 acc : 0.138\n",
      "step : 1200 / 3125 acc : 0.130\n",
      "step : 1400 / 3125 acc : 0.112\n",
      "step : 1600 / 3125 acc : 0.102\n",
      "step : 1800 / 3125 acc : 0.108\n",
      "step : 2000 / 3125 acc : 0.122\n",
      "step : 2200 / 3125 acc : 0.119\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.127\n",
      "step : 2800 / 3125 acc : 0.134\n",
      "step : 3000 / 3125 acc : 0.127\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1220 %\n",
      "======eval  end ======\n",
      "error(256~384) inserted training\n",
      "..........\n",
      "[1,   100] loss: 136.087027\n",
      "..........\n",
      "[1,   200] loss: 135.270659\n",
      "..........\n",
      "[1,   300] loss: 138.056914\n",
      "..........\n",
      "[1,   400] loss: 136.306335\n",
      "..........\n",
      "[1,   500] loss: 137.061513\n",
      "..........\n",
      "[1,   600] loss: 135.267604\n",
      "..........\n",
      "[1,   700] loss: 136.549334\n",
      "..........\n",
      "[1,   800] loss: 137.324301\n",
      "..........\n",
      "[1,   900] loss: 135.533435\n",
      "..........\n",
      "[1,  1000] loss: 134.878132\n",
      "..........\n",
      "[1,  1100] loss: 135.797550\n",
      "..........\n",
      "[1,  1200] loss: 137.289206\n",
      "..........\n",
      "[1,  1300] loss: 138.072915\n",
      "..........\n",
      "[1,  1400] loss: 136.371544\n",
      "..........\n",
      "[1,  1500] loss: 135.378187\n",
      "..........\n",
      "[1,  1600] loss: 137.409754\n",
      "..........\n",
      "[1,  1700] loss: 138.798039\n",
      "..........\n",
      "[1,  1800] loss: 136.541187\n",
      "..........\n",
      "[1,  1900] loss: 137.327039\n",
      "..........\n",
      "[1,  2000] loss: 137.249294\n",
      "..........\n",
      "[1,  2100] loss: 136.465978\n",
      "..........\n",
      "[1,  2200] loss: 137.529594\n",
      "..........\n",
      "[1,  2300] loss: 136.738872\n",
      "..........\n",
      "[1,  2400] loss: 135.982228\n",
      "..........\n",
      "[1,  2500] loss: 136.154562\n",
      "..........\n",
      "[1,  2600] loss: 140.259115\n",
      "..........\n",
      "[1,  2700] loss: 139.076900\n",
      "..........\n",
      "[1,  2800] loss: 137.035444\n",
      "..........\n",
      "[1,  2900] loss: 136.902612\n",
      "..........\n",
      "[1,  3000] loss: 136.334201\n",
      "..........\n",
      "[1,  3100] loss: 135.698190\n",
      "..........\n",
      "[1,  3200] loss: 136.727683\n",
      "..........\n",
      "[1,  3300] loss: 139.038500\n",
      "..........\n",
      "[1,  3400] loss: 138.338133\n",
      "..........\n",
      "[1,  3500] loss: 136.904574\n",
      "..........\n",
      "[1,  3600] loss: 137.264361\n",
      "..........\n",
      "[1,  3700] loss: 136.762275\n",
      "..........\n",
      "[1,  3800] loss: 139.272734\n",
      "..........\n",
      "[1,  3900] loss: 134.172649\n",
      "..........\n",
      "[1,  4000] loss: 139.251581\n",
      "..........\n",
      "[1,  4100] loss: 136.496059\n",
      "..........\n",
      "[1,  4200] loss: 136.653456\n",
      "..........\n",
      "[1,  4300] loss: 136.770878\n",
      "..........\n",
      "[1,  4400] loss: 136.931655\n",
      "..........\n",
      "[1,  4500] loss: 134.527461\n",
      "..........\n",
      "[1,  4600] loss: 138.582563\n",
      "..........\n",
      "[1,  4700] loss: 136.277054\n",
      "..........\n",
      "[1,  4800] loss: 136.171505\n",
      "..........\n",
      "[1,  4900] loss: 137.787050\n",
      "..........\n",
      "[1,  5000] loss: 135.864420\n",
      "..........\n",
      "[1,  5100] loss: 137.337549\n",
      "..........\n",
      "[1,  5200] loss: 139.305896\n",
      "..........\n",
      "[1,  5300] loss: 138.524625\n",
      "..........\n",
      "[1,  5400] loss: 136.680269\n",
      "..........\n",
      "[1,  5500] loss: 136.794456\n",
      "..........\n",
      "[1,  5600] loss: 138.556930\n",
      "..........\n",
      "[1,  5700] loss: 138.662377\n",
      "..........\n",
      "[1,  5800] loss: 138.394850\n",
      "..........\n",
      "[1,  5900] loss: 138.040671\n",
      "..........\n",
      "[1,  6000] loss: 136.920268\n",
      "..........\n",
      "[1,  6100] loss: 138.460525\n",
      "..........\n",
      "[1,  6200] loss: 138.800327\n",
      "..........\n",
      "[1,  6300] loss: 135.433715\n",
      "..........\n",
      "[1,  6400] loss: 135.480669\n",
      "..........\n",
      "[1,  6500] loss: 138.303281\n",
      "..........\n",
      "[1,  6600] loss: 137.043112\n",
      "..........\n",
      "[1,  6700] loss: 139.385168\n",
      "..........\n",
      "[1,  6800] loss: 137.875826\n",
      "..........\n",
      "[1,  6900] loss: 136.820644\n",
      "..........\n",
      "[1,  7000] loss: 135.352400\n",
      "..........\n",
      "[1,  7100] loss: 138.733471\n",
      "..........\n",
      "[1,  7200] loss: 138.472472\n",
      "..........\n",
      "[1,  7300] loss: 139.379990\n",
      "..........\n",
      "[1,  7400] loss: 140.220095\n",
      "..........\n",
      "[1,  7500] loss: 137.128877\n",
      "..........\n",
      "[1,  7600] loss: 139.603106\n",
      "..........\n",
      "[1,  7700] loss: 136.045091\n",
      "..........\n",
      "[1,  7800] loss: 138.010863\n",
      "..........\n",
      "[1,  7900] loss: 137.850533\n",
      "..........\n",
      "[1,  8000] loss: 137.200235\n",
      "total average loss : 137.242\n",
      "train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.031\n",
      "step : 400 / 3125 acc : 0.078\n",
      "step : 600 / 3125 acc : 0.083\n",
      "step : 800 / 3125 acc : 0.094\n",
      "step : 1000 / 3125 acc : 0.100\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.121\n",
      "step : 1600 / 3125 acc : 0.109\n",
      "step : 1800 / 3125 acc : 0.111\n",
      "step : 2000 / 3125 acc : 0.119\n",
      "step : 2200 / 3125 acc : 0.114\n",
      "step : 2400 / 3125 acc : 0.115\n",
      "step : 2600 / 3125 acc : 0.120\n",
      "step : 2800 / 3125 acc : 0.118\n",
      "step : 3000 / 3125 acc : 0.123\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1220 %\n",
      "======eval  end ======\n",
      "error(384~512) inserted training\n",
      "..........\n",
      "[1,   100] loss: 138.908338\n",
      "..........\n",
      "[1,   200] loss: 137.430716\n",
      "..........\n",
      "[1,   300] loss: 137.261163\n",
      "..........\n",
      "[1,   400] loss: 136.292999\n",
      "..........\n",
      "[1,   500] loss: 140.588873\n",
      "..........\n",
      "[1,   600] loss: 138.290711\n",
      "..........\n",
      "[1,   700] loss: 137.204630\n",
      "..........\n",
      "[1,   800] loss: 141.252315\n",
      "..........\n",
      "[1,   900] loss: 139.015046\n",
      "..........\n",
      "[1,  1000] loss: 137.618166\n",
      "..........\n",
      "[1,  1100] loss: 139.192027\n",
      "..........\n",
      "[1,  1200] loss: 142.527713\n",
      "..........\n",
      "[1,  1300] loss: 137.669519\n",
      "..........\n",
      "[1,  1400] loss: 139.254229\n",
      "..........\n",
      "[1,  1500] loss: 139.515935\n",
      "..........\n",
      "[1,  1600] loss: 138.028937\n",
      "..........\n",
      "[1,  1700] loss: 138.456150\n",
      "..........\n",
      "[1,  1800] loss: 139.285062\n",
      "..........\n",
      "[1,  1900] loss: 138.884945\n",
      "..........\n",
      "[1,  2000] loss: 139.169972\n",
      "..........\n",
      "[1,  2100] loss: 140.955627\n",
      "..........\n",
      "[1,  2200] loss: 136.913145\n",
      "..........\n",
      "[1,  2300] loss: 139.346093\n",
      "..........\n",
      "[1,  2400] loss: 137.613237\n",
      "..........\n",
      "[1,  2500] loss: 140.344319\n",
      "..........\n",
      "[1,  2600] loss: 140.265782\n",
      "..........\n",
      "[1,  2700] loss: 139.562272\n",
      "..........\n",
      "[1,  2800] loss: 140.886237\n",
      "..........\n",
      "[1,  2900] loss: 141.112458\n",
      "..........\n",
      "[1,  3000] loss: 138.043595\n",
      "..........\n",
      "[1,  3100] loss: 139.123631\n",
      "..........\n",
      "[1,  3200] loss: 137.086730\n",
      "..........\n",
      "[1,  3300] loss: 140.454242\n",
      "..........\n",
      "[1,  3400] loss: 138.236798\n",
      "..........\n",
      "[1,  3500] loss: 138.717021\n",
      "..........\n",
      "[1,  3600] loss: 140.921425\n",
      "..........\n",
      "[1,  3700] loss: 139.604038\n",
      "..........\n",
      "[1,  3800] loss: 139.752074\n",
      "..........\n",
      "[1,  3900] loss: 139.570718\n",
      "..........\n",
      "[1,  4000] loss: 137.565017\n",
      "..........\n",
      "[1,  4100] loss: 140.840380\n",
      "..........\n",
      "[1,  4200] loss: 137.225675\n",
      "..........\n",
      "[1,  4300] loss: 139.143771\n",
      "..........\n",
      "[1,  4400] loss: 137.807616\n",
      "..........\n",
      "[1,  4500] loss: 139.794922\n",
      "..........\n",
      "[1,  4600] loss: 139.433935\n",
      "..........\n",
      "[1,  4700] loss: 139.884494\n",
      "..........\n",
      "[1,  4800] loss: 137.871691\n",
      "..........\n",
      "[1,  4900] loss: 139.159130\n",
      "..........\n",
      "[1,  5000] loss: 139.188353\n",
      "..........\n",
      "[1,  5100] loss: 139.133408\n",
      "..........\n",
      "[1,  5200] loss: 138.627944\n",
      "..........\n",
      "[1,  5300] loss: 140.564289\n",
      "..........\n",
      "[1,  5400] loss: 138.888167\n",
      "..........\n",
      "[1,  5500] loss: 141.910245\n",
      "..........\n",
      "[1,  5600] loss: 140.767305\n",
      "..........\n",
      "[1,  5700] loss: 138.075936\n",
      "..........\n",
      "[1,  5800] loss: 139.650726\n",
      "..........\n",
      "[1,  5900] loss: 140.172949\n",
      "..........\n",
      "[1,  6000] loss: 140.454179\n",
      "..........\n",
      "[1,  6100] loss: 139.028111\n",
      "..........\n",
      "[1,  6200] loss: 139.271264\n",
      "..........\n",
      "[1,  6300] loss: 138.640914\n",
      "..........\n",
      "[1,  6400] loss: 136.421794\n",
      "..........\n",
      "[1,  6500] loss: 140.494972\n",
      "..........\n",
      "[1,  6600] loss: 137.558758\n",
      "..........\n",
      "[1,  6700] loss: 138.818589\n",
      "..........\n",
      "[1,  6800] loss: 139.205998\n",
      "..........\n",
      "[1,  6900] loss: 139.149894\n",
      "..........\n",
      "[1,  7000] loss: 138.787774\n",
      "..........\n",
      "[1,  7100] loss: 139.492234\n",
      "..........\n",
      "[1,  7200] loss: 141.158315\n",
      "..........\n",
      "[1,  7300] loss: 141.497561\n",
      "..........\n",
      "[1,  7400] loss: 140.962093\n",
      "..........\n",
      "[1,  7500] loss: 141.481526\n",
      "..........\n",
      "[1,  7600] loss: 139.702176\n",
      "..........\n",
      "[1,  7700] loss: 139.579870\n",
      "..........\n",
      "[1,  7800] loss: 142.455357\n",
      "..........\n",
      "[1,  7900] loss: 139.143424\n",
      "..........\n",
      "[1,  8000] loss: 140.582512\n",
      "total average loss : 139.299\n",
      "train acc : 0.0914\n",
      "======eval start=======\n",
      "step : 200 / 3125 acc : 0.156\n",
      "step : 400 / 3125 acc : 0.109\n",
      "step : 600 / 3125 acc : 0.104\n",
      "step : 800 / 3125 acc : 0.141\n",
      "step : 1000 / 3125 acc : 0.125\n",
      "step : 1200 / 3125 acc : 0.115\n",
      "step : 1400 / 3125 acc : 0.116\n",
      "step : 1600 / 3125 acc : 0.117\n",
      "step : 1800 / 3125 acc : 0.128\n",
      "step : 2000 / 3125 acc : 0.125\n",
      "step : 2200 / 3125 acc : 0.122\n",
      "step : 2400 / 3125 acc : 0.122\n",
      "step : 2600 / 3125 acc : 0.125\n",
      "step : 2800 / 3125 acc : 0.125\n",
      "step : 3000 / 3125 acc : 0.127\n",
      "\n",
      "0th epoch acc of Target_model on imagenet : 0.1220 %\n",
      "======eval  end ======\n",
      "======= epoch 15 =======\n",
      "error(0~128) inserted training\n",
      "..........\n",
      "[1,   100] loss: 141.134252\n",
      "..........\n",
      "[1,   200] loss: 137.379863\n",
      "..........\n",
      "[1,   300] loss: 142.367960\n",
      "..........\n",
      "[1,   400] loss: 139.264035\n",
      "..........\n",
      "[1,   500] loss: 140.838090\n",
      "..........\n",
      "[1,   600] loss: 139.419738\n",
      "..........\n",
      "[1,   700] loss: 143.485485\n",
      "..........\n",
      "[1,   800] loss: 138.757628\n",
      "..........\n",
      "[1,   900] loss: 140.078740\n",
      "..........\n",
      "[1,  1000] loss: 138.709523\n",
      "..........\n",
      "[1,  1100] loss: 139.951042\n",
      "..........\n",
      "[1,  1200] loss: 141.137714\n",
      "..........\n",
      "[1,  1300] loss: 139.448665\n",
      "..........\n",
      "[1,  1400] loss: 138.820680\n",
      "..........\n",
      "[1,  1500] loss: 142.442005\n",
      "..........\n",
      "[1,  1600] loss: 140.914431\n",
      "..........\n",
      "[1,  1700] loss: 141.948757\n",
      "..........\n",
      "[1,  1800] loss: 141.615741\n",
      "..........\n",
      "[1,  1900] loss: 139.575834\n",
      "..........\n",
      "[1,  2000] loss: 141.609472\n",
      "..........\n",
      "[1,  2100] loss: 140.884092\n",
      "..........\n",
      "[1,  2200] loss: 142.523406\n",
      "..........\n",
      "[1,  2300] loss: 139.669650\n",
      "..........\n",
      "[1,  2400] loss: 140.524945\n",
      "..........\n",
      "[1,  2500] loss: 138.863154\n",
      "..........\n",
      "[1,  2600] loss: 141.974843\n",
      "..........\n",
      "[1,  2700] loss: 141.059120\n",
      "..........\n",
      "[1,  2800] loss: 143.492760\n",
      "..........\n",
      "[1,  2900] loss: 140.996395\n",
      "..........\n",
      "[1,  3000] loss: 141.239445\n",
      "..........\n",
      "[1,  3100] loss: 140.933627\n",
      "..........\n",
      "[1,  3200] loss: 141.275325\n",
      "..........\n",
      "[1,  3300] loss: 141.328973\n",
      "..........\n",
      "[1,  3400] loss: 141.706145\n",
      "..........\n",
      "[1,  3500] loss: 143.388150\n",
      "..........\n",
      "[1,  3600] loss: 142.018579\n",
      "..........\n",
      "[1,  3700] loss: 141.291315\n",
      "..........\n",
      "[1,  3800] loss: 142.192320\n",
      "..........\n",
      "[1,  3900] loss: 139.980845\n",
      "..........\n",
      "[1,  4000] loss: 142.572719\n",
      "..........\n",
      "[1,  4100] loss: 141.484316\n",
      "..........\n",
      "[1,  4200] loss: 140.024422\n",
      "..........\n",
      "[1,  4300] loss: 143.156084\n",
      "..........\n",
      "[1,  4400] loss: 141.550301\n",
      "..........\n",
      "[1,  4500] loss: 140.788421\n",
      "...."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-129e67419b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                   \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                   \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                   error_idx,num_error,1,True)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfirst_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moriginal_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-49027496e88d>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(f4f, target_model, train_dataloader, test_dataloader, loss_fn, optimizer, error_idx, num_error, max_epochs, subset)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mf4f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf4f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/local_pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/local_pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#optimizer = torch.optim.SGD(param_list,lr=0.01,weight_decay=1e-4)\n",
    "first_feature = []\n",
    "original_out = []\n",
    "f = open(log_file,\"w\")\n",
    "f.close()\n",
    "num_error = 128\n",
    "max_epoch = 50\n",
    "for epoch in range(max_epoch):\n",
    "    print(\"======= epoch %2d =======\"%(epoch))\n",
    "    hook_register(vgg16_bn,error_index,num_error)\n",
    "    target_model = Target_model(vgg16_bn).to(device)\n",
    "    \n",
    "    for error_idx in range(0,512,num_error):\n",
    "        print(\"error(%d~%d) inserted training\"%(error_idx,error_idx+num_error))\n",
    "        tmp= training(f4f,target_model,\n",
    "                  train_dataloader,test_dataloader,\n",
    "                  loss_fn,optimizer,\n",
    "                  error_idx,num_error,1,True)\n",
    "    first_feature.append(tmp[1])\n",
    "    original_out.append(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 feature 그림 보기 위한 것들입니다.\n",
    "len(first_feature),len(original_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_out[0].size())\n",
    "w = 10\n",
    "h = 10\n",
    "cols = 32\n",
    "rows = 16\n",
    "def feature_print(pic):\n",
    "    print(\"test with 'after pooling 4 feature'\")\n",
    "    fig = plt.figure(figsize=(64,32))\n",
    "    ax = []\n",
    "    for i in range(cols*rows):\n",
    "        ch = pic[i,:,:]\n",
    "        ax.append(fig.add_subplot(rows,cols,i+1))\n",
    "        ax[-1].set_title(str(i)+\"th ch (14x14)\")\n",
    "        plt.imshow(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델 (에러없이, f4f없이)을 통과한 결과\n",
    "feature_print(original_out[0][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 1\n",
    "%matplotlib inline\n",
    "feature_print(first_feature[0][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 9\n",
    "print(\"epoch 9\")\n",
    "feature_print(first_feature[0][9].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14x14 의 feature 모두 합한 결과\n",
    "tmp = first_feature[0][6][0]\n",
    "for i in range(1,512):\n",
    "    tmp += first_feature[0][6][i]\n",
    "%matplotlib inline\n",
    "plt.imshow(tmp.cpu().detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14x14 의 feature 모두 합한 결과\n",
    "print(\"original\")\n",
    "tmp1 = original_out[0][6][0]\n",
    "for i in range(1,512):\n",
    "    tmp1 += original_out[0][6][i]\n",
    "%matplotlib inline\n",
    "plt.imshow(tmp1.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f4f을 통과한 결과  epoch 6\n",
    "print(\"epoch 6\")\n",
    "feature_print(first_feature[0][6].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "error_info = make_error_info(error_index,num_error).to(device)\n",
    "for feature,label in train_dataloader:\n",
    "    print(\"======================================\")\n",
    "    feature,label = feature.to(device),label.to(device)\n",
    "    print(\"dataloader data : \",feature.size(),label.size())\n",
    "    target_out = test_model(feature,f4f,error_info)\n",
    "    print(\"output :\",target_out.size())\n",
    "    original_out = original_model(feature)\n",
    "    if torch.equal(target_out,original_out) is False :\n",
    "        print(\"=====compare two output======\")\n",
    "        print(target_out[0][0][0][0])\n",
    "        print(original_out[0][0][0][0])\n",
    "    else :\n",
    "        print(\"same\")\n",
    "    loss = loss_fn(original_out,target_out)\n",
    "    running_loss += loss.item()\n",
    "    target_model.model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break\n",
    "#target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

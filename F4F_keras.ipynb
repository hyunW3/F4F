{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "\n",
    "model_path = \"/media/2/Network/pretrained_model/vgg_model.h5\"\n",
    "#model_path = \"/media/2/Network/pretrained_model/back_layers.h5\"\n",
    "#img_path = \"/media/2/Network/Imagenet_dup/val/n02074367\"# dugong\n",
    "data_path = \"/media/2/Network/Imagenet_dup/\"\n",
    "feature4_path = \"/media/2/Network/extracted_feature/whole_not_shuffle_to_15\"\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooling4 session\n"
     ]
    }
   ],
   "source": [
    "model=None\n",
    "def ready_model(model_path,layer):\n",
    "    model=load_model(model_path) # whole model\n",
    "    # pooling 4\n",
    "    til_pooling4_model=None\n",
    "    til_pooling4_predict = None\n",
    "    til_pooling5_model = None\n",
    "    til_pooling5_predict = None\n",
    "    if layer in 'pooling4':\n",
    "        print('pooling4 session')\n",
    "        #til_pooling4_model = Sequential([layer for layer in model.layers[:15]]) \n",
    "        #til_pooling4_model.build((None, 224,224,3))\n",
    "        til_pooling4_predict = Sequential([layer for layer in model.layers[15:]]) \n",
    "        til_pooling4_predict.build((None, 14,14,512))\n",
    "    # pooling5\n",
    "    elif layer in 'pooling5':\n",
    "        print('pooling5 session')\n",
    "        til_pooling5_model = Sequential([layer for layer in model.layers[:19]]) \n",
    "        til_pooling5_model.build((None, 224,224,3))\n",
    "        til_pooling5_predict = Sequential([layer for layer in model.layers[19:]]) \n",
    "        til_pooling5_predict.build((None, 7,7,512))\n",
    "    return til_pooling4_model,til_pooling4_predict, til_pooling5_model,til_pooling5_predict\n",
    "# load model & split pooling4 & pooling5\n",
    "#til_pooling4_model=None\n",
    "til_pooling4_predict=None\n",
    "'''\n",
    "til_pooling5_model=None\n",
    "til_pooling5_predict=None\n",
    "'''\n",
    "til_pooling4_model,til_pooling4_predict, til_pooling5_model,til_pooling5_predict = ready_model(model_path,'pooling4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original image\n",
    "#data_path = \"/media/2/Network/Imagenet_dup/\"\n",
    "def load_original_dataset(data_path):\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, horizontal_flip=True,\n",
    "                                                               validation_split=0.2)\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "    training_set = train_datagen.flow_from_directory(data_path+\"train\", target_size=(224,224),\n",
    "                                                batch_size =32,subset='training')\n",
    "    val_set = train_datagen.flow_from_directory(data_path+\"train\",target_size=(224,224),\n",
    "                                         batch_size=32, subset='validation')\n",
    "                                         \n",
    "                                         \n",
    "    test_set = val_datagen.flow_from_directory(data_path+\"val\",target_size=(224,224),\n",
    "                                         batch_size=32)\n",
    "    print(training_set[0][0][0].shape)\n",
    "    return training_set,val_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, '/media/2/Network/extracted_feature/whole_not_shuffle_to_15')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list =  os.listdir(feature4_path)\n",
    "feature_list = sorted(feature_list)\n",
    "len(feature_list),feature4_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_features_0.npy\t  training_features_24.npy  training_features_53.npy\n",
      "testing_features_1.npy\t  training_features_25.npy  training_features_54.npy\n",
      "testing_features_2.npy\t  training_features_26.npy  training_features_55.npy\n",
      "testing_features_3.npy\t  training_features_27.npy  training_features_56.npy\n",
      "testing_label.npy\t  training_features_28.npy  training_features_57.npy\n",
      "training_features_00.npy  training_features_29.npy  training_features_58.npy\n",
      "training_features_01.npy  training_features_30.npy  training_features_59.npy\n",
      "training_features_02.npy  training_features_31.npy  training_features_60.npy\n",
      "training_features_03.npy  training_features_32.npy  training_features_61.npy\n",
      "training_features_04.npy  training_features_33.npy  training_features_62.npy\n",
      "training_features_05.npy  training_features_34.npy  training_label.npy\n",
      "training_features_06.npy  training_features_35.npy  validation_features_00.npy\n",
      "training_features_07.npy  training_features_36.npy  validation_features_01.npy\n",
      "training_features_08.npy  training_features_37.npy  validation_features_02.npy\n",
      "training_features_09.npy  training_features_38.npy  validation_features_03.npy\n",
      "training_features_10.npy  training_features_39.npy  validation_features_04.npy\n",
      "training_features_11.npy  training_features_40.npy  validation_features_05.npy\n",
      "training_features_12.npy  training_features_41.npy  validation_features_06.npy\n",
      "training_features_13.npy  training_features_42.npy  validation_features_07.npy\n",
      "training_features_14.npy  training_features_43.npy  validation_features_08.npy\n",
      "training_features_15.npy  training_features_44.npy  validation_features_09.npy\n",
      "training_features_16.npy  training_features_45.npy  validation_features_10.npy\n",
      "training_features_17.npy  training_features_46.npy  validation_features_11.npy\n",
      "training_features_18.npy  training_features_47.npy  validation_features_12.npy\n",
      "training_features_19.npy  training_features_48.npy  validation_features_13.npy\n",
      "training_features_20.npy  training_features_49.npy  validation_features_14.npy\n",
      "training_features_21.npy  training_features_50.npy  validation_features_15.npy\n",
      "training_features_22.npy  training_features_51.npy  validation_label.npy\n",
      "training_features_23.npy  training_features_52.npy\n"
     ]
    }
   ],
   "source": [
    "!ls /media/2/Network/extracted_feature/whole_not_shuffle_to_15/seq_16_pkt_error\n",
    "error_feature_path = \"/media/2/Network/extracted_feature/whole_not_shuffle_to_15/seq_16_pkt_error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add error in feature -> deprecated since error feature is store in disk\n",
    "num_of_error = 16\n",
    "def error_injection(feature,num_of_error):\n",
    "    num_ch = feature.shape[3]\n",
    "    for i, data in enumerate(feature):\n",
    "        #print(i,data.shape)\n",
    "        start = (i*num_of_error) % num_ch\n",
    "        end = ((i+1)*num_of_error) % num_ch\n",
    "        data[:,:,start:end] = 0\n",
    "# classify feature\n",
    "def error_feature_store(feature_list,num_of_error,feature4_path,error_feature_path):\n",
    "    test_feature = []\n",
    "    test_label = []\n",
    "    train_feature = []\n",
    "    train_label =[]\n",
    "    val_feature = []\n",
    "    val_label =[]\n",
    "    for data in feature_list:\n",
    "    #print(feature4_path+\"/\"+data)\n",
    "        if 'train' in data:\n",
    "            if 'feature' in data:\n",
    "                tmp = np.load(feature4_path+\"/\"+data,mmap_mode='c')\n",
    "                error_injection(tmp,num_of_error)\n",
    "                np.save(error_feature_path+data,tmp)\n",
    "                tmp = None\n",
    "            else :\n",
    "                tmp = np.load(feature4_path+\"/\"+data,mmap_mode='c')\n",
    "                np.save(error_feature_path+data,tmp)\n",
    "                tmp = None\n",
    "       #print(\"train in\",data)\n",
    "        elif 'test' in data:\n",
    "        #print(\"test in\",data)\n",
    "            if 'feature' in data:\n",
    "                tmp = np.load(feature4_path+\"/\"+data,mmap_mode='c')\n",
    "                error_injection(tmp,num_of_error)\n",
    "                np.save(error_feature_path+data,tmp)\n",
    "                tmp = None\n",
    "            else:\n",
    "                tmp = np.load(feature4_path+\"/\"+data,mmap_mode='c')\n",
    "                np.save(error_feature_path+data,tmp)\n",
    "                tmp = None\n",
    "        elif 'validation' in data:\n",
    "        #print(\"val in\",data)\n",
    "            if 'feature' in data:\n",
    "                tmp = np.load(feature4_path+\"/\"+data,mmap_mode='c')\n",
    "                error_injection(tmp,num_of_error)\n",
    "                np.save(error_feature_path+data,tmp)\n",
    "                tmp = None\n",
    "            else:\n",
    "                tmp = np.load(feature4_path+\"/\"+data,mmap_mode='c')\n",
    "                np.save(error_feature_path+data,tmp)\n",
    "            tmp = None\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"error injection & store end\")\n",
    "# ex error_feature_store(feature_list,num_of_error,feature4_path,error_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list =  os.listdir(error_feature_path)\n",
    "feature_list = sorted(feature_list)\n",
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature from disk\n",
    "# classify feature\n",
    "test_feature = []\n",
    "test_label = []\n",
    "train_feature = []\n",
    "train_label =[]\n",
    "val_feature = []\n",
    "val_label =[]\n",
    "for data in feature_list:\n",
    "    #print(feature4_path+\"/\"+data)\n",
    "    if 'train' in data:\n",
    "        if 'feature' in data:\n",
    "            tmp = np.load(error_feature_path+\"/\"+data,mmap_mode='c')\n",
    "            train_feature.append(tmp)\n",
    "        else :\n",
    "            tmp = np.load(error_feature_path+\"/\"+data,mmap_mode='c')\n",
    "            train_label.append(tmp)\n",
    "       #print(\"train in\",data)\n",
    "    elif 'test' in data:\n",
    "        #print(\"test in\",data)\n",
    "        if 'feature' in data:\n",
    "            tmp = np.load(error_feature_path+\"/\"+data,mmap_mode='c')\n",
    "            test_feature.append(tmp)\n",
    "        else:\n",
    "            tmp = np.load(error_feature_path+\"/\"+data,mmap_mode='c')\n",
    "            test_label.append(tmp)\n",
    "    elif 'validation' in data:\n",
    "        #print(\"val in\",data)\n",
    "        if 'feature' in data:\n",
    "            tmp = np.load(error_feature_path+\"/\"+data,mmap_mode='c')\n",
    "            val_feature.append(tmp)\n",
    "        else:\n",
    "            tmp = np.load(error_feature_path+\"/\"+data,mmap_mode='c')\n",
    "            val_label.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/2/Network/extracted_feature/whole_not_shuffle_to_15'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature4_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100352)            1007062425\n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 10,201,346,536\n",
      "Trainable params: 10,070,624,256\n",
      "Non-trainable params: 130,722,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "back_layer = tf.keras.Sequential()\n",
    "back_layer.add(tf.keras.layers.Flatten(name='flatten1'))\n",
    "fc_layer = tf.keras.layers.Dense(14*14*512,input_dim=14*14*512,activation='tanh')\n",
    "back_layer.add(fc_layer)\n",
    "back_layer.add(tf.keras.layers.Reshape((14,14,512)))\n",
    "for layer in til_pooling4_predict.layers[:]: # til_pooling4_predict.layers[1:]\n",
    "    layer.trainable = False\n",
    "    back_layer.add(layer)\n",
    "\n",
    "#back_layer = tf.keras.layers.Concatenate()(fc_layer,til_pooling4_predict)\n",
    "# model  compile\n",
    "def scheduler(epoch,lr):\n",
    "    if epoch < 15:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*tf.math.exp(-0.1)\n",
    "MODEL_SAVE_FOLDER_PATH = './models'\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "model_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "cb_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=model_path, \n",
    "                               monitor='val_loss', verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4, decay=0)\n",
    "#til_pooling4_predict.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "back_layer.build((None,14,14,512))\n",
    "back_layer.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "back_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "max_epoch = 30\n",
    "feature_len = len(val_feature[0])\n",
    "val_feature_len = len(val_feature)\n",
    "for epoch in range(1,max_epoch+1):\n",
    "    print(\"epoch :\",epoch)\n",
    "    for i, feature in enumerate(train_feature):\n",
    "        print(feature.shape,val_feature[i].shape)\n",
    "        start = i*feature_len\n",
    "        end  = (i+1)*feature_len\n",
    "        y = back_layer.fit(feature,train_label[0][start:end],\n",
    "                                 batch_size=32,epochs=1,\n",
    "                                 validation_data=(val_feature[i%val_feature_len],val_label[0][start:end]),\n",
    "                                   callbacks=[cb_checkpoint],verbose=1,use_multiprocessing=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F4F(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_ouputs):\n",
    "        super(F4F, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간 단계에서 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===INFO===\n",
      "torch ver : 1.7.1\n",
      "torchvision ver : 0.8.2 \n",
      "GPU model : TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy as d_copy\n",
    "import random\n",
    "from condconv import CondConv2D\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "print(\"===INFO===\")\n",
    "print(\"torch ver : %s\\ntorchvision ver : %s \" %(torch.__version__, torchvision.__version__))\n",
    "print(\"GPU model :\",torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter(\"./runs/6fc_model_5e-1_offset\")\n",
    "log_file = \"./6fc_64_acc_log_0915.txt\"\n",
    "#error_index = 0\n",
    "vgg16_bn = torchvision.models.vgg16_bn(pretrained=True)#.to(device)\n",
    "vgg16_bn.eval()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomness 제어 \n",
    "# https://hoya012.github.io/blog/reproducible_pytorch/\n",
    "def set_randomness(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "# func\n",
    "\n",
    "# only apply for feature part (not pooling, classfier)\n",
    "# because of layers.feature \n",
    "def split_layer(model,start,end):\n",
    "    ct = 0\n",
    "    split_model=[] # from start to Conv5_1(include ReLU)\n",
    "    for name,layers in model.named_modules():\n",
    "        #print(name,layer)\n",
    "        #print(layers.features)\n",
    "        for idx,layer in enumerate(layers.features):\n",
    "            #print(idx,layer)\n",
    "            if start <=idx and idx <=end :\n",
    "                split_model.append(layer)\n",
    "        break\n",
    "    return nn.Sequential(*split_model)\n",
    "\n",
    "seed = 0\n",
    "set_randomness(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset[1000], test dataset[391] are loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_dataset(num_train,batch_size,\n",
    "                dataset_path,retrain_model_path):\n",
    "    if os.path.isdir(retrain_model_path) is False:\n",
    "        # make folder\n",
    "        os.mkdir(retrain_model_path)\n",
    "        print(\"retrain model path created :\",os.listdir(retrain_model_path+\"../\"))\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    transforms_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    transforms_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    train_dataset = dataset.ImageFolder(root=dataset_path+\"train\",\n",
    "                                       transform=transforms_train)\n",
    "    subset_train_dataset,_ = torch.utils.data.random_split(train_dataset, \n",
    "                                        [num_train,len(train_dataset)-num_train])\n",
    "    test_dataset = dataset.ImageFolder(root=dataset_path+\"val\",\n",
    "                                       transform=transforms_test)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(subset_train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4) # for using subset\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4)\n",
    "    print(\"train dataset[%d], test dataset[%d] are loaded\"%(len(train_dataloader),len(test_dataloader)))\n",
    "    return train_dataloader,test_dataloader\n",
    "\n",
    "\n",
    "dataset_path = \"/media/2/Network/Imagenet_dup/\"\n",
    "retrain_model_path = \"/media/0/Network/0821_to_fullmodels/\"\n",
    "batch_size = 64 # 32~ out of memory in 3080\n",
    "num_train = 128000 #640000\n",
    "\n",
    "train_dataloader,test_dataloader = get_dataset(num_train,batch_size,\n",
    "                  dataset_path,retrain_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external variable in error_index, num_error\n",
    "class F4F(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.f4f = nn.Linear(3*3*512+512,3*3*512) # 5120,4608 filter which change feature.34 (Conv5_1)\n",
    "        self.layer1 = nn.Linear(3*3*3*512+512, 128,bias=True)\n",
    "        self.layer2 = nn.Linear(5120, 5120,bias=True) #3 3 512 + 512 -> -> 3 3 512\n",
    "        self.layer3 = nn.Linear(5120, 5120,bias=True)\n",
    "        self.layer4 = nn.Linear(5120, 5120,bias=True)\n",
    "        self.layer5 = nn.Linear(5120, 5120,bias=True)\n",
    "        self.layer6 = nn.Linear(5120, 4608,bias=True)\n",
    "  \n",
    "        # 512 x5120 사이즈로 batch 저장\n",
    "    def get_f4f_weight(self):\n",
    "        # fc.weight.size(),fc.bias.size()\n",
    "        return self.parameters # torch.Size([4608, 5120])\n",
    "    def forward(self,x):\n",
    "        x1_ = self.layer1(x)\n",
    "        x1  = torch.relu(x1_) # \"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
    "        \n",
    "        x2_ = self.layer2(x1)\n",
    "        x2  = torch.relu(x2_)\n",
    "        \n",
    "        x3_ = self.layer3(x2)\n",
    "        x3   = torch.relu(x3_)\n",
    "        \n",
    "        x4_ = self.layer4(x3)\n",
    "        x4  = torch.relu(x4_)\n",
    "        \n",
    "        x5_ = self.layer5(x4)\n",
    "        x5  = torch.relu(x5_)\n",
    "        \n",
    "        x6_ = self.layer6(x5)\n",
    "        x6  = torch.tanh(x6_)\n",
    "            \n",
    "        y = x6\n",
    "        return y\n",
    "def make_error_info(error_index, num_error):\n",
    "    error_info = torch.Tensor().new_empty((512))\n",
    "    \n",
    "    error_info[:error_index] = 0 #torch.zeros((error_index))\n",
    "    error_info[error_index:error_index+num_error] = 1 #torch.ones((num_error))\n",
    "    error_info[num_error+error_index:] = 0 #torch.zeros((512-(num_error+error_index)))\n",
    "    \n",
    "    error_info  = error_info.unsqueeze(0).repeat(512,1)\n",
    "    #print(\"error_info :\",error_info)\n",
    "    return error_info # 512,521        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_index=0\n",
    "num_error = 64\n",
    "f4f = F4F().to(device)\n",
    "optimizer = torch.optim.SGD(f4f.parameters(),lr=5e-1,weight_decay=1e-4)\n",
    "if torch.cuda.device_count() >1 :\n",
    "    print(\"data parallel start\")\n",
    "    f4f = nn.DataParallel(f4f).to(device)\n",
    "def loss_fn(loss1_ratio,pred,label, filter_orig,filter_f4f):\n",
    "    if(loss1_ratio <0 or loss1_ratio >1 ):\n",
    "        print(\"wrong parameter ratio \",loss1_ratio)\n",
    "        return nan\n",
    "    loss1 = nn.CrossEntropyLoss()\n",
    "    loss2 = nn.MSELoss()\n",
    "    a = loss1(pred,label)\n",
    "    b = loss2(filter_orig,filter_f4f)\n",
    "    return loss1_ratio * a + (1-loss1_ratio) * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def error_injection(name,num_error,error_index):\n",
    "    def hook(model,input):\n",
    "        start = error_index\n",
    "        end = error_index + num_error\n",
    "        #print(input.shape) #not working\n",
    "        #normalize = nn.BatchNorm2d(512)\n",
    "        input[0][:, start:end]=0\n",
    "        #input = normalize(input)\n",
    "        #print(\"error injection shape :\",input[0][:, start:end],\"original : \",input) # dbug\n",
    "    return hook\n",
    "\n",
    "def hook_register(model,num_error,error_index):\n",
    "    for name,layers in model.named_modules():\n",
    "        #print(name,layer)\n",
    "        for idx,layer in enumerate(layers.features):\n",
    "            #print(idx,layer)\n",
    "            if idx is 34 and isinstance(layer, torch.nn.modules.conv.Conv2d) :\n",
    "                print(\"input\",name,layer) # target layer Conv5_1\n",
    "                layer.register_forward_pre_hook(error_injection(name,num_error,error_index))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "class Target_model(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def get_layer(self,idx):\n",
    "        #print(self.model._modules['34'])\n",
    "        layer =None\n",
    "        try : # target model\n",
    "            layer = self.model._modules[str(idx)]\n",
    "        except KeyError: # test_model\n",
    "            layer = self.model.features._modules[str(idx)]\n",
    "        return layer\n",
    "    def apply_f4f(self,f4f,error_info):\n",
    "        weight = self.get_layer(34).weight.data\n",
    "        weight = torch.reshape(weight,(512,3*3*512))\n",
    "        data = torch.cat( (error_info,weight), 1 ) \n",
    "        test_data.append(data) # debug\n",
    "        #print(data.size())\n",
    "        offset = f4f(data)\n",
    "        offset = torch.reshape(offset, (512,1,3,3)).repeat(1,512,1,1)\n",
    "        original_weight = self.get_layer(34).weight.data\n",
    "        print(original_weight.shape)\n",
    "        \n",
    "        #self.get_layer(34).weight.data = offset # replace -> 0916 don't work\n",
    "        self.get_layer(34).weight.data = original_weight + offset\n",
    "        print(test_filter_c.size())\n",
    "        print(self.get_layer(34).weight.data.size())\n",
    "    def forward(self,x,f4f,error_info):\n",
    "        origin_weight = self.get_layer(34).weight.data\n",
    "        # apply_f4f는 매 epoch마다 동일하므로 \n",
    "        self.apply_f4f(f4f,error_info)\n",
    "        replace_weight = self.get_layer(34).weight.data\n",
    "        y = self.model(x)\n",
    "        return y, origin_weight, replace_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [512, 512, 3, 3] at entry 0 and [512, 512, 1, 1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23556/2381157458.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moriginal_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_info\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [512, 512, 3, 3] at entry 0 and [512, 512, 1, 1] at entry 1"
     ]
    }
   ],
   "source": [
    "error_info = make_error_info(error_index,num_error).to(device)\n",
    "error_info = torch.reshape(error_info,(512,512,1,1))\n",
    "original_weight = target_model.get_layer(34).weight\n",
    "tmp = torch.reshape(original_weight,(512,512,3,3))\n",
    "torch.stack([tmp,error_info],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  torch.Size([512, 5120])\n",
      "torch.Size([512, 512, 1, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 512, 1, 3, 3]),\n",
       " torch.Size([512, 512, 1, 3, 3]),\n",
       " torch.Size([512, 512, 1, 3, 3]),\n",
       " torch.Size([512, 512, 3, 3]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print(\"input : \",test_data[0].size())\n",
    "offset = f4f(test_data[0])\n",
    "#print(offset.size())\n",
    "offset = torch.reshape(offset, (512,512,1,3,3))\n",
    "print(offset.size())\n",
    "#offset = offset.repeat(1,1,512,1,1)\n",
    "original_weight = target_model.get_layer(34).weight.data\n",
    "original_weight = original_weight.unsqueeze(2)\n",
    "offset.size(), original_weight.size(), (offset+original_weight).size(), (offset+original_weight).squeeze(2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 1, 3, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_weight[:,0,:,:].unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_model = d_copy(vgg16_bn).to(device)\n",
    "hook_register(vgg16_bn,num_error,error_index)\n",
    "target_model = Target_model(vgg16_bn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation phasetraining\n",
    "def eval(model,dataloader,epoch,loss_fn,batch_size,\n",
    "         f4f,error_info,log_file,TensorBoardWriter):\n",
    "    \n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    f4f.eval()\n",
    "    total = 0\n",
    "    correct =0\n",
    "    total_loss =0.0\n",
    "    with torch.no_grad():\n",
    "        print(\"======eval start=======\")\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "            #y_hat = model(inputs,f4f,error_info)\n",
    "            result = model(inputs,f4f,error_info)\n",
    "            y_hat,origin_weight, replace_weight = result\n",
    "            \n",
    "            _, predicted = torch.max(y_hat, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss = loss_fn(0.5,y_hat,labels,\n",
    "                            origin_weight, replace_weight)\n",
    "            total_loss +=loss.item()\n",
    "            \n",
    "            if(i%200 == 199):\n",
    "                print(\"step : %d / %d acc : %.3f\"\n",
    "                      %(i + 1,int(len(dataloader)), correct*100/total))\n",
    "                #print(\".\",end=\"\")\n",
    "        print(\"\")\n",
    "    acc = 100*correct/total\n",
    "    #print(total_loss, len(dataloader))\n",
    "    avg_loss = total_loss / (len(dataloader)*batch_size)\n",
    "    print(\"Eval acc of model on imagenet : %.4f %%, Loss : %.4f\" %(acc,avg_loss)) # model.__class__.__name__\n",
    "    f = open(log_file,\"a\")\n",
    "    print(\"Eval acc of model on imagenet : %.4f %%, Loss : %.4f\" %(acc,avg_loss),file=f) # model.__class__.__name__\n",
    "    f.close()\n",
    "    TensorBoardWriter.add_scalar(\"Model1/ACC_EVAL\",acc,epoch)\n",
    "    TensorBoardWriter.add_scalar(\"Model1/LOSS_EVAL\",avg_loss,epoch)\n",
    "    print(\"======eval  end ======\")  \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def training(f4f,target_model,original_model,\n",
    "             train_dataloader,test_dataloader,batch_size,\n",
    "             log_file,TensorBoardWriter,\n",
    "             retrain_model_path,\n",
    "             loss_fn,optimizer,\n",
    "             num_error,\n",
    "             max_epochs=30,subset=False):\n",
    "    first_feature = []\n",
    "    first_label = []\n",
    "    original_out = []\n",
    "    offset_info = []\n",
    "    target_model.to(device)\n",
    "    original_model.to(device)\n",
    "    target_model.eval()\n",
    "    original_model.eval()\n",
    "    \n",
    "    feature_num = 100\n",
    "    for epoch in range(1,max_epochs+1):\n",
    "        running_loss = 0.0\n",
    "        total_loss = []\n",
    "        total_avg_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        f4f.train()\n",
    "        # update f4f filter\n",
    "        #target_model.apply_f4f(f4f,error_info)\n",
    "    \n",
    "        # compare\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            error_index = i % (512-num_error)\n",
    "            error_info = make_error_info(error_index,num_error).to(device)\n",
    "            \n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(\".\",end=\"\")\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            #target_out = target_model(inputs,f4f,error_info)\n",
    "            result = target_model(inputs,f4f,error_info)\n",
    "            target_out,origin_weight, replace_weight = result\n",
    "            \n",
    "            if len(first_feature) < feature_num:\n",
    "                first_feature.append(target_out)\n",
    "                first_label.append(labels)\n",
    "                #first_feature.pop(0)\n",
    "                #first_label.pop(0)\n",
    "            _,predicted = torch.max(target_out,1) # target_out.data : no grad, target_out : with grad\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            #loss = loss_fn(target_out,labels)\n",
    "            \n",
    "            loss = loss_fn(0.5,target_out,labels,\n",
    "                            origin_weight, replace_weight) # check ratio is same of eval func\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            #target_model.model.zero_grad() # might be useless\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 99: \n",
    "                total_loss.append(running_loss/100)\n",
    "                print(\"\")\n",
    "                print('[%d, %5d] loss: %.6f' % (epoch+1, i+1, running_loss/(100*batch_size) )) \n",
    "                running_loss = 0.0\n",
    "        # save weight\n",
    "        #print((len(train_dataloader)/batch_size))\n",
    "        if len(total_loss) != 0:\n",
    "            total_avg_loss = sum(total_loss)/(len(total_loss)*batch_size)\n",
    "        acc = 100*correct/total\n",
    "        if total_avg_loss != 0:\n",
    "            print(\"total average loss : %.3f\" %(total_avg_loss))\n",
    "        else :\n",
    "            print(\"total loss :\" ,total_loss)\n",
    "        print(\"==epoch %d ==  train acc : %.4f\" %(epoch,acc))\n",
    "        TensorBoardWriter.add_scalar(\"Model1/ACC_Train\",acc,epoch)\n",
    "        TensorBoardWriter.add_scalar(\"Model1/LOSS_Train\",total_avg_loss,epoch)\n",
    "        acc = eval(target_model,test_dataloader,epoch,loss_fn,batch_size,\n",
    "                   f4f,error_info,log_file,TensorBoardWriter)\n",
    "        \n",
    "        offset_info.append(target_model.get_layer(34))\n",
    "        #torch.save(f4f.get_f4f_weight(), \n",
    "        #       retrain_model_path+\"%s~%s_pkt_err_f4f_epoch_%s_acc_%.4f_loss_%.4f.pt\"\n",
    "        #       %(str(error_idx).zfill(3),str(error_idx+num_error).zfill(3),\n",
    "        #        str(epoch+1).zfill(2),acc,total_avg_loss))    \n",
    "    return first_feature,first_label,offset_info\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 fc\n",
      "."
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[512, 1, 3, 3]' is invalid for input of size 2359296",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23556/1044134178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                   \u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretrain_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                   \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                   num_error,max_epoch,True)\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# tmp : first_feature,first_label,offset_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23556/3625931991.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(f4f, target_model, original_model, train_dataloader, test_dataloader, batch_size, log_file, TensorBoardWriter, retrain_model_path, loss_fn, optimizer, num_error, max_epochs, subset)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#target_out = target_model(inputs,f4f,error_info)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf4f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mtarget_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morigin_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/2/hwbae0326/anaconda3/envs/local_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23556/3002663096.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, f4f, error_info)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0morigin_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# apply_f4f는 매 epoch마다 동일하므로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_f4f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf4f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mreplace_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23556/3002663096.py\u001b[0m in \u001b[0;36mapply_f4f\u001b[0;34m(self, f4f, error_info)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(data.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf4f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moriginal_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[512, 1, 3, 3]' is invalid for input of size 2359296"
     ]
    }
   ],
   "source": [
    "\n",
    "#optimizer = torch.optim.SGD(param_list,lr=0.01,weight_decay=1e-4)\n",
    "first_feature = []\n",
    "first_label = []\n",
    "offset_info = []\n",
    "f = open(log_file,\"w\")\n",
    "print(\"6 fc\")\n",
    "f.close()\n",
    "writer.flush()\n",
    "#header.hook_register(vgg16_bn)\n",
    "target_model = Target_model(vgg16_bn).to(device)\n",
    "\n",
    "max_epoch = 190\n",
    "tmp= training(f4f,target_model,original_model,\n",
    "                  train_dataloader,test_dataloader,batch_size,\n",
    "                  log_file,writer,retrain_model_path,\n",
    "                  loss_fn,optimizer,\n",
    "                  num_error,max_epoch,True)\n",
    "        # tmp : first_feature,first_label,offset_info\n",
    "writer.close()\n",
    "first_feature.append(tmp[0])\n",
    "first_label.append(tmp[1])\n",
    "offset_info.append(tmp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

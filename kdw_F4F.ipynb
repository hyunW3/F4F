{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aceaa16-3f3f-4e1f-a6d9-797db79317e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os,json\n",
    "import gc\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc33b02c-5422-477c-a15d-70f39a28b5e3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "error index + filter_input F4F에 넣어서 offset을 뽑아서 돌려보기\n",
    "\n",
    "\"\"\"\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "pretrained_model = models.vgg16_bn(pretrained=True)\n",
    "#summary(pretrained_model, (3, 224, 224)) # (channels, width, height)\n",
    "pretrained_model.cuda() # model에 GPU 할당\n",
    "print(pretrained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d485f9-7855-47b0-a409-9309bf09d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/media/1/Imagenet_dup': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /media/1/Imagenet_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b826e6c3-5b88-4e9c-9cc4-c6de4b3296fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/2/Imagenet_dup/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-550cd972d659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m### data loader ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_DATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# numpy를 Tensor로 바꾸어 넣는다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_DATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/local_pytorch/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    207\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/local_pytorch/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/local_pytorch/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/2/Imagenet_dup/train'"
     ]
    }
   ],
   "source": [
    "new_model=models.vgg16_bn(pretrained=True).cuda()\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), # numpy를 tensor(torch)로 바꾸어 넣는다. 이미지의 경우 픽셀 값 하나는 0 ~ 255 값을 갖는다. 하지만 ToTensor()로 타입 변경시 0 ~ 1 사이의 값으로 바뀜.\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224), # 바로 resize 하지 말고 여기서 input format으로 맞춤.\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "file = open('Filter_for_Filter_result.txt', 'w')    # hello.txt 파일을 쓰기 모드(w)로 열기. 파일 객체 반환\n",
    "num_epochs = 20\n",
    "batchsize = 64\n",
    "lr = 0.001\n",
    "class_num=1000 # class 개수\n",
    "channel_per_packet=2 # channel당 packet 수 (pooling5에서 자르면 8이 된다.)\n",
    "packet_loss_per_feature=32 # feature의 총 256개 packet 중에서 packet loss 개수 (즉, feature당 channel loss는 512개 중에서 2 * 64로 128개가 loss 된다.)\n",
    "\n",
    "TRAIN_DATA_PATH = \"/media/2/Imagenet_dup/train\"\n",
    "TEST_DATA_PATH=\"/media/2/Imagenet_dup/val\"\n",
    "\n",
    "### data loader ###\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transforms_train) # numpy를 Tensor로 바꾸어 넣는다.\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, shuffle=True,num_workers=4)\n",
    "testset = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=transforms_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize, shuffle=True, num_workers=4)\n",
    "# num_workers : how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7a7ff-4742-4b0a-a6ca-64ac340dec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_start_index=0\n",
    "# hook 설정!\n",
    "def init_weights(m): # 이거는 weight를 모두 0.5로 초기화 하는 부분 (여기서는 필요없는 부분)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(0.5)\n",
    "\n",
    "def print_grad(name): # 이거는 backward 에서 쓰는 hook. (여기서는 필요없는 부분)\n",
    "    def hook(model, input, output):\n",
    "        print('BACKWARD', name, input, output)\n",
    "    return hook\n",
    "\n",
    "def preprocessing(name): # pre_hook 부분\n",
    "    def hook(model, input): # pre hook은 해당 layer의 pre processing 부분이기에 output이 없어서 output을 parameter로 놓지 않는다.\n",
    "        input[0][:,loss_start_index:loss_start_index+channel_per_packet*packet_loss_per_feature] = 0 # channel loss 발생! (input이 2D이기에 input[0]으로 해줘야 우리가 생각하는 feature이다.)\n",
    "        print(input[0].size())\n",
    "    return hook\n",
    "\n",
    "def print_forward(name): # forward hook 부분 (여기에서는 필요 없음)\n",
    "    def hook(model, input, output):\n",
    "        if name==\"module.features.34\":\n",
    "            return output\n",
    "    return hook\n",
    "\n",
    "class F4F_only_error_index(nn.Module): # error index만 넣어서 offset(3 x 3 x 512) 뽑는다. offset은 512개 각 filter에 동일하게 적용된다. \n",
    "    def __init__(self):\n",
    "        super(F4F_only_error_index, self).__init__()\n",
    "        self.fc1=nn.Linear(3*3*512+512,3*3*512) # input : error bit vector(512), output : 3 x 3 x 512 (filter offset)\n",
    "    def forward(self,x): # x는 data를 나타낸다.\n",
    "        x=self.fc1(x) # data가 fc를 지나간다.\n",
    "        output=torch.tanh(x)\n",
    "        return output\n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        if(len(self.outputs)>2): # 누적되면 앞의 것을 지운다.\n",
    "            del self.outputs[:2]\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fecf48-1932-45b4-b7f9-8e179de162b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().cuda() # cost function \n",
    "pretrained_model.features._modules\n",
    "conv5_1_layer = pretrained_model.features._modules['34']\n",
    "print(conv5_1_layer.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4f12d-d1a7-4922-a9f0-76c3761f94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for start_index in range(0,3*128+1,128): \n",
    "    before_accuracy=0.0\n",
    "    F4F=F4F_only_error_index() # model 선언\n",
    "    F4F.cuda()\n",
    "    optimizer = torch.optim.SGD(F4F.parameters(), lr=lr, weight_decay=1e-4) # optimizer\n",
    "    loss_start_index=start_index # 0~127, 128~255, 256~383, 384~511\n",
    "    error_index=[] # error 위치는 1, 아닌것은 0 (channel 위치)\n",
    "    for index in range(512):\n",
    "        if loss_start_index<=index and index < loss_start_index+channel_per_packet*packet_loss_per_feature: # error 위치\n",
    "            error_index.append(0)\n",
    "        else:\n",
    "            error_index.append(1)\n",
    "    error_index=torch.Tensor(error_index)\n",
    "    error_index  = error_index.unsqueeze(0).repeat(512,1)\n",
    "    error_index=error_index.cuda() # 512 bit vector (error location)\n",
    "    flatten_weight = torch.reshape(conv5_1_layer.weight,(512,512*3*3))\n",
    "    #print(error_index.size(),flatten_weight.size())\n",
    "    info = torch.cat( (flatten_weight,error_index), 1 )\n",
    "    #print(info.size())\n",
    "    \n",
    "    for epoch in range(num_epochs): # epoch 20번\n",
    "        F4F.train()\n",
    "        #optimizer.zero_grad() #  autograd에서 gradient가 축적 되기 때문에, gradient를 통해 가중치들을 업데이트할 때마다, 다음으로 넘어가기 전에 이 zero_() 메소드를 통해 gradient를 0로 만들어 줘야한다. \n",
    "        result = F4F(info) # result : 4608의 Tensor 형태 (512 x 3 x 3) => filter의 offset으로 사용될 예정. 더하고 빼는 경우 2가지 다 해보자.\n",
    "        result=torch.reshape(result,[512,512,3,3])  # filter에 넣을 형태로 변경\n",
    "        print(result.size())\n",
    "        exit(0)\n",
    "        #### filter를 변경한 새로운 모델 ####\n",
    "        for name, parameter in new_model.named_parameters():\n",
    "            if name == 'module.features.34.weight': # 바꿀 filter\n",
    "                parameter=parameter[:,]+result # 새로운 filter\n",
    "                new_model.module.features[34].weight.data = parameter # filter 변경!\n",
    "                break       \n",
    "        for name, module in new_model.named_modules():\n",
    "            if name==\"module.features.34\": # conv 5-1 pre hook 걸기 (input feature에 error 넣기)\n",
    "                module.register_forward_pre_hook(preprocessing(name)) # pre hook (forward)\n",
    "                break\n",
    "\n",
    "        save_output1 = SaveOutput()\n",
    "        save_output2 = SaveOutput()\n",
    "        \n",
    "        original_output=[]\n",
    "        error_output=[]\n",
    "        #### 기존 모델에서 forward hook 통해서 conv5_1의 output 저장 ###\n",
    "        for name, module in pretrained_model.named_modules():\n",
    "            if name==\"module.features.36\": # conv 5-1 위치 (ReLU까지 통과시킨 것)\n",
    "                original_output=module.register_forward_hook(save_output1) # forward hook \n",
    "                break\n",
    "            \n",
    "        #### 새로운 모델에서 conv5_1의 결과 뽑기 ####\n",
    "        for name, module in new_model.named_modules():\n",
    "            if name==\"module.features.36\": # \n",
    "                error_output=module.register_forward_hook(save_output2) # forward hook \n",
    "                break\n",
    "        \n",
    "        ####### train #######\n",
    "        for idx, (images, labels) in enumerate(trainloader):\n",
    "            images = images.cuda()\n",
    "            out1=pretrained_model(images)\n",
    "            out2=new_model(images)\n",
    "            \n",
    "            ##### 중간 feature 뽑기 #####\n",
    "            original_label=save_output1.outputs\n",
    "            original_label=torch.Tensor([item.cpu().detach().numpy() for item in original_label]).cuda() \n",
    "            \"\"\"\n",
    "                This is because gpu Upper tensor You can't go straight to numpy;  \n",
    "                You need to be in  cpu  Complete the operation on , Back to  gpu  On\n",
    "                If it's in  cpu  On , above .cpu()  and .cuda()  It can be omitted\n",
    "            \"\"\"\n",
    "            original_label=torch.reshape(original_label,[batchsize,512,14,14]) # 이상하게 512, 14, 14는 제대로 나오는데 앞쪽이 쪼개져서 나와서 이렇게 모양 바꿈\n",
    "            \n",
    "            error_label=save_output2.outputs\n",
    "            error_label=torch.Tensor([item.cpu().detach().numpy() for item in error_label]).cuda()\n",
    "            error_label=torch.reshape(error_label,[batchsize,512,14,14]) # 이상하게 512, 14, 14는 제대로 나오는데 앞쪽이 쪼개져서 나와서 이렇게 모양 바꿈\n",
    "            \n",
    "            #### 학습 #####\n",
    "            \n",
    "            original_label=original_label.cuda()\n",
    "            error_label=error_label.cuda()\n",
    "            original_label.requires_grad_(True) # requires \n",
    "            error_label.requires_grad_(True)\n",
    "            optimizer.zero_grad() #  autograd에서 gradient가 축적 되기 때문에, gradient를 통해 가중치들을 업데이트할 때마다, 다음으로 넘어가기 전에 이 zero_() 메소드를 통해 gradient를 0로 만들어 줘야한다. \n",
    "            loss = criterion(error_label,original_label) # MSE cost function 적용\n",
    "            loss.backward() # autograd 를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를 갖는 모든 텐서들에 대한 손실의 변화도를 계산합니다.\n",
    "            optimizer.step()\n",
    "            if idx>400:\n",
    "                break\n",
    "        \n",
    "        ###### test (새로운 filter가 들어간 vgg16) ######             \n",
    "        print(\"Test start!!!\")\n",
    "        new_model.eval() # Change model to 'eval' mode (BN uses moving mean/var)\n",
    "\n",
    "        correct_top1 = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # 이 컨텍스트 내부에서 새로 생성된 텐서들은 requires_grad=False 상태가 되어, 메모리 사용량을 아껴준다. (훈련 안함)\n",
    "            for idx, (images, labels) in enumerate(testloader):\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = new_model(images)\n",
    "                _, predicted = torch.max(outputs, 1) # top 1 기준\n",
    "                total += labels.size(0) # labels.size : batch size가 64이니 64이다. (맨 끝에만 16개)\n",
    "                correct_top1 += (predicted == labels).sum().item()\n",
    "                print(\"step : {} / {}\".format(idx + 1, len(testset)/int(labels.size(0))))\n",
    "                print(\"top-1 percentage :  {0:0.2f}%\".format(correct_top1 / total * 100))\n",
    "        file.write(\"error channel {0}~{1}, epoch : [{2}/{3}]\\n\".format(loss_start_index,loss_start_index+channel_per_packet*packet_loss_per_feature-1, epoch+1, num_epochs))        \n",
    "        file.write(\"top-1 percentage :  {0:0.2f}%\\n\".format(correct_top1 / total * 100))\n",
    "        if (correct_top1 / total * 100) < before_accuracy:\n",
    "            optimizer = torch.optim.SGD(F4F.parameters(), lr=before_lr*0.5, weight_decay=1e-4) # optimizer (아까 뽑아낸 훈련시킬 것만 설정)  => 안쓰는 것들은 자동으로 freeze\n",
    "            before_lr=before_lr*0.5\n",
    "        before_accuracy=(correct_top1 / total * 100)\n",
    "        \n",
    "        ###### 새로운 filter 저장 #######\n",
    "        for name, parameter in new_model.named_parameters():\n",
    "            if name == 'module.features.34.weight': # 훈련시킬 것만 뽑는다.\n",
    "                num=str(loss_start_index).zfill(3)\n",
    "                num2=str(loss_start_index+channel_per_packet*packet_loss_per_feature-1).zfill(3)\n",
    "                epoch_num=str(epoch+1).zfill(2)\n",
    "                torch.save(parameter, f\"/media/3/Network/filter/pooling4/only_error_input_F4F/conv5_1_{num}~{num2}_train_epoch_{epoch_num}.pt\") # 예를들어 128~255이면 128~255번째 channel이 깨진 것이다. 그리고 뒤의 숫자는 학습 횟수이다. 7이면 7번 epoch 돌린것\n",
    "                break\n",
    "    \n",
    "                \n",
    "file.close() # 파일 객체 닫기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc1e7a-fbb4-4a91-8820-c5ef81aef488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421a697-26ec-43a9-a320-acb4dcc64948",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2ab85-e9b3-4c44-b2c0-e4b1af0ec988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d1096-2ed8-4259-9f83-cd49f1c44742",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f4b29-3fdd-4353-9e17-af5f1de042bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b6c894-7d78-48c7-9a72-57c0131e62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy as d_copy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dbd81aa-7939-4fcd-9bcc-eb4cf950233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#error_index = 0\n",
    "writer = SummaryWriter(\"./runs/FC512_submodel_5e-1_eval\") #_BN\n",
    "vgg16_bn = torchvision.models.vgg16_bn(pretrained=True)#.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe0315b-90d2-4d99-a7a8-a53324054054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importnb import Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc28da8c-8258-4740-bf72-b1dc5d7cdae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Notebook(lazy=True):\n",
    "    import F4F_model_Submodel\n",
    "header = F4F_model_Submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d356098-fb9e-47d0-87c0-bd2d049299c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===INFO===\n",
      "torch ver : 1.7.1\n",
      "torchvision ver : 0.8.2 \n",
      "GPU model : TITAN RTX\n",
      "train dataset[4000], test dataset[1563] are loaded\n"
     ]
    }
   ],
   "source": [
    "# set randomness\n",
    "seed = 0\n",
    "\n",
    "header.set_randomness(seed) # ipynb module import means run all cell in file\n",
    "# load dataset\n",
    "dataset_path = \"/media/2/Network/Imagenet_dup/\"\n",
    "retrain_model_path = \"/media/0/Network/0821_to_fullmodels/\"\n",
    "batch_size = 32 # 32~ out of memory in 3080\n",
    "num_train = 128000 #640000 #debug\n",
    "\n",
    "train_dataloader,test_dataloader = header.get_dataset(num_train,batch_size,\n",
    "                  dataset_path,retrain_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d39c889-1ee9-4700-b3b2-7cf70f317b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d4579d-7192-4c1b-bf80-bf588383877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In_layer_number = 34 # 34 conv5_1 convolution\n",
    "#Out_layer_number = 36 # 36 conv5_1 relu \n",
    "error_index=0\n",
    "max_epoch = 90\n",
    "num_error = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25f7dbd2-b260-4f95-acd6-b73c47f80e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwriter.flush()\\nf4f = header.F4F().to(device)\\ntarget_model = header.Target_model(vgg16_bn).to(device)\\nfilter_set = target_model.get_layer(34).weight.data\\nfilter_set = torch.reshape(filter_set,(512,512*3*3)).to(device)\\nerror_info = header.make_error_info(error_index,num_error).to(device)\\n#writer.add_graph(target_model,(images,f4f,error_info))\\nwriter.add_graph(f4f,(images,filter_set,error_info))\\nwriter.close()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "writer.flush()\n",
    "f4f = header.F4F().to(device)\n",
    "target_model = header.Target_model(vgg16_bn).to(device)\n",
    "filter_set = target_model.get_layer(34).weight.data\n",
    "filter_set = torch.reshape(filter_set,(512,512*3*3)).to(device)\n",
    "error_info = header.make_error_info(error_index,num_error).to(device)\n",
    "#writer.add_graph(target_model,(images,f4f,error_info))\n",
    "writer.add_graph(f4f,(images,filter_set,error_info))\n",
    "writer.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "147afee8-cbb6-4065-8176-9237ffa31cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"./log/3fc_64_acc_log_0906.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4264bffc-c9fa-4aca-8485-2617c74830b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f4f = header.F4F().to(device)\n",
    "#optimizer = torch.optim.SGD(f4f.parameters,lr=1e-4,weight_decay=1e-4,momentum=0.9)\n",
    "optimizer = torch.optim.SGD([i for i in f4f.parameters()],lr=5e-1,weight_decay=1e-4,momentum=0.9)\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "def loss_fn(loss1_ratio,pred,label, filter_orig,filter_f4f):\n",
    "    if(loss1_ratio <0 or loss1_ratio >1 ):\n",
    "        print(\"wrong parameter ratio \",loss1_ratio)\n",
    "        return nan\n",
    "    loss1 = nn.CrossEntropyLoss()\n",
    "    loss2 = nn.MSELoss()\n",
    "    a = loss1(pred,label)\n",
    "    b = loss2(filter_orig,filter_f4f)\n",
    "    return loss1_ratio * a + (1-loss1_ratio) * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35927f2b-1bcc-4e69-aa73-1a11681238f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4608, 4608]),\n",
       " torch.Size([4608]),\n",
       " torch.Size([512, 512]),\n",
       " torch.Size([512]),\n",
       " torch.Size([512, 512]),\n",
       " torch.Size([512]),\n",
       " torch.Size([5120, 5120]),\n",
       " torch.Size([5120]),\n",
       " torch.Size([4608, 5120]),\n",
       " torch.Size([4608])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.size() for i in f4f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b07f5901-a376-4cd5-8d8f-0e2b3fd84de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() >1 :\n",
    "    print(\"data parallel start\")\n",
    "    f4f = nn.DataParallel(f4f).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7feb48a3-6382-4db2-99dd-b7d5c3934437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "#split_model = split_layer(vgg16_bn,0,Out_layer_number)\n",
    "\n",
    "original_model = d_copy(vgg16_bn).to(device)\n",
    "# subset of vgg16 (whole layer) with f4f\n",
    "header.hook_register(vgg16_bn,num_error,error_index)\n",
    "target_model = header.Target_model(vgg16_bn).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca1dae-7b1a-438d-aece-2f32ce991bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a02191-b3dd-4af5-a223-4dbe1832f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc979b91-c459-4d63-aae4-8455536b4d53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodel case\n",
      "..........\n",
      "[1,   100] loss: 6.240892\n",
      "..........\n",
      "[1,   200] loss: 6.282614\n",
      "..........\n",
      "[1,   300] loss: 6.229099\n",
      "..........\n",
      "[1,   400] loss: 6.318948\n",
      "..........\n",
      "[1,   500] loss: 6.362757\n",
      "..........\n",
      "[1,   600] loss: 6.229998\n",
      "..........\n",
      "[1,   700] loss: 6.288514\n",
      "..........\n",
      "[1,   800] loss: 6.195211\n",
      "..........\n",
      "[1,   900] loss: 6.304827\n",
      "..........\n",
      "[1,  1000] loss: 6.328884\n",
      "..........\n",
      "[1,  1100] loss: 6.239037\n",
      "..........\n",
      "[1,  1200] loss: 6.302159\n",
      "..........\n",
      "[1,  1300] loss: 6.196006\n",
      "..........\n",
      "[1,  1400] loss: 6.277868\n",
      "..........\n",
      "[1,  1500] loss: 6.368196\n",
      "..........\n",
      "[1,  1600] loss: 6.238765\n",
      "..........\n",
      "[1,  1700] loss: 6.325353\n",
      "..........\n",
      "[1,  1800] loss: 6.180549\n",
      "..........\n",
      "[1,  1900] loss: 6.284350\n",
      "..........\n",
      "[1,  2000] loss: 6.320918\n",
      "..........\n",
      "[1,  2100] loss: 6.210711\n",
      "..........\n",
      "[1,  2200] loss: 6.276591\n",
      "..........\n",
      "[1,  2300] loss: 6.204494\n",
      "..........\n",
      "[1,  2400] loss: 6.244645\n",
      "..........\n",
      "[1,  2500] loss: 6.401408\n",
      "..........\n",
      "[1,  2600] loss: 6.273409\n",
      "..........\n",
      "[1,  2700] loss: 6.293355\n",
      "..........\n",
      "[1,  2800] loss: 6.228449\n",
      "..........\n",
      "[1,  2900] loss: 6.233919\n",
      "..........\n",
      "[1,  3000] loss: 6.329326\n",
      "..........\n",
      "[1,  3100] loss: 6.311324\n",
      "..........\n",
      "[1,  3200] loss: 6.275440\n",
      "..........\n",
      "[1,  3300] loss: 6.211540\n",
      "..........\n",
      "[1,  3400] loss: 6.219326\n",
      "..........\n",
      "[1,  3500] loss: 6.364333\n",
      "..........\n",
      "[1,  3600] loss: 6.273396\n",
      "..........\n",
      "[1,  3700] loss: 6.289584\n",
      "..........\n",
      "[1,  3800] loss: 6.250633\n",
      "..........\n",
      "[1,  3900] loss: 6.267608\n",
      "..........\n",
      "[1,  4000] loss: 6.325642\n",
      "total average loss : 0.196\n",
      "== epoch  0 == train acc : 0.1055\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.047\n",
      "step : 600 / 1563 acc : 0.052\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[2,   100] loss: 6.245529\n",
      "..........\n",
      "[2,   200] loss: 6.287843\n",
      "..........\n",
      "[2,   300] loss: 6.227012\n",
      "..........\n",
      "[2,   400] loss: 6.333600\n",
      "..........\n",
      "[2,   500] loss: 6.310684\n",
      "..........\n",
      "[2,   600] loss: 6.234258\n",
      "..........\n",
      "[2,   700] loss: 6.310232\n",
      "..........\n",
      "[2,   800] loss: 6.234185\n",
      "..........\n",
      "[2,   900] loss: 6.283173\n",
      "..........\n",
      "[2,  1000] loss: 6.322636\n",
      "..........\n",
      "[2,  1100] loss: 6.249338\n",
      "..........\n",
      "[2,  1200] loss: 6.336055\n",
      "..........\n",
      "[2,  1300] loss: 6.181763\n",
      "..........\n",
      "[2,  1400] loss: 6.265645\n",
      "..........\n",
      "[2,  1500] loss: 6.350300\n",
      "..........\n",
      "[2,  1600] loss: 6.207573\n",
      "..........\n",
      "[2,  1700] loss: 6.323707\n",
      "..........\n",
      "[2,  1800] loss: 6.182281\n",
      "..........\n",
      "[2,  1900] loss: 6.221160\n",
      "..........\n",
      "[2,  2000] loss: 6.314446\n",
      "..........\n",
      "[2,  2100] loss: 6.276195\n",
      "..........\n",
      "[2,  2200] loss: 6.326840\n",
      "..........\n",
      "[2,  2300] loss: 6.177231\n",
      "..........\n",
      "[2,  2400] loss: 6.275087\n",
      "..........\n",
      "[2,  2500] loss: 6.375740\n",
      "..........\n",
      "[2,  2600] loss: 6.272133\n",
      "..........\n",
      "[2,  2700] loss: 6.278795\n",
      "..........\n",
      "[2,  2800] loss: 6.197408\n",
      "..........\n",
      "[2,  2900] loss: 6.253453\n",
      "..........\n",
      "[2,  3000] loss: 6.364108\n",
      "..........\n",
      "[2,  3100] loss: 6.297452\n",
      "..........\n",
      "[2,  3200] loss: 6.251883\n",
      "..........\n",
      "[2,  3300] loss: 6.200366\n",
      "..........\n",
      "[2,  3400] loss: 6.262638\n",
      "..........\n",
      "[2,  3500] loss: 6.353110\n",
      "..........\n",
      "[2,  3600] loss: 6.271849\n",
      "..........\n",
      "[2,  3700] loss: 6.222833\n",
      "..........\n",
      "[2,  3800] loss: 6.247651\n",
      "..........\n",
      "[2,  3900] loss: 6.273465\n",
      "..........\n",
      "[2,  4000] loss: 6.328303\n",
      "total average loss : 0.196\n",
      "== epoch  1 == train acc : 0.1031\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.047\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.062\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[3,   100] loss: 6.267495\n",
      "..........\n",
      "[3,   200] loss: 6.292316\n",
      "..........\n",
      "[3,   300] loss: 6.221779\n",
      "..........\n",
      "[3,   400] loss: 6.314903\n",
      "..........\n",
      "[3,   500] loss: 6.303746\n",
      "..........\n",
      "[3,   600] loss: 6.230455\n",
      "..........\n",
      "[3,   700] loss: 6.288488\n",
      "..........\n",
      "[3,   800] loss: 6.217594\n",
      "..........\n",
      "[3,   900] loss: 6.294054\n",
      "..........\n",
      "[3,  1000] loss: 6.349807\n",
      "..........\n",
      "[3,  1100] loss: 6.240321\n",
      "..........\n",
      "[3,  1200] loss: 6.296113\n",
      "..........\n",
      "[3,  1300] loss: 6.173980\n",
      "..........\n",
      "[3,  1400] loss: 6.305340\n",
      "..........\n",
      "[3,  1500] loss: 6.306551\n",
      "..........\n",
      "[3,  1600] loss: 6.248871\n",
      "..........\n",
      "[3,  1700] loss: 6.336504\n",
      "..........\n",
      "[3,  1800] loss: 6.177266\n",
      "..........\n",
      "[3,  1900] loss: 6.317052\n",
      "..........\n",
      "[3,  2000] loss: 6.339205\n",
      "..........\n",
      "[3,  2100] loss: 6.275868\n",
      "..........\n",
      "[3,  2200] loss: 6.230892\n",
      "..........\n",
      "[3,  2300] loss: 6.160732\n",
      "..........\n",
      "[3,  2400] loss: 6.262036\n",
      "..........\n",
      "[3,  2500] loss: 6.367634\n",
      "..........\n",
      "[3,  2600] loss: 6.273447\n",
      "..........\n",
      "[3,  2700] loss: 6.259245\n",
      "..........\n",
      "[3,  2800] loss: 6.204258\n",
      "..........\n",
      "[3,  2900] loss: 6.287007\n",
      "..........\n",
      "[3,  3000] loss: 6.351783\n",
      "..........\n",
      "[3,  3100] loss: 6.287029\n",
      "..........\n",
      "[3,  3200] loss: 6.258823\n",
      "..........\n",
      "[3,  3300] loss: 6.204804\n",
      "..........\n",
      "[3,  3400] loss: 6.263683\n",
      "..........\n",
      "[3,  3500] loss: 6.353630\n",
      "..........\n",
      "[3,  3600] loss: 6.280081\n",
      "..........\n",
      "[3,  3700] loss: 6.228312\n",
      "..........\n",
      "[3,  3800] loss: 6.211740\n",
      "..........\n",
      "[3,  3900] loss: 6.265393\n",
      "..........\n",
      "[3,  4000] loss: 6.330341\n",
      "total average loss : 0.196\n",
      "== epoch  2 == train acc : 0.1102\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.052\n",
      "step : 800 / 1563 acc : 0.062\n",
      "step : 1000 / 1563 acc : 0.056\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[4,   100] loss: 6.226955\n",
      "..........\n",
      "[4,   200] loss: 6.289873\n",
      "..........\n",
      "[4,   300] loss: 6.242189\n",
      "..........\n",
      "[4,   400] loss: 6.292632\n",
      "..........\n",
      "[4,   500] loss: 6.312669\n",
      "..........\n",
      "[4,   600] loss: 6.243970\n",
      "..........\n",
      "[4,   700] loss: 6.310570\n",
      "..........\n",
      "[4,   800] loss: 6.233984\n",
      "..........\n",
      "[4,   900] loss: 6.308510\n",
      "..........\n",
      "[4,  1000] loss: 6.320264\n",
      "..........\n",
      "[4,  1100] loss: 6.254926\n",
      "..........\n",
      "[4,  1200] loss: 6.304242\n",
      "..........\n",
      "[4,  1300] loss: 6.167901\n",
      "..........\n",
      "[4,  1400] loss: 6.258950\n",
      "..........\n",
      "[4,  1500] loss: 6.303523\n",
      "..........\n",
      "[4,  1600] loss: 6.262732\n",
      "..........\n",
      "[4,  1700] loss: 6.311719\n",
      "..........\n",
      "[4,  1800] loss: 6.170807\n",
      "..........\n",
      "[4,  1900] loss: 6.272033\n",
      "..........\n",
      "[4,  2000] loss: 6.363628\n",
      "..........\n",
      "[4,  2100] loss: 6.262398\n",
      "..........\n",
      "[4,  2200] loss: 6.322260\n",
      "..........\n",
      "[4,  2300] loss: 6.169908\n",
      "..........\n",
      "[4,  2400] loss: 6.225442\n",
      "..........\n",
      "[4,  2500] loss: 6.371210\n",
      "..........\n",
      "[4,  2600] loss: 6.277924\n",
      "..........\n",
      "[4,  2700] loss: 6.233678\n",
      "..........\n",
      "[4,  2800] loss: 6.212130\n",
      "..........\n",
      "[4,  2900] loss: 6.257520\n",
      "..........\n",
      "[4,  3000] loss: 6.340200\n",
      "..........\n",
      "[4,  3100] loss: 6.267419\n",
      "..........\n",
      "[4,  3200] loss: 6.294735\n",
      "..........\n",
      "[4,  3300] loss: 6.224714\n",
      "..........\n",
      "[4,  3400] loss: 6.237681\n",
      "..........\n",
      "[4,  3500] loss: 6.322206\n",
      "..........\n",
      "[4,  3600] loss: 6.293142\n",
      "..........\n",
      "[4,  3700] loss: 6.248094\n",
      "..........\n",
      "[4,  3800] loss: 6.265244\n",
      "..........\n",
      "[4,  3900] loss: 6.310189\n",
      "..........\n",
      "[4,  4000] loss: 6.330543\n",
      "total average loss : 0.196\n",
      "== epoch  3 == train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.016\n",
      "step : 400 / 1563 acc : 0.047\n",
      "step : 600 / 1563 acc : 0.042\n",
      "step : 800 / 1563 acc : 0.047\n",
      "step : 1000 / 1563 acc : 0.047\n",
      "step : 1200 / 1563 acc : 0.057\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[5,   100] loss: 6.214580\n",
      "..........\n",
      "[5,   200] loss: 6.291133\n",
      "..........\n",
      "[5,   300] loss: 6.227423\n",
      "..........\n",
      "[5,   400] loss: 6.340505\n",
      "..........\n",
      "[5,   500] loss: 6.336023\n",
      "..........\n",
      "[5,   600] loss: 6.226368\n",
      "..........\n",
      "[5,   700] loss: 6.322041\n",
      "..........\n",
      "[5,   800] loss: 6.195242\n",
      "..........\n",
      "[5,   900] loss: 6.290651\n",
      "..........\n",
      "[5,  1000] loss: 6.335612\n",
      "..........\n",
      "[5,  1100] loss: 6.225247\n",
      "..........\n",
      "[5,  1200] loss: 6.341505\n",
      "..........\n",
      "[5,  1300] loss: 6.211415\n",
      "..........\n",
      "[5,  1400] loss: 6.264303\n",
      "..........\n",
      "[5,  1500] loss: 6.330776\n",
      "..........\n",
      "[5,  1600] loss: 6.248823\n",
      "..........\n",
      "[5,  1700] loss: 6.308622\n",
      "..........\n",
      "[5,  1800] loss: 6.203943\n",
      "..........\n",
      "[5,  1900] loss: 6.282558\n",
      "..........\n",
      "[5,  2000] loss: 6.366390\n",
      "..........\n",
      "[5,  2100] loss: 6.221500\n",
      "..........\n",
      "[5,  2200] loss: 6.257456\n",
      "..........\n",
      "[5,  2300] loss: 6.177343\n",
      "..........\n",
      "[5,  2400] loss: 6.266241\n",
      "..........\n",
      "[5,  2500] loss: 6.342936\n",
      "..........\n",
      "[5,  2600] loss: 6.294334\n",
      "..........\n",
      "[5,  2700] loss: 6.264449\n",
      "..........\n",
      "[5,  2800] loss: 6.206737\n",
      "..........\n",
      "[5,  2900] loss: 6.240116\n",
      "..........\n",
      "[5,  3000] loss: 6.337228\n",
      "..........\n",
      "[5,  3100] loss: 6.298198\n",
      "..........\n",
      "[5,  3200] loss: 6.287403\n",
      "..........\n",
      "[5,  3300] loss: 6.243075\n",
      "..........\n",
      "[5,  3400] loss: 6.222219\n",
      "..........\n",
      "[5,  3500] loss: 6.363457\n",
      "..........\n",
      "[5,  3600] loss: 6.276654\n",
      "..........\n",
      "[5,  3700] loss: 6.223485\n",
      "..........\n",
      "[5,  3800] loss: 6.266319\n",
      "..........\n",
      "[5,  3900] loss: 6.259582\n",
      "..........\n",
      "[5,  4000] loss: 6.346575\n",
      "total average loss : 0.196\n",
      "== epoch  4 == train acc : 0.1000\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.083\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[6,   100] loss: 6.224866\n",
      "..........\n",
      "[6,   200] loss: 6.256247\n",
      "..........\n",
      "[6,   300] loss: 6.248393\n",
      "..........\n",
      "[6,   400] loss: 6.315193\n",
      "..........\n",
      "[6,   500] loss: 6.295674\n",
      "..........\n",
      "[6,   600] loss: 6.266462\n",
      "..........\n",
      "[6,   700] loss: 6.304481\n",
      "..........\n",
      "[6,   800] loss: 6.223915\n",
      "..........\n",
      "[6,   900] loss: 6.275324\n",
      "..........\n",
      "[6,  1000] loss: 6.317145\n",
      "..........\n",
      "[6,  1100] loss: 6.232195\n",
      "..........\n",
      "[6,  1200] loss: 6.303945\n",
      "..........\n",
      "[6,  1300] loss: 6.200650\n",
      "..........\n",
      "[6,  1400] loss: 6.294803\n",
      "..........\n",
      "[6,  1500] loss: 6.409199\n",
      "..........\n",
      "[6,  1600] loss: 6.200051\n",
      "..........\n",
      "[6,  1700] loss: 6.309557\n",
      "..........\n",
      "[6,  1800] loss: 6.200694\n",
      "..........\n",
      "[6,  1900] loss: 6.307505\n",
      "..........\n",
      "[6,  2000] loss: 6.375440\n",
      "..........\n",
      "[6,  2100] loss: 6.208227\n",
      "..........\n",
      "[6,  2200] loss: 6.284586\n",
      "..........\n",
      "[6,  2300] loss: 6.198813\n",
      "..........\n",
      "[6,  2400] loss: 6.267791\n",
      "..........\n",
      "[6,  2500] loss: 6.377761\n",
      "..........\n",
      "[6,  2600] loss: 6.268632\n",
      "..........\n",
      "[6,  2700] loss: 6.258255\n",
      "..........\n",
      "[6,  2800] loss: 6.234509\n",
      "..........\n",
      "[6,  2900] loss: 6.265109\n",
      "..........\n",
      "[6,  3000] loss: 6.342551\n",
      "..........\n",
      "[6,  3100] loss: 6.287705\n",
      "..........\n",
      "[6,  3200] loss: 6.239183\n",
      "..........\n",
      "[6,  3300] loss: 6.177399\n",
      "..........\n",
      "[6,  3400] loss: 6.221159\n",
      "..........\n",
      "[6,  3500] loss: 6.326104\n",
      "..........\n",
      "[6,  3600] loss: 6.283865\n",
      "..........\n",
      "[6,  3700] loss: 6.258845\n",
      "..........\n",
      "[6,  3800] loss: 6.229738\n",
      "..........\n",
      "[6,  3900] loss: 6.280160\n",
      "..........\n",
      "[6,  4000] loss: 6.339061\n",
      "total average loss : 0.196\n",
      "== epoch  5 == train acc : 0.1070\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[7,   100] loss: 6.253226\n",
      "..........\n",
      "[7,   200] loss: 6.270932\n",
      "..........\n",
      "[7,   300] loss: 6.257311\n",
      "..........\n",
      "[7,   400] loss: 6.287317\n",
      "..........\n",
      "[7,   500] loss: 6.298079\n",
      "..........\n",
      "[7,   600] loss: 6.234353\n",
      "..........\n",
      "[7,   700] loss: 6.293299\n",
      "..........\n",
      "[7,   800] loss: 6.196035\n",
      "..........\n",
      "[7,   900] loss: 6.300575\n",
      "..........\n",
      "[7,  1000] loss: 6.338019\n",
      "..........\n",
      "[7,  1100] loss: 6.249795\n",
      "..........\n",
      "[7,  1200] loss: 6.322946\n",
      "..........\n",
      "[7,  1300] loss: 6.184992\n",
      "..........\n",
      "[7,  1400] loss: 6.255148\n",
      "..........\n",
      "[7,  1500] loss: 6.343046\n",
      "..........\n",
      "[7,  1600] loss: 6.245636\n",
      "..........\n",
      "[7,  1700] loss: 6.290871\n",
      "..........\n",
      "[7,  1800] loss: 6.216362\n",
      "..........\n",
      "[7,  1900] loss: 6.273330\n",
      "..........\n",
      "[7,  2000] loss: 6.376751\n",
      "..........\n",
      "[7,  2100] loss: 6.264779\n",
      "..........\n",
      "[7,  2200] loss: 6.285101\n",
      "..........\n",
      "[7,  2300] loss: 6.181716\n",
      "..........\n",
      "[7,  2400] loss: 6.266326\n",
      "..........\n",
      "[7,  2500] loss: 6.382981\n",
      "..........\n",
      "[7,  2600] loss: 6.269426\n",
      "..........\n",
      "[7,  2700] loss: 6.262568\n",
      "..........\n",
      "[7,  2800] loss: 6.181024\n",
      "..........\n",
      "[7,  2900] loss: 6.264701\n",
      "..........\n",
      "[7,  3000] loss: 6.335856\n",
      "..........\n",
      "[7,  3100] loss: 6.255806\n",
      "..........\n",
      "[7,  3200] loss: 6.244088\n",
      "..........\n",
      "[7,  3300] loss: 6.212220\n",
      "..........\n",
      "[7,  3400] loss: 6.253325\n",
      "..........\n",
      "[7,  3500] loss: 6.342628\n",
      "..........\n",
      "[7,  3600] loss: 6.293895\n",
      "..........\n",
      "[7,  3700] loss: 6.266415\n",
      "..........\n",
      "[7,  3800] loss: 6.265135\n",
      "..........\n",
      "[7,  3900] loss: 6.262216\n",
      "..........\n",
      "[7,  4000] loss: 6.357565\n",
      "total average loss : 0.196\n",
      "== epoch  6 == train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.016\n",
      "step : 400 / 1563 acc : 0.039\n",
      "step : 600 / 1563 acc : 0.042\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.059\n",
      "step : 1200 / 1563 acc : 0.057\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[8,   100] loss: 6.217078\n",
      "..........\n",
      "[8,   200] loss: 6.253172\n",
      "..........\n",
      "[8,   300] loss: 6.235666\n",
      "..........\n",
      "[8,   400] loss: 6.288106\n",
      "..........\n",
      "[8,   500] loss: 6.300530\n",
      "..........\n",
      "[8,   600] loss: 6.246640\n",
      "..........\n",
      "[8,   700] loss: 6.333707\n",
      "..........\n",
      "[8,   800] loss: 6.194026\n",
      "..........\n",
      "[8,   900] loss: 6.302942\n",
      "..........\n",
      "[8,  1000] loss: 6.333921\n",
      "..........\n",
      "[8,  1100] loss: 6.256905\n",
      "..........\n",
      "[8,  1200] loss: 6.305857\n",
      "..........\n",
      "[8,  1300] loss: 6.181061\n",
      "..........\n",
      "[8,  1400] loss: 6.302559\n",
      "..........\n",
      "[8,  1500] loss: 6.303760\n",
      "..........\n",
      "[8,  1600] loss: 6.215569\n",
      "..........\n",
      "[8,  1700] loss: 6.305726\n",
      "..........\n",
      "[8,  1800] loss: 6.185631\n",
      "..........\n",
      "[8,  1900] loss: 6.280808\n",
      "..........\n",
      "[8,  2000] loss: 6.357114\n",
      "..........\n",
      "[8,  2100] loss: 6.258421\n",
      "..........\n",
      "[8,  2200] loss: 6.303863\n",
      "..........\n",
      "[8,  2300] loss: 6.203984\n",
      "..........\n",
      "[8,  2400] loss: 6.265701\n",
      "..........\n",
      "[8,  2500] loss: 6.383030\n",
      "..........\n",
      "[8,  2600] loss: 6.279632\n",
      "..........\n",
      "[8,  2700] loss: 6.271654\n",
      "..........\n",
      "[8,  2800] loss: 6.200794\n",
      "..........\n",
      "[8,  2900] loss: 6.287643\n",
      "..........\n",
      "[8,  3000] loss: 6.366923\n",
      "..........\n",
      "[8,  3100] loss: 6.277619\n",
      "..........\n",
      "[8,  3200] loss: 6.234990\n",
      "..........\n",
      "[8,  3300] loss: 6.248052\n",
      "..........\n",
      "[8,  3400] loss: 6.249055\n",
      "..........\n",
      "[8,  3500] loss: 6.355001\n",
      "..........\n",
      "[8,  3600] loss: 6.295603\n",
      "..........\n",
      "[8,  3700] loss: 6.231979\n",
      "..........\n",
      "[8,  3800] loss: 6.231248\n",
      "..........\n",
      "[8,  3900] loss: 6.278123\n",
      "..........\n",
      "[8,  4000] loss: 6.286488\n",
      "total average loss : 0.196\n",
      "== epoch  7 == train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.036\n",
      "step : 800 / 1563 acc : 0.051\n",
      "step : 1000 / 1563 acc : 0.050\n",
      "step : 1200 / 1563 acc : 0.052\n",
      "step : 1400 / 1563 acc : 0.058\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[9,   100] loss: 6.266876\n",
      "..........\n",
      "[9,   200] loss: 6.298798\n",
      "..........\n",
      "[9,   300] loss: 6.226487\n",
      "..........\n",
      "[9,   400] loss: 6.274493\n",
      "..........\n",
      "[9,   500] loss: 6.338944\n",
      "..........\n",
      "[9,   600] loss: 6.252762\n",
      "..........\n",
      "[9,   700] loss: 6.306063\n",
      "..........\n",
      "[9,   800] loss: 6.245763\n",
      "..........\n",
      "[9,   900] loss: 6.280746\n",
      "..........\n",
      "[9,  1000] loss: 6.312270\n",
      "..........\n",
      "[9,  1100] loss: 6.226145\n",
      "..........\n",
      "[9,  1200] loss: 6.315529\n",
      "..........\n",
      "[9,  1300] loss: 6.190852\n",
      "..........\n",
      "[9,  1400] loss: 6.294924\n",
      "..........\n",
      "[9,  1500] loss: 6.328731\n",
      "..........\n",
      "[9,  1600] loss: 6.221004\n",
      "..........\n",
      "[9,  1700] loss: 6.311225\n",
      "..........\n",
      "[9,  1800] loss: 6.191032\n",
      "..........\n",
      "[9,  1900] loss: 6.290702\n",
      "..........\n",
      "[9,  2000] loss: 6.355461\n",
      "..........\n",
      "[9,  2100] loss: 6.287801\n",
      "..........\n",
      "[9,  2200] loss: 6.289671\n",
      "..........\n",
      "[9,  2300] loss: 6.169589\n",
      "..........\n",
      "[9,  2400] loss: 6.264382\n",
      "..........\n",
      "[9,  2500] loss: 6.367509\n",
      "..........\n",
      "[9,  2600] loss: 6.275988\n",
      "..........\n",
      "[9,  2700] loss: 6.274006\n",
      "..........\n",
      "[9,  2800] loss: 6.182160\n",
      "..........\n",
      "[9,  2900] loss: 6.243303\n",
      "..........\n",
      "[9,  3000] loss: 6.382362\n",
      "..........\n",
      "[9,  3100] loss: 6.239962\n",
      "..........\n",
      "[9,  3200] loss: 6.256531\n",
      "..........\n",
      "[9,  3300] loss: 6.216821\n",
      "..........\n",
      "[9,  3400] loss: 6.263945\n",
      "..........\n",
      "[9,  3500] loss: 6.342667\n",
      "..........\n",
      "[9,  3600] loss: 6.258944\n",
      "..........\n",
      "[9,  3700] loss: 6.256163\n",
      "..........\n",
      "[9,  3800] loss: 6.261473\n",
      "..........\n",
      "[9,  3900] loss: 6.237513\n",
      "..........\n",
      "[9,  4000] loss: 6.348497\n",
      "total average loss : 0.196\n",
      "== epoch  8 == train acc : 0.1086\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.065\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[10,   100] loss: 6.281987\n",
      "..........\n",
      "[10,   200] loss: 6.268297\n",
      "..........\n",
      "[10,   300] loss: 6.228527\n",
      "..........\n",
      "[10,   400] loss: 6.309540\n",
      "..........\n",
      "[10,   500] loss: 6.316821\n",
      "..........\n",
      "[10,   600] loss: 6.227387\n",
      "..........\n",
      "[10,   700] loss: 6.288210\n",
      "..........\n",
      "[10,   800] loss: 6.214049\n",
      "..........\n",
      "[10,   900] loss: 6.278997\n",
      "..........\n",
      "[10,  1000] loss: 6.345256\n",
      "..........\n",
      "[10,  1100] loss: 6.232583\n",
      "..........\n",
      "[10,  1200] loss: 6.331696\n",
      "..........\n",
      "[10,  1300] loss: 6.199092\n",
      "..........\n",
      "[10,  1400] loss: 6.318546\n",
      "..........\n",
      "[10,  1500] loss: 6.337089\n",
      "..........\n",
      "[10,  1600] loss: 6.212889\n",
      "..........\n",
      "[10,  1700] loss: 6.297840\n",
      "..........\n",
      "[10,  1800] loss: 6.198713\n",
      "..........\n",
      "[10,  1900] loss: 6.243336\n",
      "..........\n",
      "[10,  2000] loss: 6.352585\n",
      "..........\n",
      "[10,  2100] loss: 6.242127\n",
      "..........\n",
      "[10,  2200] loss: 6.306981\n",
      "..........\n",
      "[10,  2300] loss: 6.195954\n",
      "..........\n",
      "[10,  2400] loss: 6.255218\n",
      "..........\n",
      "[10,  2500] loss: 6.363700\n",
      "..........\n",
      "[10,  2600] loss: 6.264842\n",
      "..........\n",
      "[10,  2700] loss: 6.285982\n",
      "..........\n",
      "[10,  2800] loss: 6.233628\n",
      "..........\n",
      "[10,  2900] loss: 6.260091\n",
      "..........\n",
      "[10,  3000] loss: 6.388783\n",
      "..........\n",
      "[10,  3100] loss: 6.305640\n",
      "..........\n",
      "[10,  3200] loss: 6.264299\n",
      "..........\n",
      "[10,  3300] loss: 6.218023\n",
      "..........\n",
      "[10,  3400] loss: 6.260487\n",
      "..........\n",
      "[10,  3500] loss: 6.335245\n",
      "..........\n",
      "[10,  3600] loss: 6.280137\n",
      "..........\n",
      "[10,  3700] loss: 6.258425\n",
      "..........\n",
      "[10,  3800] loss: 6.218709\n",
      "..........\n",
      "[10,  3900] loss: 6.247320\n",
      "..........\n",
      "[10,  4000] loss: 6.319208\n",
      "total average loss : 0.196\n",
      "== epoch  9 == train acc : 0.1094\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.052\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.069\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[11,   100] loss: 6.263050\n",
      "..........\n",
      "[11,   200] loss: 6.302030\n",
      "..........\n",
      "[11,   300] loss: 6.214264\n",
      "..........\n",
      "[11,   400] loss: 6.266456\n",
      "..........\n",
      "[11,   500] loss: 6.297878\n",
      "..........\n",
      "[11,   600] loss: 6.262571\n",
      "..........\n",
      "[11,   700] loss: 6.283492\n",
      "..........\n",
      "[11,   800] loss: 6.221399\n",
      "..........\n",
      "[11,   900] loss: 6.315039\n",
      "..........\n",
      "[11,  1000] loss: 6.311544\n",
      "..........\n",
      "[11,  1100] loss: 6.259760\n",
      "..........\n",
      "[11,  1200] loss: 6.319542\n",
      "..........\n",
      "[11,  1300] loss: 6.197917\n",
      "..........\n",
      "[11,  1400] loss: 6.285083\n",
      "..........\n",
      "[11,  1500] loss: 6.333054\n",
      "..........\n",
      "[11,  1600] loss: 6.242730\n",
      "..........\n",
      "[11,  1700] loss: 6.295423\n",
      "..........\n",
      "[11,  1800] loss: 6.197253\n",
      "..........\n",
      "[11,  1900] loss: 6.275497\n",
      "..........\n",
      "[11,  2000] loss: 6.356565\n",
      "..........\n",
      "[11,  2100] loss: 6.247253\n",
      "..........\n",
      "[11,  2200] loss: 6.314669\n",
      "..........\n",
      "[11,  2300] loss: 6.218849\n",
      "..........\n",
      "[11,  2400] loss: 6.282098\n",
      "..........\n",
      "[11,  2500] loss: 6.379644\n",
      "..........\n",
      "[11,  2600] loss: 6.253256\n",
      "..........\n",
      "[11,  2700] loss: 6.272233\n",
      "..........\n",
      "[11,  2800] loss: 6.201268\n",
      "..........\n",
      "[11,  2900] loss: 6.244905\n",
      "..........\n",
      "[11,  3000] loss: 6.385080\n",
      "..........\n",
      "[11,  3100] loss: 6.306286\n",
      "..........\n",
      "[11,  3200] loss: 6.248649\n",
      "..........\n",
      "[11,  3300] loss: 6.191592\n",
      "..........\n",
      "[11,  3400] loss: 6.289337\n",
      "..........\n",
      "[11,  3500] loss: 6.306541\n",
      "..........\n",
      "[11,  3600] loss: 6.264733\n",
      "..........\n",
      "[11,  3700] loss: 6.243541\n",
      "..........\n",
      "[11,  3800] loss: 6.246518\n",
      "..........\n",
      "[11,  3900] loss: 6.246861\n",
      "..........\n",
      "[11,  4000] loss: 6.294150\n",
      "total average loss : 0.196\n",
      "== epoch 10 == train acc : 0.0922\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.109\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[12,   100] loss: 6.262674\n",
      "..........\n",
      "[12,   200] loss: 6.261089\n",
      "..........\n",
      "[12,   300] loss: 6.215161\n",
      "..........\n",
      "[12,   400] loss: 6.291347\n",
      "..........\n",
      "[12,   500] loss: 6.308941\n",
      "..........\n",
      "[12,   600] loss: 6.201429\n",
      "..........\n",
      "[12,   700] loss: 6.304334\n",
      "..........\n",
      "[12,   800] loss: 6.218747\n",
      "..........\n",
      "[12,   900] loss: 6.303009\n",
      "..........\n",
      "[12,  1000] loss: 6.354654\n",
      "..........\n",
      "[12,  1100] loss: 6.249772\n",
      "..........\n",
      "[12,  1200] loss: 6.334485\n",
      "..........\n",
      "[12,  1300] loss: 6.227246\n",
      "..........\n",
      "[12,  1400] loss: 6.245238\n",
      "..........\n",
      "[12,  1500] loss: 6.344429\n",
      "..........\n",
      "[12,  1600] loss: 6.255922\n",
      "..........\n",
      "[12,  1700] loss: 6.299246\n",
      "..........\n",
      "[12,  1800] loss: 6.197532\n",
      "..........\n",
      "[12,  1900] loss: 6.269642\n",
      "..........\n",
      "[12,  2000] loss: 6.361858\n",
      "..........\n",
      "[12,  2100] loss: 6.253608\n",
      "..........\n",
      "[12,  2200] loss: 6.259576\n",
      "..........\n",
      "[12,  2300] loss: 6.211366\n",
      "..........\n",
      "[12,  2400] loss: 6.230664\n",
      "..........\n",
      "[12,  2500] loss: 6.360258\n",
      "..........\n",
      "[12,  2600] loss: 6.294061\n",
      "..........\n",
      "[12,  2700] loss: 6.301158\n",
      "..........\n",
      "[12,  2800] loss: 6.224561\n",
      "..........\n",
      "[12,  2900] loss: 6.242187\n",
      "..........\n",
      "[12,  3000] loss: 6.358629\n",
      "..........\n",
      "[12,  3100] loss: 6.255414\n",
      "..........\n",
      "[12,  3200] loss: 6.265365\n",
      "..........\n",
      "[12,  3300] loss: 6.258896\n",
      "..........\n",
      "[12,  3400] loss: 6.264371\n",
      "..........\n",
      "[12,  3500] loss: 6.340453\n",
      "..........\n",
      "[12,  3600] loss: 6.261582\n",
      "..........\n",
      "[12,  3700] loss: 6.234715\n",
      "..........\n",
      "[12,  3800] loss: 6.253371\n",
      "..........\n",
      "[12,  3900] loss: 6.255904\n",
      "..........\n",
      "[12,  4000] loss: 6.320889\n",
      "total average loss : 0.196\n",
      "== epoch 11 == train acc : 0.1031\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.052\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[13,   100] loss: 6.258227\n",
      "..........\n",
      "[13,   200] loss: 6.277737\n",
      "..........\n",
      "[13,   300] loss: 6.232799\n",
      "..........\n",
      "[13,   400] loss: 6.266031\n",
      "..........\n",
      "[13,   500] loss: 6.322704\n",
      "..........\n",
      "[13,   600] loss: 6.224748\n",
      "..........\n",
      "[13,   700] loss: 6.279667\n",
      "..........\n",
      "[13,   800] loss: 6.231382\n",
      "..........\n",
      "[13,   900] loss: 6.290114\n",
      "..........\n",
      "[13,  1000] loss: 6.325924\n",
      "..........\n",
      "[13,  1100] loss: 6.245424\n",
      "..........\n",
      "[13,  1200] loss: 6.335023\n",
      "..........\n",
      "[13,  1300] loss: 6.183861\n",
      "..........\n",
      "[13,  1400] loss: 6.281044\n",
      "..........\n",
      "[13,  1500] loss: 6.386577\n",
      "..........\n",
      "[13,  1600] loss: 6.263159\n",
      "..........\n",
      "[13,  1700] loss: 6.301829\n",
      "..........\n",
      "[13,  1800] loss: 6.174942\n",
      "..........\n",
      "[13,  1900] loss: 6.284902\n",
      "..........\n",
      "[13,  2000] loss: 6.355595\n",
      "..........\n",
      "[13,  2100] loss: 6.232136\n",
      "..........\n",
      "[13,  2200] loss: 6.294691\n",
      "..........\n",
      "[13,  2300] loss: 6.213738\n",
      "..........\n",
      "[13,  2400] loss: 6.284717\n",
      "..........\n",
      "[13,  2500] loss: 6.343045\n",
      "..........\n",
      "[13,  2600] loss: 6.278767\n",
      "..........\n",
      "[13,  2700] loss: 6.230003\n",
      "..........\n",
      "[13,  2800] loss: 6.214177\n",
      "..........\n",
      "[13,  2900] loss: 6.241785\n",
      "..........\n",
      "[13,  3000] loss: 6.381420\n",
      "..........\n",
      "[13,  3100] loss: 6.263041\n",
      "..........\n",
      "[13,  3200] loss: 6.243221\n",
      "..........\n",
      "[13,  3300] loss: 6.208112\n",
      "..........\n",
      "[13,  3400] loss: 6.248593\n",
      "..........\n",
      "[13,  3500] loss: 6.362375\n",
      "..........\n",
      "[13,  3600] loss: 6.290344\n",
      "..........\n",
      "[13,  3700] loss: 6.256539\n",
      "..........\n",
      "[13,  3800] loss: 6.227785\n",
      "..........\n",
      "[13,  3900] loss: 6.285830\n",
      "..........\n",
      "[13,  4000] loss: 6.297079\n",
      "total average loss : 0.196\n",
      "== epoch 12 == train acc : 0.0961\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.109\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.078\n",
      "step : 1000 / 1563 acc : 0.069\n",
      "step : 1200 / 1563 acc : 0.062\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[14,   100] loss: 6.248794\n",
      "..........\n",
      "[14,   200] loss: 6.322233\n",
      "..........\n",
      "[14,   300] loss: 6.209078\n",
      "..........\n",
      "[14,   400] loss: 6.317450\n",
      "..........\n",
      "[14,   500] loss: 6.320255\n",
      "..........\n",
      "[14,   600] loss: 6.210662\n",
      "..........\n",
      "[14,   700] loss: 6.321555\n",
      "..........\n",
      "[14,   800] loss: 6.226051\n",
      "..........\n",
      "[14,   900] loss: 6.276989\n",
      "..........\n",
      "[14,  1000] loss: 6.270447\n",
      "..........\n",
      "[14,  1100] loss: 6.244867\n",
      "..........\n",
      "[14,  1200] loss: 6.320829\n",
      "..........\n",
      "[14,  1300] loss: 6.193145\n",
      "..........\n",
      "[14,  1400] loss: 6.240452\n",
      "..........\n",
      "[14,  1500] loss: 6.364269\n",
      "..........\n",
      "[14,  1600] loss: 6.227220\n",
      "..........\n",
      "[14,  1700] loss: 6.342864\n",
      "..........\n",
      "[14,  1800] loss: 6.156133\n",
      "..........\n",
      "[14,  1900] loss: 6.253682\n",
      "..........\n",
      "[14,  2000] loss: 6.336024\n",
      "..........\n",
      "[14,  2100] loss: 6.265761\n",
      "..........\n",
      "[14,  2200] loss: 6.319004\n",
      "..........\n",
      "[14,  2300] loss: 6.212514\n",
      "..........\n",
      "[14,  2400] loss: 6.255500\n",
      "..........\n",
      "[14,  2500] loss: 6.372582\n",
      "..........\n",
      "[14,  2600] loss: 6.264812\n",
      "..........\n",
      "[14,  2700] loss: 6.258241\n",
      "..........\n",
      "[14,  2800] loss: 6.207590\n",
      "..........\n",
      "[14,  2900] loss: 6.272501\n",
      "..........\n",
      "[14,  3000] loss: 6.342811\n",
      "..........\n",
      "[14,  3100] loss: 6.260882\n",
      "..........\n",
      "[14,  3200] loss: 6.283981\n",
      "..........\n",
      "[14,  3300] loss: 6.241518\n",
      "..........\n",
      "[14,  3400] loss: 6.285451\n",
      "..........\n",
      "[14,  3500] loss: 6.362294\n",
      "..........\n",
      "[14,  3600] loss: 6.282093\n",
      "..........\n",
      "[14,  3700] loss: 6.249462\n",
      "..........\n",
      "[14,  3800] loss: 6.226767\n",
      "..........\n",
      "[14,  3900] loss: 6.258651\n",
      "..........\n",
      "[14,  4000] loss: 6.322413\n",
      "total average loss : 0.196\n",
      "== epoch 13 == train acc : 0.0984\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.031\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.069\n",
      "step : 1200 / 1563 acc : 0.065\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[15,   100] loss: 6.246053\n",
      "..........\n",
      "[15,   200] loss: 6.277740\n",
      "..........\n",
      "[15,   300] loss: 6.238889\n",
      "..........\n",
      "[15,   400] loss: 6.295503\n",
      "..........\n",
      "[15,   500] loss: 6.338917\n",
      "..........\n",
      "[15,   600] loss: 6.238069\n",
      "..........\n",
      "[15,   700] loss: 6.305164\n",
      "..........\n",
      "[15,   800] loss: 6.219806\n",
      "..........\n",
      "[15,   900] loss: 6.266853\n",
      "..........\n",
      "[15,  1000] loss: 6.314584\n",
      "..........\n",
      "[15,  1100] loss: 6.228853\n",
      "..........\n",
      "[15,  1200] loss: 6.306170\n",
      "..........\n",
      "[15,  1300] loss: 6.169720\n",
      "..........\n",
      "[15,  1400] loss: 6.289128\n",
      "..........\n",
      "[15,  1500] loss: 6.348972\n",
      "..........\n",
      "[15,  1600] loss: 6.195016\n",
      "..........\n",
      "[15,  1700] loss: 6.327298\n",
      "..........\n",
      "[15,  1800] loss: 6.224748\n",
      "..........\n",
      "[15,  1900] loss: 6.248339\n",
      "..........\n",
      "[15,  2000] loss: 6.362125\n",
      "..........\n",
      "[15,  2100] loss: 6.254236\n",
      "..........\n",
      "[15,  2200] loss: 6.288212\n",
      "..........\n",
      "[15,  2300] loss: 6.205516\n",
      "..........\n",
      "[15,  2400] loss: 6.262219\n",
      "..........\n",
      "[15,  2500] loss: 6.335904\n",
      "..........\n",
      "[15,  2600] loss: 6.255774\n",
      "..........\n",
      "[15,  2700] loss: 6.325202\n",
      "..........\n",
      "[15,  2800] loss: 6.222091\n",
      "..........\n",
      "[15,  2900] loss: 6.246870\n",
      "..........\n",
      "[15,  3000] loss: 6.358646\n",
      "..........\n",
      "[15,  3100] loss: 6.267468\n",
      "..........\n",
      "[15,  3200] loss: 6.238612\n",
      "..........\n",
      "[15,  3300] loss: 6.227171\n",
      "..........\n",
      "[15,  3400] loss: 6.244041\n",
      "..........\n",
      "[15,  3500] loss: 6.336559\n",
      "..........\n",
      "[15,  3600] loss: 6.278401\n",
      "..........\n",
      "[15,  3700] loss: 6.278175\n",
      "..........\n",
      "[15,  3800] loss: 6.240906\n",
      "..........\n",
      "[15,  3900] loss: 6.260697\n",
      "..........\n",
      "[15,  4000] loss: 6.359268\n",
      "total average loss : 0.196\n",
      "== epoch 14 == train acc : 0.0961\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.070\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[16,   100] loss: 6.263984\n",
      "..........\n",
      "[16,   200] loss: 6.295878\n",
      "..........\n",
      "[16,   300] loss: 6.219805\n",
      "..........\n",
      "[16,   400] loss: 6.291169\n",
      "..........\n",
      "[16,   500] loss: 6.313706\n",
      "..........\n",
      "[16,   600] loss: 6.228792\n",
      "..........\n",
      "[16,   700] loss: 6.313371\n",
      "..........\n",
      "[16,   800] loss: 6.190768\n",
      "..........\n",
      "[16,   900] loss: 6.294014\n",
      "..........\n",
      "[16,  1000] loss: 6.329565\n",
      "..........\n",
      "[16,  1100] loss: 6.237413\n",
      "..........\n",
      "[16,  1200] loss: 6.331464\n",
      "..........\n",
      "[16,  1300] loss: 6.182856\n",
      "..........\n",
      "[16,  1400] loss: 6.303162\n",
      "..........\n",
      "[16,  1500] loss: 6.347476\n",
      "..........\n",
      "[16,  1600] loss: 6.193943\n",
      "..........\n",
      "[16,  1700] loss: 6.320977\n",
      "..........\n",
      "[16,  1800] loss: 6.194860\n",
      "..........\n",
      "[16,  1900] loss: 6.294924\n",
      "..........\n",
      "[16,  2000] loss: 6.352419\n",
      "..........\n",
      "[16,  2100] loss: 6.267399\n",
      "..........\n",
      "[16,  2200] loss: 6.303120\n",
      "..........\n",
      "[16,  2300] loss: 6.228896\n",
      "..........\n",
      "[16,  2400] loss: 6.286550\n",
      "..........\n",
      "[16,  2500] loss: 6.370349\n",
      "..........\n",
      "[16,  2600] loss: 6.278730\n",
      "..........\n",
      "[16,  2700] loss: 6.251281\n",
      "..........\n",
      "[16,  2800] loss: 6.188966\n",
      "..........\n",
      "[16,  2900] loss: 6.213010\n",
      "..........\n",
      "[16,  3000] loss: 6.345949\n",
      "..........\n",
      "[16,  3100] loss: 6.271620\n",
      "..........\n",
      "[16,  3200] loss: 6.271352\n",
      "..........\n",
      "[16,  3300] loss: 6.242431\n",
      "..........\n",
      "[16,  3400] loss: 6.262042\n",
      "..........\n",
      "[16,  3500] loss: 6.319882\n",
      "..........\n",
      "[16,  3600] loss: 6.280443\n",
      "..........\n",
      "[16,  3700] loss: 6.209455\n",
      "..........\n",
      "[16,  3800] loss: 6.252798\n",
      "..........\n",
      "[16,  3900] loss: 6.280715\n",
      "..........\n",
      "[16,  4000] loss: 6.339530\n",
      "total average loss : 0.196\n",
      "== epoch 15 == train acc : 0.0992\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.125\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.094\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.081\n",
      "step : 1200 / 1563 acc : 0.078\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[17,   100] loss: 6.242240\n",
      "..........\n",
      "[17,   200] loss: 6.268554\n",
      "..........\n",
      "[17,   300] loss: 6.221782\n",
      "..........\n",
      "[17,   400] loss: 6.275037\n",
      "..........\n",
      "[17,   500] loss: 6.336720\n",
      "..........\n",
      "[17,   600] loss: 6.249903\n",
      "..........\n",
      "[17,   700] loss: 6.315807\n",
      "..........\n",
      "[17,   800] loss: 6.214697\n",
      "..........\n",
      "[17,   900] loss: 6.324447\n",
      "..........\n",
      "[17,  1000] loss: 6.348016\n",
      "..........\n",
      "[17,  1100] loss: 6.220067\n",
      "..........\n",
      "[17,  1200] loss: 6.318761\n",
      "..........\n",
      "[17,  1300] loss: 6.230955\n",
      "..........\n",
      "[17,  1400] loss: 6.288206\n",
      "..........\n",
      "[17,  1500] loss: 6.342325\n",
      "..........\n",
      "[17,  1600] loss: 6.233645\n",
      "..........\n",
      "[17,  1700] loss: 6.313233\n",
      "..........\n",
      "[17,  1800] loss: 6.210459\n",
      "..........\n",
      "[17,  1900] loss: 6.276473\n",
      "..........\n",
      "[17,  2000] loss: 6.368817\n",
      "..........\n",
      "[17,  2100] loss: 6.231057\n",
      "..........\n",
      "[17,  2200] loss: 6.268267\n",
      "..........\n",
      "[17,  2300] loss: 6.222565\n",
      "..........\n",
      "[17,  2400] loss: 6.258601\n",
      "..........\n",
      "[17,  2500] loss: 6.317882\n",
      "..........\n",
      "[17,  2600] loss: 6.264594\n",
      "..........\n",
      "[17,  2700] loss: 6.253522\n",
      "..........\n",
      "[17,  2800] loss: 6.204213\n",
      "..........\n",
      "[17,  2900] loss: 6.260695\n",
      "..........\n",
      "[17,  3000] loss: 6.322908\n",
      "..........\n",
      "[17,  3100] loss: 6.264667\n",
      "..........\n",
      "[17,  3200] loss: 6.234530\n",
      "..........\n",
      "[17,  3300] loss: 6.252781\n",
      "..........\n",
      "[17,  3400] loss: 6.270282\n",
      "..........\n",
      "[17,  3500] loss: 6.356232\n",
      "..........\n",
      "[17,  3600] loss: 6.275834\n",
      "..........\n",
      "[17,  3700] loss: 6.244070\n",
      "..........\n",
      "[17,  3800] loss: 6.212400\n",
      "..........\n",
      "[17,  3900] loss: 6.259564\n",
      "..........\n",
      "[17,  4000] loss: 6.330598\n",
      "total average loss : 0.196\n",
      "== epoch 16 == train acc : 0.1102\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.102\n",
      "step : 600 / 1563 acc : 0.089\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.078\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[18,   100] loss: 6.264580\n",
      "..........\n",
      "[18,   200] loss: 6.275062\n",
      "..........\n",
      "[18,   300] loss: 6.215640\n",
      "..........\n",
      "[18,   400] loss: 6.307517\n",
      "..........\n",
      "[18,   500] loss: 6.304400\n",
      "..........\n",
      "[18,   600] loss: 6.228152\n",
      "..........\n",
      "[18,   700] loss: 6.288126\n",
      "..........\n",
      "[18,   800] loss: 6.198867\n",
      "..........\n",
      "[18,   900] loss: 6.295599\n",
      "..........\n",
      "[18,  1000] loss: 6.355846\n",
      "..........\n",
      "[18,  1100] loss: 6.244145\n",
      "..........\n",
      "[18,  1200] loss: 6.309968\n",
      "..........\n",
      "[18,  1300] loss: 6.217464\n",
      "..........\n",
      "[18,  1400] loss: 6.252851\n",
      "..........\n",
      "[18,  1500] loss: 6.350488\n",
      "..........\n",
      "[18,  1600] loss: 6.238219\n",
      "..........\n",
      "[18,  1700] loss: 6.292580\n",
      "..........\n",
      "[18,  1800] loss: 6.173419\n",
      "..........\n",
      "[18,  1900] loss: 6.250979\n",
      "..........\n",
      "[18,  2000] loss: 6.367269\n",
      "..........\n",
      "[18,  2100] loss: 6.269095\n",
      "..........\n",
      "[18,  2200] loss: 6.275054\n",
      "..........\n",
      "[18,  2300] loss: 6.224742\n",
      "..........\n",
      "[18,  2400] loss: 6.244231\n",
      "..........\n",
      "[18,  2500] loss: 6.395010\n",
      "..........\n",
      "[18,  2600] loss: 6.250195\n",
      "..........\n",
      "[18,  2700] loss: 6.264486\n",
      "..........\n",
      "[18,  2800] loss: 6.199321\n",
      "..........\n",
      "[18,  2900] loss: 6.244597\n",
      "..........\n",
      "[18,  3000] loss: 6.370409\n",
      "..........\n",
      "[18,  3100] loss: 6.274080\n",
      "..........\n",
      "[18,  3200] loss: 6.268198\n",
      "..........\n",
      "[18,  3300] loss: 6.249252\n",
      "..........\n",
      "[18,  3400] loss: 6.254381\n",
      "..........\n",
      "[18,  3500] loss: 6.356193\n",
      "..........\n",
      "[18,  3600] loss: 6.259368\n",
      "..........\n",
      "[18,  3700] loss: 6.241257\n",
      "..........\n",
      "[18,  3800] loss: 6.248179\n",
      "..........\n",
      "[18,  3900] loss: 6.259121\n",
      "..........\n",
      "[18,  4000] loss: 6.343851\n",
      "total average loss : 0.196\n",
      "== epoch 17 == train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[19,   100] loss: 6.246838\n",
      "..........\n",
      "[19,   200] loss: 6.259521\n",
      "..........\n",
      "[19,   300] loss: 6.250973\n",
      "..........\n",
      "[19,   400] loss: 6.291073\n",
      "..........\n",
      "[19,   500] loss: 6.298379\n",
      "..........\n",
      "[19,   600] loss: 6.261298\n",
      "..........\n",
      "[19,   700] loss: 6.298420\n",
      "..........\n",
      "[19,   800] loss: 6.165821\n",
      "..........\n",
      "[19,   900] loss: 6.314282\n",
      "..........\n",
      "[19,  1000] loss: 6.332399\n",
      "..........\n",
      "[19,  1100] loss: 6.252822\n",
      "..........\n",
      "[19,  1200] loss: 6.325860\n",
      "..........\n",
      "[19,  1300] loss: 6.195652\n",
      "..........\n",
      "[19,  1400] loss: 6.281625\n",
      "..........\n",
      "[19,  1500] loss: 6.371009\n",
      "..........\n",
      "[19,  1600] loss: 6.250473\n",
      "..........\n",
      "[19,  1700] loss: 6.319283\n",
      "..........\n",
      "[19,  1800] loss: 6.189889\n",
      "..........\n",
      "[19,  1900] loss: 6.218250\n",
      "..........\n",
      "[19,  2000] loss: 6.360031\n",
      "..........\n",
      "[19,  2100] loss: 6.250934\n",
      "..........\n",
      "[19,  2200] loss: 6.308114\n",
      "..........\n",
      "[19,  2300] loss: 6.206287\n",
      "..........\n",
      "[19,  2400] loss: 6.257311\n",
      "..........\n",
      "[19,  2500] loss: 6.351148\n",
      "..........\n",
      "[19,  2600] loss: 6.245916\n",
      "..........\n",
      "[19,  2700] loss: 6.267050\n",
      "..........\n",
      "[19,  2800] loss: 6.210940\n",
      "..........\n",
      "[19,  2900] loss: 6.259991\n",
      "..........\n",
      "[19,  3000] loss: 6.357005\n",
      "..........\n",
      "[19,  3100] loss: 6.322042\n",
      "..........\n",
      "[19,  3200] loss: 6.246076\n",
      "..........\n",
      "[19,  3300] loss: 6.220385\n",
      "..........\n",
      "[19,  3400] loss: 6.271173\n",
      "..........\n",
      "[19,  3500] loss: 6.330332\n",
      "..........\n",
      "[19,  3600] loss: 6.285671\n",
      "..........\n",
      "[19,  3700] loss: 6.240659\n",
      "..........\n",
      "[19,  3800] loss: 6.232981\n",
      "..........\n",
      "[19,  3900] loss: 6.270023\n",
      "..........\n",
      "[19,  4000] loss: 6.353802\n",
      "total average loss : 0.196\n",
      "== epoch 18 == train acc : 0.0945\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.078\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[20,   100] loss: 6.276910\n",
      "..........\n",
      "[20,   200] loss: 6.280403\n",
      "..........\n",
      "[20,   300] loss: 6.234841\n",
      "..........\n",
      "[20,   400] loss: 6.277030\n",
      "..........\n",
      "[20,   500] loss: 6.308823\n",
      "..........\n",
      "[20,   600] loss: 6.255697\n",
      "..........\n",
      "[20,   700] loss: 6.301535\n",
      "..........\n",
      "[20,   800] loss: 6.255738\n",
      "..........\n",
      "[20,   900] loss: 6.339093\n",
      "..........\n",
      "[20,  1000] loss: 6.345272\n",
      "..........\n",
      "[20,  1100] loss: 6.218800\n",
      "..........\n",
      "[20,  1200] loss: 6.250416\n",
      "..........\n",
      "[20,  1300] loss: 6.199004\n",
      "..........\n",
      "[20,  1400] loss: 6.263163\n",
      "..........\n",
      "[20,  1500] loss: 6.310118\n",
      "..........\n",
      "[20,  1600] loss: 6.283252\n",
      "..........\n",
      "[20,  1700] loss: 6.337447\n",
      "..........\n",
      "[20,  1800] loss: 6.193020\n",
      "..........\n",
      "[20,  1900] loss: 6.249194\n",
      "..........\n",
      "[20,  2000] loss: 6.382097\n",
      "..........\n",
      "[20,  2100] loss: 6.252262\n",
      "..........\n",
      "[20,  2200] loss: 6.278827\n",
      "..........\n",
      "[20,  2300] loss: 6.237866\n",
      "..........\n",
      "[20,  2400] loss: 6.277668\n",
      "..........\n",
      "[20,  2500] loss: 6.328940\n",
      "..........\n",
      "[20,  2600] loss: 6.265413\n",
      "..........\n",
      "[20,  2700] loss: 6.286879\n",
      "..........\n",
      "[20,  2800] loss: 6.207511\n",
      "..........\n",
      "[20,  2900] loss: 6.262867\n",
      "..........\n",
      "[20,  3000] loss: 6.363098\n",
      "..........\n",
      "[20,  3100] loss: 6.273137\n",
      "..........\n",
      "[20,  3200] loss: 6.269535\n",
      "..........\n",
      "[20,  3300] loss: 6.230989\n",
      "..........\n",
      "[20,  3400] loss: 6.236775\n",
      "..........\n",
      "[20,  3500] loss: 6.297715\n",
      "..........\n",
      "[20,  3600] loss: 6.270559\n",
      "..........\n",
      "[20,  3700] loss: 6.244909\n",
      "..........\n",
      "[20,  3800] loss: 6.263401\n",
      "..........\n",
      "[20,  3900] loss: 6.261243\n",
      "..........\n",
      "[20,  4000] loss: 6.328257\n",
      "total average loss : 0.196\n",
      "== epoch 19 == train acc : 0.0906\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.078\n",
      "step : 1000 / 1563 acc : 0.081\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[21,   100] loss: 6.272222\n",
      "..........\n",
      "[21,   200] loss: 6.268716\n",
      "..........\n",
      "[21,   300] loss: 6.223444\n",
      "..........\n",
      "[21,   400] loss: 6.266232\n",
      "..........\n",
      "[21,   500] loss: 6.296048\n",
      "..........\n",
      "[21,   600] loss: 6.218165\n",
      "..........\n",
      "[21,   700] loss: 6.275007\n",
      "..........\n",
      "[21,   800] loss: 6.218461\n",
      "..........\n",
      "[21,   900] loss: 6.265297\n",
      "..........\n",
      "[21,  1000] loss: 6.333927\n",
      "..........\n",
      "[21,  1100] loss: 6.234358\n",
      "..........\n",
      "[21,  1200] loss: 6.287243\n",
      "..........\n",
      "[21,  1300] loss: 6.173048\n",
      "..........\n",
      "[21,  1400] loss: 6.292476\n",
      "..........\n",
      "[21,  1500] loss: 6.345284\n",
      "..........\n",
      "[21,  1600] loss: 6.242574\n",
      "..........\n",
      "[21,  1700] loss: 6.342398\n",
      "..........\n",
      "[21,  1800] loss: 6.200581\n",
      "..........\n",
      "[21,  1900] loss: 6.268817\n",
      "..........\n",
      "[21,  2000] loss: 6.360116\n",
      "..........\n",
      "[21,  2100] loss: 6.253145\n",
      "..........\n",
      "[21,  2200] loss: 6.308033\n",
      "..........\n",
      "[21,  2300] loss: 6.226367\n",
      "..........\n",
      "[21,  2400] loss: 6.304660\n",
      "..........\n",
      "[21,  2500] loss: 6.398445\n",
      "..........\n",
      "[21,  2600] loss: 6.259398\n",
      "..........\n",
      "[21,  2700] loss: 6.263977\n",
      "..........\n",
      "[21,  2800] loss: 6.201274\n",
      "..........\n",
      "[21,  2900] loss: 6.260044\n",
      "..........\n",
      "[21,  3000] loss: 6.336852\n",
      "..........\n",
      "[21,  3100] loss: 6.234559\n",
      "..........\n",
      "[21,  3200] loss: 6.281926\n",
      "..........\n",
      "[21,  3300] loss: 6.215345\n",
      "..........\n",
      "[21,  3400] loss: 6.273462\n",
      "..........\n",
      "[21,  3500] loss: 6.321081\n",
      "..........\n",
      "[21,  3600] loss: 6.272653\n",
      "..........\n",
      "[21,  3700] loss: 6.292887\n",
      "..........\n",
      "[21,  3800] loss: 6.232385\n",
      "..........\n",
      "[21,  3900] loss: 6.267829\n",
      "..........\n",
      "[21,  4000] loss: 6.349959\n",
      "total average loss : 0.196\n",
      "== epoch 20 == train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[22,   100] loss: 6.246058\n",
      "..........\n",
      "[22,   200] loss: 6.280241\n",
      "..........\n",
      "[22,   300] loss: 6.240906\n",
      "..........\n",
      "[22,   400] loss: 6.292819\n",
      "..........\n",
      "[22,   500] loss: 6.338772\n",
      "..........\n",
      "[22,   600] loss: 6.239621\n",
      "..........\n",
      "[22,   700] loss: 6.289424\n",
      "..........\n",
      "[22,   800] loss: 6.191206\n",
      "..........\n",
      "[22,   900] loss: 6.278378\n",
      "..........\n",
      "[22,  1000] loss: 6.323333\n",
      "..........\n",
      "[22,  1100] loss: 6.208528\n",
      "..........\n",
      "[22,  1200] loss: 6.302520\n",
      "..........\n",
      "[22,  1300] loss: 6.200348\n",
      "..........\n",
      "[22,  1400] loss: 6.297777\n",
      "..........\n",
      "[22,  1500] loss: 6.355709\n",
      "..........\n",
      "[22,  1600] loss: 6.252710\n",
      "..........\n",
      "[22,  1700] loss: 6.320703\n",
      "..........\n",
      "[22,  1800] loss: 6.193047\n",
      "..........\n",
      "[22,  1900] loss: 6.294031\n",
      "..........\n",
      "[22,  2000] loss: 6.325629\n",
      "..........\n",
      "[22,  2100] loss: 6.260304\n",
      "..........\n",
      "[22,  2200] loss: 6.298212\n",
      "..........\n",
      "[22,  2300] loss: 6.185321\n",
      "..........\n",
      "[22,  2400] loss: 6.285129\n",
      "..........\n",
      "[22,  2500] loss: 6.357818\n",
      "..........\n",
      "[22,  2600] loss: 6.270812\n",
      "..........\n",
      "[22,  2700] loss: 6.268157\n",
      "..........\n",
      "[22,  2800] loss: 6.236892\n",
      "..........\n",
      "[22,  2900] loss: 6.238454\n",
      "..........\n",
      "[22,  3000] loss: 6.358105\n",
      "..........\n",
      "[22,  3100] loss: 6.283724\n",
      "..........\n",
      "[22,  3200] loss: 6.247639\n",
      "..........\n",
      "[22,  3300] loss: 6.206865\n",
      "..........\n",
      "[22,  3400] loss: 6.265581\n",
      "..........\n",
      "[22,  3500] loss: 6.325181\n",
      "..........\n",
      "[22,  3600] loss: 6.302644\n",
      "..........\n",
      "[22,  3700] loss: 6.252420\n",
      "..........\n",
      "[22,  3800] loss: 6.224507\n",
      "..........\n",
      "[22,  3900] loss: 6.275109\n",
      "..........\n",
      "[22,  4000] loss: 6.329876\n",
      "total average loss : 0.196\n",
      "== epoch 21 == train acc : 0.1094\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.083\n",
      "step : 800 / 1563 acc : 0.070\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.062\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[23,   100] loss: 6.232614\n",
      "..........\n",
      "[23,   200] loss: 6.265784\n",
      "..........\n",
      "[23,   300] loss: 6.209812\n",
      "..........\n",
      "[23,   400] loss: 6.313617\n",
      "..........\n",
      "[23,   500] loss: 6.323655\n",
      "..........\n",
      "[23,   600] loss: 6.232777\n",
      "..........\n",
      "[23,   700] loss: 6.288295\n",
      "..........\n",
      "[23,   800] loss: 6.216263\n",
      "..........\n",
      "[23,   900] loss: 6.302221\n",
      "..........\n",
      "[23,  1000] loss: 6.347563\n",
      "..........\n",
      "[23,  1100] loss: 6.251299\n",
      "..........\n",
      "[23,  1200] loss: 6.299678\n",
      "..........\n",
      "[23,  1300] loss: 6.186000\n",
      "..........\n",
      "[23,  1400] loss: 6.241991\n",
      "..........\n",
      "[23,  1500] loss: 6.339007\n",
      "..........\n",
      "[23,  1600] loss: 6.236839\n",
      "..........\n",
      "[23,  1700] loss: 6.333758\n",
      "..........\n",
      "[23,  1800] loss: 6.193230\n",
      "..........\n",
      "[23,  1900] loss: 6.283725\n",
      "..........\n",
      "[23,  2000] loss: 6.380429\n",
      "..........\n",
      "[23,  2100] loss: 6.283683\n",
      "..........\n",
      "[23,  2200] loss: 6.290677\n",
      "..........\n",
      "[23,  2300] loss: 6.187461\n",
      "..........\n",
      "[23,  2400] loss: 6.228616\n",
      "..........\n",
      "[23,  2500] loss: 6.373192\n",
      "..........\n",
      "[23,  2600] loss: 6.262557\n",
      "..........\n",
      "[23,  2700] loss: 6.250433\n",
      "..........\n",
      "[23,  2800] loss: 6.237564\n",
      "..........\n",
      "[23,  2900] loss: 6.255695\n",
      "..........\n",
      "[23,  3000] loss: 6.358906\n",
      "..........\n",
      "[23,  3100] loss: 6.258745\n",
      "..........\n",
      "[23,  3200] loss: 6.243903\n",
      "..........\n",
      "[23,  3300] loss: 6.237097\n",
      "..........\n",
      "[23,  3400] loss: 6.255287\n",
      "..........\n",
      "[23,  3500] loss: 6.371411\n",
      "..........\n",
      "[23,  3600] loss: 6.287452\n",
      "..........\n",
      "[23,  3700] loss: 6.267509\n",
      "..........\n",
      "[23,  3800] loss: 6.251677\n",
      "..........\n",
      "[23,  3900] loss: 6.258864\n",
      "..........\n",
      "[23,  4000] loss: 6.319741\n",
      "total average loss : 0.196\n",
      "== epoch 22 == train acc : 0.0984\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.156\n",
      "step : 400 / 1563 acc : 0.109\n",
      "step : 600 / 1563 acc : 0.094\n",
      "step : 800 / 1563 acc : 0.094\n",
      "step : 1000 / 1563 acc : 0.087\n",
      "step : 1200 / 1563 acc : 0.078\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[24,   100] loss: 6.215774\n",
      "..........\n",
      "[24,   200] loss: 6.256743\n",
      "..........\n",
      "[24,   300] loss: 6.246386\n",
      "..........\n",
      "[24,   400] loss: 6.307734\n",
      "..........\n",
      "[24,   500] loss: 6.283233\n",
      "..........\n",
      "[24,   600] loss: 6.260902\n",
      "..........\n",
      "[24,   700] loss: 6.294508\n",
      "..........\n",
      "[24,   800] loss: 6.254852\n",
      "..........\n",
      "[24,   900] loss: 6.281939\n",
      "..........\n",
      "[24,  1000] loss: 6.345522\n",
      "..........\n",
      "[24,  1100] loss: 6.277557\n",
      "..........\n",
      "[24,  1200] loss: 6.330988\n",
      "..........\n",
      "[24,  1300] loss: 6.176221\n",
      "..........\n",
      "[24,  1400] loss: 6.297564\n",
      "..........\n",
      "[24,  1500] loss: 6.364298\n",
      "..........\n",
      "[24,  1600] loss: 6.257423\n",
      "..........\n",
      "[24,  1700] loss: 6.322321\n",
      "..........\n",
      "[24,  1800] loss: 6.196485\n",
      "..........\n",
      "[24,  1900] loss: 6.261949\n",
      "..........\n",
      "[24,  2000] loss: 6.373115\n",
      "..........\n",
      "[24,  2100] loss: 6.267826\n",
      "..........\n",
      "[24,  2200] loss: 6.272489\n",
      "..........\n",
      "[24,  2300] loss: 6.182762\n",
      "..........\n",
      "[24,  2400] loss: 6.276872\n",
      "..........\n",
      "[24,  2500] loss: 6.353256\n",
      "..........\n",
      "[24,  2600] loss: 6.251951\n",
      "..........\n",
      "[24,  2700] loss: 6.246041\n",
      "..........\n",
      "[24,  2800] loss: 6.205975\n",
      "..........\n",
      "[24,  2900] loss: 6.225414\n",
      "..........\n",
      "[24,  3000] loss: 6.362815\n",
      "..........\n",
      "[24,  3100] loss: 6.276518\n",
      "..........\n",
      "[24,  3200] loss: 6.250825\n",
      "..........\n",
      "[24,  3300] loss: 6.178297\n",
      "..........\n",
      "[24,  3400] loss: 6.282371\n",
      "..........\n",
      "[24,  3500] loss: 6.322580\n",
      "..........\n",
      "[24,  3600] loss: 6.269459\n",
      "..........\n",
      "[24,  3700] loss: 6.218744\n",
      "..........\n",
      "[24,  3800] loss: 6.263287\n",
      "..........\n",
      "[24,  3900] loss: 6.270586\n",
      "..........\n",
      "[24,  4000] loss: 6.334440\n",
      "total average loss : 0.196\n",
      "== epoch 23 == train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.031\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.078\n",
      "step : 1000 / 1563 acc : 0.078\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[25,   100] loss: 6.211352\n",
      "..........\n",
      "[25,   200] loss: 6.263175\n",
      "..........\n",
      "[25,   300] loss: 6.219210\n",
      "..........\n",
      "[25,   400] loss: 6.271386\n",
      "..........\n",
      "[25,   500] loss: 6.317912\n",
      "..........\n",
      "[25,   600] loss: 6.222875\n",
      "..........\n",
      "[25,   700] loss: 6.303862\n",
      "..........\n",
      "[25,   800] loss: 6.212549\n",
      "..........\n",
      "[25,   900] loss: 6.288834\n",
      "..........\n",
      "[25,  1000] loss: 6.336203\n",
      "..........\n",
      "[25,  1100] loss: 6.229113\n",
      "..........\n",
      "[25,  1200] loss: 6.306834\n",
      "..........\n",
      "[25,  1300] loss: 6.208101\n",
      "..........\n",
      "[25,  1400] loss: 6.265449\n",
      "..........\n",
      "[25,  1500] loss: 6.361216\n",
      "..........\n",
      "[25,  1600] loss: 6.217167\n",
      "..........\n",
      "[25,  1700] loss: 6.334120\n",
      "..........\n",
      "[25,  1800] loss: 6.176522\n",
      "..........\n",
      "[25,  1900] loss: 6.267640\n",
      "..........\n",
      "[25,  2000] loss: 6.394558\n",
      "..........\n",
      "[25,  2100] loss: 6.271547\n",
      "..........\n",
      "[25,  2200] loss: 6.304833\n",
      "..........\n",
      "[25,  2300] loss: 6.217617\n",
      "..........\n",
      "[25,  2400] loss: 6.294657\n",
      "..........\n",
      "[25,  2500] loss: 6.345180\n",
      "..........\n",
      "[25,  2600] loss: 6.260141\n",
      "..........\n",
      "[25,  2700] loss: 6.261810\n",
      "..........\n",
      "[25,  2800] loss: 6.185851\n",
      "..........\n",
      "[25,  2900] loss: 6.270330\n",
      "..........\n",
      "[25,  3000] loss: 6.349424\n",
      "..........\n",
      "[25,  3100] loss: 6.261049\n",
      "..........\n",
      "[25,  3200] loss: 6.304642\n",
      "..........\n",
      "[25,  3300] loss: 6.235243\n",
      "..........\n",
      "[25,  3400] loss: 6.258736\n",
      "..........\n",
      "[25,  3500] loss: 6.313437\n",
      "..........\n",
      "[25,  3600] loss: 6.279428\n",
      "..........\n",
      "[25,  3700] loss: 6.211401\n",
      "..........\n",
      "[25,  3800] loss: 6.246529\n",
      "..........\n",
      "[25,  3900] loss: 6.289841\n",
      "..........\n",
      "[25,  4000] loss: 6.329776\n",
      "total average loss : 0.196\n",
      "== epoch 24 == train acc : 0.1031\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.062\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[26,   100] loss: 6.244648\n",
      "..........\n",
      "[26,   200] loss: 6.282988\n",
      "..........\n",
      "[26,   300] loss: 6.242188\n",
      "..........\n",
      "[26,   400] loss: 6.316155\n",
      "..........\n",
      "[26,   500] loss: 6.322067\n",
      "..........\n",
      "[26,   600] loss: 6.244639\n",
      "..........\n",
      "[26,   700] loss: 6.309854\n",
      "..........\n",
      "[26,   800] loss: 6.201279\n",
      "..........\n",
      "[26,   900] loss: 6.295748\n",
      "..........\n",
      "[26,  1000] loss: 6.359792\n",
      "..........\n",
      "[26,  1100] loss: 6.219970\n",
      "..........\n",
      "[26,  1200] loss: 6.321539\n",
      "..........\n",
      "[26,  1300] loss: 6.201869\n",
      "..........\n",
      "[26,  1400] loss: 6.319195\n",
      "..........\n",
      "[26,  1500] loss: 6.332530\n",
      "..........\n",
      "[26,  1600] loss: 6.223417\n",
      "..........\n",
      "[26,  1700] loss: 6.289592\n",
      "..........\n",
      "[26,  1800] loss: 6.193154\n",
      "..........\n",
      "[26,  1900] loss: 6.297640\n",
      "..........\n",
      "[26,  2000] loss: 6.358725\n",
      "..........\n",
      "[26,  2100] loss: 6.296533\n",
      "..........\n",
      "[26,  2200] loss: 6.292874\n",
      "..........\n",
      "[26,  2300] loss: 6.202629\n",
      "..........\n",
      "[26,  2400] loss: 6.263644\n",
      "..........\n",
      "[26,  2500] loss: 6.349280\n",
      "..........\n",
      "[26,  2600] loss: 6.229684\n",
      "..........\n",
      "[26,  2700] loss: 6.247401\n",
      "..........\n",
      "[26,  2800] loss: 6.222299\n",
      "..........\n",
      "[26,  2900] loss: 6.251365\n",
      "..........\n",
      "[26,  3000] loss: 6.369306\n",
      "..........\n",
      "[26,  3100] loss: 6.248702\n",
      "..........\n",
      "[26,  3200] loss: 6.244182\n",
      "..........\n",
      "[26,  3300] loss: 6.199136\n",
      "..........\n",
      "[26,  3400] loss: 6.232129\n",
      "..........\n",
      "[26,  3500] loss: 6.340088\n",
      "..........\n",
      "[26,  3600] loss: 6.311543\n",
      "..........\n",
      "[26,  3700] loss: 6.239207\n",
      "..........\n",
      "[26,  3800] loss: 6.251291\n",
      "..........\n",
      "[26,  3900] loss: 6.276109\n",
      "..........\n",
      "[26,  4000] loss: 6.335642\n",
      "total average loss : 0.196\n",
      "== epoch 25 == train acc : 0.1180\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.089\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[27,   100] loss: 6.206673\n",
      "..........\n",
      "[27,   200] loss: 6.278931\n",
      "..........\n",
      "[27,   300] loss: 6.216695\n",
      "..........\n",
      "[27,   400] loss: 6.329231\n",
      "..........\n",
      "[27,   500] loss: 6.292896\n",
      "..........\n",
      "[27,   600] loss: 6.238789\n",
      "..........\n",
      "[27,   700] loss: 6.296712\n",
      "..........\n",
      "[27,   800] loss: 6.239625\n",
      "..........\n",
      "[27,   900] loss: 6.303407\n",
      "..........\n",
      "[27,  1000] loss: 6.316549\n",
      "..........\n",
      "[27,  1100] loss: 6.214461\n",
      "..........\n",
      "[27,  1200] loss: 6.325033\n",
      "..........\n",
      "[27,  1300] loss: 6.195413\n",
      "..........\n",
      "[27,  1400] loss: 6.305241\n",
      "..........\n",
      "[27,  1500] loss: 6.354344\n",
      "..........\n",
      "[27,  1600] loss: 6.237753\n",
      "..........\n",
      "[27,  1700] loss: 6.355278\n",
      "..........\n",
      "[27,  1800] loss: 6.166496\n",
      "..........\n",
      "[27,  1900] loss: 6.278944\n",
      "..........\n",
      "[27,  2000] loss: 6.346804\n",
      "..........\n",
      "[27,  2100] loss: 6.231574\n",
      "..........\n",
      "[27,  2200] loss: 6.270118\n",
      "..........\n",
      "[27,  2300] loss: 6.191079\n",
      "..........\n",
      "[27,  2400] loss: 6.241699\n",
      "..........\n",
      "[27,  2500] loss: 6.421731\n",
      "..........\n",
      "[27,  2600] loss: 6.247168\n",
      "..........\n",
      "[27,  2700] loss: 6.270352\n",
      "..........\n",
      "[27,  2800] loss: 6.227887\n",
      "..........\n",
      "[27,  2900] loss: 6.243430\n",
      "..........\n",
      "[27,  3000] loss: 6.362261\n",
      "..........\n",
      "[27,  3100] loss: 6.239410\n",
      "..........\n",
      "[27,  3200] loss: 6.253303\n",
      "..........\n",
      "[27,  3300] loss: 6.230576\n",
      "..........\n",
      "[27,  3400] loss: 6.242154\n",
      "..........\n",
      "[27,  3500] loss: 6.316714\n",
      "..........\n",
      "[27,  3600] loss: 6.298321\n",
      "..........\n",
      "[27,  3700] loss: 6.264459\n",
      "..........\n",
      "[27,  3800] loss: 6.243784\n",
      "..........\n",
      "[27,  3900] loss: 6.282323\n",
      "..........\n",
      "[27,  4000] loss: 6.323301\n",
      "total average loss : 0.196\n",
      "== epoch 26 == train acc : 0.1023\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.031\n",
      "step : 600 / 1563 acc : 0.047\n",
      "step : 800 / 1563 acc : 0.078\n",
      "step : 1000 / 1563 acc : 0.084\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[28,   100] loss: 6.240175\n",
      "..........\n",
      "[28,   200] loss: 6.277403\n",
      "..........\n",
      "[28,   300] loss: 6.234201\n",
      "..........\n",
      "[28,   400] loss: 6.325780\n",
      "..........\n",
      "[28,   500] loss: 6.350015\n",
      "..........\n",
      "[28,   600] loss: 6.237325\n",
      "..........\n",
      "[28,   700] loss: 6.299259\n",
      "..........\n",
      "[28,   800] loss: 6.229148\n",
      "..........\n",
      "[28,   900] loss: 6.281663\n",
      "..........\n",
      "[28,  1000] loss: 6.333146\n",
      "..........\n",
      "[28,  1100] loss: 6.220531\n",
      "..........\n",
      "[28,  1200] loss: 6.325827\n",
      "..........\n",
      "[28,  1300] loss: 6.176663\n",
      "..........\n",
      "[28,  1400] loss: 6.302477\n",
      "..........\n",
      "[28,  1500] loss: 6.327045\n",
      "..........\n",
      "[28,  1600] loss: 6.250295\n",
      "..........\n",
      "[28,  1700] loss: 6.314587\n",
      "..........\n",
      "[28,  1800] loss: 6.176344\n",
      "..........\n",
      "[28,  1900] loss: 6.264882\n",
      "..........\n",
      "[28,  2000] loss: 6.336025\n",
      "..........\n",
      "[28,  2100] loss: 6.246647\n",
      "..........\n",
      "[28,  2200] loss: 6.328212\n",
      "..........\n",
      "[28,  2300] loss: 6.210192\n",
      "..........\n",
      "[28,  2400] loss: 6.247944\n",
      "..........\n",
      "[28,  2500] loss: 6.342557\n",
      "..........\n",
      "[28,  2600] loss: 6.252732\n",
      "..........\n",
      "[28,  2700] loss: 6.290469\n",
      "..........\n",
      "[28,  2800] loss: 6.215673\n",
      "..........\n",
      "[28,  2900] loss: 6.256049\n",
      "..........\n",
      "[28,  3000] loss: 6.350946\n",
      "..........\n",
      "[28,  3100] loss: 6.258865\n",
      "..........\n",
      "[28,  3200] loss: 6.243073\n",
      "..........\n",
      "[28,  3300] loss: 6.233864\n",
      "..........\n",
      "[28,  3400] loss: 6.242656\n",
      "..........\n",
      "[28,  3500] loss: 6.377583\n",
      "..........\n",
      "[28,  3600] loss: 6.271058\n",
      "..........\n",
      "[28,  3700] loss: 6.254578\n",
      "..........\n",
      "[28,  3800] loss: 6.219404\n",
      "..........\n",
      "[28,  3900] loss: 6.241199\n",
      "..........\n",
      "[28,  4000] loss: 6.323450\n",
      "total average loss : 0.196\n",
      "== epoch 27 == train acc : 0.1023\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.125\n",
      "step : 400 / 1563 acc : 0.109\n",
      "step : 600 / 1563 acc : 0.099\n",
      "step : 800 / 1563 acc : 0.102\n",
      "step : 1000 / 1563 acc : 0.100\n",
      "step : 1200 / 1563 acc : 0.086\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[29,   100] loss: 6.236387\n",
      "..........\n",
      "[29,   200] loss: 6.276515\n",
      "..........\n",
      "[29,   300] loss: 6.218636\n",
      "..........\n",
      "[29,   400] loss: 6.284124\n",
      "..........\n",
      "[29,   500] loss: 6.324735\n",
      "..........\n",
      "[29,   600] loss: 6.228956\n",
      "..........\n",
      "[29,   700] loss: 6.280829\n",
      "..........\n",
      "[29,   800] loss: 6.234091\n",
      "..........\n",
      "[29,   900] loss: 6.294699\n",
      "..........\n",
      "[29,  1000] loss: 6.318356\n",
      "..........\n",
      "[29,  1100] loss: 6.223321\n",
      "..........\n",
      "[29,  1200] loss: 6.311164\n",
      "..........\n",
      "[29,  1300] loss: 6.190214\n",
      "..........\n",
      "[29,  1400] loss: 6.286149\n",
      "..........\n",
      "[29,  1500] loss: 6.357135\n",
      "..........\n",
      "[29,  1600] loss: 6.254434\n",
      "..........\n",
      "[29,  1700] loss: 6.296061\n",
      "..........\n",
      "[29,  1800] loss: 6.182412\n",
      "..........\n",
      "[29,  1900] loss: 6.268875\n",
      "..........\n",
      "[29,  2000] loss: 6.374477\n",
      "..........\n",
      "[29,  2100] loss: 6.251357\n",
      "..........\n",
      "[29,  2200] loss: 6.285476\n",
      "..........\n",
      "[29,  2300] loss: 6.229313\n",
      "..........\n",
      "[29,  2400] loss: 6.249282\n",
      "..........\n",
      "[29,  2500] loss: 6.377684\n",
      "..........\n",
      "[29,  2600] loss: 6.281436\n",
      "..........\n",
      "[29,  2700] loss: 6.265497\n",
      "..........\n",
      "[29,  2800] loss: 6.246379\n",
      "..........\n",
      "[29,  2900] loss: 6.250790\n",
      "..........\n",
      "[29,  3000] loss: 6.339618\n",
      "..........\n",
      "[29,  3100] loss: 6.257067\n",
      "..........\n",
      "[29,  3200] loss: 6.246274\n",
      "..........\n",
      "[29,  3300] loss: 6.218010\n",
      "..........\n",
      "[29,  3400] loss: 6.272073\n",
      "..........\n",
      "[29,  3500] loss: 6.345613\n",
      "..........\n",
      "[29,  3600] loss: 6.295638\n",
      "..........\n",
      "[29,  3700] loss: 6.251114\n",
      "..........\n",
      "[29,  3800] loss: 6.256184\n",
      "..........\n",
      "[29,  3900] loss: 6.255098\n",
      "..........\n",
      "[29,  4000] loss: 6.320707\n",
      "total average loss : 0.196\n",
      "== epoch 28 == train acc : 0.1078\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.156\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.099\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.084\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[30,   100] loss: 6.247924\n",
      "..........\n",
      "[30,   200] loss: 6.301911\n",
      "..........\n",
      "[30,   300] loss: 6.237063\n",
      "..........\n",
      "[30,   400] loss: 6.307540\n",
      "..........\n",
      "[30,   500] loss: 6.296217\n",
      "..........\n",
      "[30,   600] loss: 6.262015\n",
      "..........\n",
      "[30,   700] loss: 6.289612\n",
      "..........\n",
      "[30,   800] loss: 6.225826\n",
      "..........\n",
      "[30,   900] loss: 6.317493\n",
      "..........\n",
      "[30,  1000] loss: 6.305889\n",
      "..........\n",
      "[30,  1100] loss: 6.232781\n",
      "..........\n",
      "[30,  1200] loss: 6.339469\n",
      "..........\n",
      "[30,  1300] loss: 6.209949\n",
      "..........\n",
      "[30,  1400] loss: 6.237006\n",
      "..........\n",
      "[30,  1500] loss: 6.355081\n",
      "..........\n",
      "[30,  1600] loss: 6.251458\n",
      "..........\n",
      "[30,  1700] loss: 6.307887\n",
      "..........\n",
      "[30,  1800] loss: 6.189905\n",
      "..........\n",
      "[30,  1900] loss: 6.279462\n",
      "..........\n",
      "[30,  2000] loss: 6.346530\n",
      "..........\n",
      "[30,  2100] loss: 6.264407\n",
      "..........\n",
      "[30,  2200] loss: 6.309919\n",
      "..........\n",
      "[30,  2300] loss: 6.164040\n",
      "..........\n",
      "[30,  2400] loss: 6.263041\n",
      "..........\n",
      "[30,  2500] loss: 6.392588\n",
      "..........\n",
      "[30,  2600] loss: 6.249792\n",
      "..........\n",
      "[30,  2700] loss: 6.247343\n",
      "..........\n",
      "[30,  2800] loss: 6.195106\n",
      "..........\n",
      "[30,  2900] loss: 6.272289\n",
      "..........\n",
      "[30,  3000] loss: 6.363175\n",
      "..........\n",
      "[30,  3100] loss: 6.261139\n",
      "..........\n",
      "[30,  3200] loss: 6.258297\n",
      "..........\n",
      "[30,  3300] loss: 6.222449\n",
      "..........\n",
      "[30,  3400] loss: 6.248811\n",
      "..........\n",
      "[30,  3500] loss: 6.351927\n",
      "..........\n",
      "[30,  3600] loss: 6.270937\n",
      "..........\n",
      "[30,  3700] loss: 6.249334\n",
      "..........\n",
      "[30,  3800] loss: 6.277780\n",
      "..........\n",
      "[30,  3900] loss: 6.255096\n",
      "..........\n",
      "[30,  4000] loss: 6.293478\n",
      "total average loss : 0.196\n",
      "== epoch 29 == train acc : 0.0945\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.094\n",
      "step : 600 / 1563 acc : 0.089\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[31,   100] loss: 6.271932\n",
      "..........\n",
      "[31,   200] loss: 6.239528\n",
      "..........\n",
      "[31,   300] loss: 6.194253\n",
      "..........\n",
      "[31,   400] loss: 6.328653\n",
      "..........\n",
      "[31,   500] loss: 6.304310\n",
      "..........\n",
      "[31,   600] loss: 6.243815\n",
      "..........\n",
      "[31,   700] loss: 6.282142\n",
      "..........\n",
      "[31,   800] loss: 6.192852\n",
      "..........\n",
      "[31,   900] loss: 6.307603\n",
      "..........\n",
      "[31,  1000] loss: 6.333449\n",
      "..........\n",
      "[31,  1100] loss: 6.215184\n",
      "..........\n",
      "[31,  1200] loss: 6.296288\n",
      "..........\n",
      "[31,  1300] loss: 6.203671\n",
      "..........\n",
      "[31,  1400] loss: 6.295094\n",
      "..........\n",
      "[31,  1500] loss: 6.352969\n",
      "..........\n",
      "[31,  1600] loss: 6.217942\n",
      "..........\n",
      "[31,  1700] loss: 6.316180\n",
      "..........\n",
      "[31,  1800] loss: 6.228799\n",
      "..........\n",
      "[31,  1900] loss: 6.271396\n",
      "..........\n",
      "[31,  2000] loss: 6.355371\n",
      "..........\n",
      "[31,  2100] loss: 6.247563\n",
      "..........\n",
      "[31,  2200] loss: 6.262367\n",
      "..........\n",
      "[31,  2300] loss: 6.207251\n",
      "..........\n",
      "[31,  2400] loss: 6.243218\n",
      "..........\n",
      "[31,  2500] loss: 6.388698\n",
      "..........\n",
      "[31,  2600] loss: 6.239153\n",
      "..........\n",
      "[31,  2700] loss: 6.282870\n",
      "..........\n",
      "[31,  2800] loss: 6.221502\n",
      "..........\n",
      "[31,  2900] loss: 6.273443\n",
      "..........\n",
      "[31,  3000] loss: 6.379574\n",
      "..........\n",
      "[31,  3100] loss: 6.302223\n",
      "..........\n",
      "[31,  3200] loss: 6.263723\n",
      "..........\n",
      "[31,  3300] loss: 6.202224\n",
      "..........\n",
      "[31,  3400] loss: 6.224099\n",
      "..........\n",
      "[31,  3500] loss: 6.354374\n",
      "..........\n",
      "[31,  3600] loss: 6.242603\n",
      "..........\n",
      "[31,  3700] loss: 6.256947\n",
      "..........\n",
      "[31,  3800] loss: 6.261745\n",
      "..........\n",
      "[31,  3900] loss: 6.243393\n",
      "..........\n",
      "[31,  4000] loss: 6.314650\n",
      "total average loss : 0.196\n",
      "== epoch 30 == train acc : 0.1031\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.016\n",
      "step : 400 / 1563 acc : 0.039\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.078\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[32,   100] loss: 6.191906\n",
      "..........\n",
      "[32,   200] loss: 6.254746\n",
      "..........\n",
      "[32,   300] loss: 6.268060\n",
      "..........\n",
      "[32,   400] loss: 6.313368\n",
      "..........\n",
      "[32,   500] loss: 6.317515\n",
      "..........\n",
      "[32,   600] loss: 6.207175\n",
      "..........\n",
      "[32,   700] loss: 6.280628\n",
      "..........\n",
      "[32,   800] loss: 6.203340\n",
      "..........\n",
      "[32,   900] loss: 6.283100\n",
      "..........\n",
      "[32,  1000] loss: 6.301244\n",
      "..........\n",
      "[32,  1100] loss: 6.240672\n",
      "..........\n",
      "[32,  1200] loss: 6.309414\n",
      "..........\n",
      "[32,  1300] loss: 6.208716\n",
      "..........\n",
      "[32,  1400] loss: 6.290121\n",
      "..........\n",
      "[32,  1500] loss: 6.376071\n",
      "..........\n",
      "[32,  1600] loss: 6.240156\n",
      "..........\n",
      "[32,  1700] loss: 6.308841\n",
      "..........\n",
      "[32,  1800] loss: 6.191561\n",
      "..........\n",
      "[32,  1900] loss: 6.273777\n",
      "..........\n",
      "[32,  2000] loss: 6.364114\n",
      "..........\n",
      "[32,  2100] loss: 6.297477\n",
      "..........\n",
      "[32,  2200] loss: 6.293564\n",
      "..........\n",
      "[32,  2300] loss: 6.153239\n",
      "..........\n",
      "[32,  2400] loss: 6.235819\n",
      "..........\n",
      "[32,  2500] loss: 6.366305\n",
      "..........\n",
      "[32,  2600] loss: 6.248437\n",
      "..........\n",
      "[32,  2700] loss: 6.288549\n",
      "..........\n",
      "[32,  2800] loss: 6.221455\n",
      "..........\n",
      "[32,  2900] loss: 6.279172\n",
      "..........\n",
      "[32,  3000] loss: 6.390987\n",
      "..........\n",
      "[32,  3100] loss: 6.314402\n",
      "..........\n",
      "[32,  3200] loss: 6.252204\n",
      "..........\n",
      "[32,  3300] loss: 6.200748\n",
      "..........\n",
      "[32,  3400] loss: 6.270023\n",
      "..........\n",
      "[32,  3500] loss: 6.346401\n",
      "..........\n",
      "[32,  3600] loss: 6.314691\n",
      "..........\n",
      "[32,  3700] loss: 6.253702\n",
      "..........\n",
      "[32,  3800] loss: 6.231543\n",
      "..........\n",
      "[32,  3900] loss: 6.247787\n",
      "..........\n",
      "[32,  4000] loss: 6.297666\n",
      "total average loss : 0.196\n",
      "== epoch 31 == train acc : 0.1039\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[33,   100] loss: 6.259690\n",
      "..........\n",
      "[33,   200] loss: 6.283363\n",
      "..........\n",
      "[33,   300] loss: 6.267518\n",
      "..........\n",
      "[33,   400] loss: 6.322971\n",
      "..........\n",
      "[33,   500] loss: 6.340441\n",
      "..........\n",
      "[33,   600] loss: 6.258613\n",
      "..........\n",
      "[33,   700] loss: 6.293064\n",
      "..........\n",
      "[33,   800] loss: 6.217842\n",
      "..........\n",
      "[33,   900] loss: 6.268316\n",
      "..........\n",
      "[33,  1000] loss: 6.318128\n",
      "..........\n",
      "[33,  1100] loss: 6.215833\n",
      "..........\n",
      "[33,  1200] loss: 6.292269\n",
      "..........\n",
      "[33,  1300] loss: 6.184179\n",
      "..........\n",
      "[33,  1400] loss: 6.267900\n",
      "..........\n",
      "[33,  1500] loss: 6.372970\n",
      "..........\n",
      "[33,  1600] loss: 6.248796\n",
      "..........\n",
      "[33,  1700] loss: 6.327840\n",
      "..........\n",
      "[33,  1800] loss: 6.195474\n",
      "..........\n",
      "[33,  1900] loss: 6.273211\n",
      "..........\n",
      "[33,  2000] loss: 6.318431\n",
      "..........\n",
      "[33,  2100] loss: 6.253224\n",
      "..........\n",
      "[33,  2200] loss: 6.264489\n",
      "..........\n",
      "[33,  2300] loss: 6.171361\n",
      "..........\n",
      "[33,  2400] loss: 6.256633\n",
      "..........\n",
      "[33,  2500] loss: 6.373490\n",
      "..........\n",
      "[33,  2600] loss: 6.328210\n",
      "..........\n",
      "[33,  2700] loss: 6.245741\n",
      "..........\n",
      "[33,  2800] loss: 6.199936\n",
      "..........\n",
      "[33,  2900] loss: 6.253706\n",
      "..........\n",
      "[33,  3000] loss: 6.351414\n",
      "..........\n",
      "[33,  3100] loss: 6.281779\n",
      "..........\n",
      "[33,  3200] loss: 6.255074\n",
      "..........\n",
      "[33,  3300] loss: 6.235618\n",
      "..........\n",
      "[33,  3400] loss: 6.277064\n",
      "..........\n",
      "[33,  3500] loss: 6.363043\n",
      "..........\n",
      "[33,  3600] loss: 6.286545\n",
      "..........\n",
      "[33,  3700] loss: 6.218248\n",
      "..........\n",
      "[33,  3800] loss: 6.253815\n",
      "..........\n",
      "[33,  3900] loss: 6.263252\n",
      "..........\n",
      "[33,  4000] loss: 6.334114\n",
      "total average loss : 0.196\n",
      "== epoch 32 == train acc : 0.1023\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.099\n",
      "step : 800 / 1563 acc : 0.094\n",
      "step : 1000 / 1563 acc : 0.087\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[34,   100] loss: 6.285369\n",
      "..........\n",
      "[34,   200] loss: 6.267053\n",
      "..........\n",
      "[34,   300] loss: 6.227403\n",
      "..........\n",
      "[34,   400] loss: 6.312619\n",
      "..........\n",
      "[34,   500] loss: 6.328231\n",
      "..........\n",
      "[34,   600] loss: 6.276780\n",
      "..........\n",
      "[34,   700] loss: 6.278007\n",
      "..........\n",
      "[34,   800] loss: 6.207556\n",
      "..........\n",
      "[34,   900] loss: 6.294900\n",
      "..........\n",
      "[34,  1000] loss: 6.311812\n",
      "..........\n",
      "[34,  1100] loss: 6.225469\n",
      "..........\n",
      "[34,  1200] loss: 6.305791\n",
      "..........\n",
      "[34,  1300] loss: 6.238081\n",
      "..........\n",
      "[34,  1400] loss: 6.265745\n",
      "..........\n",
      "[34,  1500] loss: 6.329339\n",
      "..........\n",
      "[34,  1600] loss: 6.227471\n",
      "..........\n",
      "[34,  1700] loss: 6.323611\n",
      "..........\n",
      "[34,  1800] loss: 6.190796\n",
      "..........\n",
      "[34,  1900] loss: 6.278579\n",
      "..........\n",
      "[34,  2000] loss: 6.366900\n",
      "..........\n",
      "[34,  2100] loss: 6.263516\n",
      "..........\n",
      "[34,  2200] loss: 6.310579\n",
      "..........\n",
      "[34,  2300] loss: 6.183550\n",
      "..........\n",
      "[34,  2400] loss: 6.269822\n",
      "..........\n",
      "[34,  2500] loss: 6.349133\n",
      "..........\n",
      "[34,  2600] loss: 6.261454\n",
      "..........\n",
      "[34,  2700] loss: 6.253257\n",
      "..........\n",
      "[34,  2800] loss: 6.186146\n",
      "..........\n",
      "[34,  2900] loss: 6.266968\n",
      "..........\n",
      "[34,  3000] loss: 6.376741\n",
      "..........\n",
      "[34,  3100] loss: 6.238973\n",
      "..........\n",
      "[34,  3200] loss: 6.278673\n",
      "..........\n",
      "[34,  3300] loss: 6.186372\n",
      "..........\n",
      "[34,  3400] loss: 6.242789\n",
      "..........\n",
      "[34,  3500] loss: 6.368569\n",
      "..........\n",
      "[34,  3600] loss: 6.296779\n",
      "..........\n",
      "[34,  3700] loss: 6.264621\n",
      "..........\n",
      "[34,  3800] loss: 6.227325\n",
      "..........\n",
      "[34,  3900] loss: 6.244181\n",
      "..........\n",
      "[34,  4000] loss: 6.335695\n",
      "total average loss : 0.196\n",
      "== epoch 33 == train acc : 0.1008\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.047\n",
      "step : 800 / 1563 acc : 0.062\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[35,   100] loss: 6.254254\n",
      "..........\n",
      "[35,   200] loss: 6.225521\n",
      "..........\n",
      "[35,   300] loss: 6.256945\n",
      "..........\n",
      "[35,   400] loss: 6.329321\n",
      "..........\n",
      "[35,   500] loss: 6.305158\n",
      "..........\n",
      "[35,   600] loss: 6.208302\n",
      "..........\n",
      "[35,   700] loss: 6.296164\n",
      "..........\n",
      "[35,   800] loss: 6.211884\n",
      "..........\n",
      "[35,   900] loss: 6.298426\n",
      "..........\n",
      "[35,  1000] loss: 6.347646\n",
      "..........\n",
      "[35,  1100] loss: 6.233731\n",
      "..........\n",
      "[35,  1200] loss: 6.331487\n",
      "..........\n",
      "[35,  1300] loss: 6.204465\n",
      "..........\n",
      "[35,  1400] loss: 6.257637\n",
      "..........\n",
      "[35,  1500] loss: 6.362687\n",
      "..........\n",
      "[35,  1600] loss: 6.225017\n",
      "..........\n",
      "[35,  1700] loss: 6.323495\n",
      "..........\n",
      "[35,  1800] loss: 6.181971\n",
      "..........\n",
      "[35,  1900] loss: 6.273058\n",
      "..........\n",
      "[35,  2000] loss: 6.373000\n",
      "..........\n",
      "[35,  2100] loss: 6.228321\n",
      "..........\n",
      "[35,  2200] loss: 6.293437\n",
      "..........\n",
      "[35,  2300] loss: 6.216120\n",
      "..........\n",
      "[35,  2400] loss: 6.264799\n",
      "..........\n",
      "[35,  2500] loss: 6.369233\n",
      "..........\n",
      "[35,  2600] loss: 6.235123\n",
      "..........\n",
      "[35,  2700] loss: 6.265526\n",
      "..........\n",
      "[35,  2800] loss: 6.199610\n",
      "..........\n",
      "[35,  2900] loss: 6.253737\n",
      "..........\n",
      "[35,  3000] loss: 6.357316\n",
      "..........\n",
      "[35,  3100] loss: 6.245712\n",
      "..........\n",
      "[35,  3200] loss: 6.238249\n",
      "..........\n",
      "[35,  3300] loss: 6.207621\n",
      "..........\n",
      "[35,  3400] loss: 6.268835\n",
      "..........\n",
      "[35,  3500] loss: 6.348644\n",
      "..........\n",
      "[35,  3600] loss: 6.268898\n",
      "..........\n",
      "[35,  3700] loss: 6.277254\n",
      "..........\n",
      "[35,  3800] loss: 6.254831\n",
      "..........\n",
      "[35,  3900] loss: 6.253451\n",
      "..........\n",
      "[35,  4000] loss: 6.359695\n",
      "total average loss : 0.196\n",
      "== epoch 34 == train acc : 0.1023\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.089\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.084\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[36,   100] loss: 6.275883\n",
      "..........\n",
      "[36,   200] loss: 6.282279\n",
      "..........\n",
      "[36,   300] loss: 6.221338\n",
      "..........\n",
      "[36,   400] loss: 6.287283\n",
      "..........\n",
      "[36,   500] loss: 6.331290\n",
      "..........\n",
      "[36,   600] loss: 6.256779\n",
      "..........\n",
      "[36,   700] loss: 6.281979\n",
      "..........\n",
      "[36,   800] loss: 6.223561\n",
      "..........\n",
      "[36,   900] loss: 6.272499\n",
      "..........\n",
      "[36,  1000] loss: 6.314056\n",
      "..........\n",
      "[36,  1100] loss: 6.185508\n",
      "..........\n",
      "[36,  1200] loss: 6.342261\n",
      "..........\n",
      "[36,  1300] loss: 6.191856\n",
      "..........\n",
      "[36,  1400] loss: 6.296647\n",
      "..........\n",
      "[36,  1500] loss: 6.345224\n",
      "..........\n",
      "[36,  1600] loss: 6.229195\n",
      "..........\n",
      "[36,  1700] loss: 6.316655\n",
      "..........\n",
      "[36,  1800] loss: 6.185079\n",
      "..........\n",
      "[36,  1900] loss: 6.286611\n",
      "..........\n",
      "[36,  2000] loss: 6.347928\n",
      "..........\n",
      "[36,  2100] loss: 6.255978\n",
      "..........\n",
      "[36,  2200] loss: 6.276392\n",
      "..........\n",
      "[36,  2300] loss: 6.205334\n",
      "..........\n",
      "[36,  2400] loss: 6.249786\n",
      "..........\n",
      "[36,  2500] loss: 6.391045\n",
      "..........\n",
      "[36,  2600] loss: 6.270082\n",
      "..........\n",
      "[36,  2700] loss: 6.284737\n",
      "..........\n",
      "[36,  2800] loss: 6.221107\n",
      "..........\n",
      "[36,  2900] loss: 6.273983\n",
      "..........\n",
      "[36,  3000] loss: 6.371866\n",
      "..........\n",
      "[36,  3100] loss: 6.289980\n",
      "..........\n",
      "[36,  3200] loss: 6.277852\n",
      "..........\n",
      "[36,  3300] loss: 6.216770\n",
      "..........\n",
      "[36,  3400] loss: 6.273251\n",
      "..........\n",
      "[36,  3500] loss: 6.317615\n",
      "..........\n",
      "[36,  3600] loss: 6.273467\n",
      "..........\n",
      "[36,  3700] loss: 6.205178\n",
      "..........\n",
      "[36,  3800] loss: 6.252236\n",
      "..........\n",
      "[36,  3900] loss: 6.233707\n",
      "..........\n",
      "[36,  4000] loss: 6.313727\n",
      "total average loss : 0.196\n",
      "== epoch 35 == train acc : 0.1031\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.086\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[37,   100] loss: 6.242336\n",
      "..........\n",
      "[37,   200] loss: 6.246888\n",
      "..........\n",
      "[37,   300] loss: 6.225197\n",
      "..........\n",
      "[37,   400] loss: 6.342864\n",
      "..........\n",
      "[37,   500] loss: 6.325473\n",
      "..........\n",
      "[37,   600] loss: 6.231358\n",
      "..........\n",
      "[37,   700] loss: 6.298364\n",
      "..........\n",
      "[37,   800] loss: 6.210005\n",
      "..........\n",
      "[37,   900] loss: 6.319760\n",
      "..........\n",
      "[37,  1000] loss: 6.315009\n",
      "..........\n",
      "[37,  1100] loss: 6.268678\n",
      "..........\n",
      "[37,  1200] loss: 6.316006\n",
      "..........\n",
      "[37,  1300] loss: 6.219912\n",
      "..........\n",
      "[37,  1400] loss: 6.273393\n",
      "..........\n",
      "[37,  1500] loss: 6.343666\n",
      "..........\n",
      "[37,  1600] loss: 6.233420\n",
      "..........\n",
      "[37,  1700] loss: 6.313759\n",
      "..........\n",
      "[37,  1800] loss: 6.205037\n",
      "..........\n",
      "[37,  1900] loss: 6.271903\n",
      "..........\n",
      "[37,  2000] loss: 6.375727\n",
      "..........\n",
      "[37,  2100] loss: 6.225935\n",
      "..........\n",
      "[37,  2200] loss: 6.304433\n",
      "..........\n",
      "[37,  2300] loss: 6.172587\n",
      "..........\n",
      "[37,  2400] loss: 6.278488\n",
      "..........\n",
      "[37,  2500] loss: 6.368936\n",
      "..........\n",
      "[37,  2600] loss: 6.248381\n",
      "..........\n",
      "[37,  2700] loss: 6.244485\n",
      "..........\n",
      "[37,  2800] loss: 6.205539\n",
      "..........\n",
      "[37,  2900] loss: 6.260124\n",
      "..........\n",
      "[37,  3000] loss: 6.345506\n",
      "..........\n",
      "[37,  3100] loss: 6.262515\n",
      "..........\n",
      "[37,  3200] loss: 6.274980\n",
      "..........\n",
      "[37,  3300] loss: 6.219926\n",
      "..........\n",
      "[37,  3400] loss: 6.254927\n",
      "..........\n",
      "[37,  3500] loss: 6.319773\n",
      "..........\n",
      "[37,  3600] loss: 6.299656\n",
      "..........\n",
      "[37,  3700] loss: 6.245204\n",
      "..........\n",
      "[37,  3800] loss: 6.241708\n",
      "..........\n",
      "[37,  3900] loss: 6.258356\n",
      "..........\n",
      "[37,  4000] loss: 6.353825\n",
      "total average loss : 0.196\n",
      "== epoch 36 == train acc : 0.1078\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[38,   100] loss: 6.260525\n",
      "..........\n",
      "[38,   200] loss: 6.274431\n",
      "..........\n",
      "[38,   300] loss: 6.249759\n",
      "..........\n",
      "[38,   400] loss: 6.324974\n",
      "..........\n",
      "[38,   500] loss: 6.323685\n",
      "..........\n",
      "[38,   600] loss: 6.246405\n",
      "..........\n",
      "[38,   700] loss: 6.310194\n",
      "..........\n",
      "[38,   800] loss: 6.199414\n",
      "..........\n",
      "[38,   900] loss: 6.296453\n",
      "..........\n",
      "[38,  1000] loss: 6.306972\n",
      "..........\n",
      "[38,  1100] loss: 6.239966\n",
      "..........\n",
      "[38,  1200] loss: 6.336754\n",
      "..........\n",
      "[38,  1300] loss: 6.196807\n",
      "..........\n",
      "[38,  1400] loss: 6.295068\n",
      "..........\n",
      "[38,  1500] loss: 6.343805\n",
      "..........\n",
      "[38,  1600] loss: 6.245371\n",
      "..........\n",
      "[38,  1700] loss: 6.307406\n",
      "..........\n",
      "[38,  1800] loss: 6.191695\n",
      "..........\n",
      "[38,  1900] loss: 6.266614\n",
      "..........\n",
      "[38,  2000] loss: 6.340569\n",
      "..........\n",
      "[38,  2100] loss: 6.243556\n",
      "..........\n",
      "[38,  2200] loss: 6.263277\n",
      "..........\n",
      "[38,  2300] loss: 6.190993\n",
      "..........\n",
      "[38,  2400] loss: 6.270600\n",
      "..........\n",
      "[38,  2500] loss: 6.390549\n",
      "..........\n",
      "[38,  2600] loss: 6.260160\n",
      "..........\n",
      "[38,  2700] loss: 6.273812\n",
      "..........\n",
      "[38,  2800] loss: 6.204719\n",
      "..........\n",
      "[38,  2900] loss: 6.223904\n",
      "..........\n",
      "[38,  3000] loss: 6.357384\n",
      "..........\n",
      "[38,  3100] loss: 6.267851\n",
      "..........\n",
      "[38,  3200] loss: 6.261654\n",
      "..........\n",
      "[38,  3300] loss: 6.218074\n",
      "..........\n",
      "[38,  3400] loss: 6.261563\n",
      "..........\n",
      "[38,  3500] loss: 6.318603\n",
      "..........\n",
      "[38,  3600] loss: 6.279593\n",
      "..........\n",
      "[38,  3700] loss: 6.265803\n",
      "..........\n",
      "[38,  3800] loss: 6.229934\n",
      "..........\n",
      "[38,  3900] loss: 6.274364\n",
      "..........\n",
      "[38,  4000] loss: 6.312294\n",
      "total average loss : 0.196\n",
      "== epoch 37 == train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.109\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.083\n",
      "step : 800 / 1563 acc : 0.062\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[39,   100] loss: 6.238371\n",
      "..........\n",
      "[39,   200] loss: 6.294652\n",
      "..........\n",
      "[39,   300] loss: 6.221833\n",
      "..........\n",
      "[39,   400] loss: 6.336107\n",
      "..........\n",
      "[39,   500] loss: 6.304072\n",
      "..........\n",
      "[39,   600] loss: 6.246630\n",
      "..........\n",
      "[39,   700] loss: 6.278678\n",
      "..........\n",
      "[39,   800] loss: 6.193594\n",
      "..........\n",
      "[39,   900] loss: 6.283448\n",
      "..........\n",
      "[39,  1000] loss: 6.323430\n",
      "..........\n",
      "[39,  1100] loss: 6.211568\n",
      "..........\n",
      "[39,  1200] loss: 6.320806\n",
      "..........\n",
      "[39,  1300] loss: 6.189828\n",
      "..........\n",
      "[39,  1400] loss: 6.290361\n",
      "..........\n",
      "[39,  1500] loss: 6.344933\n",
      "..........\n",
      "[39,  1600] loss: 6.233611\n",
      "..........\n",
      "[39,  1700] loss: 6.324537\n",
      "..........\n",
      "[39,  1800] loss: 6.208864\n",
      "..........\n",
      "[39,  1900] loss: 6.279787\n",
      "..........\n",
      "[39,  2000] loss: 6.330881\n",
      "..........\n",
      "[39,  2100] loss: 6.274847\n",
      "..........\n",
      "[39,  2200] loss: 6.281871\n",
      "..........\n",
      "[39,  2300] loss: 6.206861\n",
      "..........\n",
      "[39,  2400] loss: 6.268544\n",
      "..........\n",
      "[39,  2500] loss: 6.383081\n",
      "..........\n",
      "[39,  2600] loss: 6.271302\n",
      "..........\n",
      "[39,  2700] loss: 6.260416\n",
      "..........\n",
      "[39,  2800] loss: 6.202968\n",
      "..........\n",
      "[39,  2900] loss: 6.266803\n",
      "..........\n",
      "[39,  3000] loss: 6.374550\n",
      "..........\n",
      "[39,  3100] loss: 6.251803\n",
      "..........\n",
      "[39,  3200] loss: 6.270220\n",
      "..........\n",
      "[39,  3300] loss: 6.204397\n",
      "..........\n",
      "[39,  3400] loss: 6.259669\n",
      "..........\n",
      "[39,  3500] loss: 6.312197\n",
      "..........\n",
      "[39,  3600] loss: 6.272210\n",
      "..........\n",
      "[39,  3700] loss: 6.256592\n",
      "..........\n",
      "[39,  3800] loss: 6.269903\n",
      "..........\n",
      "[39,  3900] loss: 6.270486\n",
      "..........\n",
      "[39,  4000] loss: 6.319629\n",
      "total average loss : 0.196\n",
      "== epoch 38 == train acc : 0.1047\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.016\n",
      "step : 400 / 1563 acc : 0.047\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.060\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[40,   100] loss: 6.253222\n",
      "..........\n",
      "[40,   200] loss: 6.312637\n",
      "..........\n",
      "[40,   300] loss: 6.222286\n",
      "..........\n",
      "[40,   400] loss: 6.313448\n",
      "..........\n",
      "[40,   500] loss: 6.283113\n",
      "..........\n",
      "[40,   600] loss: 6.240010\n",
      "..........\n",
      "[40,   700] loss: 6.305896\n",
      "..........\n",
      "[40,   800] loss: 6.233587\n",
      "..........\n",
      "[40,   900] loss: 6.245691\n",
      "..........\n",
      "[40,  1000] loss: 6.273452\n",
      "..........\n",
      "[40,  1100] loss: 6.243399\n",
      "..........\n",
      "[40,  1200] loss: 6.304858\n",
      "..........\n",
      "[40,  1300] loss: 6.181597\n",
      "..........\n",
      "[40,  1400] loss: 6.293019\n",
      "..........\n",
      "[40,  1500] loss: 6.354305\n",
      "..........\n",
      "[40,  1600] loss: 6.241610\n",
      "..........\n",
      "[40,  1700] loss: 6.343871\n",
      "..........\n",
      "[40,  1800] loss: 6.202817\n",
      "..........\n",
      "[40,  1900] loss: 6.281868\n",
      "..........\n",
      "[40,  2000] loss: 6.353326\n",
      "..........\n",
      "[40,  2100] loss: 6.256921\n",
      "..........\n",
      "[40,  2200] loss: 6.278789\n",
      "..........\n",
      "[40,  2300] loss: 6.201866\n",
      "..........\n",
      "[40,  2400] loss: 6.300954\n",
      "..........\n",
      "[40,  2500] loss: 6.328082\n",
      "..........\n",
      "[40,  2600] loss: 6.262395\n",
      "..........\n",
      "[40,  2700] loss: 6.276128\n",
      "..........\n",
      "[40,  2800] loss: 6.185246\n",
      "..........\n",
      "[40,  2900] loss: 6.258556\n",
      "..........\n",
      "[40,  3000] loss: 6.349101\n",
      "..........\n",
      "[40,  3100] loss: 6.272228\n",
      "..........\n",
      "[40,  3200] loss: 6.257907\n",
      "..........\n",
      "[40,  3300] loss: 6.212985\n",
      "..........\n",
      "[40,  3400] loss: 6.285766\n",
      "..........\n",
      "[40,  3500] loss: 6.327573\n",
      "..........\n",
      "[40,  3600] loss: 6.257100\n",
      "..........\n",
      "[40,  3700] loss: 6.260240\n",
      "..........\n",
      "[40,  3800] loss: 6.226484\n",
      "..........\n",
      "[40,  3900] loss: 6.273588\n",
      "..........\n",
      "[40,  4000] loss: 6.349969\n",
      "total average loss : 0.196\n",
      "== epoch 39 == train acc : 0.1141\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.047\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[41,   100] loss: 6.222322\n",
      "..........\n",
      "[41,   200] loss: 6.271069\n",
      "..........\n",
      "[41,   300] loss: 6.242574\n",
      "..........\n",
      "[41,   400] loss: 6.322457\n",
      "..........\n",
      "[41,   500] loss: 6.337370\n",
      "..........\n",
      "[41,   600] loss: 6.212146\n",
      "..........\n",
      "[41,   700] loss: 6.289417\n",
      "..........\n",
      "[41,   800] loss: 6.216349\n",
      "..........\n",
      "[41,   900] loss: 6.299248\n",
      "..........\n",
      "[41,  1000] loss: 6.370325\n",
      "..........\n",
      "[41,  1100] loss: 6.243316\n",
      "..........\n",
      "[41,  1200] loss: 6.304560\n",
      "..........\n",
      "[41,  1300] loss: 6.202393\n",
      "..........\n",
      "[41,  1400] loss: 6.312192\n",
      "..........\n",
      "[41,  1500] loss: 6.337189\n",
      "..........\n",
      "[41,  1600] loss: 6.217050\n",
      "..........\n",
      "[41,  1700] loss: 6.268576\n",
      "..........\n",
      "[41,  1800] loss: 6.183534\n",
      "..........\n",
      "[41,  1900] loss: 6.321115\n",
      "..........\n",
      "[41,  2000] loss: 6.366446\n",
      "..........\n",
      "[41,  2100] loss: 6.242670\n",
      "..........\n",
      "[41,  2200] loss: 6.271270\n",
      "..........\n",
      "[41,  2300] loss: 6.199617\n",
      "..........\n",
      "[41,  2400] loss: 6.243759\n",
      "..........\n",
      "[41,  2500] loss: 6.334105\n",
      "..........\n",
      "[41,  2600] loss: 6.256034\n",
      "..........\n",
      "[41,  2700] loss: 6.303986\n",
      "..........\n",
      "[41,  2800] loss: 6.210624\n",
      "..........\n",
      "[41,  2900] loss: 6.265363\n",
      "..........\n",
      "[41,  3000] loss: 6.394528\n",
      "..........\n",
      "[41,  3100] loss: 6.271311\n",
      "..........\n",
      "[41,  3200] loss: 6.262153\n",
      "..........\n",
      "[41,  3300] loss: 6.235763\n",
      "..........\n",
      "[41,  3400] loss: 6.259525\n",
      "..........\n",
      "[41,  3500] loss: 6.333193\n",
      "..........\n",
      "[41,  3600] loss: 6.281188\n",
      "..........\n",
      "[41,  3700] loss: 6.194064\n",
      "..........\n",
      "[41,  3800] loss: 6.252712\n",
      "..........\n",
      "[41,  3900] loss: 6.240906\n",
      "..........\n",
      "[41,  4000] loss: 6.324080\n",
      "total average loss : 0.196\n",
      "== epoch 40 == train acc : 0.1047\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.068\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[42,   100] loss: 6.232496\n",
      "..........\n",
      "[42,   200] loss: 6.295250\n",
      "..........\n",
      "[42,   300] loss: 6.246165\n",
      "..........\n",
      "[42,   400] loss: 6.298054\n",
      "..........\n",
      "[42,   500] loss: 6.314601\n",
      "..........\n",
      "[42,   600] loss: 6.227588\n",
      "..........\n",
      "[42,   700] loss: 6.298358\n",
      "..........\n",
      "[42,   800] loss: 6.216797\n",
      "..........\n",
      "[42,   900] loss: 6.285430\n",
      "..........\n",
      "[42,  1000] loss: 6.294385\n",
      "..........\n",
      "[42,  1100] loss: 6.231378\n",
      "..........\n",
      "[42,  1200] loss: 6.319073\n",
      "..........\n",
      "[42,  1300] loss: 6.213075\n",
      "..........\n",
      "[42,  1400] loss: 6.279683\n",
      "..........\n",
      "[42,  1500] loss: 6.332106\n",
      "..........\n",
      "[42,  1600] loss: 6.230172\n",
      "..........\n",
      "[42,  1700] loss: 6.325594\n",
      "..........\n",
      "[42,  1800] loss: 6.196104\n",
      "..........\n",
      "[42,  1900] loss: 6.256240\n",
      "..........\n",
      "[42,  2000] loss: 6.363163\n",
      "..........\n",
      "[42,  2100] loss: 6.270507\n",
      "..........\n",
      "[42,  2200] loss: 6.282755\n",
      "..........\n",
      "[42,  2300] loss: 6.182669\n",
      "..........\n",
      "[42,  2400] loss: 6.248570\n",
      "..........\n",
      "[42,  2500] loss: 6.363915\n",
      "..........\n",
      "[42,  2600] loss: 6.246503\n",
      "..........\n",
      "[42,  2700] loss: 6.309106\n",
      "..........\n",
      "[42,  2800] loss: 6.194094\n",
      "..........\n",
      "[42,  2900] loss: 6.255088\n",
      "..........\n",
      "[42,  3000] loss: 6.359443\n",
      "..........\n",
      "[42,  3100] loss: 6.258356\n",
      "..........\n",
      "[42,  3200] loss: 6.317703\n",
      "..........\n",
      "[42,  3300] loss: 6.221461\n",
      "..........\n",
      "[42,  3400] loss: 6.258309\n",
      "..........\n",
      "[42,  3500] loss: 6.336232\n",
      "..........\n",
      "[42,  3600] loss: 6.248382\n",
      "..........\n",
      "[42,  3700] loss: 6.245408\n",
      "..........\n",
      "[42,  3800] loss: 6.222684\n",
      "..........\n",
      "[42,  3900] loss: 6.300342\n",
      "..........\n",
      "[42,  4000] loss: 6.324032\n",
      "total average loss : 0.196\n",
      "== epoch 41 == train acc : 0.0977\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[43,   100] loss: 6.239546\n",
      "..........\n",
      "[43,   200] loss: 6.282441\n",
      "..........\n",
      "[43,   300] loss: 6.244836\n",
      "..........\n",
      "[43,   400] loss: 6.341599\n",
      "..........\n",
      "[43,   500] loss: 6.333835\n",
      "..........\n",
      "[43,   600] loss: 6.236082\n",
      "..........\n",
      "[43,   700] loss: 6.273320\n",
      "..........\n",
      "[43,   800] loss: 6.203915\n",
      "..........\n",
      "[43,   900] loss: 6.253897\n",
      "..........\n",
      "[43,  1000] loss: 6.282020\n",
      "..........\n",
      "[43,  1100] loss: 6.282295\n",
      "..........\n",
      "[43,  1200] loss: 6.313516\n",
      "..........\n",
      "[43,  1300] loss: 6.201800\n",
      "..........\n",
      "[43,  1400] loss: 6.279908\n",
      "..........\n",
      "[43,  1500] loss: 6.348241\n",
      "..........\n",
      "[43,  1600] loss: 6.235159\n",
      "..........\n",
      "[43,  1700] loss: 6.306564\n",
      "..........\n",
      "[43,  1800] loss: 6.183919\n",
      "..........\n",
      "[43,  1900] loss: 6.309238\n",
      "..........\n",
      "[43,  2000] loss: 6.364694\n",
      "..........\n",
      "[43,  2100] loss: 6.286089\n",
      "..........\n",
      "[43,  2200] loss: 6.268180\n",
      "..........\n",
      "[43,  2300] loss: 6.217077\n",
      "..........\n",
      "[43,  2400] loss: 6.219655\n",
      "..........\n",
      "[43,  2500] loss: 6.366914\n",
      "..........\n",
      "[43,  2600] loss: 6.267230\n",
      "..........\n",
      "[43,  2700] loss: 6.290629\n",
      "..........\n",
      "[43,  2800] loss: 6.212259\n",
      "..........\n",
      "[43,  2900] loss: 6.258836\n",
      "..........\n",
      "[43,  3000] loss: 6.336302\n",
      "..........\n",
      "[43,  3100] loss: 6.272398\n",
      "..........\n",
      "[43,  3200] loss: 6.261551\n",
      "..........\n",
      "[43,  3300] loss: 6.236605\n",
      "..........\n",
      "[43,  3400] loss: 6.250060\n",
      "..........\n",
      "[43,  3500] loss: 6.325630\n",
      "..........\n",
      "[43,  3600] loss: 6.276074\n",
      "..........\n",
      "[43,  3700] loss: 6.221737\n",
      "..........\n",
      "[43,  3800] loss: 6.220127\n",
      "..........\n",
      "[43,  3900] loss: 6.273937\n",
      "..........\n",
      "[43,  4000] loss: 6.326073\n",
      "total average loss : 0.196\n",
      "== epoch 42 == train acc : 0.0961\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.094\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.090\n",
      "step : 1000 / 1563 acc : 0.081\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[44,   100] loss: 6.221378\n",
      "..........\n",
      "[44,   200] loss: 6.274389\n",
      "..........\n",
      "[44,   300] loss: 6.231515\n",
      "..........\n",
      "[44,   400] loss: 6.288216\n",
      "..........\n",
      "[44,   500] loss: 6.295238\n",
      "..........\n",
      "[44,   600] loss: 6.248364\n",
      "..........\n",
      "[44,   700] loss: 6.316717\n",
      "..........\n",
      "[44,   800] loss: 6.204767\n",
      "..........\n",
      "[44,   900] loss: 6.258816\n",
      "..........\n",
      "[44,  1000] loss: 6.320027\n",
      "..........\n",
      "[44,  1100] loss: 6.247606\n",
      "..........\n",
      "[44,  1200] loss: 6.330369\n",
      "..........\n",
      "[44,  1300] loss: 6.204943\n",
      "..........\n",
      "[44,  1400] loss: 6.307509\n",
      "..........\n",
      "[44,  1500] loss: 6.317285\n",
      "..........\n",
      "[44,  1600] loss: 6.237990\n",
      "..........\n",
      "[44,  1700] loss: 6.268654\n",
      "..........\n",
      "[44,  1800] loss: 6.192020\n",
      "..........\n",
      "[44,  1900] loss: 6.253166\n",
      "..........\n",
      "[44,  2000] loss: 6.381415\n",
      "..........\n",
      "[44,  2100] loss: 6.253366\n",
      "..........\n",
      "[44,  2200] loss: 6.278919\n",
      "..........\n",
      "[44,  2300] loss: 6.188770\n",
      "..........\n",
      "[44,  2400] loss: 6.311146\n",
      "..........\n",
      "[44,  2500] loss: 6.394185\n",
      "..........\n",
      "[44,  2600] loss: 6.242454\n",
      "..........\n",
      "[44,  2700] loss: 6.288446\n",
      "..........\n",
      "[44,  2800] loss: 6.205493\n",
      "..........\n",
      "[44,  2900] loss: 6.283383\n",
      "..........\n",
      "[44,  3000] loss: 6.340536\n",
      "..........\n",
      "[44,  3100] loss: 6.290708\n",
      "..........\n",
      "[44,  3200] loss: 6.259062\n",
      "..........\n",
      "[44,  3300] loss: 6.228319\n",
      "..........\n",
      "[44,  3400] loss: 6.253614\n",
      "..........\n",
      "[44,  3500] loss: 6.338407\n",
      "..........\n",
      "[44,  3600] loss: 6.282962\n",
      "..........\n",
      "[44,  3700] loss: 6.267919\n",
      "..........\n",
      "[44,  3800] loss: 6.245532\n",
      "..........\n",
      "[44,  3900] loss: 6.241530\n",
      "..........\n",
      "[44,  4000] loss: 6.339421\n",
      "total average loss : 0.196\n",
      "== epoch 43 == train acc : 0.1062\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.057\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[45,   100] loss: 6.255888\n",
      "..........\n",
      "[45,   200] loss: 6.305595\n",
      "..........\n",
      "[45,   300] loss: 6.270925\n",
      "..........\n",
      "[45,   400] loss: 6.304117\n",
      "..........\n",
      "[45,   500] loss: 6.280162\n",
      "..........\n",
      "[45,   600] loss: 6.235001\n",
      "..........\n",
      "[45,   700] loss: 6.315639\n",
      "..........\n",
      "[45,   800] loss: 6.207769\n",
      "..........\n",
      "[45,   900] loss: 6.318361\n",
      "..........\n",
      "[45,  1000] loss: 6.345823\n",
      "..........\n",
      "[45,  1100] loss: 6.229252\n",
      "..........\n",
      "[45,  1200] loss: 6.326472\n",
      "..........\n",
      "[45,  1300] loss: 6.205239\n",
      "..........\n",
      "[45,  1400] loss: 6.273394\n",
      "..........\n",
      "[45,  1500] loss: 6.366658\n",
      "..........\n",
      "[45,  1600] loss: 6.241515\n",
      "..........\n",
      "[45,  1700] loss: 6.267394\n",
      "..........\n",
      "[45,  1800] loss: 6.207465\n",
      "..........\n",
      "[45,  1900] loss: 6.256891\n",
      "..........\n",
      "[45,  2000] loss: 6.340336\n",
      "..........\n",
      "[45,  2100] loss: 6.229424\n",
      "..........\n",
      "[45,  2200] loss: 6.297378\n",
      "..........\n",
      "[45,  2300] loss: 6.196661\n",
      "..........\n",
      "[45,  2400] loss: 6.242782\n",
      "..........\n",
      "[45,  2500] loss: 6.377021\n",
      "..........\n",
      "[45,  2600] loss: 6.272507\n",
      "..........\n",
      "[45,  2700] loss: 6.283113\n",
      "..........\n",
      "[45,  2800] loss: 6.237463\n",
      "..........\n",
      "[45,  2900] loss: 6.293934\n",
      "..........\n",
      "[45,  3000] loss: 6.382016\n",
      "..........\n",
      "[45,  3100] loss: 6.237179\n",
      "..........\n",
      "[45,  3200] loss: 6.210851\n",
      "..........\n",
      "[45,  3300] loss: 6.227524\n",
      "..........\n",
      "[45,  3400] loss: 6.250345\n",
      "..........\n",
      "[45,  3500] loss: 6.351710\n",
      "..........\n",
      "[45,  3600] loss: 6.273573\n",
      "..........\n",
      "[45,  3700] loss: 6.234252\n",
      "..........\n",
      "[45,  3800] loss: 6.238180\n",
      "..........\n",
      "[45,  3900] loss: 6.284693\n",
      "..........\n",
      "[45,  4000] loss: 6.292089\n",
      "total average loss : 0.196\n",
      "== epoch 44 == train acc : 0.1047\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.109\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.070\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[46,   100] loss: 6.232610\n",
      "..........\n",
      "[46,   200] loss: 6.323457\n",
      "..........\n",
      "[46,   300] loss: 6.203449\n",
      "..........\n",
      "[46,   400] loss: 6.319732\n",
      "..........\n",
      "[46,   500] loss: 6.314380\n",
      "..........\n",
      "[46,   600] loss: 6.223121\n",
      "..........\n",
      "[46,   700] loss: 6.296805\n",
      "..........\n",
      "[46,   800] loss: 6.215878\n",
      "..........\n",
      "[46,   900] loss: 6.277647\n",
      "..........\n",
      "[46,  1000] loss: 6.342084\n",
      "..........\n",
      "[46,  1100] loss: 6.246522\n",
      "..........\n",
      "[46,  1200] loss: 6.289884\n",
      "..........\n",
      "[46,  1300] loss: 6.199415\n",
      "..........\n",
      "[46,  1400] loss: 6.281298\n",
      "..........\n",
      "[46,  1500] loss: 6.335168\n",
      "..........\n",
      "[46,  1600] loss: 6.243820\n",
      "..........\n",
      "[46,  1700] loss: 6.306474\n",
      "..........\n",
      "[46,  1800] loss: 6.205318\n",
      "..........\n",
      "[46,  1900] loss: 6.279205\n",
      "..........\n",
      "[46,  2000] loss: 6.329207\n",
      "..........\n",
      "[46,  2100] loss: 6.260119\n",
      "..........\n",
      "[46,  2200] loss: 6.274413\n",
      "..........\n",
      "[46,  2300] loss: 6.220432\n",
      "..........\n",
      "[46,  2400] loss: 6.265244\n",
      "..........\n",
      "[46,  2500] loss: 6.377723\n",
      "..........\n",
      "[46,  2600] loss: 6.257250\n",
      "..........\n",
      "[46,  2700] loss: 6.281556\n",
      "..........\n",
      "[46,  2800] loss: 6.193116\n",
      "..........\n",
      "[46,  2900] loss: 6.264355\n",
      "..........\n",
      "[46,  3000] loss: 6.290672\n",
      "..........\n",
      "[46,  3100] loss: 6.288095\n",
      "..........\n",
      "[46,  3200] loss: 6.279037\n",
      "..........\n",
      "[46,  3300] loss: 6.232608\n",
      "..........\n",
      "[46,  3400] loss: 6.253994\n",
      "..........\n",
      "[46,  3500] loss: 6.321722\n",
      "..........\n",
      "[46,  3600] loss: 6.288143\n",
      "..........\n",
      "[46,  3700] loss: 6.236289\n",
      "..........\n",
      "[46,  3800] loss: 6.243463\n",
      "..........\n",
      "[46,  3900] loss: 6.250722\n",
      "..........\n",
      "[46,  4000] loss: 6.340400\n",
      "total average loss : 0.196\n",
      "== epoch 45 == train acc : 0.0883\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.099\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.078\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[47,   100] loss: 6.238760\n",
      "..........\n",
      "[47,   200] loss: 6.273033\n",
      "..........\n",
      "[47,   300] loss: 6.240278\n",
      "..........\n",
      "[47,   400] loss: 6.277193\n",
      "..........\n",
      "[47,   500] loss: 6.360753\n",
      "..........\n",
      "[47,   600] loss: 6.243924\n",
      "..........\n",
      "[47,   700] loss: 6.271116\n",
      "..........\n",
      "[47,   800] loss: 6.203723\n",
      "..........\n",
      "[47,   900] loss: 6.296004\n",
      "..........\n",
      "[47,  1000] loss: 6.349729\n",
      "..........\n",
      "[47,  1100] loss: 6.276098\n",
      "..........\n",
      "[47,  1200] loss: 6.333207\n",
      "..........\n",
      "[47,  1300] loss: 6.203336\n",
      "..........\n",
      "[47,  1400] loss: 6.272350\n",
      "..........\n",
      "[47,  1500] loss: 6.327717\n",
      "..........\n",
      "[47,  1600] loss: 6.215368\n",
      "..........\n",
      "[47,  1700] loss: 6.322031\n",
      "..........\n",
      "[47,  1800] loss: 6.201821\n",
      "..........\n",
      "[47,  1900] loss: 6.214632\n",
      "..........\n",
      "[47,  2000] loss: 6.386530\n",
      "..........\n",
      "[47,  2100] loss: 6.251470\n",
      "..........\n",
      "[47,  2200] loss: 6.286496\n",
      "..........\n",
      "[47,  2300] loss: 6.178437\n",
      "..........\n",
      "[47,  2400] loss: 6.260365\n",
      "..........\n",
      "[47,  2500] loss: 6.375644\n",
      "..........\n",
      "[47,  2600] loss: 6.264056\n",
      "..........\n",
      "[47,  2700] loss: 6.289619\n",
      "..........\n",
      "[47,  2800] loss: 6.188318\n",
      "..........\n",
      "[47,  2900] loss: 6.229094\n",
      "..........\n",
      "[47,  3000] loss: 6.352774\n",
      "..........\n",
      "[47,  3100] loss: 6.274057\n",
      "..........\n",
      "[47,  3200] loss: 6.245989\n",
      "..........\n",
      "[47,  3300] loss: 6.245526\n",
      "..........\n",
      "[47,  3400] loss: 6.231679\n",
      "..........\n",
      "[47,  3500] loss: 6.353939\n",
      "..........\n",
      "[47,  3600] loss: 6.271334\n",
      "..........\n",
      "[47,  3700] loss: 6.220415\n",
      "..........\n",
      "[47,  3800] loss: 6.246039\n",
      "..........\n",
      "[47,  3900] loss: 6.244127\n",
      "..........\n",
      "[47,  4000] loss: 6.366460\n",
      "total average loss : 0.196\n",
      "== epoch 46 == train acc : 0.1133\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.065\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[48,   100] loss: 6.227637\n",
      "..........\n",
      "[48,   200] loss: 6.278970\n",
      "..........\n",
      "[48,   300] loss: 6.251727\n",
      "..........\n",
      "[48,   400] loss: 6.328661\n",
      "..........\n",
      "[48,   500] loss: 6.320238\n",
      "..........\n",
      "[48,   600] loss: 6.265805\n",
      "..........\n",
      "[48,   700] loss: 6.304366\n",
      "..........\n",
      "[48,   800] loss: 6.198275\n",
      "..........\n",
      "[48,   900] loss: 6.263953\n",
      "..........\n",
      "[48,  1000] loss: 6.321864\n",
      "..........\n",
      "[48,  1100] loss: 6.247580\n",
      "..........\n",
      "[48,  1200] loss: 6.342363\n",
      "..........\n",
      "[48,  1300] loss: 6.207976\n",
      "..........\n",
      "[48,  1400] loss: 6.293495\n",
      "..........\n",
      "[48,  1500] loss: 6.329596\n",
      "..........\n",
      "[48,  1600] loss: 6.227681\n",
      "..........\n",
      "[48,  1700] loss: 6.294492\n",
      "..........\n",
      "[48,  1800] loss: 6.206687\n",
      "..........\n",
      "[48,  1900] loss: 6.285254\n",
      "..........\n",
      "[48,  2000] loss: 6.342394\n",
      "..........\n",
      "[48,  2100] loss: 6.231969\n",
      "..........\n",
      "[48,  2200] loss: 6.284680\n",
      "..........\n",
      "[48,  2300] loss: 6.197447\n",
      "..........\n",
      "[48,  2400] loss: 6.276477\n",
      "..........\n",
      "[48,  2500] loss: 6.330972\n",
      "..........\n",
      "[48,  2600] loss: 6.240223\n",
      "..........\n",
      "[48,  2700] loss: 6.275877\n",
      "..........\n",
      "[48,  2800] loss: 6.194593\n",
      "..........\n",
      "[48,  2900] loss: 6.268810\n",
      "..........\n",
      "[48,  3000] loss: 6.373102\n",
      "..........\n",
      "[48,  3100] loss: 6.270037\n",
      "..........\n",
      "[48,  3200] loss: 6.269614\n",
      "..........\n",
      "[48,  3300] loss: 6.213574\n",
      "..........\n",
      "[48,  3400] loss: 6.258251\n",
      "..........\n",
      "[48,  3500] loss: 6.323468\n",
      "..........\n",
      "[48,  3600] loss: 6.304117\n",
      "..........\n",
      "[48,  3700] loss: 6.263863\n",
      "..........\n",
      "[48,  3800] loss: 6.266410\n",
      "..........\n",
      "[48,  3900] loss: 6.242195\n",
      "..........\n",
      "[48,  4000] loss: 6.333656\n",
      "total average loss : 0.196\n",
      "== epoch 47 == train acc : 0.1039\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.031\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.068\n",
      "step : 800 / 1563 acc : 0.070\n",
      "step : 1000 / 1563 acc : 0.059\n",
      "step : 1200 / 1563 acc : 0.055\n",
      "step : 1400 / 1563 acc : 0.062\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[49,   100] loss: 6.196370\n",
      "..........\n",
      "[49,   200] loss: 6.281052\n",
      "..........\n",
      "[49,   300] loss: 6.239332\n",
      "..........\n",
      "[49,   400] loss: 6.326361\n",
      "..........\n",
      "[49,   500] loss: 6.343596\n",
      "..........\n",
      "[49,   600] loss: 6.233936\n",
      "..........\n",
      "[49,   700] loss: 6.315905\n",
      "..........\n",
      "[49,   800] loss: 6.206956\n",
      "..........\n",
      "[49,   900] loss: 6.285661\n",
      "..........\n",
      "[49,  1000] loss: 6.352073\n",
      "..........\n",
      "[49,  1100] loss: 6.239395\n",
      "..........\n",
      "[49,  1200] loss: 6.305986\n",
      "..........\n",
      "[49,  1300] loss: 6.214635\n",
      "..........\n",
      "[49,  1400] loss: 6.288468\n",
      "..........\n",
      "[49,  1500] loss: 6.344288\n",
      "..........\n",
      "[49,  1600] loss: 6.222305\n",
      "..........\n",
      "[49,  1700] loss: 6.319003\n",
      "..........\n",
      "[49,  1800] loss: 6.183356\n",
      "..........\n",
      "[49,  1900] loss: 6.260096\n",
      "..........\n",
      "[49,  2000] loss: 6.353192\n",
      "..........\n",
      "[49,  2100] loss: 6.284601\n",
      "..........\n",
      "[49,  2200] loss: 6.298158\n",
      "..........\n",
      "[49,  2300] loss: 6.193805\n",
      "..........\n",
      "[49,  2400] loss: 6.271752\n",
      "..........\n",
      "[49,  2500] loss: 6.348009\n",
      "..........\n",
      "[49,  2600] loss: 6.261750\n",
      "..........\n",
      "[49,  2700] loss: 6.276606\n",
      "..........\n",
      "[49,  2800] loss: 6.178636\n",
      "..........\n",
      "[49,  2900] loss: 6.282423\n",
      "..........\n",
      "[49,  3000] loss: 6.386731\n",
      "..........\n",
      "[49,  3100] loss: 6.255826\n",
      "..........\n",
      "[49,  3200] loss: 6.281486\n",
      "..........\n",
      "[49,  3300] loss: 6.222264\n",
      "..........\n",
      "[49,  3400] loss: 6.278995\n",
      "..........\n",
      "[49,  3500] loss: 6.350673\n",
      "..........\n",
      "[49,  3600] loss: 6.280017\n",
      "..........\n",
      "[49,  3700] loss: 6.231582\n",
      "..........\n",
      "[49,  3800] loss: 6.216748\n",
      "..........\n",
      "[49,  3900] loss: 6.230409\n",
      "..........\n",
      "[49,  4000] loss: 6.323748\n",
      "total average loss : 0.196\n",
      "== epoch 48 == train acc : 0.0922\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.031\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.078\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[50,   100] loss: 6.217539\n",
      "..........\n",
      "[50,   200] loss: 6.321199\n",
      "..........\n",
      "[50,   300] loss: 6.240525\n",
      "..........\n",
      "[50,   400] loss: 6.326781\n",
      "..........\n",
      "[50,   500] loss: 6.307797\n",
      "..........\n",
      "[50,   600] loss: 6.195342\n",
      "..........\n",
      "[50,   700] loss: 6.269601\n",
      "..........\n",
      "[50,   800] loss: 6.213125\n",
      "..........\n",
      "[50,   900] loss: 6.268359\n",
      "..........\n",
      "[50,  1000] loss: 6.298761\n",
      "..........\n",
      "[50,  1100] loss: 6.230812\n",
      "..........\n",
      "[50,  1200] loss: 6.342683\n",
      "..........\n",
      "[50,  1300] loss: 6.193112\n",
      "..........\n",
      "[50,  1400] loss: 6.249483\n",
      "..........\n",
      "[50,  1500] loss: 6.372305\n",
      "..........\n",
      "[50,  1600] loss: 6.215804\n",
      "..........\n",
      "[50,  1700] loss: 6.292336\n",
      "..........\n",
      "[50,  1800] loss: 6.204254\n",
      "..........\n",
      "[50,  1900] loss: 6.268117\n",
      "..........\n",
      "[50,  2000] loss: 6.357639\n",
      "..........\n",
      "[50,  2100] loss: 6.262127\n",
      "..........\n",
      "[50,  2200] loss: 6.301162\n",
      "..........\n",
      "[50,  2300] loss: 6.192189\n",
      "..........\n",
      "[50,  2400] loss: 6.264099\n",
      "..........\n",
      "[50,  2500] loss: 6.394797\n",
      "..........\n",
      "[50,  2600] loss: 6.277236\n",
      "..........\n",
      "[50,  2700] loss: 6.213070\n",
      "..........\n",
      "[50,  2800] loss: 6.207976\n",
      "..........\n",
      "[50,  2900] loss: 6.252677\n",
      "..........\n",
      "[50,  3000] loss: 6.330909\n",
      "..........\n",
      "[50,  3100] loss: 6.248974\n",
      "..........\n",
      "[50,  3200] loss: 6.270873\n",
      "..........\n",
      "[50,  3300] loss: 6.226674\n",
      "..........\n",
      "[50,  3400] loss: 6.269352\n",
      "..........\n",
      "[50,  3500] loss: 6.350919\n",
      "..........\n",
      "[50,  3600] loss: 6.279732\n",
      "..........\n",
      "[50,  3700] loss: 6.287566\n",
      "..........\n",
      "[50,  3800] loss: 6.260468\n",
      "..........\n",
      "[50,  3900] loss: 6.271329\n",
      "..........\n",
      "[50,  4000] loss: 6.359034\n",
      "total average loss : 0.196\n",
      "== epoch 49 == train acc : 0.1078\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.109\n",
      "step : 400 / 1563 acc : 0.125\n",
      "step : 600 / 1563 acc : 0.094\n",
      "step : 800 / 1563 acc : 0.078\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[51,   100] loss: 6.268229\n",
      "..........\n",
      "[51,   200] loss: 6.268563\n",
      "..........\n",
      "[51,   300] loss: 6.273875\n",
      "..........\n",
      "[51,   400] loss: 6.274439\n",
      "..........\n",
      "[51,   500] loss: 6.294392\n",
      "..........\n",
      "[51,   600] loss: 6.256445\n",
      "..........\n",
      "[51,   700] loss: 6.269364\n",
      "..........\n",
      "[51,   800] loss: 6.208679\n",
      "..........\n",
      "[51,   900] loss: 6.282083\n",
      "..........\n",
      "[51,  1000] loss: 6.299182\n",
      "..........\n",
      "[51,  1100] loss: 6.218702\n",
      "..........\n",
      "[51,  1200] loss: 6.326394\n",
      "..........\n",
      "[51,  1300] loss: 6.217026\n",
      "..........\n",
      "[51,  1400] loss: 6.317533\n",
      "..........\n",
      "[51,  1500] loss: 6.359394\n",
      "..........\n",
      "[51,  1600] loss: 6.258740\n",
      "..........\n",
      "[51,  1700] loss: 6.294792\n",
      "..........\n",
      "[51,  1800] loss: 6.154935\n",
      "..........\n",
      "[51,  1900] loss: 6.260693\n",
      "..........\n",
      "[51,  2000] loss: 6.364951\n",
      "..........\n",
      "[51,  2100] loss: 6.279207\n",
      "..........\n",
      "[51,  2200] loss: 6.308676\n",
      "..........\n",
      "[51,  2300] loss: 6.185610\n",
      "..........\n",
      "[51,  2400] loss: 6.271134\n",
      "..........\n",
      "[51,  2500] loss: 6.357954\n",
      "..........\n",
      "[51,  2600] loss: 6.248235\n",
      "..........\n",
      "[51,  2700] loss: 6.296770\n",
      "..........\n",
      "[51,  2800] loss: 6.191750\n",
      "..........\n",
      "[51,  2900] loss: 6.245772\n",
      "..........\n",
      "[51,  3000] loss: 6.336097\n",
      "..........\n",
      "[51,  3100] loss: 6.270916\n",
      "..........\n",
      "[51,  3200] loss: 6.237074\n",
      "..........\n",
      "[51,  3300] loss: 6.242499\n",
      "..........\n",
      "[51,  3400] loss: 6.283396\n",
      "..........\n",
      "[51,  3500] loss: 6.341607\n",
      "..........\n",
      "[51,  3600] loss: 6.281429\n",
      "..........\n",
      "[51,  3700] loss: 6.251191\n",
      "..........\n",
      "[51,  3800] loss: 6.278876\n",
      "..........\n",
      "[51,  3900] loss: 6.242601\n",
      "..........\n",
      "[51,  4000] loss: 6.321741\n",
      "total average loss : 0.196\n",
      "== epoch 50 == train acc : 0.1008\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.102\n",
      "step : 600 / 1563 acc : 0.089\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[52,   100] loss: 6.238655\n",
      "..........\n",
      "[52,   200] loss: 6.238725\n",
      "..........\n",
      "[52,   300] loss: 6.230343\n",
      "..........\n",
      "[52,   400] loss: 6.309231\n",
      "..........\n",
      "[52,   500] loss: 6.315527\n",
      "..........\n",
      "[52,   600] loss: 6.265631\n",
      "..........\n",
      "[52,   700] loss: 6.294628\n",
      "..........\n",
      "[52,   800] loss: 6.215895\n",
      "..........\n",
      "[52,   900] loss: 6.325697\n",
      "..........\n",
      "[52,  1000] loss: 6.344153\n",
      "..........\n",
      "[52,  1100] loss: 6.202306\n",
      "..........\n",
      "[52,  1200] loss: 6.313336\n",
      "..........\n",
      "[52,  1300] loss: 6.197722\n",
      "..........\n",
      "[52,  1400] loss: 6.314082\n",
      "..........\n",
      "[52,  1500] loss: 6.374528\n",
      "..........\n",
      "[52,  1600] loss: 6.188455\n",
      "..........\n",
      "[52,  1700] loss: 6.279864\n",
      "..........\n",
      "[52,  1800] loss: 6.181303\n",
      "..........\n",
      "[52,  1900] loss: 6.270412\n",
      "..........\n",
      "[52,  2000] loss: 6.356398\n",
      "..........\n",
      "[52,  2100] loss: 6.262664\n",
      "..........\n",
      "[52,  2200] loss: 6.298097\n",
      "..........\n",
      "[52,  2300] loss: 6.225499\n",
      "..........\n",
      "[52,  2400] loss: 6.252503\n",
      "..........\n",
      "[52,  2500] loss: 6.401632\n",
      "..........\n",
      "[52,  2600] loss: 6.243383\n",
      "..........\n",
      "[52,  2700] loss: 6.270628\n",
      "..........\n",
      "[52,  2800] loss: 6.212744\n",
      "..........\n",
      "[52,  2900] loss: 6.262319\n",
      "..........\n",
      "[52,  3000] loss: 6.341617\n",
      "..........\n",
      "[52,  3100] loss: 6.264079\n",
      "..........\n",
      "[52,  3200] loss: 6.252289\n",
      "..........\n",
      "[52,  3300] loss: 6.248599\n",
      "..........\n",
      "[52,  3400] loss: 6.252609\n",
      "..........\n",
      "[52,  3500] loss: 6.356234\n",
      "..........\n",
      "[52,  3600] loss: 6.269225\n",
      "..........\n",
      "[52,  3700] loss: 6.231145\n",
      "..........\n",
      "[52,  3800] loss: 6.218975\n",
      "..........\n",
      "[52,  3900] loss: 6.253285\n",
      "..........\n",
      "[52,  4000] loss: 6.358384\n",
      "total average loss : 0.196\n",
      "== epoch 51 == train acc : 0.1094\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.094\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.094\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.081\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[53,   100] loss: 6.262487\n",
      "..........\n",
      "[53,   200] loss: 6.284355\n",
      "..........\n",
      "[53,   300] loss: 6.220707\n",
      "..........\n",
      "[53,   400] loss: 6.285734\n",
      "..........\n",
      "[53,   500] loss: 6.306788\n",
      "..........\n",
      "[53,   600] loss: 6.194988\n",
      "..........\n",
      "[53,   700] loss: 6.314505\n",
      "..........\n",
      "[53,   800] loss: 6.224798\n",
      "..........\n",
      "[53,   900] loss: 6.293171\n",
      "..........\n",
      "[53,  1000] loss: 6.359106\n",
      "..........\n",
      "[53,  1100] loss: 6.209757\n",
      "..........\n",
      "[53,  1200] loss: 6.293128\n",
      "..........\n",
      "[53,  1300] loss: 6.183819\n",
      "..........\n",
      "[53,  1400] loss: 6.322531\n",
      "..........\n",
      "[53,  1500] loss: 6.311585\n",
      "..........\n",
      "[53,  1600] loss: 6.244446\n",
      "..........\n",
      "[53,  1700] loss: 6.280485\n",
      "..........\n",
      "[53,  1800] loss: 6.206819\n",
      "..........\n",
      "[53,  1900] loss: 6.270291\n",
      "..........\n",
      "[53,  2000] loss: 6.366396\n",
      "..........\n",
      "[53,  2100] loss: 6.288813\n",
      "..........\n",
      "[53,  2200] loss: 6.272524\n",
      "..........\n",
      "[53,  2300] loss: 6.182444\n",
      "..........\n",
      "[53,  2400] loss: 6.267797\n",
      "..........\n",
      "[53,  2500] loss: 6.367702\n",
      "..........\n",
      "[53,  2600] loss: 6.263947\n",
      "..........\n",
      "[53,  2700] loss: 6.244822\n",
      "..........\n",
      "[53,  2800] loss: 6.215382\n",
      "..........\n",
      "[53,  2900] loss: 6.284022\n",
      "..........\n",
      "[53,  3000] loss: 6.340340\n",
      "..........\n",
      "[53,  3100] loss: 6.275218\n",
      "..........\n",
      "[53,  3200] loss: 6.310228\n",
      "..........\n",
      "[53,  3300] loss: 6.215195\n",
      "..........\n",
      "[53,  3400] loss: 6.253463\n",
      "..........\n",
      "[53,  3500] loss: 6.361214\n",
      "..........\n",
      "[53,  3600] loss: 6.308262\n",
      "..........\n",
      "[53,  3700] loss: 6.249091\n",
      "..........\n",
      "[53,  3800] loss: 6.231249\n",
      "..........\n",
      "[53,  3900] loss: 6.241567\n",
      "..........\n",
      "[53,  4000] loss: 6.315136\n",
      "total average loss : 0.196\n",
      "== epoch 52 == train acc : 0.1016\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.047\n",
      "step : 600 / 1563 acc : 0.068\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.065\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[54,   100] loss: 6.249588\n",
      "..........\n",
      "[54,   200] loss: 6.278834\n",
      "..........\n",
      "[54,   300] loss: 6.209396\n",
      "..........\n",
      "[54,   400] loss: 6.301502\n",
      "..........\n",
      "[54,   500] loss: 6.317620\n",
      "..........\n",
      "[54,   600] loss: 6.231491\n",
      "..........\n",
      "[54,   700] loss: 6.301930\n",
      "..........\n",
      "[54,   800] loss: 6.193370\n",
      "..........\n",
      "[54,   900] loss: 6.276941\n",
      "..........\n",
      "[54,  1000] loss: 6.306082\n",
      "..........\n",
      "[54,  1100] loss: 6.239129\n",
      "..........\n",
      "[54,  1200] loss: 6.317356\n",
      "..........\n",
      "[54,  1300] loss: 6.209778\n",
      "..........\n",
      "[54,  1400] loss: 6.275117\n",
      "..........\n",
      "[54,  1500] loss: 6.343718\n",
      "..........\n",
      "[54,  1600] loss: 6.246658\n",
      "..........\n",
      "[54,  1700] loss: 6.302271\n",
      "..........\n",
      "[54,  1800] loss: 6.191323\n",
      "..........\n",
      "[54,  1900] loss: 6.239295\n",
      "..........\n",
      "[54,  2000] loss: 6.349047\n",
      "..........\n",
      "[54,  2100] loss: 6.278364\n",
      "..........\n",
      "[54,  2200] loss: 6.297806\n",
      "..........\n",
      "[54,  2300] loss: 6.167579\n",
      "..........\n",
      "[54,  2400] loss: 6.235993\n",
      "..........\n",
      "[54,  2500] loss: 6.374681\n",
      "..........\n",
      "[54,  2600] loss: 6.295033\n",
      "..........\n",
      "[54,  2700] loss: 6.285257\n",
      "..........\n",
      "[54,  2800] loss: 6.204451\n",
      "..........\n",
      "[54,  2900] loss: 6.276303\n",
      "..........\n",
      "[54,  3000] loss: 6.360301\n",
      "..........\n",
      "[54,  3100] loss: 6.283397\n",
      "..........\n",
      "[54,  3200] loss: 6.252606\n",
      "..........\n",
      "[54,  3300] loss: 6.257833\n",
      "..........\n",
      "[54,  3400] loss: 6.317226\n",
      "..........\n",
      "[54,  3500] loss: 6.383044\n",
      "..........\n",
      "[54,  3600] loss: 6.267822\n",
      "..........\n",
      "[54,  3700] loss: 6.222347\n",
      "..........\n",
      "[54,  3800] loss: 6.230373\n",
      "..........\n",
      "[54,  3900] loss: 6.237349\n",
      "..........\n",
      "[54,  4000] loss: 6.350452\n",
      "total average loss : 0.196\n",
      "== epoch 53 == train acc : 0.1055\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.047\n",
      "step : 600 / 1563 acc : 0.052\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.069\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[55,   100] loss: 6.212294\n",
      "..........\n",
      "[55,   200] loss: 6.325194\n",
      "..........\n",
      "[55,   300] loss: 6.245854\n",
      "..........\n",
      "[55,   400] loss: 6.293501\n",
      "..........\n",
      "[55,   500] loss: 6.280556\n",
      "..........\n",
      "[55,   600] loss: 6.236230\n",
      "..........\n",
      "[55,   700] loss: 6.298993\n",
      "..........\n",
      "[55,   800] loss: 6.244982\n",
      "..........\n",
      "[55,   900] loss: 6.297958\n",
      "..........\n",
      "[55,  1000] loss: 6.350602\n",
      "..........\n",
      "[55,  1100] loss: 6.224870\n",
      "..........\n",
      "[55,  1200] loss: 6.273981\n",
      "..........\n",
      "[55,  1300] loss: 6.188722\n",
      "..........\n",
      "[55,  1400] loss: 6.297497\n",
      "..........\n",
      "[55,  1500] loss: 6.354765\n",
      "..........\n",
      "[55,  1600] loss: 6.229123\n",
      "..........\n",
      "[55,  1700] loss: 6.280610\n",
      "..........\n",
      "[55,  1800] loss: 6.217010\n",
      "..........\n",
      "[55,  1900] loss: 6.254815\n",
      "..........\n",
      "[55,  2000] loss: 6.324601\n",
      "..........\n",
      "[55,  2100] loss: 6.229244\n",
      "..........\n",
      "[55,  2200] loss: 6.306381\n",
      "..........\n",
      "[55,  2300] loss: 6.195735\n",
      "..........\n",
      "[55,  2400] loss: 6.259317\n",
      "..........\n",
      "[55,  2500] loss: 6.363078\n",
      "..........\n",
      "[55,  2600] loss: 6.283832\n",
      "..........\n",
      "[55,  2700] loss: 6.273811\n",
      "..........\n",
      "[55,  2800] loss: 6.206443\n",
      "..........\n",
      "[55,  2900] loss: 6.246071\n",
      "..........\n",
      "[55,  3000] loss: 6.376699\n",
      "..........\n",
      "[55,  3100] loss: 6.267709\n",
      "..........\n",
      "[55,  3200] loss: 6.257343\n",
      "..........\n",
      "[55,  3300] loss: 6.231299\n",
      "..........\n",
      "[55,  3400] loss: 6.267929\n",
      "..........\n",
      "[55,  3500] loss: 6.344431\n",
      "..........\n",
      "[55,  3600] loss: 6.294064\n",
      "..........\n",
      "[55,  3700] loss: 6.242846\n",
      "..........\n",
      "[55,  3800] loss: 6.280592\n",
      "..........\n",
      "[55,  3900] loss: 6.242542\n",
      "..........\n",
      "[55,  4000] loss: 6.328195\n",
      "total average loss : 0.196\n",
      "== epoch 54 == train acc : 0.1070\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.141\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.062\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[56,   100] loss: 6.264569\n",
      "..........\n",
      "[56,   200] loss: 6.252360\n",
      "..........\n",
      "[56,   300] loss: 6.227240\n",
      "..........\n",
      "[56,   400] loss: 6.302170\n",
      "..........\n",
      "[56,   500] loss: 6.299335\n",
      "..........\n",
      "[56,   600] loss: 6.197701\n",
      "..........\n",
      "[56,   700] loss: 6.276352\n",
      "..........\n",
      "[56,   800] loss: 6.225589\n",
      "..........\n",
      "[56,   900] loss: 6.304109\n",
      "..........\n",
      "[56,  1000] loss: 6.349020\n",
      "..........\n",
      "[56,  1100] loss: 6.240361\n",
      "..........\n",
      "[56,  1200] loss: 6.320214\n",
      "..........\n",
      "[56,  1300] loss: 6.196104\n",
      "..........\n",
      "[56,  1400] loss: 6.300133\n",
      "..........\n",
      "[56,  1500] loss: 6.335178\n",
      "..........\n",
      "[56,  1600] loss: 6.245567\n",
      "..........\n",
      "[56,  1700] loss: 6.280480\n",
      "..........\n",
      "[56,  1800] loss: 6.182365\n",
      "..........\n",
      "[56,  1900] loss: 6.246621\n",
      "..........\n",
      "[56,  2000] loss: 6.380672\n",
      "..........\n",
      "[56,  2100] loss: 6.278893\n",
      "..........\n",
      "[56,  2200] loss: 6.268360\n",
      "..........\n",
      "[56,  2300] loss: 6.211740\n",
      "..........\n",
      "[56,  2400] loss: 6.264173\n",
      "..........\n",
      "[56,  2500] loss: 6.369949\n",
      "..........\n",
      "[56,  2600] loss: 6.247692\n",
      "..........\n",
      "[56,  2700] loss: 6.260759\n",
      "..........\n",
      "[56,  2800] loss: 6.208969\n",
      "..........\n",
      "[56,  2900] loss: 6.252315\n",
      "..........\n",
      "[56,  3000] loss: 6.335351\n",
      "..........\n",
      "[56,  3100] loss: 6.302910\n",
      "..........\n",
      "[56,  3200] loss: 6.234449\n",
      "..........\n",
      "[56,  3300] loss: 6.234395\n",
      "..........\n",
      "[56,  3400] loss: 6.257527\n",
      "..........\n",
      "[56,  3500] loss: 6.371737\n",
      "..........\n",
      "[56,  3600] loss: 6.309482\n",
      "..........\n",
      "[56,  3700] loss: 6.258125\n",
      "..........\n",
      "[56,  3800] loss: 6.246203\n",
      "..........\n",
      "[56,  3900] loss: 6.275476\n",
      "..........\n",
      "[56,  4000] loss: 6.358475\n",
      "total average loss : 0.196\n",
      "== epoch 55 == train acc : 0.0977\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.065\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[57,   100] loss: 6.261444\n",
      "..........\n",
      "[57,   200] loss: 6.262502\n",
      "..........\n",
      "[57,   300] loss: 6.194661\n",
      "..........\n",
      "[57,   400] loss: 6.336063\n",
      "..........\n",
      "[57,   500] loss: 6.345335\n",
      "..........\n",
      "[57,   600] loss: 6.247373\n",
      "..........\n",
      "[57,   700] loss: 6.297407\n",
      "..........\n",
      "[57,   800] loss: 6.231110\n",
      "..........\n",
      "[57,   900] loss: 6.287656\n",
      "..........\n",
      "[57,  1000] loss: 6.320648\n",
      "..........\n",
      "[57,  1100] loss: 6.248471\n",
      "..........\n",
      "[57,  1200] loss: 6.330339\n",
      "..........\n",
      "[57,  1300] loss: 6.182023\n",
      "..........\n",
      "[57,  1400] loss: 6.269958\n",
      "..........\n",
      "[57,  1500] loss: 6.381930\n",
      "..........\n",
      "[57,  1600] loss: 6.235788\n",
      "..........\n",
      "[57,  1700] loss: 6.325138\n",
      "..........\n",
      "[57,  1800] loss: 6.202807\n",
      "..........\n",
      "[57,  1900] loss: 6.288204\n",
      "..........\n",
      "[57,  2000] loss: 6.344485\n",
      "..........\n",
      "[57,  2100] loss: 6.224265\n",
      "..........\n",
      "[57,  2200] loss: 6.284822\n",
      "..........\n",
      "[57,  2300] loss: 6.202502\n",
      "..........\n",
      "[57,  2400] loss: 6.244562\n",
      "..........\n",
      "[57,  2500] loss: 6.377898\n",
      "..........\n",
      "[57,  2600] loss: 6.261631\n",
      "..........\n",
      "[57,  2700] loss: 6.260795\n",
      "..........\n",
      "[57,  2800] loss: 6.180737\n",
      "..........\n",
      "[57,  2900] loss: 6.262766\n",
      "..........\n",
      "[57,  3000] loss: 6.369117\n",
      "..........\n",
      "[57,  3100] loss: 6.270669\n",
      "..........\n",
      "[57,  3200] loss: 6.250777\n",
      "..........\n",
      "[57,  3300] loss: 6.239153\n",
      "..........\n",
      "[57,  3400] loss: 6.245560\n",
      "..........\n",
      "[57,  3500] loss: 6.323289\n",
      "..........\n",
      "[57,  3600] loss: 6.266304\n",
      "..........\n",
      "[57,  3700] loss: 6.238666\n",
      "..........\n",
      "[57,  3800] loss: 6.266262\n",
      "..........\n",
      "[57,  3900] loss: 6.259947\n",
      "..........\n",
      "[57,  4000] loss: 6.368600\n",
      "total average loss : 0.196\n",
      "== epoch 56 == train acc : 0.1094\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.057\n",
      "step : 800 / 1563 acc : 0.047\n",
      "step : 1000 / 1563 acc : 0.056\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[58,   100] loss: 6.257592\n",
      "..........\n",
      "[58,   200] loss: 6.274356\n",
      "..........\n",
      "[58,   300] loss: 6.246756\n",
      "..........\n",
      "[58,   400] loss: 6.320418\n",
      "..........\n",
      "[58,   500] loss: 6.316037\n",
      "..........\n",
      "[58,   600] loss: 6.229974\n",
      "..........\n",
      "[58,   700] loss: 6.301595\n",
      "..........\n",
      "[58,   800] loss: 6.195793\n",
      "..........\n",
      "[58,   900] loss: 6.251461\n",
      "..........\n",
      "[58,  1000] loss: 6.351845\n",
      "..........\n",
      "[58,  1100] loss: 6.228684\n",
      "..........\n",
      "[58,  1200] loss: 6.303437\n",
      "..........\n",
      "[58,  1300] loss: 6.212786\n",
      "..........\n",
      "[58,  1400] loss: 6.281737\n",
      "..........\n",
      "[58,  1500] loss: 6.369857\n",
      "..........\n",
      "[58,  1600] loss: 6.216245\n",
      "..........\n",
      "[58,  1700] loss: 6.314863\n",
      "..........\n",
      "[58,  1800] loss: 6.172829\n",
      "..........\n",
      "[58,  1900] loss: 6.263300\n",
      "..........\n",
      "[58,  2000] loss: 6.334846\n",
      "..........\n",
      "[58,  2100] loss: 6.256522\n",
      "..........\n",
      "[58,  2200] loss: 6.270206\n",
      "..........\n",
      "[58,  2300] loss: 6.185314\n",
      "..........\n",
      "[58,  2400] loss: 6.262861\n",
      "..........\n",
      "[58,  2500] loss: 6.375312\n",
      "..........\n",
      "[58,  2600] loss: 6.253122\n",
      "..........\n",
      "[58,  2700] loss: 6.275511\n",
      "..........\n",
      "[58,  2800] loss: 6.191703\n",
      "..........\n",
      "[58,  2900] loss: 6.286064\n",
      "..........\n",
      "[58,  3000] loss: 6.326264\n",
      "..........\n",
      "[58,  3100] loss: 6.276525\n",
      "..........\n",
      "[58,  3200] loss: 6.280015\n",
      "..........\n",
      "[58,  3300] loss: 6.218979\n",
      "..........\n",
      "[58,  3400] loss: 6.242887\n",
      "..........\n",
      "[58,  3500] loss: 6.371823\n",
      "..........\n",
      "[58,  3600] loss: 6.285645\n",
      "..........\n",
      "[58,  3700] loss: 6.288053\n",
      "..........\n",
      "[58,  3800] loss: 6.228235\n",
      "..........\n",
      "[58,  3900] loss: 6.240146\n",
      "..........\n",
      "[58,  4000] loss: 6.387762\n",
      "total average loss : 0.196\n",
      "== epoch 57 == train acc : 0.1000\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.109\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.099\n",
      "step : 800 / 1563 acc : 0.090\n",
      "step : 1000 / 1563 acc : 0.078\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[59,   100] loss: 6.225587\n",
      "..........\n",
      "[59,   200] loss: 6.252401\n",
      "..........\n",
      "[59,   300] loss: 6.236175\n",
      "..........\n",
      "[59,   400] loss: 6.293448\n",
      "..........\n",
      "[59,   500] loss: 6.329839\n",
      "..........\n",
      "[59,   600] loss: 6.242353\n",
      "..........\n",
      "[59,   700] loss: 6.344534\n",
      "..........\n",
      "[59,   800] loss: 6.226785\n",
      "..........\n",
      "[59,   900] loss: 6.276933\n",
      "..........\n",
      "[59,  1000] loss: 6.333468\n",
      "..........\n",
      "[59,  1100] loss: 6.221864\n",
      "..........\n",
      "[59,  1200] loss: 6.281443\n",
      "..........\n",
      "[59,  1300] loss: 6.189479\n",
      "..........\n",
      "[59,  1400] loss: 6.277570\n",
      "..........\n",
      "[59,  1500] loss: 6.339151\n",
      "..........\n",
      "[59,  1600] loss: 6.269127\n",
      "..........\n",
      "[59,  1700] loss: 6.312628\n",
      "..........\n",
      "[59,  1800] loss: 6.209306\n",
      "..........\n",
      "[59,  1900] loss: 6.245215\n",
      "..........\n",
      "[59,  2000] loss: 6.339987\n",
      "..........\n",
      "[59,  2100] loss: 6.264198\n",
      "..........\n",
      "[59,  2200] loss: 6.317514\n",
      "..........\n",
      "[59,  2300] loss: 6.213353\n",
      "..........\n",
      "[59,  2400] loss: 6.234905\n",
      "..........\n",
      "[59,  2500] loss: 6.385247\n",
      "..........\n",
      "[59,  2600] loss: 6.279044\n",
      "..........\n",
      "[59,  2700] loss: 6.278545\n",
      "..........\n",
      "[59,  2800] loss: 6.177246\n",
      "..........\n",
      "[59,  2900] loss: 6.268102\n",
      "..........\n",
      "[59,  3000] loss: 6.346474\n",
      "..........\n",
      "[59,  3100] loss: 6.266383\n",
      "..........\n",
      "[59,  3200] loss: 6.234255\n",
      "..........\n",
      "[59,  3300] loss: 6.194173\n",
      "..........\n",
      "[59,  3400] loss: 6.304771\n",
      "..........\n",
      "[59,  3500] loss: 6.337697\n",
      "..........\n",
      "[59,  3600] loss: 6.298590\n",
      "..........\n",
      "[59,  3700] loss: 6.256204\n",
      "..........\n",
      "[59,  3800] loss: 6.257513\n",
      "..........\n",
      "[59,  3900] loss: 6.262484\n",
      "..........\n",
      "[59,  4000] loss: 6.323687\n",
      "total average loss : 0.196\n",
      "== epoch 58 == train acc : 0.1016\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.052\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[60,   100] loss: 6.237070\n",
      "..........\n",
      "[60,   200] loss: 6.242938\n",
      "..........\n",
      "[60,   300] loss: 6.240501\n",
      "..........\n",
      "[60,   400] loss: 6.312297\n",
      "..........\n",
      "[60,   500] loss: 6.308838\n",
      "..........\n",
      "[60,   600] loss: 6.274447\n",
      "..........\n",
      "[60,   700] loss: 6.312378\n",
      "..........\n",
      "[60,   800] loss: 6.209708\n",
      "..........\n",
      "[60,   900] loss: 6.283526\n",
      "..........\n",
      "[60,  1000] loss: 6.327612\n",
      "..........\n",
      "[60,  1100] loss: 6.219620\n",
      "..........\n",
      "[60,  1200] loss: 6.320514\n",
      "..........\n",
      "[60,  1300] loss: 6.170532\n",
      "..........\n",
      "[60,  1400] loss: 6.267366\n",
      "..........\n",
      "[60,  1500] loss: 6.335143\n",
      "..........\n",
      "[60,  1600] loss: 6.248784\n",
      "..........\n",
      "[60,  1700] loss: 6.327144\n",
      "..........\n",
      "[60,  1800] loss: 6.165602\n",
      "..........\n",
      "[60,  1900] loss: 6.242377\n",
      "..........\n",
      "[60,  2000] loss: 6.340789\n",
      "..........\n",
      "[60,  2100] loss: 6.269300\n",
      "..........\n",
      "[60,  2200] loss: 6.284214\n",
      "..........\n",
      "[60,  2300] loss: 6.220983\n",
      "..........\n",
      "[60,  2400] loss: 6.281096\n",
      "..........\n",
      "[60,  2500] loss: 6.380621\n",
      "..........\n",
      "[60,  2600] loss: 6.275054\n",
      "..........\n",
      "[60,  2700] loss: 6.258587\n",
      "..........\n",
      "[60,  2800] loss: 6.249657\n",
      "..........\n",
      "[60,  2900] loss: 6.258318\n",
      "..........\n",
      "[60,  3000] loss: 6.355871\n",
      "..........\n",
      "[60,  3100] loss: 6.270919\n",
      "..........\n",
      "[60,  3200] loss: 6.255101\n",
      "..........\n",
      "[60,  3300] loss: 6.204079\n",
      "..........\n",
      "[60,  3400] loss: 6.283769\n",
      "..........\n",
      "[60,  3500] loss: 6.328619\n",
      "..........\n",
      "[60,  3600] loss: 6.266886\n",
      "..........\n",
      "[60,  3700] loss: 6.262292\n",
      "..........\n",
      "[60,  3800] loss: 6.257330\n",
      "..........\n",
      "[60,  3900] loss: 6.240525\n",
      "..........\n",
      "[60,  4000] loss: 6.337253\n",
      "total average loss : 0.196\n",
      "== epoch 59 == train acc : 0.1031\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.094\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.068\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.065\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[61,   100] loss: 6.243514\n",
      "..........\n",
      "[61,   200] loss: 6.274157\n",
      "..........\n",
      "[61,   300] loss: 6.254939\n",
      "..........\n",
      "[61,   400] loss: 6.289009\n",
      "..........\n",
      "[61,   500] loss: 6.313619\n",
      "..........\n",
      "[61,   600] loss: 6.227026\n",
      "..........\n",
      "[61,   700] loss: 6.291916\n",
      "..........\n",
      "[61,   800] loss: 6.214689\n",
      "..........\n",
      "[61,   900] loss: 6.275877\n",
      "..........\n",
      "[61,  1000] loss: 6.312787\n",
      "..........\n",
      "[61,  1100] loss: 6.236209\n",
      "..........\n",
      "[61,  1200] loss: 6.314242\n",
      "..........\n",
      "[61,  1300] loss: 6.168883\n",
      "..........\n",
      "[61,  1400] loss: 6.307579\n",
      "..........\n",
      "[61,  1500] loss: 6.330478\n",
      "..........\n",
      "[61,  1600] loss: 6.220698\n",
      "..........\n",
      "[61,  1700] loss: 6.317733\n",
      "..........\n",
      "[61,  1800] loss: 6.214399\n",
      "..........\n",
      "[61,  1900] loss: 6.289981\n",
      "..........\n",
      "[61,  2000] loss: 6.355514\n",
      "..........\n",
      "[61,  2100] loss: 6.263070\n",
      "..........\n",
      "[61,  2200] loss: 6.287503\n",
      "..........\n",
      "[61,  2300] loss: 6.168468\n",
      "..........\n",
      "[61,  2400] loss: 6.269005\n",
      "..........\n",
      "[61,  2500] loss: 6.374197\n",
      "..........\n",
      "[61,  2600] loss: 6.250637\n",
      "..........\n",
      "[61,  2700] loss: 6.270904\n",
      "..........\n",
      "[61,  2800] loss: 6.225071\n",
      "..........\n",
      "[61,  2900] loss: 6.294689\n",
      "..........\n",
      "[61,  3000] loss: 6.360942\n",
      "..........\n",
      "[61,  3100] loss: 6.286123\n",
      "..........\n",
      "[61,  3200] loss: 6.279403\n",
      "..........\n",
      "[61,  3300] loss: 6.206533\n",
      "..........\n",
      "[61,  3400] loss: 6.267694\n",
      "..........\n",
      "[61,  3500] loss: 6.330398\n",
      "..........\n",
      "[61,  3600] loss: 6.286612\n",
      "..........\n",
      "[61,  3700] loss: 6.218489\n",
      "..........\n",
      "[61,  3800] loss: 6.229409\n",
      "..........\n",
      "[61,  3900] loss: 6.292689\n",
      "..........\n",
      "[61,  4000] loss: 6.311771\n",
      "total average loss : 0.196\n",
      "== epoch 60 == train acc : 0.0984\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.094\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.078\n",
      "step : 1000 / 1563 acc : 0.084\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[62,   100] loss: 6.269378\n",
      "..........\n",
      "[62,   200] loss: 6.300432\n",
      "..........\n",
      "[62,   300] loss: 6.221477\n",
      "..........\n",
      "[62,   400] loss: 6.290543\n",
      "..........\n",
      "[62,   500] loss: 6.315379\n",
      "..........\n",
      "[62,   600] loss: 6.251821\n",
      "..........\n",
      "[62,   700] loss: 6.269885\n",
      "..........\n",
      "[62,   800] loss: 6.235843\n",
      "..........\n",
      "[62,   900] loss: 6.251110\n",
      "..........\n",
      "[62,  1000] loss: 6.331592\n",
      "..........\n",
      "[62,  1100] loss: 6.250112\n",
      "..........\n",
      "[62,  1200] loss: 6.304035\n",
      "..........\n",
      "[62,  1300] loss: 6.187037\n",
      "..........\n",
      "[62,  1400] loss: 6.317869\n",
      "..........\n",
      "[62,  1500] loss: 6.335683\n",
      "..........\n",
      "[62,  1600] loss: 6.241445\n",
      "..........\n",
      "[62,  1700] loss: 6.336197\n",
      "..........\n",
      "[62,  1800] loss: 6.197494\n",
      "..........\n",
      "[62,  1900] loss: 6.252920\n",
      "..........\n",
      "[62,  2000] loss: 6.348669\n",
      "..........\n",
      "[62,  2100] loss: 6.234981\n",
      "..........\n",
      "[62,  2200] loss: 6.296963\n",
      "..........\n",
      "[62,  2300] loss: 6.190205\n",
      "..........\n",
      "[62,  2400] loss: 6.242649\n",
      "..........\n",
      "[62,  2500] loss: 6.383248\n",
      "..........\n",
      "[62,  2600] loss: 6.269351\n",
      "..........\n",
      "[62,  2700] loss: 6.270034\n",
      "..........\n",
      "[62,  2800] loss: 6.209742\n",
      "..........\n",
      "[62,  2900] loss: 6.276193\n",
      "..........\n",
      "[62,  3000] loss: 6.373946\n",
      "..........\n",
      "[62,  3100] loss: 6.278836\n",
      "..........\n",
      "[62,  3200] loss: 6.243322\n",
      "..........\n",
      "[62,  3300] loss: 6.221761\n",
      "..........\n",
      "[62,  3400] loss: 6.265263\n",
      "..........\n",
      "[62,  3500] loss: 6.346615\n",
      "..........\n",
      "[62,  3600] loss: 6.267652\n",
      "..........\n",
      "[62,  3700] loss: 6.223629\n",
      "..........\n",
      "[62,  3800] loss: 6.197862\n",
      "..........\n",
      "[62,  3900] loss: 6.264537\n",
      "..........\n",
      "[62,  4000] loss: 6.361037\n",
      "total average loss : 0.196\n",
      "== epoch 61 == train acc : 0.1039\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.057\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[63,   100] loss: 6.264244\n",
      "..........\n",
      "[63,   200] loss: 6.271480\n",
      "..........\n",
      "[63,   300] loss: 6.213077\n",
      "..........\n",
      "[63,   400] loss: 6.318413\n",
      "..........\n",
      "[63,   500] loss: 6.292372\n",
      "..........\n",
      "[63,   600] loss: 6.227568\n",
      "..........\n",
      "[63,   700] loss: 6.326387\n",
      "..........\n",
      "[63,   800] loss: 6.204417\n",
      "..........\n",
      "[63,   900] loss: 6.279195\n",
      "..........\n",
      "[63,  1000] loss: 6.335012\n",
      "..........\n",
      "[63,  1100] loss: 6.250731\n",
      "..........\n",
      "[63,  1200] loss: 6.310069\n",
      "..........\n",
      "[63,  1300] loss: 6.168300\n",
      "..........\n",
      "[63,  1400] loss: 6.249198\n",
      "..........\n",
      "[63,  1500] loss: 6.322492\n",
      "..........\n",
      "[63,  1600] loss: 6.252325\n",
      "..........\n",
      "[63,  1700] loss: 6.311820\n",
      "..........\n",
      "[63,  1800] loss: 6.226009\n",
      "..........\n",
      "[63,  1900] loss: 6.291313\n",
      "..........\n",
      "[63,  2000] loss: 6.380430\n",
      "..........\n",
      "[63,  2100] loss: 6.259036\n",
      "..........\n",
      "[63,  2200] loss: 6.302036\n",
      "..........\n",
      "[63,  2300] loss: 6.184782\n",
      "..........\n",
      "[63,  2400] loss: 6.264872\n",
      "..........\n",
      "[63,  2500] loss: 6.337404\n",
      "..........\n",
      "[63,  2600] loss: 6.237923\n",
      "..........\n",
      "[63,  2700] loss: 6.255729\n",
      "..........\n",
      "[63,  2800] loss: 6.220090\n",
      "..........\n",
      "[63,  2900] loss: 6.257750\n",
      "..........\n",
      "[63,  3000] loss: 6.338704\n",
      "..........\n",
      "[63,  3100] loss: 6.269964\n",
      "..........\n",
      "[63,  3200] loss: 6.261889\n",
      "..........\n",
      "[63,  3300] loss: 6.205680\n",
      "..........\n",
      "[63,  3400] loss: 6.288584\n",
      "..........\n",
      "[63,  3500] loss: 6.350780\n",
      "..........\n",
      "[63,  3600] loss: 6.273776\n",
      "..........\n",
      "[63,  3700] loss: 6.259301\n",
      "..........\n",
      "[63,  3800] loss: 6.237965\n",
      "..........\n",
      "[63,  3900] loss: 6.282342\n",
      "..........\n",
      "[63,  4000] loss: 6.332454\n",
      "total average loss : 0.196\n",
      "== epoch 62 == train acc : 0.1023\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.102\n",
      "step : 600 / 1563 acc : 0.099\n",
      "step : 800 / 1563 acc : 0.078\n",
      "step : 1000 / 1563 acc : 0.078\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[64,   100] loss: 6.256845\n",
      "..........\n",
      "[64,   200] loss: 6.277509\n",
      "..........\n",
      "[64,   300] loss: 6.224865\n",
      "..........\n",
      "[64,   400] loss: 6.279514\n",
      "..........\n",
      "[64,   500] loss: 6.294949\n",
      "..........\n",
      "[64,   600] loss: 6.246317\n",
      "..........\n",
      "[64,   700] loss: 6.345485\n",
      "..........\n",
      "[64,   800] loss: 6.211613\n",
      "..........\n",
      "[64,   900] loss: 6.256252\n",
      "..........\n",
      "[64,  1000] loss: 6.325836\n",
      "..........\n",
      "[64,  1100] loss: 6.225163\n",
      "..........\n",
      "[64,  1200] loss: 6.300962\n",
      "..........\n",
      "[64,  1300] loss: 6.192025\n",
      "..........\n",
      "[64,  1400] loss: 6.281365\n",
      "..........\n",
      "[64,  1500] loss: 6.363252\n",
      "..........\n",
      "[64,  1600] loss: 6.237752\n",
      "..........\n",
      "[64,  1700] loss: 6.316774\n",
      "..........\n",
      "[64,  1800] loss: 6.176622\n",
      "..........\n",
      "[64,  1900] loss: 6.277780\n",
      "..........\n",
      "[64,  2000] loss: 6.383136\n",
      "..........\n",
      "[64,  2100] loss: 6.252536\n",
      "..........\n",
      "[64,  2200] loss: 6.278691\n",
      "..........\n",
      "[64,  2300] loss: 6.215536\n",
      "..........\n",
      "[64,  2400] loss: 6.285773\n",
      "..........\n",
      "[64,  2500] loss: 6.352363\n",
      "..........\n",
      "[64,  2600] loss: 6.245086\n",
      "..........\n",
      "[64,  2700] loss: 6.282926\n",
      "..........\n",
      "[64,  2800] loss: 6.208528\n",
      "..........\n",
      "[64,  2900] loss: 6.276805\n",
      "..........\n",
      "[64,  3000] loss: 6.330304\n",
      "..........\n",
      "[64,  3100] loss: 6.317575\n",
      "..........\n",
      "[64,  3200] loss: 6.241523\n",
      "..........\n",
      "[64,  3300] loss: 6.206926\n",
      "..........\n",
      "[64,  3400] loss: 6.253163\n",
      "..........\n",
      "[64,  3500] loss: 6.345036\n",
      "..........\n",
      "[64,  3600] loss: 6.302321\n",
      "..........\n",
      "[64,  3700] loss: 6.263487\n",
      "..........\n",
      "[64,  3800] loss: 6.252939\n",
      "..........\n",
      "[64,  3900] loss: 6.248761\n",
      "..........\n",
      "[64,  4000] loss: 6.313778\n",
      "total average loss : 0.196\n",
      "== epoch 63 == train acc : 0.1008\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.016\n",
      "step : 400 / 1563 acc : 0.039\n",
      "step : 600 / 1563 acc : 0.052\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.065\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[65,   100] loss: 6.242716\n",
      "..........\n",
      "[65,   200] loss: 6.272299\n",
      "..........\n",
      "[65,   300] loss: 6.257947\n",
      "..........\n",
      "[65,   400] loss: 6.331941\n",
      "..........\n",
      "[65,   500] loss: 6.316111\n",
      "..........\n",
      "[65,   600] loss: 6.239676\n",
      "..........\n",
      "[65,   700] loss: 6.269056\n",
      "..........\n",
      "[65,   800] loss: 6.213419\n",
      "..........\n",
      "[65,   900] loss: 6.308178\n",
      "..........\n",
      "[65,  1000] loss: 6.339403\n",
      "..........\n",
      "[65,  1100] loss: 6.221662\n",
      "..........\n",
      "[65,  1200] loss: 6.287424\n",
      "..........\n",
      "[65,  1300] loss: 6.158332\n",
      "..........\n",
      "[65,  1400] loss: 6.269728\n",
      "..........\n",
      "[65,  1500] loss: 6.364144\n",
      "..........\n",
      "[65,  1600] loss: 6.239229\n",
      "..........\n",
      "[65,  1700] loss: 6.342496\n",
      "..........\n",
      "[65,  1800] loss: 6.167170\n",
      "..........\n",
      "[65,  1900] loss: 6.225938\n",
      "..........\n",
      "[65,  2000] loss: 6.328993\n",
      "..........\n",
      "[65,  2100] loss: 6.262018\n",
      "..........\n",
      "[65,  2200] loss: 6.283646\n",
      "..........\n",
      "[65,  2300] loss: 6.158161\n",
      "..........\n",
      "[65,  2400] loss: 6.238758\n",
      "..........\n",
      "[65,  2500] loss: 6.356524\n",
      "..........\n",
      "[65,  2600] loss: 6.254577\n",
      "..........\n",
      "[65,  2700] loss: 6.278329\n",
      "..........\n",
      "[65,  2800] loss: 6.194490\n",
      "..........\n",
      "[65,  2900] loss: 6.261986\n",
      "..........\n",
      "[65,  3000] loss: 6.325338\n",
      "..........\n",
      "[65,  3100] loss: 6.280228\n",
      "..........\n",
      "[65,  3200] loss: 6.285924\n",
      "..........\n",
      "[65,  3300] loss: 6.234644\n",
      "..........\n",
      "[65,  3400] loss: 6.322054\n",
      "..........\n",
      "[65,  3500] loss: 6.352832\n",
      "..........\n",
      "[65,  3600] loss: 6.288908\n",
      "..........\n",
      "[65,  3700] loss: 6.274582\n",
      "..........\n",
      "[65,  3800] loss: 6.249454\n",
      "..........\n",
      "[65,  3900] loss: 6.249524\n",
      "..........\n",
      "[65,  4000] loss: 6.339296\n",
      "total average loss : 0.196\n",
      "== epoch 64 == train acc : 0.0914\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.039\n",
      "step : 600 / 1563 acc : 0.047\n",
      "step : 800 / 1563 acc : 0.055\n",
      "step : 1000 / 1563 acc : 0.069\n",
      "step : 1200 / 1563 acc : 0.076\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[66,   100] loss: 6.238834\n",
      "..........\n",
      "[66,   200] loss: 6.295282\n",
      "..........\n",
      "[66,   300] loss: 6.242377\n",
      "..........\n",
      "[66,   400] loss: 6.269023\n",
      "..........\n",
      "[66,   500] loss: 6.301588\n",
      "..........\n",
      "[66,   600] loss: 6.253556\n",
      "..........\n",
      "[66,   700] loss: 6.287503\n",
      "..........\n",
      "[66,   800] loss: 6.232995\n",
      "..........\n",
      "[66,   900] loss: 6.302932\n",
      "..........\n",
      "[66,  1000] loss: 6.332190\n",
      "..........\n",
      "[66,  1100] loss: 6.258180\n",
      "..........\n",
      "[66,  1200] loss: 6.290353\n",
      "..........\n",
      "[66,  1300] loss: 6.185657\n",
      "..........\n",
      "[66,  1400] loss: 6.287488\n",
      "..........\n",
      "[66,  1500] loss: 6.326959\n",
      "..........\n",
      "[66,  1600] loss: 6.226568\n",
      "..........\n",
      "[66,  1700] loss: 6.335139\n",
      "..........\n",
      "[66,  1800] loss: 6.187401\n",
      "..........\n",
      "[66,  1900] loss: 6.269044\n",
      "..........\n",
      "[66,  2000] loss: 6.382911\n",
      "..........\n",
      "[66,  2100] loss: 6.198436\n",
      "..........\n",
      "[66,  2200] loss: 6.295146\n",
      "..........\n",
      "[66,  2300] loss: 6.193743\n",
      "..........\n",
      "[66,  2400] loss: 6.268264\n",
      "..........\n",
      "[66,  2500] loss: 6.390346\n",
      "..........\n",
      "[66,  2600] loss: 6.262407\n",
      "..........\n",
      "[66,  2700] loss: 6.270073\n",
      "..........\n",
      "[66,  2800] loss: 6.217974\n",
      "..........\n",
      "[66,  2900] loss: 6.266569\n",
      "..........\n",
      "[66,  3000] loss: 6.364871\n",
      "..........\n",
      "[66,  3100] loss: 6.286105\n",
      "..........\n",
      "[66,  3200] loss: 6.248138\n",
      "..........\n",
      "[66,  3300] loss: 6.230981\n",
      "..........\n",
      "[66,  3400] loss: 6.264272\n",
      "..........\n",
      "[66,  3500] loss: 6.347900\n",
      "..........\n",
      "[66,  3600] loss: 6.305210\n",
      "..........\n",
      "[66,  3700] loss: 6.240267\n",
      "..........\n",
      "[66,  3800] loss: 6.194049\n",
      "..........\n",
      "[66,  3900] loss: 6.269335\n",
      "..........\n",
      "[66,  4000] loss: 6.304432\n",
      "total average loss : 0.196\n",
      "== epoch 65 == train acc : 0.1008\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.070\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[67,   100] loss: 6.231086\n",
      "..........\n",
      "[67,   200] loss: 6.266170\n",
      "..........\n",
      "[67,   300] loss: 6.234839\n",
      "..........\n",
      "[67,   400] loss: 6.314924\n",
      "..........\n",
      "[67,   500] loss: 6.269139\n",
      "..........\n",
      "[67,   600] loss: 6.207292\n",
      "..........\n",
      "[67,   700] loss: 6.320724\n",
      "..........\n",
      "[67,   800] loss: 6.211254\n",
      "..........\n",
      "[67,   900] loss: 6.297277\n",
      "..........\n",
      "[67,  1000] loss: 6.368129\n",
      "..........\n",
      "[67,  1100] loss: 6.243825\n",
      "..........\n",
      "[67,  1200] loss: 6.307021\n",
      "..........\n",
      "[67,  1300] loss: 6.166260\n",
      "..........\n",
      "[67,  1400] loss: 6.258087\n",
      "..........\n",
      "[67,  1500] loss: 6.341805\n",
      "..........\n",
      "[67,  1600] loss: 6.250395\n",
      "..........\n",
      "[67,  1700] loss: 6.348005\n",
      "..........\n",
      "[67,  1800] loss: 6.204591\n",
      "..........\n",
      "[67,  1900] loss: 6.276442\n",
      "..........\n",
      "[67,  2000] loss: 6.355150\n",
      "..........\n",
      "[67,  2100] loss: 6.257137\n",
      "..........\n",
      "[67,  2200] loss: 6.281086\n",
      "..........\n",
      "[67,  2300] loss: 6.204510\n",
      "..........\n",
      "[67,  2400] loss: 6.246459\n",
      "..........\n",
      "[67,  2500] loss: 6.362473\n",
      "..........\n",
      "[67,  2600] loss: 6.246739\n",
      "..........\n",
      "[67,  2700] loss: 6.285778\n",
      "..........\n",
      "[67,  2800] loss: 6.204356\n",
      "..........\n",
      "[67,  2900] loss: 6.224250\n",
      "..........\n",
      "[67,  3000] loss: 6.341659\n",
      "..........\n",
      "[67,  3100] loss: 6.302099\n",
      "..........\n",
      "[67,  3200] loss: 6.266363\n",
      "..........\n",
      "[67,  3300] loss: 6.221554\n",
      "..........\n",
      "[67,  3400] loss: 6.248866\n",
      "..........\n",
      "[67,  3500] loss: 6.347317\n",
      "..........\n",
      "[67,  3600] loss: 6.271337\n",
      "..........\n",
      "[67,  3700] loss: 6.269406\n",
      "..........\n",
      "[67,  3800] loss: 6.265112\n",
      "..........\n",
      "[67,  3900] loss: 6.293180\n",
      "..........\n",
      "[67,  4000] loss: 6.309751\n",
      "total average loss : 0.196\n",
      "== epoch 66 == train acc : 0.1062\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.057\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.062\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[68,   100] loss: 6.223182\n",
      "..........\n",
      "[68,   200] loss: 6.280068\n",
      "..........\n",
      "[68,   300] loss: 6.245925\n",
      "..........\n",
      "[68,   400] loss: 6.314817\n",
      "..........\n",
      "[68,   500] loss: 6.290883\n",
      "..........\n",
      "[68,   600] loss: 6.222920\n",
      "..........\n",
      "[68,   700] loss: 6.301442\n",
      "..........\n",
      "[68,   800] loss: 6.191961\n",
      "..........\n",
      "[68,   900] loss: 6.277656\n",
      "..........\n",
      "[68,  1000] loss: 6.326165\n",
      "..........\n",
      "[68,  1100] loss: 6.254291\n",
      "..........\n",
      "[68,  1200] loss: 6.344167\n",
      "..........\n",
      "[68,  1300] loss: 6.221697\n",
      "..........\n",
      "[68,  1400] loss: 6.300084\n",
      "..........\n",
      "[68,  1500] loss: 6.385946\n",
      "..........\n",
      "[68,  1600] loss: 6.226069\n",
      "..........\n",
      "[68,  1700] loss: 6.321068\n",
      "..........\n",
      "[68,  1800] loss: 6.200270\n",
      "..........\n",
      "[68,  1900] loss: 6.282111\n",
      "..........\n",
      "[68,  2000] loss: 6.330456\n",
      "..........\n",
      "[68,  2100] loss: 6.277060\n",
      "..........\n",
      "[68,  2200] loss: 6.285260\n",
      "..........\n",
      "[68,  2300] loss: 6.214071\n",
      "..........\n",
      "[68,  2400] loss: 6.291529\n",
      "..........\n",
      "[68,  2500] loss: 6.369057\n",
      "..........\n",
      "[68,  2600] loss: 6.246114\n",
      "..........\n",
      "[68,  2700] loss: 6.285211\n",
      "..........\n",
      "[68,  2800] loss: 6.194940\n",
      "..........\n",
      "[68,  2900] loss: 6.245650\n",
      "..........\n",
      "[68,  3000] loss: 6.320837\n",
      "..........\n",
      "[68,  3100] loss: 6.259172\n",
      "..........\n",
      "[68,  3200] loss: 6.229615\n",
      "..........\n",
      "[68,  3300] loss: 6.257600\n",
      "..........\n",
      "[68,  3400] loss: 6.287995\n",
      "..........\n",
      "[68,  3500] loss: 6.323864\n",
      "..........\n",
      "[68,  3600] loss: 6.293649\n",
      "..........\n",
      "[68,  3700] loss: 6.251247\n",
      "..........\n",
      "[68,  3800] loss: 6.260550\n",
      "..........\n",
      "[68,  3900] loss: 6.237515\n",
      "..........\n",
      "[68,  4000] loss: 6.346717\n",
      "total average loss : 0.196\n",
      "== epoch 67 == train acc : 0.1055\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.069\n",
      "step : 1200 / 1563 acc : 0.062\n",
      "step : 1400 / 1563 acc : 0.062\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[69,   100] loss: 6.234549\n",
      "..........\n",
      "[69,   200] loss: 6.294598\n",
      "..........\n",
      "[69,   300] loss: 6.242307\n",
      "..........\n",
      "[69,   400] loss: 6.294613\n",
      "..........\n",
      "[69,   500] loss: 6.291803\n",
      "..........\n",
      "[69,   600] loss: 6.216620\n",
      "..........\n",
      "[69,   700] loss: 6.315489\n",
      "..........\n",
      "[69,   800] loss: 6.246391\n",
      "..........\n",
      "[69,   900] loss: 6.295678\n",
      "..........\n",
      "[69,  1000] loss: 6.330304\n",
      "..........\n",
      "[69,  1100] loss: 6.250171\n",
      "..........\n",
      "[69,  1200] loss: 6.303434\n",
      "..........\n",
      "[69,  1300] loss: 6.223313\n",
      "..........\n",
      "[69,  1400] loss: 6.235483\n",
      "..........\n",
      "[69,  1500] loss: 6.312230\n",
      "..........\n",
      "[69,  1600] loss: 6.195671\n",
      "..........\n",
      "[69,  1700] loss: 6.325099\n",
      "..........\n",
      "[69,  1800] loss: 6.196393\n",
      "..........\n",
      "[69,  1900] loss: 6.255864\n",
      "..........\n",
      "[69,  2000] loss: 6.373735\n",
      "..........\n",
      "[69,  2100] loss: 6.242688\n",
      "..........\n",
      "[69,  2200] loss: 6.298718\n",
      "..........\n",
      "[69,  2300] loss: 6.178691\n",
      "..........\n",
      "[69,  2400] loss: 6.280186\n",
      "..........\n",
      "[69,  2500] loss: 6.374353\n",
      "..........\n",
      "[69,  2600] loss: 6.252179\n",
      "..........\n",
      "[69,  2700] loss: 6.283394\n",
      "..........\n",
      "[69,  2800] loss: 6.210266\n",
      "..........\n",
      "[69,  2900] loss: 6.282903\n",
      "..........\n",
      "[69,  3000] loss: 6.349739\n",
      "..........\n",
      "[69,  3100] loss: 6.260040\n",
      "..........\n",
      "[69,  3200] loss: 6.274916\n",
      "..........\n",
      "[69,  3300] loss: 6.232137\n",
      "..........\n",
      "[69,  3400] loss: 6.257449\n",
      "..........\n",
      "[69,  3500] loss: 6.336411\n",
      "..........\n",
      "[69,  3600] loss: 6.272187\n",
      "..........\n",
      "[69,  3700] loss: 6.268763\n",
      "..........\n",
      "[69,  3800] loss: 6.263427\n",
      "..........\n",
      "[69,  3900] loss: 6.221329\n",
      "..........\n",
      "[69,  4000] loss: 6.341745\n",
      "total average loss : 0.196\n",
      "== epoch 68 == train acc : 0.1000\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.083\n",
      "step : 800 / 1563 acc : 0.086\n",
      "step : 1000 / 1563 acc : 0.069\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.060\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[70,   100] loss: 6.240123\n",
      "..........\n",
      "[70,   200] loss: 6.271877\n",
      "..........\n",
      "[70,   300] loss: 6.205444\n",
      "..........\n",
      "[70,   400] loss: 6.309516\n",
      "..........\n",
      "[70,   500] loss: 6.318855\n",
      "..........\n",
      "[70,   600] loss: 6.207311\n",
      "..........\n",
      "[70,   700] loss: 6.312402\n",
      "..........\n",
      "[70,   800] loss: 6.207706\n",
      "..........\n",
      "[70,   900] loss: 6.286999\n",
      "..........\n",
      "[70,  1000] loss: 6.310040\n",
      "..........\n",
      "[70,  1100] loss: 6.266584\n",
      "..........\n",
      "[70,  1200] loss: 6.287112\n",
      "..........\n",
      "[70,  1300] loss: 6.221941\n",
      "..........\n",
      "[70,  1400] loss: 6.265177\n",
      "..........\n",
      "[70,  1500] loss: 6.342740\n",
      "..........\n",
      "[70,  1600] loss: 6.258413\n",
      "..........\n",
      "[70,  1700] loss: 6.345205\n",
      "..........\n",
      "[70,  1800] loss: 6.223648\n",
      "..........\n",
      "[70,  1900] loss: 6.256914\n",
      "..........\n",
      "[70,  2000] loss: 6.335990\n",
      "..........\n",
      "[70,  2100] loss: 6.257262\n",
      "..........\n",
      "[70,  2200] loss: 6.332626\n",
      "..........\n",
      "[70,  2300] loss: 6.210628\n",
      "..........\n",
      "[70,  2400] loss: 6.276973\n",
      "..........\n",
      "[70,  2500] loss: 6.384502\n",
      "..........\n",
      "[70,  2600] loss: 6.259233\n",
      "..........\n",
      "[70,  2700] loss: 6.268128\n",
      "..........\n",
      "[70,  2800] loss: 6.226817\n",
      "..........\n",
      "[70,  2900] loss: 6.235115\n",
      "..........\n",
      "[70,  3000] loss: 6.353452\n",
      "..........\n",
      "[70,  3100] loss: 6.260878\n",
      "..........\n",
      "[70,  3200] loss: 6.286613\n",
      "..........\n",
      "[70,  3300] loss: 6.193284\n",
      "..........\n",
      "[70,  3400] loss: 6.240647\n",
      "..........\n",
      "[70,  3500] loss: 6.336013\n",
      "..........\n",
      "[70,  3600] loss: 6.289345\n",
      "..........\n",
      "[70,  3700] loss: 6.271500\n",
      "..........\n",
      "[70,  3800] loss: 6.231421\n",
      "..........\n",
      "[70,  3900] loss: 6.252533\n",
      "..........\n",
      "[70,  4000] loss: 6.284812\n",
      "total average loss : 0.196\n",
      "== epoch 69 == train acc : 0.1102\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.094\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.068\n",
      "step : 800 / 1563 acc : 0.078\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[71,   100] loss: 6.260946\n",
      "..........\n",
      "[71,   200] loss: 6.290115\n",
      "..........\n",
      "[71,   300] loss: 6.270514\n",
      "..........\n",
      "[71,   400] loss: 6.335174\n",
      "..........\n",
      "[71,   500] loss: 6.305691\n",
      "..........\n",
      "[71,   600] loss: 6.235762\n",
      "..........\n",
      "[71,   700] loss: 6.273738\n",
      "..........\n",
      "[71,   800] loss: 6.234319\n",
      "..........\n",
      "[71,   900] loss: 6.286649\n",
      "..........\n",
      "[71,  1000] loss: 6.350587\n",
      "..........\n",
      "[71,  1100] loss: 6.236145\n",
      "..........\n",
      "[71,  1200] loss: 6.321175\n",
      "..........\n",
      "[71,  1300] loss: 6.192744\n",
      "..........\n",
      "[71,  1400] loss: 6.278688\n",
      "..........\n",
      "[71,  1500] loss: 6.334883\n",
      "..........\n",
      "[71,  1600] loss: 6.192671\n",
      "..........\n",
      "[71,  1700] loss: 6.308006\n",
      "..........\n",
      "[71,  1800] loss: 6.179644\n",
      "..........\n",
      "[71,  1900] loss: 6.280479\n",
      "..........\n",
      "[71,  2000] loss: 6.353434\n",
      "..........\n",
      "[71,  2100] loss: 6.251389\n",
      "..........\n",
      "[71,  2200] loss: 6.286213\n",
      "..........\n",
      "[71,  2300] loss: 6.199116\n",
      "..........\n",
      "[71,  2400] loss: 6.294692\n",
      "..........\n",
      "[71,  2500] loss: 6.357267\n",
      "..........\n",
      "[71,  2600] loss: 6.251991\n",
      "..........\n",
      "[71,  2700] loss: 6.260918\n",
      "..........\n",
      "[71,  2800] loss: 6.204255\n",
      "..........\n",
      "[71,  2900] loss: 6.253246\n",
      "..........\n",
      "[71,  3000] loss: 6.351699\n",
      "..........\n",
      "[71,  3100] loss: 6.271219\n",
      "..........\n",
      "[71,  3200] loss: 6.265322\n",
      "..........\n",
      "[71,  3300] loss: 6.215565\n",
      "..........\n",
      "[71,  3400] loss: 6.282430\n",
      "..........\n",
      "[71,  3500] loss: 6.345943\n",
      "..........\n",
      "[71,  3600] loss: 6.292516\n",
      "..........\n",
      "[71,  3700] loss: 6.272294\n",
      "..........\n",
      "[71,  3800] loss: 6.222328\n",
      "..........\n",
      "[71,  3900] loss: 6.265145\n",
      "..........\n",
      "[71,  4000] loss: 6.330745\n",
      "total average loss : 0.196\n",
      "== epoch 70 == train acc : 0.1078\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.070\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.070\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[72,   100] loss: 6.261556\n",
      "..........\n",
      "[72,   200] loss: 6.293340\n",
      "..........\n",
      "[72,   300] loss: 6.259887\n",
      "..........\n",
      "[72,   400] loss: 6.320390\n",
      "..........\n",
      "[72,   500] loss: 6.313998\n",
      "..........\n",
      "[72,   600] loss: 6.257415\n",
      "..........\n",
      "[72,   700] loss: 6.304299\n",
      "..........\n",
      "[72,   800] loss: 6.237422\n",
      "..........\n",
      "[72,   900] loss: 6.282213\n",
      "..........\n",
      "[72,  1000] loss: 6.349232\n",
      "..........\n",
      "[72,  1100] loss: 6.267311\n",
      "..........\n",
      "[72,  1200] loss: 6.302641\n",
      "..........\n",
      "[72,  1300] loss: 6.208451\n",
      "..........\n",
      "[72,  1400] loss: 6.291533\n",
      "..........\n",
      "[72,  1500] loss: 6.332460\n",
      "..........\n",
      "[72,  1600] loss: 6.219324\n",
      "..........\n",
      "[72,  1700] loss: 6.296233\n",
      "..........\n",
      "[72,  1800] loss: 6.183348\n",
      "..........\n",
      "[72,  1900] loss: 6.254369\n",
      "..........\n",
      "[72,  2000] loss: 6.352306\n",
      "..........\n",
      "[72,  2100] loss: 6.219905\n",
      "..........\n",
      "[72,  2200] loss: 6.302214\n",
      "..........\n",
      "[72,  2300] loss: 6.173642\n",
      "..........\n",
      "[72,  2400] loss: 6.257936\n",
      "..........\n",
      "[72,  2500] loss: 6.333629\n",
      "..........\n",
      "[72,  2600] loss: 6.244600\n",
      "..........\n",
      "[72,  2700] loss: 6.238129\n",
      "..........\n",
      "[72,  2800] loss: 6.206606\n",
      "..........\n",
      "[72,  2900] loss: 6.261211\n",
      "..........\n",
      "[72,  3000] loss: 6.372211\n",
      "..........\n",
      "[72,  3100] loss: 6.254614\n",
      "..........\n",
      "[72,  3200] loss: 6.243561\n",
      "..........\n",
      "[72,  3300] loss: 6.212499\n",
      "..........\n",
      "[72,  3400] loss: 6.267472\n",
      "..........\n",
      "[72,  3500] loss: 6.328744\n",
      "..........\n",
      "[72,  3600] loss: 6.306033\n",
      "..........\n",
      "[72,  3700] loss: 6.230443\n",
      "..........\n",
      "[72,  3800] loss: 6.215922\n",
      "..........\n",
      "[72,  3900] loss: 6.288324\n",
      "..........\n",
      "[72,  4000] loss: 6.343695\n",
      "total average loss : 0.196\n",
      "== epoch 71 == train acc : 0.1062\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.109\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.062\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[73,   100] loss: 6.236267\n",
      "..........\n",
      "[73,   200] loss: 6.246421\n",
      "..........\n",
      "[73,   300] loss: 6.230276\n",
      "..........\n",
      "[73,   400] loss: 6.311432\n",
      "..........\n",
      "[73,   500] loss: 6.329032\n",
      "..........\n",
      "[73,   600] loss: 6.184852\n",
      "..........\n",
      "[73,   700] loss: 6.307348\n",
      "..........\n",
      "[73,   800] loss: 6.225546\n",
      "..........\n",
      "[73,   900] loss: 6.288555\n",
      "..........\n",
      "[73,  1000] loss: 6.339783\n",
      "..........\n",
      "[73,  1100] loss: 6.230793\n",
      "..........\n",
      "[73,  1200] loss: 6.318588\n",
      "..........\n",
      "[73,  1300] loss: 6.213698\n",
      "..........\n",
      "[73,  1400] loss: 6.283248\n",
      "..........\n",
      "[73,  1500] loss: 6.321182\n",
      "..........\n",
      "[73,  1600] loss: 6.233540\n",
      "..........\n",
      "[73,  1700] loss: 6.339089\n",
      "..........\n",
      "[73,  1800] loss: 6.176325\n",
      "..........\n",
      "[73,  1900] loss: 6.255524\n",
      "..........\n",
      "[73,  2000] loss: 6.337598\n",
      "..........\n",
      "[73,  2100] loss: 6.255149\n",
      "..........\n",
      "[73,  2200] loss: 6.282283\n",
      "..........\n",
      "[73,  2300] loss: 6.207652\n",
      "..........\n",
      "[73,  2400] loss: 6.268851\n",
      "..........\n",
      "[73,  2500] loss: 6.346605\n",
      "..........\n",
      "[73,  2600] loss: 6.303799\n",
      "..........\n",
      "[73,  2700] loss: 6.294396\n",
      "..........\n",
      "[73,  2800] loss: 6.202008\n",
      "..........\n",
      "[73,  2900] loss: 6.275295\n",
      "..........\n",
      "[73,  3000] loss: 6.361463\n",
      "..........\n",
      "[73,  3100] loss: 6.290232\n",
      "..........\n",
      "[73,  3200] loss: 6.284519\n",
      "..........\n",
      "[73,  3300] loss: 6.221339\n",
      "..........\n",
      "[73,  3400] loss: 6.278876\n",
      "..........\n",
      "[73,  3500] loss: 6.344989\n",
      "..........\n",
      "[73,  3600] loss: 6.265116\n",
      "..........\n",
      "[73,  3700] loss: 6.260186\n",
      "..........\n",
      "[73,  3800] loss: 6.238322\n",
      "..........\n",
      "[73,  3900] loss: 6.250298\n",
      "..........\n",
      "[73,  4000] loss: 6.333241\n",
      "total average loss : 0.196\n",
      "== epoch 72 == train acc : 0.0953\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.094\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.057\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.053\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[74,   100] loss: 6.233470\n",
      "..........\n",
      "[74,   200] loss: 6.278469\n",
      "..........\n",
      "[74,   300] loss: 6.234438\n",
      "..........\n",
      "[74,   400] loss: 6.289791\n",
      "..........\n",
      "[74,   500] loss: 6.312239\n",
      "..........\n",
      "[74,   600] loss: 6.236471\n",
      "..........\n",
      "[74,   700] loss: 6.303179\n",
      "..........\n",
      "[74,   800] loss: 6.220929\n",
      "..........\n",
      "[74,   900] loss: 6.302082\n",
      "..........\n",
      "[74,  1000] loss: 6.298713\n",
      "..........\n",
      "[74,  1100] loss: 6.251049\n",
      "..........\n",
      "[74,  1200] loss: 6.326947\n",
      "..........\n",
      "[74,  1300] loss: 6.203573\n",
      "..........\n",
      "[74,  1400] loss: 6.297149\n",
      "..........\n",
      "[74,  1500] loss: 6.345150\n",
      "..........\n",
      "[74,  1600] loss: 6.250815\n",
      "..........\n",
      "[74,  1700] loss: 6.315472\n",
      "..........\n",
      "[74,  1800] loss: 6.181526\n",
      "..........\n",
      "[74,  1900] loss: 6.273668\n",
      "..........\n",
      "[74,  2000] loss: 6.347917\n",
      "..........\n",
      "[74,  2100] loss: 6.261857\n",
      "..........\n",
      "[74,  2200] loss: 6.288825\n",
      "..........\n",
      "[74,  2300] loss: 6.194547\n",
      "..........\n",
      "[74,  2400] loss: 6.230852\n",
      "..........\n",
      "[74,  2500] loss: 6.380016\n",
      "..........\n",
      "[74,  2600] loss: 6.245063\n",
      "..........\n",
      "[74,  2700] loss: 6.276924\n",
      "..........\n",
      "[74,  2800] loss: 6.197218\n",
      "..........\n",
      "[74,  2900] loss: 6.257531\n",
      "..........\n",
      "[74,  3000] loss: 6.360443\n",
      "..........\n",
      "[74,  3100] loss: 6.251096\n",
      "..........\n",
      "[74,  3200] loss: 6.296875\n",
      "..........\n",
      "[74,  3300] loss: 6.222190\n",
      "..........\n",
      "[74,  3400] loss: 6.298927\n",
      "..........\n",
      "[74,  3500] loss: 6.311183\n",
      "..........\n",
      "[74,  3600] loss: 6.261791\n",
      "..........\n",
      "[74,  3700] loss: 6.258580\n",
      "..........\n",
      "[74,  3800] loss: 6.255159\n",
      "..........\n",
      "[74,  3900] loss: 6.269035\n",
      "..........\n",
      "[74,  4000] loss: 6.305489\n",
      "total average loss : 0.196\n",
      "== epoch 73 == train acc : 0.1086\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.042\n",
      "step : 800 / 1563 acc : 0.047\n",
      "step : 1000 / 1563 acc : 0.056\n",
      "step : 1200 / 1563 acc : 0.065\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[75,   100] loss: 6.242890\n",
      "..........\n",
      "[75,   200] loss: 6.285484\n",
      "..........\n",
      "[75,   300] loss: 6.226387\n",
      "..........\n",
      "[75,   400] loss: 6.306671\n",
      "..........\n",
      "[75,   500] loss: 6.325071\n",
      "..........\n",
      "[75,   600] loss: 6.234435\n",
      "..........\n",
      "[75,   700] loss: 6.299418\n",
      "..........\n",
      "[75,   800] loss: 6.205288\n",
      "..........\n",
      "[75,   900] loss: 6.322475\n",
      "..........\n",
      "[75,  1000] loss: 6.314124\n",
      "..........\n",
      "[75,  1100] loss: 6.211839\n",
      "..........\n",
      "[75,  1200] loss: 6.329165\n",
      "..........\n",
      "[75,  1300] loss: 6.133459\n",
      "..........\n",
      "[75,  1400] loss: 6.307177\n",
      "..........\n",
      "[75,  1500] loss: 6.350231\n",
      "..........\n",
      "[75,  1600] loss: 6.261298\n",
      "..........\n",
      "[75,  1700] loss: 6.268002\n",
      "..........\n",
      "[75,  1800] loss: 6.206089\n",
      "..........\n",
      "[75,  1900] loss: 6.289514\n",
      "..........\n",
      "[75,  2000] loss: 6.350611\n",
      "..........\n",
      "[75,  2100] loss: 6.229999\n",
      "..........\n",
      "[75,  2200] loss: 6.311212\n",
      "..........\n",
      "[75,  2300] loss: 6.199124\n",
      "..........\n",
      "[75,  2400] loss: 6.247262\n",
      "..........\n",
      "[75,  2500] loss: 6.378459\n",
      "..........\n",
      "[75,  2600] loss: 6.262410\n",
      "..........\n",
      "[75,  2700] loss: 6.263465\n",
      "..........\n",
      "[75,  2800] loss: 6.204284\n",
      "..........\n",
      "[75,  2900] loss: 6.281554\n",
      "..........\n",
      "[75,  3000] loss: 6.359298\n",
      "..........\n",
      "[75,  3100] loss: 6.270995\n",
      "..........\n",
      "[75,  3200] loss: 6.282760\n",
      "..........\n",
      "[75,  3300] loss: 6.206877\n",
      "..........\n",
      "[75,  3400] loss: 6.267347\n",
      "..........\n",
      "[75,  3500] loss: 6.401785\n",
      "..........\n",
      "[75,  3600] loss: 6.313856\n",
      "..........\n",
      "[75,  3700] loss: 6.249866\n",
      "..........\n",
      "[75,  3800] loss: 6.210128\n",
      "..........\n",
      "[75,  3900] loss: 6.246828\n",
      "..........\n",
      "[75,  4000] loss: 6.318419\n",
      "total average loss : 0.196\n",
      "== epoch 74 == train acc : 0.1047\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.055\n",
      "step : 600 / 1563 acc : 0.068\n",
      "step : 800 / 1563 acc : 0.070\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.078\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[76,   100] loss: 6.259325\n",
      "..........\n",
      "[76,   200] loss: 6.299650\n",
      "..........\n",
      "[76,   300] loss: 6.221302\n",
      "..........\n",
      "[76,   400] loss: 6.322882\n",
      "..........\n",
      "[76,   500] loss: 6.347924\n",
      "..........\n",
      "[76,   600] loss: 6.248103\n",
      "..........\n",
      "[76,   700] loss: 6.303375\n",
      "..........\n",
      "[76,   800] loss: 6.202942\n",
      "..........\n",
      "[76,   900] loss: 6.298571\n",
      "..........\n",
      "[76,  1000] loss: 6.329587\n",
      "..........\n",
      "[76,  1100] loss: 6.214146\n",
      "..........\n",
      "[76,  1200] loss: 6.318061\n",
      "..........\n",
      "[76,  1300] loss: 6.217993\n",
      "..........\n",
      "[76,  1400] loss: 6.295027\n",
      "..........\n",
      "[76,  1500] loss: 6.333235\n",
      "..........\n",
      "[76,  1600] loss: 6.223823\n",
      "..........\n",
      "[76,  1700] loss: 6.317266\n",
      "..........\n",
      "[76,  1800] loss: 6.192453\n",
      "..........\n",
      "[76,  1900] loss: 6.299398\n",
      "..........\n",
      "[76,  2000] loss: 6.371459\n",
      "..........\n",
      "[76,  2100] loss: 6.250128\n",
      "..........\n",
      "[76,  2200] loss: 6.314829\n",
      "..........\n",
      "[76,  2300] loss: 6.204019\n",
      "..........\n",
      "[76,  2400] loss: 6.274984\n",
      "..........\n",
      "[76,  2500] loss: 6.374059\n",
      "..........\n",
      "[76,  2600] loss: 6.244933\n",
      "..........\n",
      "[76,  2700] loss: 6.273767\n",
      "..........\n",
      "[76,  2800] loss: 6.193571\n",
      "..........\n",
      "[76,  2900] loss: 6.281905\n",
      "..........\n",
      "[76,  3000] loss: 6.322462\n",
      "..........\n",
      "[76,  3100] loss: 6.248896\n",
      "..........\n",
      "[76,  3200] loss: 6.274084\n",
      "..........\n",
      "[76,  3300] loss: 6.230326\n",
      "..........\n",
      "[76,  3400] loss: 6.259857\n",
      "..........\n",
      "[76,  3500] loss: 6.299089\n",
      "..........\n",
      "[76,  3600] loss: 6.265572\n",
      "..........\n",
      "[76,  3700] loss: 6.236811\n",
      "..........\n",
      "[76,  3800] loss: 6.255495\n",
      "..........\n",
      "[76,  3900] loss: 6.236532\n",
      "..........\n",
      "[76,  4000] loss: 6.328639\n",
      "total average loss : 0.196\n",
      "== epoch 75 == train acc : 0.1055\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.047\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.062\n",
      "step : 1400 / 1563 acc : 0.065\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[77,   100] loss: 6.229471\n",
      "..........\n",
      "[77,   200] loss: 6.284266\n",
      "..........\n",
      "[77,   300] loss: 6.224408\n",
      "..........\n",
      "[77,   400] loss: 6.308049\n",
      "..........\n",
      "[77,   500] loss: 6.314623\n",
      "..........\n",
      "[77,   600] loss: 6.205998\n",
      "..........\n",
      "[77,   700] loss: 6.296209\n",
      "..........\n",
      "[77,   800] loss: 6.204275\n",
      "..........\n",
      "[77,   900] loss: 6.305094\n",
      "..........\n",
      "[77,  1000] loss: 6.323135\n",
      "..........\n",
      "[77,  1100] loss: 6.199728\n",
      "..........\n",
      "[77,  1200] loss: 6.318072\n",
      "..........\n",
      "[77,  1300] loss: 6.193018\n",
      "..........\n",
      "[77,  1400] loss: 6.276211\n",
      "..........\n",
      "[77,  1500] loss: 6.342189\n",
      "..........\n",
      "[77,  1600] loss: 6.248209\n",
      "..........\n",
      "[77,  1700] loss: 6.294280\n",
      "..........\n",
      "[77,  1800] loss: 6.200525\n",
      "..........\n",
      "[77,  1900] loss: 6.250860\n",
      "..........\n",
      "[77,  2000] loss: 6.366415\n",
      "..........\n",
      "[77,  2100] loss: 6.267789\n",
      "..........\n",
      "[77,  2200] loss: 6.312360\n",
      "..........\n",
      "[77,  2300] loss: 6.213136\n",
      "..........\n",
      "[77,  2400] loss: 6.258407\n",
      "..........\n",
      "[77,  2500] loss: 6.381359\n",
      "..........\n",
      "[77,  2600] loss: 6.290791\n",
      "..........\n",
      "[77,  2700] loss: 6.276483\n",
      "..........\n",
      "[77,  2800] loss: 6.255083\n",
      "..........\n",
      "[77,  2900] loss: 6.258794\n",
      "..........\n",
      "[77,  3000] loss: 6.330054\n",
      "..........\n",
      "[77,  3100] loss: 6.308463\n",
      "..........\n",
      "[77,  3200] loss: 6.231408\n",
      "..........\n",
      "[77,  3300] loss: 6.236410\n",
      "..........\n",
      "[77,  3400] loss: 6.313425\n",
      "..........\n",
      "[77,  3500] loss: 6.328202\n",
      "..........\n",
      "[77,  3600] loss: 6.276953\n",
      "..........\n",
      "[77,  3700] loss: 6.261703\n",
      "..........\n",
      "[77,  3800] loss: 6.229225\n",
      "..........\n",
      "[77,  3900] loss: 6.220025\n",
      "..........\n",
      "[77,  4000] loss: 6.307856\n",
      "total average loss : 0.196\n",
      "== epoch 76 == train acc : 0.0992\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.094\n",
      "step : 400 / 1563 acc : 0.062\n",
      "step : 600 / 1563 acc : 0.057\n",
      "step : 800 / 1563 acc : 0.059\n",
      "step : 1000 / 1563 acc : 0.053\n",
      "step : 1200 / 1563 acc : 0.062\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[78,   100] loss: 6.231786\n",
      "..........\n",
      "[78,   200] loss: 6.281898\n",
      "..........\n",
      "[78,   300] loss: 6.232523\n",
      "..........\n",
      "[78,   400] loss: 6.342513\n",
      "..........\n",
      "[78,   500] loss: 6.301598\n",
      "..........\n",
      "[78,   600] loss: 6.248457\n",
      "..........\n",
      "[78,   700] loss: 6.269229\n",
      "..........\n",
      "[78,   800] loss: 6.230160\n",
      "..........\n",
      "[78,   900] loss: 6.297680\n",
      "..........\n",
      "[78,  1000] loss: 6.341696\n",
      "..........\n",
      "[78,  1100] loss: 6.201281\n",
      "..........\n",
      "[78,  1200] loss: 6.312752\n",
      "..........\n",
      "[78,  1300] loss: 6.189307\n",
      "..........\n",
      "[78,  1400] loss: 6.275807\n",
      "..........\n",
      "[78,  1500] loss: 6.377810\n",
      "..........\n",
      "[78,  1600] loss: 6.210482\n",
      "..........\n",
      "[78,  1700] loss: 6.281514\n",
      "..........\n",
      "[78,  1800] loss: 6.209569\n",
      "..........\n",
      "[78,  1900] loss: 6.284838\n",
      "..........\n",
      "[78,  2000] loss: 6.336069\n",
      "..........\n",
      "[78,  2100] loss: 6.289763\n",
      "..........\n",
      "[78,  2200] loss: 6.312667\n",
      "..........\n",
      "[78,  2300] loss: 6.208728\n",
      "..........\n",
      "[78,  2400] loss: 6.285225\n",
      "..........\n",
      "[78,  2500] loss: 6.346332\n",
      "..........\n",
      "[78,  2600] loss: 6.265278\n",
      "..........\n",
      "[78,  2700] loss: 6.287227\n",
      "..........\n",
      "[78,  2800] loss: 6.210582\n",
      "..........\n",
      "[78,  2900] loss: 6.270189\n",
      "..........\n",
      "[78,  3000] loss: 6.394217\n",
      "..........\n",
      "[78,  3100] loss: 6.235138\n",
      "..........\n",
      "[78,  3200] loss: 6.261645\n",
      "..........\n",
      "[78,  3300] loss: 6.211263\n",
      "..........\n",
      "[78,  3400] loss: 6.267105\n",
      "..........\n",
      "[78,  3500] loss: 6.342909\n",
      "..........\n",
      "[78,  3600] loss: 6.284459\n",
      "..........\n",
      "[78,  3700] loss: 6.225139\n",
      "..........\n",
      "[78,  3800] loss: 6.236629\n",
      "..........\n",
      "[78,  3900] loss: 6.261413\n",
      "..........\n",
      "[78,  4000] loss: 6.337750\n",
      "total average loss : 0.196\n",
      "== epoch 77 == train acc : 0.1055\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.081\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[79,   100] loss: 6.233891\n",
      "..........\n",
      "[79,   200] loss: 6.286879\n",
      "..........\n",
      "[79,   300] loss: 6.248653\n",
      "..........\n",
      "[79,   400] loss: 6.305888\n",
      "..........\n",
      "[79,   500] loss: 6.289125\n",
      "..........\n",
      "[79,   600] loss: 6.232937\n",
      "..........\n",
      "[79,   700] loss: 6.284896\n",
      "..........\n",
      "[79,   800] loss: 6.260045\n",
      "..........\n",
      "[79,   900] loss: 6.288524\n",
      "..........\n",
      "[79,  1000] loss: 6.357508\n",
      "..........\n",
      "[79,  1100] loss: 6.276251\n",
      "..........\n",
      "[79,  1200] loss: 6.344782\n",
      "..........\n",
      "[79,  1300] loss: 6.215817\n",
      "..........\n",
      "[79,  1400] loss: 6.243012\n",
      "..........\n",
      "[79,  1500] loss: 6.360826\n",
      "..........\n",
      "[79,  1600] loss: 6.249116\n",
      "..........\n",
      "[79,  1700] loss: 6.322538\n",
      "..........\n",
      "[79,  1800] loss: 6.188795\n",
      "..........\n",
      "[79,  1900] loss: 6.267780\n",
      "..........\n",
      "[79,  2000] loss: 6.336536\n",
      "..........\n",
      "[79,  2100] loss: 6.237557\n",
      "..........\n",
      "[79,  2200] loss: 6.275320\n",
      "..........\n",
      "[79,  2300] loss: 6.202745\n",
      "..........\n",
      "[79,  2400] loss: 6.267566\n",
      "..........\n",
      "[79,  2500] loss: 6.356598\n",
      "..........\n",
      "[79,  2600] loss: 6.279552\n",
      "..........\n",
      "[79,  2700] loss: 6.265343\n",
      "..........\n",
      "[79,  2800] loss: 6.190201\n",
      "..........\n",
      "[79,  2900] loss: 6.251229\n",
      "..........\n",
      "[79,  3000] loss: 6.346365\n",
      "..........\n",
      "[79,  3100] loss: 6.268702\n",
      "..........\n",
      "[79,  3200] loss: 6.234727\n",
      "..........\n",
      "[79,  3300] loss: 6.221806\n",
      "..........\n",
      "[79,  3400] loss: 6.243076\n",
      "..........\n",
      "[79,  3500] loss: 6.351331\n",
      "..........\n",
      "[79,  3600] loss: 6.264545\n",
      "..........\n",
      "[79,  3700] loss: 6.263624\n",
      "..........\n",
      "[79,  3800] loss: 6.231024\n",
      "..........\n",
      "[79,  3900] loss: 6.286460\n",
      "..........\n",
      "[79,  4000] loss: 6.328078\n",
      "total average loss : 0.196\n",
      "== epoch 78 == train acc : 0.0930\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.141\n",
      "step : 400 / 1563 acc : 0.148\n",
      "step : 600 / 1563 acc : 0.115\n",
      "step : 800 / 1563 acc : 0.113\n",
      "step : 1000 / 1563 acc : 0.100\n",
      "step : 1200 / 1563 acc : 0.091\n",
      "step : 1400 / 1563 acc : 0.078\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[80,   100] loss: 6.232450\n",
      "..........\n",
      "[80,   200] loss: 6.276873\n",
      "..........\n",
      "[80,   300] loss: 6.221572\n",
      "..........\n",
      "[80,   400] loss: 6.307562\n",
      "..........\n",
      "[80,   500] loss: 6.307210\n",
      "..........\n",
      "[80,   600] loss: 6.267818\n",
      "..........\n",
      "[80,   700] loss: 6.294470\n",
      "..........\n",
      "[80,   800] loss: 6.228100\n",
      "..........\n",
      "[80,   900] loss: 6.322996\n",
      "..........\n",
      "[80,  1000] loss: 6.317428\n",
      "..........\n",
      "[80,  1100] loss: 6.272842\n",
      "..........\n",
      "[80,  1200] loss: 6.330266\n",
      "..........\n",
      "[80,  1300] loss: 6.201546\n",
      "..........\n",
      "[80,  1400] loss: 6.245505\n",
      "..........\n",
      "[80,  1500] loss: 6.342801\n",
      "..........\n",
      "[80,  1600] loss: 6.235885\n",
      "..........\n",
      "[80,  1700] loss: 6.256138\n",
      "..........\n",
      "[80,  1800] loss: 6.204136\n",
      "..........\n",
      "[80,  1900] loss: 6.277888\n",
      "..........\n",
      "[80,  2000] loss: 6.357904\n",
      "..........\n",
      "[80,  2100] loss: 6.255273\n",
      "..........\n",
      "[80,  2200] loss: 6.278539\n",
      "..........\n",
      "[80,  2300] loss: 6.254692\n",
      "..........\n",
      "[80,  2400] loss: 6.231869\n",
      "..........\n",
      "[80,  2500] loss: 6.338404\n",
      "..........\n",
      "[80,  2600] loss: 6.279517\n",
      "..........\n",
      "[80,  2700] loss: 6.279519\n",
      "..........\n",
      "[80,  2800] loss: 6.237140\n",
      "..........\n",
      "[80,  2900] loss: 6.233410\n",
      "..........\n",
      "[80,  3000] loss: 6.387741\n",
      "..........\n",
      "[80,  3100] loss: 6.257772\n",
      "..........\n",
      "[80,  3200] loss: 6.225923\n",
      "..........\n",
      "[80,  3300] loss: 6.223847\n",
      "..........\n",
      "[80,  3400] loss: 6.249942\n",
      "..........\n",
      "[80,  3500] loss: 6.329044\n",
      "..........\n",
      "[80,  3600] loss: 6.278325\n",
      "..........\n",
      "[80,  3700] loss: 6.269457\n",
      "..........\n",
      "[80,  3800] loss: 6.235582\n",
      "..........\n",
      "[80,  3900] loss: 6.253493\n",
      "..........\n",
      "[80,  4000] loss: 6.350884\n",
      "total average loss : 0.196\n",
      "== epoch 79 == train acc : 0.0969\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.109\n",
      "step : 400 / 1563 acc : 0.094\n",
      "step : 600 / 1563 acc : 0.083\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[81,   100] loss: 6.194190\n",
      "..........\n",
      "[81,   200] loss: 6.296428\n",
      "..........\n",
      "[81,   300] loss: 6.212051\n",
      "..........\n",
      "[81,   400] loss: 6.304521\n",
      "..........\n",
      "[81,   500] loss: 6.332320\n",
      "..........\n",
      "[81,   600] loss: 6.230506\n",
      "..........\n",
      "[81,   700] loss: 6.317640\n",
      "..........\n",
      "[81,   800] loss: 6.220609\n",
      "..........\n",
      "[81,   900] loss: 6.315228\n",
      "..........\n",
      "[81,  1000] loss: 6.332177\n",
      "..........\n",
      "[81,  1100] loss: 6.270135\n",
      "..........\n",
      "[81,  1200] loss: 6.317156\n",
      "..........\n",
      "[81,  1300] loss: 6.220576\n",
      "..........\n",
      "[81,  1400] loss: 6.292904\n",
      "..........\n",
      "[81,  1500] loss: 6.350076\n",
      "..........\n",
      "[81,  1600] loss: 6.253951\n",
      "..........\n",
      "[81,  1700] loss: 6.314784\n",
      "..........\n",
      "[81,  1800] loss: 6.171068\n",
      "..........\n",
      "[81,  1900] loss: 6.293463\n",
      "..........\n",
      "[81,  2000] loss: 6.347346\n",
      "..........\n",
      "[81,  2100] loss: 6.257107\n",
      "..........\n",
      "[81,  2200] loss: 6.264603\n",
      "..........\n",
      "[81,  2300] loss: 6.211565\n",
      "..........\n",
      "[81,  2400] loss: 6.256690\n",
      "..........\n",
      "[81,  2500] loss: 6.372446\n",
      "..........\n",
      "[81,  2600] loss: 6.252221\n",
      "..........\n",
      "[81,  2700] loss: 6.270634\n",
      "..........\n",
      "[81,  2800] loss: 6.202835\n",
      "..........\n",
      "[81,  2900] loss: 6.233089\n",
      "..........\n",
      "[81,  3000] loss: 6.382902\n",
      "..........\n",
      "[81,  3100] loss: 6.253010\n",
      "..........\n",
      "[81,  3200] loss: 6.250246\n",
      "..........\n",
      "[81,  3300] loss: 6.231598\n",
      "..........\n",
      "[81,  3400] loss: 6.250535\n",
      "..........\n",
      "[81,  3500] loss: 6.352314\n",
      "..........\n",
      "[81,  3600] loss: 6.265422\n",
      "..........\n",
      "[81,  3700] loss: 6.224023\n",
      "..........\n",
      "[81,  3800] loss: 6.277243\n",
      "..........\n",
      "[81,  3900] loss: 6.231015\n",
      "..........\n",
      "[81,  4000] loss: 6.342837\n",
      "total average loss : 0.196\n",
      "== epoch 80 == train acc : 0.0898\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.016\n",
      "step : 400 / 1563 acc : 0.039\n",
      "step : 600 / 1563 acc : 0.031\n",
      "step : 800 / 1563 acc : 0.055\n",
      "step : 1000 / 1563 acc : 0.062\n",
      "step : 1200 / 1563 acc : 0.062\n",
      "step : 1400 / 1563 acc : 0.071\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[82,   100] loss: 6.233119\n",
      "..........\n",
      "[82,   200] loss: 6.284997\n",
      "..........\n",
      "[82,   300] loss: 6.248067\n",
      "..........\n",
      "[82,   400] loss: 6.309244\n",
      "..........\n",
      "[82,   500] loss: 6.293352\n",
      "..........\n",
      "[82,   600] loss: 6.226221\n",
      "..........\n",
      "[82,   700] loss: 6.303749\n",
      "..........\n",
      "[82,   800] loss: 6.236137\n",
      "..........\n",
      "[82,   900] loss: 6.303689\n",
      "..........\n",
      "[82,  1000] loss: 6.299411\n",
      "..........\n",
      "[82,  1100] loss: 6.238136\n",
      "..........\n",
      "[82,  1200] loss: 6.303125\n",
      "..........\n",
      "[82,  1300] loss: 6.220794\n",
      "..........\n",
      "[82,  1400] loss: 6.248775\n",
      "..........\n",
      "[82,  1500] loss: 6.346069\n",
      "..........\n",
      "[82,  1600] loss: 6.222331\n",
      "..........\n",
      "[82,  1700] loss: 6.311430\n",
      "..........\n",
      "[82,  1800] loss: 6.208338\n",
      "..........\n",
      "[82,  1900] loss: 6.280655\n",
      "..........\n",
      "[82,  2000] loss: 6.333162\n",
      "..........\n",
      "[82,  2100] loss: 6.279987\n",
      "..........\n",
      "[82,  2200] loss: 6.276001\n",
      "..........\n",
      "[82,  2300] loss: 6.206614\n",
      "..........\n",
      "[82,  2400] loss: 6.255446\n",
      "..........\n",
      "[82,  2500] loss: 6.387264\n",
      "..........\n",
      "[82,  2600] loss: 6.242305\n",
      "..........\n",
      "[82,  2700] loss: 6.282670\n",
      "..........\n",
      "[82,  2800] loss: 6.219024\n",
      "..........\n",
      "[82,  2900] loss: 6.262898\n",
      "..........\n",
      "[82,  3000] loss: 6.344438\n",
      "..........\n",
      "[82,  3100] loss: 6.266382\n",
      "..........\n",
      "[82,  3200] loss: 6.289368\n",
      "..........\n",
      "[82,  3300] loss: 6.234594\n",
      "..........\n",
      "[82,  3400] loss: 6.271151\n",
      "..........\n",
      "[82,  3500] loss: 6.361303\n",
      "..........\n",
      "[82,  3600] loss: 6.288946\n",
      "..........\n",
      "[82,  3700] loss: 6.229984\n",
      "..........\n",
      "[82,  3800] loss: 6.254034\n",
      "..........\n",
      "[82,  3900] loss: 6.246029\n",
      "..........\n",
      "[82,  4000] loss: 6.317658\n",
      "total average loss : 0.196\n",
      "== epoch 81 == train acc : 0.1008\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.078\n",
      "step : 800 / 1563 acc : 0.066\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.065\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[83,   100] loss: 6.279609\n",
      "..........\n",
      "[83,   200] loss: 6.283780\n",
      "..........\n",
      "[83,   300] loss: 6.228478\n",
      "..........\n",
      "[83,   400] loss: 6.283586\n",
      "..........\n",
      "[83,   500] loss: 6.303164\n",
      "..........\n",
      "[83,   600] loss: 6.213714\n",
      "..........\n",
      "[83,   700] loss: 6.293197\n",
      "..........\n",
      "[83,   800] loss: 6.236426\n",
      "..........\n",
      "[83,   900] loss: 6.294372\n",
      "..........\n",
      "[83,  1000] loss: 6.357814\n",
      "..........\n",
      "[83,  1100] loss: 6.197591\n",
      "..........\n",
      "[83,  1200] loss: 6.333437\n",
      "..........\n",
      "[83,  1300] loss: 6.227188\n",
      "..........\n",
      "[83,  1400] loss: 6.275100\n",
      "..........\n",
      "[83,  1500] loss: 6.376743\n",
      "..........\n",
      "[83,  1600] loss: 6.226077\n",
      "..........\n",
      "[83,  1700] loss: 6.323061\n",
      "..........\n",
      "[83,  1800] loss: 6.168181\n",
      "..........\n",
      "[83,  1900] loss: 6.273399\n",
      "..........\n",
      "[83,  2000] loss: 6.343913\n",
      "..........\n",
      "[83,  2100] loss: 6.236649\n",
      "..........\n",
      "[83,  2200] loss: 6.253850\n",
      "..........\n",
      "[83,  2300] loss: 6.200989\n",
      "..........\n",
      "[83,  2400] loss: 6.274741\n",
      "..........\n",
      "[83,  2500] loss: 6.313269\n",
      "..........\n",
      "[83,  2600] loss: 6.270165\n",
      "..........\n",
      "[83,  2700] loss: 6.269267\n",
      "..........\n",
      "[83,  2800] loss: 6.211499\n",
      "..........\n",
      "[83,  2900] loss: 6.254787\n",
      "..........\n",
      "[83,  3000] loss: 6.352062\n",
      "..........\n",
      "[83,  3100] loss: 6.292090\n",
      "..........\n",
      "[83,  3200] loss: 6.269923\n",
      "..........\n",
      "[83,  3300] loss: 6.183692\n",
      "..........\n",
      "[83,  3400] loss: 6.275391\n",
      "..........\n",
      "[83,  3500] loss: 6.381314\n",
      "..........\n",
      "[83,  3600] loss: 6.291424\n",
      "..........\n",
      "[83,  3700] loss: 6.244606\n",
      "..........\n",
      "[83,  3800] loss: 6.246165\n",
      "..........\n",
      "[83,  3900] loss: 6.224501\n",
      "..........\n",
      "[83,  4000] loss: 6.371698\n",
      "total average loss : 0.196\n",
      "== epoch 82 == train acc : 0.1117\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.094\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.083\n",
      "step : 800 / 1563 acc : 0.082\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[84,   100] loss: 6.251289\n",
      "..........\n",
      "[84,   200] loss: 6.299334\n",
      "..........\n",
      "[84,   300] loss: 6.222920\n",
      "..........\n",
      "[84,   400] loss: 6.296305\n",
      "..........\n",
      "[84,   500] loss: 6.307256\n",
      "..........\n",
      "[84,   600] loss: 6.244566\n",
      "..........\n",
      "[84,   700] loss: 6.285188\n",
      "..........\n",
      "[84,   800] loss: 6.206965\n",
      "..........\n",
      "[84,   900] loss: 6.255192\n",
      "..........\n",
      "[84,  1000] loss: 6.338104\n",
      "..........\n",
      "[84,  1100] loss: 6.242039\n",
      "..........\n",
      "[84,  1200] loss: 6.324611\n",
      "..........\n",
      "[84,  1300] loss: 6.202963\n",
      "..........\n",
      "[84,  1400] loss: 6.278277\n",
      "..........\n",
      "[84,  1500] loss: 6.346594\n",
      "..........\n",
      "[84,  1600] loss: 6.234624\n",
      "..........\n",
      "[84,  1700] loss: 6.316562\n",
      "..........\n",
      "[84,  1800] loss: 6.171323\n",
      "..........\n",
      "[84,  1900] loss: 6.262306\n",
      "..........\n",
      "[84,  2000] loss: 6.372320\n",
      "..........\n",
      "[84,  2100] loss: 6.243140\n",
      "..........\n",
      "[84,  2200] loss: 6.315477\n",
      "..........\n",
      "[84,  2300] loss: 6.191056\n",
      "..........\n",
      "[84,  2400] loss: 6.250215\n",
      "..........\n",
      "[84,  2500] loss: 6.368648\n",
      "..........\n",
      "[84,  2600] loss: 6.248432\n",
      "..........\n",
      "[84,  2700] loss: 6.286554\n",
      "..........\n",
      "[84,  2800] loss: 6.225889\n",
      "..........\n",
      "[84,  2900] loss: 6.281214\n",
      "..........\n",
      "[84,  3000] loss: 6.352051\n",
      "..........\n",
      "[84,  3100] loss: 6.286802\n",
      "..........\n",
      "[84,  3200] loss: 6.274645\n",
      "..........\n",
      "[84,  3300] loss: 6.191390\n",
      "..........\n",
      "[84,  3400] loss: 6.272543\n",
      "..........\n",
      "[84,  3500] loss: 6.317272\n",
      "..........\n",
      "[84,  3600] loss: 6.286701\n",
      "..........\n",
      "[84,  3700] loss: 6.260726\n",
      "..........\n",
      "[84,  3800] loss: 6.237466\n",
      "..........\n",
      "[84,  3900] loss: 6.235392\n",
      "..........\n",
      "[84,  4000] loss: 6.313188\n",
      "total average loss : 0.196\n",
      "== epoch 83 == train acc : 0.1008\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.125\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.062\n",
      "step : 1000 / 1563 acc : 0.075\n",
      "step : 1200 / 1563 acc : 0.068\n",
      "step : 1400 / 1563 acc : 0.067\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[85,   100] loss: 6.223435\n",
      "..........\n",
      "[85,   200] loss: 6.273677\n",
      "..........\n",
      "[85,   300] loss: 6.245353\n",
      "..........\n",
      "[85,   400] loss: 6.280763\n",
      "..........\n",
      "[85,   500] loss: 6.316776\n",
      "..........\n",
      "[85,   600] loss: 6.255815\n",
      "..........\n",
      "[85,   700] loss: 6.320827\n",
      "..........\n",
      "[85,   800] loss: 6.187992\n",
      "..........\n",
      "[85,   900] loss: 6.324921\n",
      "..........\n",
      "[85,  1000] loss: 6.356218\n",
      "..........\n",
      "[85,  1100] loss: 6.215372\n",
      "..........\n",
      "[85,  1200] loss: 6.317473\n",
      "..........\n",
      "[85,  1300] loss: 6.174968\n",
      "..........\n",
      "[85,  1400] loss: 6.310470\n",
      "..........\n",
      "[85,  1500] loss: 6.354114\n",
      "..........\n",
      "[85,  1600] loss: 6.243527\n",
      "..........\n",
      "[85,  1700] loss: 6.306165\n",
      "..........\n",
      "[85,  1800] loss: 6.192652\n",
      "..........\n",
      "[85,  1900] loss: 6.293207\n",
      "..........\n",
      "[85,  2000] loss: 6.335831\n",
      "..........\n",
      "[85,  2100] loss: 6.245497\n",
      "..........\n",
      "[85,  2200] loss: 6.306656\n",
      "..........\n",
      "[85,  2300] loss: 6.199504\n",
      "..........\n",
      "[85,  2400] loss: 6.243931\n",
      "..........\n",
      "[85,  2500] loss: 6.381864\n",
      "..........\n",
      "[85,  2600] loss: 6.247693\n",
      "..........\n",
      "[85,  2700] loss: 6.277682\n",
      "..........\n",
      "[85,  2800] loss: 6.188767\n",
      "..........\n",
      "[85,  2900] loss: 6.258658\n",
      "..........\n",
      "[85,  3000] loss: 6.389977\n",
      "..........\n",
      "[85,  3100] loss: 6.278007\n",
      "..........\n",
      "[85,  3200] loss: 6.274150\n",
      "..........\n",
      "[85,  3300] loss: 6.210840\n",
      "..........\n",
      "[85,  3400] loss: 6.259360\n",
      "..........\n",
      "[85,  3500] loss: 6.355766\n",
      "..........\n",
      "[85,  3600] loss: 6.292186\n",
      "..........\n",
      "[85,  3700] loss: 6.256754\n",
      "..........\n",
      "[85,  3800] loss: 6.239307\n",
      "..........\n",
      "[85,  3900] loss: 6.231455\n",
      "..........\n",
      "[85,  4000] loss: 6.340462\n",
      "total average loss : 0.196\n",
      "== epoch 84 == train acc : 0.1016\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.125\n",
      "step : 400 / 1563 acc : 0.102\n",
      "step : 600 / 1563 acc : 0.099\n",
      "step : 800 / 1563 acc : 0.094\n",
      "step : 1000 / 1563 acc : 0.087\n",
      "step : 1200 / 1563 acc : 0.081\n",
      "step : 1400 / 1563 acc : 0.076\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[86,   100] loss: 6.223375\n",
      "..........\n",
      "[86,   200] loss: 6.264219\n",
      "..........\n",
      "[86,   300] loss: 6.221180\n",
      "..........\n",
      "[86,   400] loss: 6.322396\n",
      "..........\n",
      "[86,   500] loss: 6.281968\n",
      "..........\n",
      "[86,   600] loss: 6.239095\n",
      "..........\n",
      "[86,   700] loss: 6.291836\n",
      "..........\n",
      "[86,   800] loss: 6.207818\n",
      "..........\n",
      "[86,   900] loss: 6.245882\n",
      "..........\n",
      "[86,  1000] loss: 6.374480\n",
      "..........\n",
      "[86,  1100] loss: 6.258146\n",
      "..........\n",
      "[86,  1200] loss: 6.306097\n",
      "..........\n",
      "[86,  1300] loss: 6.209089\n",
      "..........\n",
      "[86,  1400] loss: 6.275083\n",
      "..........\n",
      "[86,  1500] loss: 6.365851\n",
      "..........\n",
      "[86,  1600] loss: 6.244394\n",
      "..........\n",
      "[86,  1700] loss: 6.320791\n",
      "..........\n",
      "[86,  1800] loss: 6.153456\n",
      "..........\n",
      "[86,  1900] loss: 6.287634\n",
      "..........\n",
      "[86,  2000] loss: 6.369610\n",
      "..........\n",
      "[86,  2100] loss: 6.253828\n",
      "..........\n",
      "[86,  2200] loss: 6.277442\n",
      "..........\n",
      "[86,  2300] loss: 6.206011\n",
      "..........\n",
      "[86,  2400] loss: 6.253678\n",
      "..........\n",
      "[86,  2500] loss: 6.356500\n",
      "..........\n",
      "[86,  2600] loss: 6.264205\n",
      "..........\n",
      "[86,  2700] loss: 6.259822\n",
      "..........\n",
      "[86,  2800] loss: 6.260406\n",
      "..........\n",
      "[86,  2900] loss: 6.272729\n",
      "..........\n",
      "[86,  3000] loss: 6.360088\n",
      "..........\n",
      "[86,  3100] loss: 6.280578\n",
      "..........\n",
      "[86,  3200] loss: 6.277657\n",
      "..........\n",
      "[86,  3300] loss: 6.259199\n",
      "..........\n",
      "[86,  3400] loss: 6.308275\n",
      "..........\n",
      "[86,  3500] loss: 6.313191\n",
      "..........\n",
      "[86,  3600] loss: 6.259481\n",
      "..........\n",
      "[86,  3700] loss: 6.252458\n",
      "..........\n",
      "[86,  3800] loss: 6.208529\n",
      "..........\n",
      "[86,  3900] loss: 6.260719\n",
      "..........\n",
      "[86,  4000] loss: 6.328179\n",
      "total average loss : 0.196\n",
      "== epoch 85 == train acc : 0.1000\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.078\n",
      "step : 400 / 1563 acc : 0.086\n",
      "step : 600 / 1563 acc : 0.073\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.084\n",
      "step : 1200 / 1563 acc : 0.078\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[87,   100] loss: 6.288187\n",
      "..........\n",
      "[87,   200] loss: 6.251088\n",
      "..........\n",
      "[87,   300] loss: 6.234507\n",
      "..........\n",
      "[87,   400] loss: 6.320521\n",
      "..........\n",
      "[87,   500] loss: 6.311276\n",
      "..........\n",
      "[87,   600] loss: 6.237655\n",
      "..........\n",
      "[87,   700] loss: 6.289944\n",
      "..........\n",
      "[87,   800] loss: 6.229627\n",
      "..........\n",
      "[87,   900] loss: 6.303940\n",
      "..........\n",
      "[87,  1000] loss: 6.319272\n",
      "..........\n",
      "[87,  1100] loss: 6.232103\n",
      "..........\n",
      "[87,  1200] loss: 6.291703\n",
      "..........\n",
      "[87,  1300] loss: 6.221700\n",
      "..........\n",
      "[87,  1400] loss: 6.295767\n",
      "..........\n",
      "[87,  1500] loss: 6.336843\n",
      "..........\n",
      "[87,  1600] loss: 6.263600\n",
      "..........\n",
      "[87,  1700] loss: 6.276684\n",
      "..........\n",
      "[87,  1800] loss: 6.172735\n",
      "..........\n",
      "[87,  1900] loss: 6.280631\n",
      "..........\n",
      "[87,  2000] loss: 6.350946\n",
      "..........\n",
      "[87,  2100] loss: 6.221953\n",
      "..........\n",
      "[87,  2200] loss: 6.277016\n",
      "..........\n",
      "[87,  2300] loss: 6.210602\n",
      "..........\n",
      "[87,  2400] loss: 6.268013\n",
      "..........\n",
      "[87,  2500] loss: 6.404769\n",
      "..........\n",
      "[87,  2600] loss: 6.282483\n",
      "..........\n",
      "[87,  2700] loss: 6.294801\n",
      "..........\n",
      "[87,  2800] loss: 6.203664\n",
      "..........\n",
      "[87,  2900] loss: 6.233321\n",
      "..........\n",
      "[87,  3000] loss: 6.326552\n",
      "..........\n",
      "[87,  3100] loss: 6.275129\n",
      "..........\n",
      "[87,  3200] loss: 6.282994\n",
      "..........\n",
      "[87,  3300] loss: 6.193944\n",
      "..........\n",
      "[87,  3400] loss: 6.252440\n",
      "..........\n",
      "[87,  3500] loss: 6.333120\n",
      "..........\n",
      "[87,  3600] loss: 6.287388\n",
      "..........\n",
      "[87,  3700] loss: 6.247394\n",
      "..........\n",
      "[87,  3800] loss: 6.238374\n",
      "..........\n",
      "[87,  3900] loss: 6.260804\n",
      "..........\n",
      "[87,  4000] loss: 6.283113\n",
      "total average loss : 0.196\n",
      "== epoch 86 == train acc : 0.1039\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.094\n",
      "step : 400 / 1563 acc : 0.078\n",
      "step : 600 / 1563 acc : 0.089\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.070\n",
      "step : 1400 / 1563 acc : 0.069\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[88,   100] loss: 6.239349\n",
      "..........\n",
      "[88,   200] loss: 6.279263\n",
      "..........\n",
      "[88,   300] loss: 6.237003\n",
      "..........\n",
      "[88,   400] loss: 6.294940\n",
      "..........\n",
      "[88,   500] loss: 6.289883\n",
      "..........\n",
      "[88,   600] loss: 6.247798\n",
      "..........\n",
      "[88,   700] loss: 6.283605\n",
      "..........\n",
      "[88,   800] loss: 6.199692\n",
      "..........\n",
      "[88,   900] loss: 6.266423\n",
      "..........\n",
      "[88,  1000] loss: 6.313058\n",
      "..........\n",
      "[88,  1100] loss: 6.239753\n",
      "..........\n",
      "[88,  1200] loss: 6.319254\n",
      "..........\n",
      "[88,  1300] loss: 6.217436\n",
      "..........\n",
      "[88,  1400] loss: 6.290650\n",
      "..........\n",
      "[88,  1500] loss: 6.324289\n",
      "..........\n",
      "[88,  1600] loss: 6.237503\n",
      "..........\n",
      "[88,  1700] loss: 6.265842\n",
      "..........\n",
      "[88,  1800] loss: 6.238027\n",
      "..........\n",
      "[88,  1900] loss: 6.267563\n",
      "..........\n",
      "[88,  2000] loss: 6.373153\n",
      "..........\n",
      "[88,  2100] loss: 6.275075\n",
      "..........\n",
      "[88,  2200] loss: 6.316906\n",
      "..........\n",
      "[88,  2300] loss: 6.202754\n",
      "..........\n",
      "[88,  2400] loss: 6.239933\n",
      "..........\n",
      "[88,  2500] loss: 6.362185\n",
      "..........\n",
      "[88,  2600] loss: 6.265708\n",
      "..........\n",
      "[88,  2700] loss: 6.287837\n",
      "..........\n",
      "[88,  2800] loss: 6.219799\n",
      "..........\n",
      "[88,  2900] loss: 6.260200\n",
      "..........\n",
      "[88,  3000] loss: 6.372468\n",
      "..........\n",
      "[88,  3100] loss: 6.234741\n",
      "..........\n",
      "[88,  3200] loss: 6.263272\n",
      "..........\n",
      "[88,  3300] loss: 6.219300\n",
      "..........\n",
      "[88,  3400] loss: 6.270677\n",
      "..........\n",
      "[88,  3500] loss: 6.335626\n",
      "..........\n",
      "[88,  3600] loss: 6.286267\n",
      "..........\n",
      "[88,  3700] loss: 6.239235\n",
      "..........\n",
      "[88,  3800] loss: 6.225211\n",
      "..........\n",
      "[88,  3900] loss: 6.283406\n",
      "..........\n",
      "[88,  4000] loss: 6.328393\n",
      "total average loss : 0.196\n",
      "== epoch 87 == train acc : 0.0992\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.094\n",
      "step : 600 / 1563 acc : 0.083\n",
      "step : 800 / 1563 acc : 0.074\n",
      "step : 1000 / 1563 acc : 0.066\n",
      "step : 1200 / 1563 acc : 0.065\n",
      "step : 1400 / 1563 acc : 0.062\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[89,   100] loss: 6.285281\n",
      "..........\n",
      "[89,   200] loss: 6.308844\n",
      "..........\n",
      "[89,   300] loss: 6.220123\n",
      "..........\n",
      "[89,   400] loss: 6.300149\n",
      "..........\n",
      "[89,   500] loss: 6.319727\n",
      "..........\n",
      "[89,   600] loss: 6.247385\n",
      "..........\n",
      "[89,   700] loss: 6.285135\n",
      "..........\n",
      "[89,   800] loss: 6.213206\n",
      "..........\n",
      "[89,   900] loss: 6.253458\n",
      "..........\n",
      "[89,  1000] loss: 6.317014\n",
      "..........\n",
      "[89,  1100] loss: 6.221893\n",
      "..........\n",
      "[89,  1200] loss: 6.341414\n",
      "..........\n",
      "[89,  1300] loss: 6.246704\n",
      "..........\n",
      "[89,  1400] loss: 6.300664\n",
      "..........\n",
      "[89,  1500] loss: 6.329035\n",
      "..........\n",
      "[89,  1600] loss: 6.231592\n",
      "..........\n",
      "[89,  1700] loss: 6.335421\n",
      "..........\n",
      "[89,  1800] loss: 6.223636\n",
      "..........\n",
      "[89,  1900] loss: 6.272789\n",
      "..........\n",
      "[89,  2000] loss: 6.315727\n",
      "..........\n",
      "[89,  2100] loss: 6.287305\n",
      "..........\n",
      "[89,  2200] loss: 6.263147\n",
      "..........\n",
      "[89,  2300] loss: 6.174399\n",
      "..........\n",
      "[89,  2400] loss: 6.264238\n",
      "..........\n",
      "[89,  2500] loss: 6.359943\n",
      "..........\n",
      "[89,  2600] loss: 6.263355\n",
      "..........\n",
      "[89,  2700] loss: 6.296753\n",
      "..........\n",
      "[89,  2800] loss: 6.198715\n",
      "..........\n",
      "[89,  2900] loss: 6.241599\n",
      "..........\n",
      "[89,  3000] loss: 6.396423\n",
      "..........\n",
      "[89,  3100] loss: 6.240980\n",
      "..........\n",
      "[89,  3200] loss: 6.222110\n",
      "..........\n",
      "[89,  3300] loss: 6.193159\n",
      "..........\n",
      "[89,  3400] loss: 6.280455\n",
      "..........\n",
      "[89,  3500] loss: 6.329740\n",
      "..........\n",
      "[89,  3600] loss: 6.281615\n",
      "..........\n",
      "[89,  3700] loss: 6.240576\n",
      "..........\n",
      "[89,  3800] loss: 6.254980\n",
      "..........\n",
      "[89,  3900] loss: 6.250732\n",
      "..........\n",
      "[89,  4000] loss: 6.329404\n",
      "total average loss : 0.196\n",
      "== epoch 88 == train acc : 0.1047\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.062\n",
      "step : 400 / 1563 acc : 0.039\n",
      "step : 600 / 1563 acc : 0.062\n",
      "step : 800 / 1563 acc : 0.062\n",
      "step : 1000 / 1563 acc : 0.072\n",
      "step : 1200 / 1563 acc : 0.073\n",
      "step : 1400 / 1563 acc : 0.074\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n",
      "..........\n",
      "[90,   100] loss: 6.262750\n",
      "..........\n",
      "[90,   200] loss: 6.270254\n",
      "..........\n",
      "[90,   300] loss: 6.258833\n",
      "..........\n",
      "[90,   400] loss: 6.295771\n",
      "..........\n",
      "[90,   500] loss: 6.310756\n",
      "..........\n",
      "[90,   600] loss: 6.237204\n",
      "..........\n",
      "[90,   700] loss: 6.317228\n",
      "..........\n",
      "[90,   800] loss: 6.220717\n",
      "..........\n",
      "[90,   900] loss: 6.272385\n",
      "..........\n",
      "[90,  1000] loss: 6.320393\n",
      "..........\n",
      "[90,  1100] loss: 6.235024\n",
      "..........\n",
      "[90,  1200] loss: 6.293707\n",
      "..........\n",
      "[90,  1300] loss: 6.213973\n",
      "..........\n",
      "[90,  1400] loss: 6.295503\n",
      "..........\n",
      "[90,  1500] loss: 6.357089\n",
      "..........\n",
      "[90,  1600] loss: 6.237099\n",
      "..........\n",
      "[90,  1700] loss: 6.287245\n",
      "..........\n",
      "[90,  1800] loss: 6.188738\n",
      "..........\n",
      "[90,  1900] loss: 6.273501\n",
      "..........\n",
      "[90,  2000] loss: 6.337090\n",
      "..........\n",
      "[90,  2100] loss: 6.223072\n",
      "..........\n",
      "[90,  2200] loss: 6.297802\n",
      "..........\n",
      "[90,  2300] loss: 6.188165\n",
      "..........\n",
      "[90,  2400] loss: 6.245797\n",
      "..........\n",
      "[90,  2500] loss: 6.384786\n",
      "..........\n",
      "[90,  2600] loss: 6.272308\n",
      "..........\n",
      "[90,  2700] loss: 6.256443\n",
      "..........\n",
      "[90,  2800] loss: 6.185021\n",
      "..........\n",
      "[90,  2900] loss: 6.231342\n",
      "..........\n",
      "[90,  3000] loss: 6.350091\n",
      "..........\n",
      "[90,  3100] loss: 6.257770\n",
      "..........\n",
      "[90,  3200] loss: 6.253015\n",
      "..........\n",
      "[90,  3300] loss: 6.229380\n",
      "..........\n",
      "[90,  3400] loss: 6.276901\n",
      "..........\n",
      "[90,  3500] loss: 6.359919\n",
      "..........\n",
      "[90,  3600] loss: 6.285017\n",
      "..........\n",
      "[90,  3700] loss: 6.256175\n",
      "..........\n",
      "[90,  3800] loss: 6.237742\n",
      "..........\n",
      "[90,  3900] loss: 6.241746\n",
      "..........\n",
      "[90,  4000] loss: 6.357353\n",
      "total average loss : 0.196\n",
      "== epoch 89 == train acc : 0.1008\n",
      "======eval start=======\n",
      "step : 200 / 1563 acc : 0.047\n",
      "step : 400 / 1563 acc : 0.039\n",
      "step : 600 / 1563 acc : 0.036\n",
      "step : 800 / 1563 acc : 0.043\n",
      "step : 1000 / 1563 acc : 0.059\n",
      "step : 1200 / 1563 acc : 0.060\n",
      "step : 1400 / 1563 acc : 0.062\n",
      "\n",
      "Eval acc of model on imagenet : 0.0720 %, Loss : 0.1994\n",
      "======eval  end ======\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#optimizer = torch.optim.SGD(param_list,lr=0.01,weight_decay=1e-4)\n",
    "first_feature = []\n",
    "first_label = []\n",
    "offset_info = []\n",
    "f = open(log_file,\"w\")\n",
    "print(\"Submodel case\")\n",
    "f.close()\n",
    "\n",
    "target_model = header.Target_model(vgg16_bn).to(device)\n",
    "    \n",
    "tmp= header.training(f4f,target_model,original_model,\n",
    "                  train_dataloader,test_dataloader,batch_size,\n",
    "                  log_file,writer,retrain_model_path,\n",
    "                  loss_fn,optimizer,\n",
    "                  num_error,max_epoch,True)\n",
    "        # tmp : first_feature,first_label,offset_info\n",
    "first_feature.append(tmp[0])\n",
    "first_label.append(tmp[1])\n",
    "offset_info.append(tmp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a7b445e-6394-4ebe-8b6c-7b6926377bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d087fe7c-688e-4bff-9a23-565a0dbbf158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2949e+00, -7.4835e-01, -2.8660e+00, -1.3078e+00, -6.9675e-01,\n",
      "        -1.1274e+00, -7.4309e-01, -1.4525e+00, -1.0095e+00,  8.0502e-01,\n",
      "        -4.5142e-02, -1.9161e-02,  6.6789e-01,  8.0750e-01,  9.4017e-01,\n",
      "        -2.7363e-01, -2.4230e-01,  4.4678e-01,  3.5910e-01,  1.4490e+00,\n",
      "        -1.7707e+00, -1.0178e+00, -1.7609e+00,  3.6564e-01, -1.2447e+00,\n",
      "        -4.6765e-01, -7.3489e-01,  1.1904e-01, -1.0987e-01,  3.9440e-01,\n",
      "        -7.4150e-01,  4.6591e-01, -3.0082e-01, -4.9465e-01,  6.2079e-01,\n",
      "        -1.4249e+00,  3.1941e-01, -7.3470e-01,  2.0024e-01, -2.3518e-01,\n",
      "         3.5912e-01,  3.9649e-01,  1.0416e+00,  3.4948e-01,  2.9197e-01,\n",
      "         8.6047e-01,  9.3698e-01,  4.3177e-01,  9.6084e-02, -1.5583e+00,\n",
      "        -1.2716e+00, -2.3165e+00,  1.1884e+00,  1.5262e+00,  1.5222e+00,\n",
      "         1.4590e-01,  6.0542e-01,  1.8448e-02,  3.0675e-01,  9.8754e-01,\n",
      "         1.7141e+00,  1.5020e+00, -6.0426e-03,  2.1591e+00,  8.0917e-01,\n",
      "         2.8540e-01,  1.3659e+00,  1.7023e+00,  9.6647e-01,  7.6580e-01,\n",
      "        -1.1198e+00,  1.5748e+00, -1.2038e+00, -4.0572e-01, -1.0116e+00,\n",
      "         6.0285e-02,  2.9761e+00,  1.0750e+00,  2.4118e+00,  9.6896e-01,\n",
      "        -1.5704e+00, -2.5138e+00, -2.4358e-02, -1.0614e+00, -8.3989e-01,\n",
      "         7.5715e-01, -1.1535e+00,  3.3679e-01, -6.4302e-01, -1.8979e+00,\n",
      "        -1.1957e+00, -1.2492e+00, -5.6246e-01, -1.2447e+00,  7.5295e-01,\n",
      "        -2.0348e+00, -1.0266e+00, -1.4911e+00, -1.7105e+00, -1.1446e-01,\n",
      "        -1.3417e+00, -1.6173e+00,  8.2946e-01,  1.7934e+00,  2.1148e+00,\n",
      "         1.0552e+00,  1.7062e+00, -1.9979e+00, -2.0963e+00, -1.5024e+00,\n",
      "        -1.1203e+00, -7.7001e-01,  7.4687e-01,  3.8242e-01,  9.8714e-01,\n",
      "        -1.8188e+00, -1.4756e+00, -1.1497e+00, -4.8795e-01, -8.0776e-01,\n",
      "        -5.2672e-01, -1.6549e+00, -1.5798e+00, -1.9480e+00,  1.5544e-01,\n",
      "         1.0710e+00, -8.5044e-01, -2.0972e+00, -2.2312e+00, -1.0037e+00,\n",
      "        -1.2554e+00, -1.7256e+00, -1.9654e+00, -1.8100e+00, -1.2874e+00,\n",
      "        -2.0754e+00, -2.2491e+00, -1.2680e+00, -9.9411e-01, -1.4030e+00,\n",
      "         1.7525e-02, -1.4762e+00, -1.3793e+00, -2.0564e+00, -7.9425e-01,\n",
      "        -1.0275e+00, -2.4028e+00, -3.1886e+00, -2.9257e+00, -2.5228e+00,\n",
      "        -4.9217e-01,  2.3637e+00, -1.8909e+00, -9.3047e-01,  2.3314e-01,\n",
      "        -5.4752e-01,  5.9655e-02,  2.2675e-01,  2.9763e-01,  6.6971e-01,\n",
      "        -1.4618e+00,  6.0350e-01,  1.3263e+00,  5.1660e-02,  1.2951e+00,\n",
      "        -1.8486e-01, -7.7261e-02, -1.0068e+00,  1.0784e+00, -6.8284e-01,\n",
      "        -5.7541e-01,  2.1194e+00,  5.5473e-01,  3.7084e-01,  1.3482e+00,\n",
      "        -1.7513e+00, -3.2669e-01,  7.7348e-02,  1.0124e+00,  2.0773e-01,\n",
      "         4.2192e-01, -4.4889e-01,  1.4830e+00, -5.3851e-01,  8.0305e-01,\n",
      "         1.0753e+00,  1.0855e+00,  3.4012e-01, -6.5406e-01,  1.3751e-02,\n",
      "        -2.0141e+00,  9.6676e-01,  3.7002e-01, -7.6139e-01, -2.3290e+00,\n",
      "         1.0811e+00,  3.2045e-01, -1.1138e+00, -8.9094e-01, -5.0290e-02,\n",
      "        -2.1520e+00, -1.9015e-01,  1.4058e-01,  2.7566e-01, -4.1598e-01,\n",
      "        -3.8442e-01, -2.5997e-01,  7.3633e-01,  1.3445e+00,  1.3178e+00,\n",
      "        -4.7008e-01,  4.9074e-01,  7.3485e-02, -4.4673e-02, -9.7702e-01,\n",
      "         8.7298e-01, -1.6441e+00, -3.4720e-01, -2.4110e-01,  2.9838e-01,\n",
      "        -1.5962e+00, -6.4198e-01, -8.1538e-01,  1.8567e+00,  6.7753e-01,\n",
      "         8.6158e-01, -1.4042e+00,  1.4045e+00, -2.2336e+00, -4.3481e-01,\n",
      "         8.6707e-01,  3.8607e-01,  5.0202e-01, -1.5751e+00,  6.3910e-01,\n",
      "         1.2207e+00,  6.9294e-01,  1.5847e+00,  5.7114e-01,  2.5474e-02,\n",
      "         9.8729e-01,  6.6243e-01,  2.3989e-01, -2.2247e-02, -1.8323e+00,\n",
      "         4.2749e-01, -3.7855e-01, -1.0075e+00,  7.6592e-01,  7.6770e-01,\n",
      "         1.1941e+00,  3.1680e-01, -1.1671e+00,  1.3725e+00,  9.4875e-01,\n",
      "        -1.8363e+00, -1.1404e+00, -6.7499e-01, -2.0353e-02,  1.3117e+00,\n",
      "         2.4563e-01, -1.1957e-01, -1.2394e-01,  1.8958e+00,  7.1416e-01,\n",
      "         9.6713e-01, -3.9026e-01, -8.3652e-01, -4.1689e-01,  1.4824e-01,\n",
      "        -5.5736e-01,  4.8534e-01,  8.1310e-01,  1.3652e+00, -1.0098e-01,\n",
      "         1.5691e-02,  1.5533e+00,  9.8362e-01,  8.4083e-01,  2.0165e-01,\n",
      "         1.4084e+00,  2.5648e+00,  1.9945e+00,  1.1492e+00,  2.6783e+00,\n",
      "         3.3077e+00,  9.3571e-01,  1.2855e+00,  6.2692e-03, -7.2551e-01,\n",
      "        -2.2455e-01, -8.4184e-02, -8.3161e-01,  2.2082e+00, -1.0274e+00,\n",
      "        -6.4791e-01, -1.2088e+00, -2.5149e-01,  2.9640e+00,  3.1788e+00,\n",
      "         8.6688e-02,  9.9118e-01,  4.7615e-01,  5.8321e-01,  8.2558e-02,\n",
      "         1.7716e+00,  2.3217e+00,  8.6911e-02,  1.5112e-01,  2.6083e-01,\n",
      "         1.3412e+00,  2.3195e-01,  6.1505e-01,  1.2830e+00,  1.8430e+00,\n",
      "         1.1785e+00,  2.9466e-01,  7.6772e-02, -5.0913e-01,  5.5477e-02,\n",
      "         4.0840e-01, -1.2741e+00, -1.2110e+00, -1.2211e+00, -2.9760e-01,\n",
      "        -1.3313e+00, -4.5241e-01,  6.8223e-01,  2.7319e-02, -1.5005e+00,\n",
      "         1.1919e+00,  4.1669e-01, -3.4532e-01,  2.0649e+00,  9.4291e-01,\n",
      "         5.0064e-01,  7.6321e-01,  6.4930e-01,  1.1850e+00, -1.4798e+00,\n",
      "         2.4742e-01,  3.7141e-01, -3.4886e-01, -9.1172e-02, -1.4145e+00,\n",
      "        -1.3624e+00, -8.7948e-01, -1.4029e+00, -1.6576e+00, -1.1905e+00,\n",
      "        -7.2903e-01, -8.0043e-01, -3.4744e-01,  6.5944e-01,  1.5847e-01,\n",
      "        -2.6979e-01,  2.0749e+00,  5.1375e-01,  1.9170e+00,  2.0332e+00,\n",
      "         9.4979e-01,  1.9379e+00,  2.1790e+00,  1.1195e+00, -1.2854e-01,\n",
      "        -1.1113e-02, -2.2107e-01, -8.4693e-04,  3.8301e-01, -5.0575e-01,\n",
      "         9.1209e-01,  6.9090e-02,  2.0330e+00,  1.5506e+00,  1.7485e+00,\n",
      "        -4.7484e-01,  6.1061e-01,  1.2955e+00,  9.0091e-01, -3.2881e-01,\n",
      "         6.3722e-01,  8.8596e-01,  1.5457e+00,  1.2658e+00,  5.9873e-01,\n",
      "        -1.1197e+00, -1.7083e+00,  1.5608e-01,  2.4932e-03, -1.6412e+00,\n",
      "        -3.3317e-02, -1.0178e+00, -2.3371e+00, -2.3217e+00, -1.5659e+00,\n",
      "        -9.7095e-01, -2.1703e+00,  2.5494e-01, -1.3115e-02,  9.1909e-02,\n",
      "        -1.6504e+00,  1.8822e+00,  1.2544e+00, -2.4965e+00, -1.7801e+00,\n",
      "        -1.1466e+00, -7.2944e-01, -6.8099e-01, -2.4810e+00,  3.6838e-01,\n",
      "        -1.1024e-01, -1.2166e+00,  1.1173e+00,  6.9604e-01,  6.5191e-02,\n",
      "        -1.0296e+00,  4.8125e-01, -7.6205e-01,  1.6371e+00,  4.3915e+00,\n",
      "         1.0779e+00,  2.4214e-02, -1.0215e-01,  1.8969e-01,  3.4168e-02,\n",
      "        -1.3109e+00, -3.0477e-01, -3.1327e-01,  4.8453e-01,  1.2011e+00,\n",
      "        -7.1061e-01, -1.1750e+00,  4.6490e-01, -8.4077e-01,  1.1352e+00,\n",
      "        -1.5410e-01, -4.0523e-01, -3.2134e-01,  1.8162e-01, -6.5587e-01,\n",
      "         9.7785e-01,  8.0765e-01, -5.9311e-01,  7.6379e-01, -1.1841e+00,\n",
      "        -9.8886e-01,  1.6581e+00,  1.8023e+00,  1.1255e+00, -1.6809e+00,\n",
      "        -2.5041e+00, -2.9784e-01, -5.6713e-02, -3.3795e-01, -9.4124e-01,\n",
      "         2.3698e+00, -5.2220e-01,  8.2829e-01, -4.1237e-01, -9.0366e-01,\n",
      "        -5.4280e-01, -6.9078e-01,  2.1070e-01,  2.7109e-01,  1.8903e+00,\n",
      "        -4.9895e-01, -1.9105e+00, -1.6341e+00,  9.7614e-02, -3.0989e-01,\n",
      "         1.5133e+00, -7.6797e-01, -1.4053e+00,  1.6225e+00,  4.9824e-01,\n",
      "         2.1825e+00, -1.7639e+00,  3.4044e-01,  1.9215e+00, -3.8927e-01,\n",
      "        -9.0173e-01,  9.8184e-01,  3.4390e-01, -1.1548e+00, -2.0110e+00,\n",
      "         8.4419e-01,  2.3121e-01,  2.8153e+00,  4.8684e-01,  2.3442e-01,\n",
      "         5.7478e-02, -4.8504e-01,  1.0831e+00, -1.7980e+00,  1.8149e-02,\n",
      "        -1.6510e+00,  1.2088e-01, -7.9014e-01,  4.4350e-02,  2.8119e-01,\n",
      "        -3.9133e-01, -1.1672e-01,  8.2855e-01,  8.5157e-02,  2.4686e+00,\n",
      "         9.7555e-01, -5.6162e-01,  4.9611e-01,  2.5667e+00, -1.9745e+00,\n",
      "        -1.6424e+00, -7.1645e-01,  1.4464e+00,  1.6149e-01,  9.6574e-01,\n",
      "         2.1139e+00, -4.4877e-01, -7.5636e-01, -7.7047e-02,  2.8181e-01,\n",
      "        -8.4339e-01, -6.2216e-01, -6.5940e-01,  1.1713e+00, -8.4508e-01,\n",
      "        -1.8964e+00,  5.8038e-01,  8.3855e-01,  4.1211e-01,  5.5808e-01,\n",
      "         8.1347e-01,  1.0903e+00,  4.2107e-01,  6.4872e-01, -1.8961e+00,\n",
      "        -9.1426e-01, -1.5712e+00, -1.9103e+00, -1.5460e+00,  3.4843e+00,\n",
      "        -1.7945e+00,  5.4384e-01,  1.6384e+00,  1.9563e+00, -7.1993e-01,\n",
      "         2.1587e-01,  8.5394e-01, -3.3991e+00, -8.9772e-01,  7.1420e-01,\n",
      "        -3.1149e-01,  1.8605e+00,  2.4529e-01,  2.7182e-01, -2.3801e+00,\n",
      "        -1.3700e+00,  6.0337e-01, -9.3824e-02,  6.5141e-01,  9.6720e-01,\n",
      "        -1.1539e+00,  1.6323e-01, -8.6987e-01,  1.5525e+00, -5.9371e-01,\n",
      "        -9.4882e-01, -7.1716e-01, -6.0965e-01,  9.0823e-01, -1.7054e+00,\n",
      "         1.6394e-01, -3.2645e-01,  4.4535e-01,  7.7207e-01,  1.2561e+00,\n",
      "         2.4978e-01, -1.9888e+00, -5.7109e-01, -2.7842e+00, -3.8074e-01,\n",
      "        -1.0986e+00, -1.0777e+00, -1.6963e+00,  1.3967e+00,  6.6780e-01,\n",
      "         1.0761e+00, -4.8562e-01,  2.5852e+00,  1.1743e+00,  1.0246e+00,\n",
      "         5.2522e-01, -9.0149e-02,  4.9222e-01,  1.3607e+00,  2.8857e-01,\n",
      "        -6.6448e-01,  1.0379e+00,  8.6388e-01,  3.2891e-01,  1.1726e+00,\n",
      "         2.2094e+00, -2.0997e+00, -2.9789e-02, -1.2088e+00,  1.6245e+00,\n",
      "         2.2724e+00,  2.0285e+00, -8.4342e-01,  9.8729e-01, -3.9513e-01,\n",
      "        -3.5304e-01,  2.9584e+00, -2.0090e+00,  1.8884e+00, -5.0115e-01,\n",
      "         6.9488e-01,  1.2166e-01, -1.1005e+00,  5.5606e-01,  2.0072e+00,\n",
      "         3.2271e+00,  5.0938e-01,  3.1851e+00,  1.1390e+00, -6.5526e-01,\n",
      "        -2.7464e+00,  4.8931e+00, -8.0207e-01, -1.8233e+00,  1.2891e+00,\n",
      "         1.2451e+00,  9.8941e-01,  1.5066e+00,  2.6032e+00, -2.9774e-01,\n",
      "         8.3650e-02,  2.0123e-01, -6.4540e-01, -1.1328e+00, -9.6272e-01,\n",
      "         3.2052e-02, -2.4410e-01, -6.0964e-01,  3.5693e-01,  2.3076e+00,\n",
      "        -2.8030e+00, -6.2405e-02, -8.4051e-02, -5.5225e-01, -1.1560e-02,\n",
      "         2.7089e+00, -1.1442e+00,  7.9065e-01,  3.4161e-01, -3.8697e-01,\n",
      "        -2.1048e-01,  2.4702e-01, -1.3287e+00,  1.3922e+00, -9.1472e-01,\n",
      "        -8.9463e-01, -1.6074e+00,  1.1825e+00, -2.3223e+00,  1.0903e+00,\n",
      "         2.2545e-01,  1.3969e+00,  5.9428e-02, -1.4753e+00,  9.7471e-02,\n",
      "         3.4604e-01, -2.7249e-01, -1.0394e+00,  3.2224e+00,  4.0864e+00,\n",
      "        -1.1234e+00, -2.7304e-01,  3.6110e+00,  1.2663e+00,  5.1541e-01,\n",
      "         9.5612e-01,  2.4039e+00,  4.0260e-01,  2.2937e-01,  3.0102e-01,\n",
      "        -9.7107e-01,  9.9822e-01, -1.8685e+00, -3.6056e-01, -2.6463e+00,\n",
      "        -2.0754e+00, -9.0201e-01,  6.3190e-01, -1.1931e+00, -2.6771e+00,\n",
      "         1.4912e+00,  1.5942e+00, -4.7413e-01, -2.0379e+00,  2.8079e-01,\n",
      "         1.4288e+00, -3.9882e-01, -8.4740e-02,  1.7996e+00,  1.0676e+00,\n",
      "        -1.8424e+00, -5.0156e-01, -9.6205e-01,  4.3931e-02,  1.5776e+00,\n",
      "         2.0644e+00,  1.3962e+00, -1.1259e-01, -3.1513e-01,  1.5011e+00,\n",
      "        -1.4883e+00,  2.2560e-01, -1.1940e+00, -5.5801e-01,  8.3740e-01,\n",
      "         1.9367e+00,  8.8719e-02,  1.8532e+00, -1.0205e+00, -2.6775e+00,\n",
      "         1.0462e+00,  4.6799e-01, -1.6549e+00, -5.8376e-01, -2.8325e+00,\n",
      "        -1.0979e+00,  6.0131e-01,  6.6512e-01,  1.2921e+00, -1.4166e+00,\n",
      "        -3.0428e-01,  5.0202e-01,  8.2295e-01,  1.0129e+00, -2.7229e-01,\n",
      "         7.7363e-01,  1.5847e+00, -3.4215e-01,  4.0559e-01, -1.2971e+00,\n",
      "         5.1875e-01,  8.4083e-01,  2.5555e-01,  1.1195e+00,  1.1316e+00,\n",
      "         7.4051e-01,  2.3498e-01,  7.5700e-02,  8.7452e-01,  8.2521e-01,\n",
      "        -1.4042e+00, -4.2338e-01, -7.1617e-01,  2.2929e-01,  1.0229e+00,\n",
      "         4.0347e-02,  3.9235e+00, -7.1051e-01,  2.5973e+00,  1.6792e+00,\n",
      "         9.0375e-01, -1.8529e+00,  2.0197e+00, -3.9606e-01,  2.1070e+00,\n",
      "         1.1627e+00, -6.2193e-01,  1.7900e+00,  2.6294e+00,  1.9759e+00,\n",
      "        -1.3199e+00,  4.8954e-01,  5.7347e-01,  1.4583e+00, -1.6530e+00,\n",
      "        -1.5198e+00, -1.4597e+00,  8.4490e-01,  3.5432e+00,  2.7098e+00,\n",
      "         1.8892e+00,  1.2352e+00, -3.0775e-01, -2.9367e-01,  3.9732e-01,\n",
      "        -1.0880e+00,  2.9502e-02,  1.4277e-01, -2.3091e-01,  5.5121e-01,\n",
      "        -1.1999e+00,  1.1111e+00,  8.6176e-01, -3.5642e-03,  1.3204e+00,\n",
      "        -1.3853e+00, -3.4200e-02, -1.5940e+00, -1.5336e+00,  8.8788e-01,\n",
      "         4.4613e-01,  1.4039e+00, -6.2037e-01,  6.0099e-01,  3.4433e-01,\n",
      "         1.0562e+00,  2.0625e+00, -1.6948e+00,  7.9614e-01, -2.0968e+00,\n",
      "        -8.0471e-01,  5.5047e-01, -1.8415e+00,  1.7997e+00, -1.9556e-01,\n",
      "        -3.7081e+00, -2.6625e+00, -1.7309e+00,  5.6180e-01,  4.5033e-01,\n",
      "        -2.5753e-01,  8.6644e-01,  4.4889e-01,  5.0260e-01, -1.9969e+00,\n",
      "        -7.9499e-02,  1.0626e+00, -2.5675e+00, -2.3860e+00,  3.6346e-01,\n",
      "         4.7478e-02,  1.5610e+00,  1.8305e+00,  1.3167e+00, -9.8384e-01,\n",
      "        -8.4447e-01,  2.3670e-01, -2.7049e-01,  1.6209e+00,  1.3299e+00,\n",
      "         1.9795e+00,  2.0299e+00, -8.1128e-01,  5.3677e-01,  1.3126e+00,\n",
      "         1.6848e+00,  1.7144e+00,  1.7521e+00, -1.1056e+00, -2.4774e-01,\n",
      "         3.6538e+00, -2.8366e+00, -8.2153e-01,  1.2711e+00,  1.6094e+00,\n",
      "        -1.5824e+00,  4.5870e-01,  1.2853e+00, -1.2830e-01, -1.0422e+00,\n",
      "        -1.9748e+00,  1.2068e-01, -5.1745e-01,  9.1725e-01, -1.1426e+00,\n",
      "         7.4517e-01, -1.9057e+00,  1.8468e+00, -8.5169e-01, -2.5634e+00,\n",
      "        -7.7155e-01,  9.7268e-01, -8.1436e-01,  4.9769e-01, -2.1392e-02,\n",
      "         5.4354e-01, -6.6138e-01,  1.7866e+00,  1.1215e+00, -9.5892e-02,\n",
      "        -4.7874e-02, -1.2591e+00, -1.5721e+00, -1.1929e+00,  1.3344e+00,\n",
      "        -2.0003e+00,  1.3739e-01,  5.2716e-01,  2.6298e+00,  1.9345e-02,\n",
      "        -2.1531e+00,  1.0556e+00, -8.8885e-01,  1.8017e+00,  8.9669e-01,\n",
      "        -1.3866e+00, -8.1994e-01,  7.0045e-01, -6.1119e-02,  3.1711e+00,\n",
      "         1.3395e+00,  4.9484e-03,  2.5288e-01, -2.4004e-01, -9.8817e-01,\n",
      "         1.1155e+00,  9.0312e-01, -5.3667e-01, -1.3451e+00, -3.1234e+00,\n",
      "        -1.7613e+00,  8.4999e-02, -1.4245e-01,  1.2969e+00, -1.1701e-01,\n",
      "        -4.7503e-01,  1.7145e+00, -3.6930e-01, -1.6906e+00, -8.1884e-01,\n",
      "        -1.0687e+00, -1.9480e+00, -5.1949e-01, -5.4896e-01,  1.2317e+00,\n",
      "        -9.3525e-01, -1.1906e+00, -2.1931e-01, -1.9791e+00, -1.2978e+00,\n",
      "        -2.1869e+00, -2.3390e+00,  1.7092e-01, -1.8013e+00,  1.3784e-01,\n",
      "        -1.9892e+00, -1.7601e+00, -5.3568e-01, -4.4742e-01, -2.9368e-01,\n",
      "        -1.1127e+00, -1.3621e+00,  3.2197e-01,  6.9892e-01,  9.5660e-02,\n",
      "         4.7850e-01, -5.9495e-01, -2.5938e-01,  5.0991e-01,  3.6372e-01,\n",
      "        -3.5070e-01, -1.0022e+00,  7.8836e-01,  3.5848e-01, -1.6943e+00,\n",
      "        -1.7255e-01, -3.6842e-02, -1.4916e+00,  9.0578e-01, -2.1193e-01,\n",
      "        -2.8244e-01,  9.2462e-01,  9.5429e-01,  1.7777e+00,  6.2004e-01,\n",
      "        -1.9584e+00,  1.0798e+00, -1.1670e+00, -1.5579e+00, -1.3959e+00,\n",
      "         2.5502e-01, -1.9339e+00,  6.1252e-01,  8.4572e-01, -2.4323e+00,\n",
      "        -1.8260e+00,  5.5335e-01, -9.8583e-01, -1.5458e+00, -8.2196e-01,\n",
      "        -1.0024e+00, -2.5831e+00,  2.0357e+00,  8.7273e-01, -1.2556e+00,\n",
      "         5.8264e-01, -2.0158e+00, -1.3773e+00, -2.4440e+00, -1.3392e+00,\n",
      "        -6.5464e-01, -5.8261e-01,  3.5509e-01,  1.0910e+00,  1.3650e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-1.1312, -1.0539, -3.0464, -0.9692, -0.8575, -0.6410, -0.6204, -2.0320,\n",
      "        -1.0311,  0.4478,  0.2684, -0.4308,  1.2670,  1.0063,  1.7205, -0.2186,\n",
      "        -0.3767,  0.9071,  0.8683,  1.6304, -2.0398, -1.2978, -2.5092,  0.3198,\n",
      "        -2.2454, -0.6214, -0.4546,  0.1490,  0.3556,  1.0148, -0.8228, -0.1480,\n",
      "         0.0370, -0.0889,  0.7137, -1.3390,  0.3578, -0.7655, -0.0472, -0.4819,\n",
      "         0.1541,  1.3271,  1.4339,  0.0655,  0.5530,  1.2442,  0.7667,  0.1970,\n",
      "        -0.1356, -1.7719, -1.3369, -3.0756,  1.6597,  1.8265,  2.4128, -0.5138,\n",
      "         0.9813,  0.1886,  0.1141,  0.5295,  1.3842,  1.4383,  0.2400,  1.9825,\n",
      "         0.2206,  0.2149,  1.6315,  1.9975,  1.5627,  0.1503, -1.3315,  1.4890,\n",
      "        -1.2795, -0.8842, -1.3154,  0.3549,  3.4285,  1.1652,  1.9913,  0.3397,\n",
      "        -1.6705, -2.1806,  0.3112, -1.0243, -1.2577,  0.8938, -1.1928,  0.4453,\n",
      "        -1.0542, -1.9359, -1.5854, -0.9449, -0.8885, -1.1199, -0.3405, -2.0381,\n",
      "        -1.5733, -2.0660, -1.9764, -0.2921, -2.3628, -1.9474,  0.8506,  1.2402,\n",
      "         2.2343,  0.9256,  1.3702, -2.6931, -3.0660, -0.9160, -1.3848, -1.5087,\n",
      "         0.3682, -0.8485,  0.8183, -1.7231, -2.0211, -1.8203, -0.2546, -0.7109,\n",
      "        -0.9232, -0.9256, -1.5891, -1.8218, -0.0183,  0.1760, -1.4574, -2.7601,\n",
      "        -2.5203, -1.9419, -1.8078, -1.5741, -1.9366, -1.9844, -1.2864, -2.5499,\n",
      "        -2.7609, -1.9262, -0.8340, -1.3162,  1.2071, -1.7664, -1.2586, -2.3695,\n",
      "        -0.9284, -0.3715, -2.1484, -2.9499, -2.5172, -2.2995, -0.2452,  2.4159,\n",
      "        -1.4387, -0.9923,  0.3401, -0.8034,  0.1989,  0.3726,  0.2870,  0.3854,\n",
      "        -1.5298,  0.1613,  1.0265, -0.1388,  1.2939, -0.4005, -0.3532, -1.3663,\n",
      "         0.9270, -0.6373, -1.0226,  1.6142,  0.2683,  0.4653,  1.1937, -1.7812,\n",
      "        -0.7554, -0.1800,  0.1265, -0.2175,  0.2422, -0.9625,  1.2756, -1.0026,\n",
      "         0.8472,  0.8588,  1.1263, -0.0939, -0.8093, -0.0558, -2.1695,  0.6986,\n",
      "         0.3447, -0.5552, -2.5853,  0.6016, -0.5291, -1.3052, -1.4132, -0.3221,\n",
      "        -2.2841, -0.4904,  0.0492,  0.5854, -0.4425, -0.3082, -0.3529,  1.1134,\n",
      "         1.6675,  0.8779, -0.5729,  0.2699,  0.4392, -0.1497, -1.0329,  1.4694,\n",
      "        -1.3056, -0.4231, -0.3623,  0.2933, -1.6938, -1.2146, -0.2117,  1.7427,\n",
      "         0.0910,  0.3932, -1.1021,  1.5514, -2.1481, -0.6234,  0.7971,  0.5712,\n",
      "         0.4140, -1.7591,  0.2592,  0.9431,  0.1553,  1.4121,  0.2670, -0.5283,\n",
      "         0.6476,  0.1828, -0.3300,  0.1806, -1.9622,  0.3639, -0.4964, -1.1441,\n",
      "         0.5515,  0.3506,  0.8945,  0.3493, -1.0384,  1.0243,  1.1456, -1.9458,\n",
      "        -1.2306, -0.1836,  0.6611,  1.6715,  0.4796, -0.3639, -0.6416,  1.9107,\n",
      "         0.4618,  0.6278, -0.4210, -0.9993, -0.3402,  0.0985, -0.0899,  0.1810,\n",
      "         0.3917,  1.8041, -0.2297, -0.5073,  2.0528,  0.4811,  0.7641,  0.5100,\n",
      "         0.9942,  2.2502,  2.0356,  1.0737,  2.8567,  3.3840,  0.8605,  1.9417,\n",
      "        -0.0768, -0.8176,  0.0534, -0.1843, -0.9464,  2.2620, -1.2705, -0.5782,\n",
      "        -0.7408, -0.4674,  2.6476,  3.4035,  0.3478,  0.1829,  1.2834,  0.6158,\n",
      "        -0.5008,  2.1967,  2.4465, -0.5001, -0.8351, -1.1390,  0.9051, -0.3679,\n",
      "         0.1493,  1.2491,  1.5458,  0.4068, -0.6255, -1.3008, -2.1045, -0.7456,\n",
      "         0.4019, -2.4779, -2.2151, -1.4679, -1.4803, -2.8873, -1.2235,  0.4519,\n",
      "        -0.5278, -0.9596,  1.0309,  0.1437,  0.0591,  2.1989,  0.6413,  0.2809,\n",
      "         0.8185,  0.8284,  1.0924, -1.5648, -0.0277,  0.6320, -0.6163, -0.1813,\n",
      "        -1.2652, -2.0588, -1.4944, -1.7295, -2.2076, -1.8562, -1.2410, -0.5432,\n",
      "        -0.3691,  0.8773, -0.5070,  0.2025,  1.6707,  0.1987,  0.9667,  1.3213,\n",
      "         1.1249,  1.4465,  1.3517,  0.4222, -0.6790, -0.0391, -0.4780, -0.7454,\n",
      "         0.3807, -0.5863,  0.7561,  0.2336,  1.7162,  1.2960,  2.1403, -0.2193,\n",
      "         1.3030,  0.8930,  0.7164, -0.5442,  0.3717,  0.1958,  0.5583,  0.7800,\n",
      "         0.6510, -1.8864, -1.8312, -0.2385,  0.1799, -1.7461, -0.0975, -0.9444,\n",
      "        -2.5824, -3.2873, -1.2252, -0.4075, -2.7017,  0.1381, -0.2790,  0.2121,\n",
      "        -1.4836,  2.1268,  0.9049, -2.9909, -1.1568, -1.4677, -0.8224, -0.6377,\n",
      "        -2.2917,  0.4445, -0.3737, -1.0727,  1.5570,  2.1608,  0.7811, -0.4166,\n",
      "         0.0480, -1.5760,  1.7503,  3.8591,  1.3094, -0.6357, -0.0523,  0.6086,\n",
      "        -0.0238, -2.0677, -0.1594, -0.3807,  0.1703,  0.5931, -1.3623, -0.6132,\n",
      "         0.7930, -0.8259,  0.8567,  0.4868, -0.0520, -0.3084,  0.3874, -0.9201,\n",
      "         0.1518,  0.6410, -1.2456,  0.6024, -1.7960, -1.2398,  2.1896,  2.8185,\n",
      "         0.5231, -1.6423, -2.2543, -0.0902, -0.4528, -0.5329, -0.6742,  1.6821,\n",
      "        -1.0627,  1.0640,  0.3346, -1.3500, -0.2360, -0.7473, -0.5885, -0.2750,\n",
      "         2.2480, -0.0991, -1.0200, -1.1455, -0.7348,  0.4001,  1.7345, -0.4063,\n",
      "        -1.6590,  1.9972,  0.1795,  2.7031, -2.4134,  0.9065,  2.0179, -0.7388,\n",
      "        -0.0993,  1.2076,  1.4429, -1.3231, -1.9266,  2.1873,  0.0161,  4.3555,\n",
      "         0.0163,  0.2805, -0.5617, -0.7840,  0.8489, -1.9156, -0.5064, -1.8345,\n",
      "        -0.9200, -1.1611,  0.1808,  0.6856,  0.2528, -0.3386,  0.2608,  0.1176,\n",
      "         3.9251,  1.6151, -1.6816,  0.4715,  3.7598, -2.1549, -1.4630, -0.0206,\n",
      "         1.1578, -0.3586,  1.0970,  2.2496, -0.4004, -1.0137, -0.1385,  0.4340,\n",
      "        -0.1527,  0.9072, -1.6468,  1.2716, -1.3840, -1.4845,  1.5408,  2.2545,\n",
      "         1.0043,  0.3007,  1.2145,  1.6070,  1.0920,  1.1248, -1.6617, -0.5309,\n",
      "        -1.0047, -2.7155, -1.6709,  3.0138, -2.1600,  0.7766,  1.5077,  1.6467,\n",
      "         0.2515, -0.2394,  1.1516, -3.3899,  0.1089,  0.4062,  1.1183,  2.7255,\n",
      "        -0.4813,  0.4813, -2.1941, -2.3963,  1.2945, -0.7155,  1.2304,  1.3533,\n",
      "        -1.5563,  0.3936, -1.5704,  2.0972, -0.0448, -0.4689, -1.2530, -0.0695,\n",
      "         0.8609, -1.8847, -0.0518,  0.3464,  0.3107,  0.5835,  0.9799,  0.9177,\n",
      "        -2.0020, -1.1333, -3.1746, -0.6424, -1.2862, -1.1135, -1.1625,  0.9204,\n",
      "        -0.0760,  1.0899, -0.1923,  2.2765,  1.6302,  1.7244,  2.8929,  0.3587,\n",
      "         1.8291,  2.2489, -0.2984, -0.8810,  1.1535,  1.8878,  2.0757,  1.1599,\n",
      "         1.9475, -2.6002, -0.4538, -1.3380,  0.5147,  3.2582,  1.4816, -1.3870,\n",
      "         0.5159, -0.2469, -0.2535,  3.2625, -2.5343,  3.0143, -0.7255,  0.6590,\n",
      "        -0.2794, -0.5170,  0.6883,  1.6671,  4.7546, -0.1395,  3.3715,  1.1618,\n",
      "        -0.3930, -2.3210,  5.5126, -0.1113, -1.8643,  1.5915,  0.8374,  1.7419,\n",
      "         3.1706,  2.8105,  0.1561, -0.1774,  0.2069, -0.6285, -1.2383, -0.9786,\n",
      "         0.3792, -1.1122, -0.8329, -0.4113,  1.0888, -3.9678, -0.2560,  0.4294,\n",
      "        -0.1716, -0.1625,  2.8832, -0.3463,  1.3919,  0.2133,  0.4107, -0.6042,\n",
      "         0.7351, -0.9630,  1.0493, -0.3618, -0.7562, -1.9286,  3.0122, -2.3666,\n",
      "         2.3909, -0.4714,  1.9413,  0.2966, -1.5089,  0.1368, -0.0400, -0.6949,\n",
      "        -1.1857,  3.9846,  3.3896, -0.4613, -0.4648,  2.9464,  2.1378, -0.0860,\n",
      "         0.2913,  3.7733,  0.3827,  0.8658,  1.4074, -0.7025,  2.1205, -1.7593,\n",
      "         0.4501, -2.8051, -2.3339, -1.0770,  1.0624, -1.3417, -3.0786,  1.3391,\n",
      "         0.9933, -0.1261, -2.4593,  0.5691,  2.0233, -1.2194, -0.5727,  1.6786,\n",
      "         0.8431, -1.2264,  0.5098, -0.6296,  0.6550,  1.9221,  1.7429,  1.2630,\n",
      "        -0.3284,  0.5535,  1.1738, -1.4180, -0.7474, -1.1255, -0.5330,  0.5467,\n",
      "         1.3797,  0.2141,  0.9391, -2.1619, -2.6715,  0.8742,  0.4322, -1.7671,\n",
      "        -0.9647, -2.7736, -1.9048,  0.5399,  1.5414,  1.2955, -1.0589, -0.5747,\n",
      "         0.0966,  0.0587,  1.6056,  0.6257,  0.9651,  1.6381,  1.0494,  1.1510,\n",
      "        -0.5687,  1.6084,  0.7037,  0.0849,  1.6422,  0.6060,  1.3330, -0.2866,\n",
      "        -0.8315,  1.3478,  2.3125, -1.8378,  0.4166,  0.2160,  0.5084,  1.8311,\n",
      "         0.4747,  5.5166, -0.1880,  3.2504,  3.0413,  0.4489, -0.9042,  1.5242,\n",
      "        -1.2156,  1.3633,  0.2720, -0.0782,  1.1988,  2.7863,  0.9216, -1.3342,\n",
      "        -0.0470,  1.1434,  2.2746, -1.9873, -1.3022, -1.1466,  2.3973,  2.9526,\n",
      "         2.2821,  2.9309,  1.3586, -0.4356, -1.2052,  0.7149, -0.6899, -0.3219,\n",
      "        -0.1088, -0.6300,  1.2893, -2.1825,  1.1092,  1.3444,  0.3780,  1.8399,\n",
      "        -1.2787,  0.2963, -1.8490, -2.0231,  1.7138, -0.1698,  0.6267,  0.9666,\n",
      "         0.5091,  1.1808,  2.7887,  2.6930, -1.1591,  1.3326, -2.1205, -1.3324,\n",
      "         0.2853, -1.8526,  1.1963, -0.0416, -3.5819, -2.6203, -1.6950,  0.8981,\n",
      "         0.4191, -0.1345,  1.1478,  0.9465,  0.6258, -2.0665,  0.3364,  1.9009,\n",
      "        -3.3135, -1.4846,  0.6818, -0.0696,  1.6560,  1.9908,  1.8700, -1.0219,\n",
      "        -1.2172,  0.2874,  0.2194,  1.2276,  2.0266,  1.4073,  2.2502, -0.6175,\n",
      "         1.8021,  1.4177,  1.2769,  2.6890,  0.7365, -2.0487, -0.5499,  3.2098,\n",
      "        -2.8757, -1.6596,  1.6439,  2.5004, -1.2781,  1.2415,  1.2600, -0.4607,\n",
      "        -0.9136, -2.2173, -0.1061,  0.0455,  1.6852, -0.8398, -0.6099, -1.9123,\n",
      "         2.9958, -1.4803, -2.5280, -1.1882,  1.8276, -0.2118,  1.3608, -0.9404,\n",
      "        -0.0673, -0.6892,  1.8764,  0.7996, -0.4599,  0.1130, -0.6438, -1.9203,\n",
      "        -2.0440,  1.1921, -3.1160,  1.8545,  0.3090,  2.9922,  0.0682, -2.1134,\n",
      "         1.3635, -0.8102,  1.7485,  1.5031, -1.8651, -0.1545,  0.8311, -0.7308,\n",
      "         3.7394,  1.4959,  0.5487, -0.0574, -0.4617, -0.6415,  0.8522,  0.7037,\n",
      "        -0.2612, -1.5891, -2.5880, -1.9357,  0.1005,  0.0707,  2.2419,  0.0436,\n",
      "        -0.8297,  2.4741, -0.0494, -1.0771, -0.0962, -0.3049, -0.7618, -0.0807,\n",
      "        -0.3768,  0.9326,  0.1017, -0.3562, -0.2799, -1.7374, -0.3039, -0.8914,\n",
      "        -2.1360,  0.1728, -0.8131,  1.1730, -1.9495, -1.6317, -0.2135, -0.0229,\n",
      "        -0.1558, -1.4266, -2.1504,  0.3521,  0.1398, -0.3751, -0.0537, -1.5146,\n",
      "        -1.0013,  0.0758, -0.7793, -1.0305, -0.6893, -0.1584, -0.4084, -1.1121,\n",
      "         0.3413,  1.3501, -0.4013,  2.0176,  1.4271,  1.0175,  0.6616,  1.9558,\n",
      "         2.4254,  0.7324, -2.2520, -0.0890, -1.1245, -1.1008, -0.7750,  0.5173,\n",
      "        -1.7029,  1.2981,  0.5643, -2.3335, -1.9849,  0.4397, -1.2470, -1.5661,\n",
      "        -1.6887, -2.5791, -3.6102,  2.0828, -0.0854, -2.5617, -0.3860, -1.7138,\n",
      "        -1.3735, -1.4898, -1.2358,  0.0616,  0.0102,  0.6930,  1.0876,  1.6238],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-9.9938e-01, -9.4951e-01, -3.1436e+00, -1.0600e+00, -1.0198e+00,\n",
      "        -3.6266e-01, -4.4545e-01, -1.8559e+00, -7.9052e-01,  8.9521e-01,\n",
      "         7.2338e-01,  2.6231e-01,  1.8145e+00,  1.5388e+00,  2.2363e+00,\n",
      "        -6.6517e-02,  6.9445e-02,  1.1957e+00,  7.4096e-01,  2.1372e+00,\n",
      "        -1.8396e+00, -7.6704e-01, -2.4507e+00,  5.9662e-01, -1.5498e+00,\n",
      "        -4.2154e-01,  5.3090e-02,  3.5849e-01,  1.8129e-01,  1.2274e+00,\n",
      "        -7.4266e-01,  3.5544e-02,  3.9391e-01, -1.2058e-01,  6.5762e-01,\n",
      "        -1.2694e+00,  5.1302e-01, -5.8609e-01,  4.3396e-01, -1.2765e-01,\n",
      "         6.0296e-01,  1.8039e+00,  1.9397e+00,  6.0630e-01,  8.5017e-01,\n",
      "         1.3323e+00,  1.1193e+00,  7.2790e-01,  3.9923e-01, -1.8865e+00,\n",
      "        -1.4185e+00, -3.1208e+00,  1.7906e+00,  1.6727e+00,  2.1519e+00,\n",
      "        -3.7844e-01,  6.0882e-01,  1.0007e-01,  2.5741e-01,  5.2312e-01,\n",
      "         1.5387e+00,  1.3762e+00,  1.2401e-01,  2.3688e+00,  4.1648e-01,\n",
      "         1.8290e-01,  2.0228e+00,  1.9150e+00,  1.7283e+00,  3.6842e-01,\n",
      "        -7.0257e-01,  1.5764e+00, -9.1381e-01, -4.5666e-02, -8.2779e-01,\n",
      "         9.4860e-01,  3.6248e+00,  1.8639e+00,  2.5218e+00,  6.0894e-01,\n",
      "        -1.4495e+00, -2.0091e+00,  6.6751e-01, -5.0998e-01, -9.1657e-01,\n",
      "         1.2315e+00, -8.2719e-01,  6.8202e-01, -9.2417e-01, -1.8672e+00,\n",
      "        -1.4939e+00, -5.2114e-01, -4.5008e-01, -1.1396e+00,  4.5776e-01,\n",
      "        -1.5443e+00, -1.5919e+00, -1.9056e+00, -1.9277e+00, -1.8839e-01,\n",
      "        -2.2889e+00, -1.8182e+00,  1.0961e+00,  1.8446e+00,  2.8056e+00,\n",
      "         1.5571e+00,  2.1229e+00, -2.4028e+00, -2.8521e+00, -1.0337e+00,\n",
      "        -8.6215e-01, -8.7776e-01,  4.2629e-01, -2.3780e-01,  1.1221e+00,\n",
      "        -1.5840e+00, -1.8296e+00, -1.8162e+00, -6.4046e-01, -8.3061e-01,\n",
      "        -5.4406e-01, -1.2710e+00, -1.8345e+00, -1.7310e+00,  1.7479e-02,\n",
      "         7.3587e-01, -8.6193e-01, -2.5672e+00, -2.2937e+00, -1.6496e+00,\n",
      "        -1.9503e+00, -1.4370e+00, -1.9364e+00, -1.7097e+00, -1.0115e+00,\n",
      "        -2.0982e+00, -2.5087e+00, -1.7174e+00, -5.9243e-01, -1.2029e+00,\n",
      "         1.2764e+00, -1.4171e+00, -7.2001e-01, -2.3770e+00, -7.7346e-01,\n",
      "        -4.4469e-01, -2.3193e+00, -3.0768e+00, -2.8888e+00, -2.1733e+00,\n",
      "        -8.7640e-02,  2.6745e+00, -1.4886e+00, -7.6715e-01,  5.6146e-01,\n",
      "        -5.0690e-01,  3.5090e-01,  4.1994e-01,  5.1118e-01,  7.1064e-01,\n",
      "        -1.2570e+00,  3.7129e-01,  1.2017e+00,  1.1160e-01,  1.5192e+00,\n",
      "        -3.3599e-01, -1.4942e-01, -1.0584e+00,  1.2059e+00, -4.2168e-01,\n",
      "        -3.5544e-01,  1.8473e+00,  4.3474e-01,  4.9833e-01,  1.6656e+00,\n",
      "        -1.3213e+00, -4.0665e-01,  2.7913e-01,  4.0662e-01,  6.6837e-03,\n",
      "         4.5939e-01, -4.6842e-01,  2.0485e+00, -8.0091e-01,  1.1888e+00,\n",
      "         1.4365e+00,  1.6297e+00,  3.4067e-01, -3.5997e-01,  3.2960e-01,\n",
      "        -1.7810e+00,  1.1295e+00,  9.1773e-01, -2.3707e-01, -2.0671e+00,\n",
      "         5.6087e-01, -6.4938e-02, -1.1714e+00, -7.2185e-01, -2.6142e-01,\n",
      "        -2.2060e+00, -2.6209e-02,  4.9662e-01,  7.3945e-01, -3.6315e-01,\n",
      "        -1.3577e-01, -2.1608e-01,  1.4092e+00,  1.8217e+00,  1.0991e+00,\n",
      "        -1.9468e-01,  4.8034e-01,  5.9235e-01,  6.8404e-02, -8.1977e-01,\n",
      "         1.5381e+00, -1.0868e+00, -2.6543e-01, -8.4785e-02,  4.8192e-01,\n",
      "        -1.4594e+00, -8.4028e-01, -3.3763e-02,  1.7973e+00,  1.8167e-01,\n",
      "         1.0452e+00, -9.1978e-01,  1.7965e+00, -1.8707e+00, -4.2192e-01,\n",
      "         7.4749e-01,  4.6698e-01,  2.0440e-01, -1.5148e+00,  5.1025e-01,\n",
      "         1.5352e+00,  3.1841e-01,  1.5321e+00,  1.2644e-01, -5.8281e-01,\n",
      "         7.0530e-01,  1.8413e-01, -1.6534e-01,  4.0226e-01, -1.8322e+00,\n",
      "         5.1711e-01, -2.1455e-01, -9.6635e-01,  7.3227e-01,  5.0871e-01,\n",
      "         1.0858e+00,  3.9540e-01, -9.0705e-01,  9.7575e-01,  1.6189e+00,\n",
      "        -1.4089e+00, -1.2901e+00, -1.4411e-01,  6.2023e-01,  1.8125e+00,\n",
      "         7.5103e-01,  6.3781e-02,  1.1218e-01,  1.9722e+00,  5.1947e-01,\n",
      "         8.3456e-01, -3.2247e-01, -8.3024e-01, -2.1930e-02,  6.1808e-01,\n",
      "         2.9226e-01,  8.9572e-01,  1.0676e+00,  1.8901e+00,  2.3393e-01,\n",
      "         1.6107e-01,  2.6408e+00,  1.0787e+00,  1.4605e+00,  9.2889e-01,\n",
      "         1.8625e+00,  2.7789e+00,  2.3403e+00,  1.3805e+00,  3.3916e+00,\n",
      "         3.6847e+00,  1.4564e+00,  2.3853e+00,  3.0795e-01, -4.0536e-01,\n",
      "         2.4421e-01,  1.1752e-01, -6.9653e-01,  2.8173e+00, -7.2750e-01,\n",
      "        -4.2376e-01, -4.2383e-01, -3.4033e-01,  3.9010e+00,  4.3955e+00,\n",
      "         9.3708e-01,  6.3693e-01,  1.6775e+00,  1.2173e+00,  1.4160e-01,\n",
      "         2.5496e+00,  2.7978e+00,  1.7741e-01, -1.4360e-01, -1.0030e-01,\n",
      "         1.3710e+00,  2.2248e-01,  8.0142e-01,  1.7669e+00,  1.8911e+00,\n",
      "         1.0896e+00, -5.1587e-02, -4.9134e-01, -1.1130e+00, -1.0429e-01,\n",
      "         8.3443e-01, -1.9564e+00, -1.1699e+00, -1.2069e+00, -5.5064e-01,\n",
      "        -2.0572e+00, -7.9520e-01,  6.8955e-01, -1.8780e-01, -9.4664e-01,\n",
      "         1.8019e+00,  8.1473e-01,  2.4850e-01,  2.8387e+00,  1.3726e+00,\n",
      "         1.4540e+00,  1.7776e+00,  1.3518e+00,  1.3104e+00, -1.7189e+00,\n",
      "         1.8175e-01,  8.0623e-01, -1.3949e-01,  1.2781e-01, -1.2491e+00,\n",
      "        -1.9829e+00, -1.3172e+00, -1.6845e+00, -2.1410e+00, -1.7115e+00,\n",
      "        -1.0234e+00, -5.7467e-01, -2.4427e-01,  9.5606e-01,  1.9832e-02,\n",
      "         2.8458e-01,  2.5579e+00,  1.1571e+00,  1.8512e+00,  2.3059e+00,\n",
      "         1.6732e+00,  1.7479e+00,  1.9519e+00,  1.2086e+00,  4.4633e-02,\n",
      "         4.8492e-01, -1.1055e-01, -2.3465e-01,  9.5625e-01, -2.5745e-01,\n",
      "         1.6517e+00,  1.0845e+00,  2.6329e+00,  2.1355e+00,  2.7441e+00,\n",
      "         1.7887e-01,  1.6423e+00,  1.6793e+00,  1.4113e+00,  2.7751e-01,\n",
      "         1.2324e+00,  1.1108e+00,  1.7432e+00,  1.5055e+00,  1.3904e+00,\n",
      "        -1.4525e+00, -1.6906e+00,  1.6693e-01,  2.4261e-01, -1.6031e+00,\n",
      "         1.8978e-01, -1.0803e+00, -2.2600e+00, -3.3656e+00, -1.2424e+00,\n",
      "        -3.8423e-01, -2.5802e+00,  6.0049e-01, -5.2874e-01,  6.8431e-03,\n",
      "        -1.6519e+00,  1.6228e+00,  4.4997e-01, -2.9985e+00, -1.4428e+00,\n",
      "        -1.6550e+00, -1.0405e+00, -1.0969e+00, -2.4636e+00,  2.5256e-01,\n",
      "         2.5578e-01, -1.2422e+00,  1.6423e+00,  1.6737e+00,  6.0566e-02,\n",
      "        -1.0928e+00,  4.2091e-02, -1.5894e+00,  1.4562e+00,  3.9405e+00,\n",
      "         6.7348e-01, -8.6992e-01, -2.9772e-01, -1.6394e-01, -7.2868e-01,\n",
      "        -2.2171e+00, -2.5175e-01, -5.4523e-01, -5.6034e-02,  6.3658e-01,\n",
      "        -1.1681e+00, -5.6193e-01,  4.1903e-01, -9.9357e-01,  7.0861e-01,\n",
      "         1.0041e-01, -4.2289e-01, -6.6913e-01,  1.2491e-01, -8.4982e-01,\n",
      "        -1.0417e-01,  5.0575e-01, -1.3102e+00,  6.7949e-01, -2.0161e+00,\n",
      "        -1.3482e+00,  1.6334e+00,  2.4119e+00,  8.3490e-01, -2.0042e+00,\n",
      "        -2.6452e+00, -3.1138e-01, -2.0835e-01, -6.5478e-01, -1.2653e+00,\n",
      "         1.4628e+00, -9.9408e-01,  6.8794e-01, -3.0514e-02, -1.2642e+00,\n",
      "        -8.8409e-01, -4.2930e-01, -3.9439e-01, -1.1365e-01,  1.9471e+00,\n",
      "        -3.3292e-01, -1.5203e+00, -1.6625e+00, -8.3116e-01,  1.4277e-01,\n",
      "         1.3218e+00, -7.4371e-01, -1.7186e+00,  1.5202e+00,  3.4619e-01,\n",
      "         2.3860e+00, -2.4444e+00,  3.7524e-01,  1.7107e+00, -8.9810e-01,\n",
      "        -2.4830e-01,  1.1487e+00,  1.2528e+00, -1.6287e+00, -2.0192e+00,\n",
      "         1.6596e+00, -3.5293e-01,  3.8385e+00, -1.2084e-01,  1.1553e-01,\n",
      "        -5.4812e-02, -9.8037e-01,  7.5328e-01, -1.7918e+00, -3.3648e-01,\n",
      "        -1.6655e+00, -6.4236e-01, -1.4502e+00, -2.9739e-01,  1.4569e-01,\n",
      "        -3.2093e-01, -3.8937e-01,  5.4225e-01, -2.7328e-02,  3.1620e+00,\n",
      "         1.2231e+00, -1.4488e+00,  2.0305e-01,  3.1531e+00, -2.5820e+00,\n",
      "        -2.0182e+00, -4.0256e-01,  8.4235e-01, -6.9093e-01,  7.7022e-01,\n",
      "         2.2204e+00, -6.2746e-01, -1.1373e+00, -3.2854e-01,  2.9287e-01,\n",
      "        -5.5428e-01,  2.4581e-01, -1.4527e+00,  8.5939e-01, -9.1845e-01,\n",
      "        -2.0188e+00,  1.1747e+00,  1.6895e+00,  7.4676e-01,  1.1791e-01,\n",
      "         8.2456e-01,  9.7974e-01,  6.0272e-01,  1.1916e+00, -1.8023e+00,\n",
      "        -6.3751e-01, -1.5504e+00, -2.4199e+00, -1.8242e+00,  3.3053e+00,\n",
      "        -2.2079e+00,  4.1184e-01,  1.2170e+00,  1.2290e+00, -4.1317e-01,\n",
      "        -1.9042e-01,  6.3944e-01, -3.4347e+00,  8.9265e-02,  1.7918e-01,\n",
      "         6.1008e-01,  2.2503e+00, -3.6655e-01,  2.6294e-01, -2.4258e+00,\n",
      "        -2.3885e+00,  1.2055e+00, -7.7521e-01,  9.9340e-01,  1.2001e+00,\n",
      "        -1.6797e+00, -8.0465e-02, -1.7601e+00,  1.6768e+00, -1.7204e-01,\n",
      "        -8.8799e-01, -1.3726e+00, -6.4300e-01,  1.1651e+00, -2.1264e+00,\n",
      "        -1.7993e-01, -1.7256e-01,  3.6518e-02,  1.0792e-01,  1.0080e+00,\n",
      "         2.3688e-01, -2.3855e+00, -8.9294e-01, -3.3496e+00, -8.9855e-01,\n",
      "        -1.3992e+00, -1.4340e+00, -1.9917e+00,  1.1660e+00,  1.1877e-01,\n",
      "         6.2856e-01, -3.0352e-01,  1.8755e+00,  1.6665e+00,  1.4520e+00,\n",
      "         2.3296e+00,  1.1152e-01,  1.2710e+00,  1.5632e+00, -3.3664e-01,\n",
      "        -9.1416e-01,  1.0169e+00,  1.7308e+00,  1.3507e+00,  1.4400e+00,\n",
      "         2.0926e+00, -2.6332e+00, -3.1292e-01, -1.3915e+00,  6.9972e-01,\n",
      "         3.0034e+00,  1.2346e+00, -1.7936e+00,  3.5659e-01, -5.9474e-01,\n",
      "        -7.0750e-01,  2.7788e+00, -2.7480e+00,  2.5829e+00, -7.2572e-01,\n",
      "         5.0083e-01, -1.3958e-01, -1.0151e+00,  3.3896e-01,  1.6763e+00,\n",
      "         3.9413e+00, -3.4379e-01,  3.0684e+00,  9.4586e-01, -8.5170e-01,\n",
      "        -2.8694e+00,  5.1957e+00, -7.1388e-01, -1.9454e+00,  1.0281e+00,\n",
      "         1.0501e+00,  1.1015e+00,  2.9284e+00,  2.5903e+00, -1.9272e-01,\n",
      "        -1.8921e-01, -7.2108e-02, -7.8833e-01, -1.2394e+00, -1.0445e+00,\n",
      "         1.5163e-01, -1.0850e+00, -9.5099e-01, -3.7887e-01,  1.1411e+00,\n",
      "        -3.8012e+00, -4.9190e-01,  1.9318e-01, -2.1289e-01, -5.4158e-01,\n",
      "         2.8103e+00, -3.7425e-01,  9.0120e-01,  1.3880e-01, -2.6804e-01,\n",
      "        -6.4774e-01,  7.3109e-02, -1.2193e+00,  1.4858e+00, -6.6310e-01,\n",
      "        -9.4873e-01, -2.0740e+00,  2.5111e+00, -2.3974e+00,  1.9327e+00,\n",
      "        -7.6574e-01,  1.6372e+00,  2.2226e-01, -1.5888e+00,  2.3547e-01,\n",
      "        -3.2556e-01, -9.0829e-01, -1.4314e+00,  3.8630e+00,  4.1866e+00,\n",
      "        -9.8455e-01, -1.7213e-01,  3.1405e+00,  1.6317e+00,  2.3624e-02,\n",
      "         3.4136e-01,  3.0180e+00,  6.5886e-02,  3.5825e-01,  1.0812e+00,\n",
      "        -8.7340e-01,  1.8534e+00, -2.0901e+00,  3.3934e-01, -2.7103e+00,\n",
      "        -2.5394e+00, -1.0799e+00,  6.3291e-01, -1.4629e+00, -3.3189e+00,\n",
      "         1.1207e+00,  6.8131e-01, -2.6487e-01, -2.5970e+00,  3.3310e-01,\n",
      "         1.8545e+00, -1.1532e+00, -6.0305e-01,  1.2849e+00,  6.3915e-01,\n",
      "        -1.7153e+00,  4.7689e-02, -7.7457e-01,  5.9837e-01,  1.6365e+00,\n",
      "         1.9701e+00,  1.0445e+00, -2.0452e-01,  3.1776e-01,  9.6325e-01,\n",
      "        -1.2300e+00, -8.6942e-01, -1.0735e+00, -8.2551e-01,  3.9204e-01,\n",
      "         9.8182e-01, -2.4863e-01,  1.1236e+00, -2.0751e+00, -3.1029e+00,\n",
      "         8.2174e-01,  4.7571e-01, -1.9728e+00, -1.0600e+00, -2.7683e+00,\n",
      "        -1.9006e+00,  4.4674e-01,  1.2093e+00,  1.3416e+00, -1.3639e+00,\n",
      "        -7.7311e-02, -1.5611e-01, -2.3124e-01,  1.5426e+00,  3.3020e-01,\n",
      "         6.1232e-01,  1.4660e+00,  6.5249e-01,  6.0482e-01, -8.8210e-01,\n",
      "         9.6787e-01,  4.5757e-01, -2.2678e-01,  1.4341e+00,  4.2840e-01,\n",
      "         7.5929e-01, -3.6742e-01, -9.5581e-01,  9.8105e-01,  2.0574e+00,\n",
      "        -1.8485e+00, -3.3069e-02, -1.7414e-01,  3.8955e-01,  1.6303e+00,\n",
      "         1.8652e-01,  4.8485e+00, -9.2106e-01,  2.8725e+00,  2.4889e+00,\n",
      "         2.8116e-01, -1.1120e+00,  1.3776e+00, -1.3079e+00,  1.5834e+00,\n",
      "         1.2021e-01, -3.8933e-01,  1.2101e+00,  2.6740e+00,  1.1185e+00,\n",
      "        -1.6287e+00, -2.8203e-01,  1.2477e+00,  1.9368e+00, -1.9788e+00,\n",
      "        -1.6865e+00, -1.4465e+00,  1.8899e+00,  3.0419e+00,  2.1753e+00,\n",
      "         2.4177e+00,  1.2688e+00, -3.2609e-01, -1.1546e+00,  5.6699e-01,\n",
      "        -9.1295e-01, -5.7577e-01, -2.0486e-01, -8.0045e-01,  1.0006e+00,\n",
      "        -2.2312e+00,  1.3034e+00,  8.2650e-01,  1.7107e-01,  1.6655e+00,\n",
      "        -1.5199e+00,  1.6614e-01, -1.9663e+00, -2.0870e+00,  1.4114e+00,\n",
      "        -4.0692e-01,  1.0192e+00,  7.0316e-01,  5.3012e-01,  7.1851e-01,\n",
      "         2.1713e+00,  2.7115e+00, -1.2834e+00,  6.6869e-01, -2.2293e+00,\n",
      "        -1.0019e+00,  6.1137e-01, -2.0503e+00,  1.2544e+00, -4.6730e-01,\n",
      "        -3.8355e+00, -2.8356e+00, -1.6592e+00,  2.7591e-01,  3.9182e-01,\n",
      "        -6.2195e-01,  6.8315e-01,  7.5715e-01,  7.8901e-01, -2.2455e+00,\n",
      "        -3.0154e-01,  1.1139e+00, -3.3095e+00, -1.8345e+00,  3.3979e-01,\n",
      "        -1.7481e-01,  1.4481e+00,  1.8051e+00,  1.2598e+00, -1.2453e+00,\n",
      "        -1.1370e+00, -1.0082e-01, -2.4785e-01,  1.4682e+00,  1.7888e+00,\n",
      "         1.2224e+00,  2.1117e+00, -8.7659e-01,  1.6676e+00,  1.2506e+00,\n",
      "         1.3899e+00,  2.4239e+00,  6.9838e-01, -1.8493e+00, -7.4766e-01,\n",
      "         3.5859e+00, -2.7892e+00, -1.5632e+00,  1.3554e+00,  1.9826e+00,\n",
      "        -1.8468e+00,  7.0147e-01,  1.0917e+00, -8.6812e-01, -1.1184e+00,\n",
      "        -2.4321e+00, -4.7479e-01, -3.9796e-01,  1.2826e+00, -1.1467e+00,\n",
      "        -7.6275e-01, -2.0779e+00,  2.3627e+00, -1.5096e+00, -2.5574e+00,\n",
      "        -1.3099e+00,  1.4376e+00, -4.1049e-01,  6.5677e-01, -1.0078e+00,\n",
      "        -2.2665e-01, -1.0188e+00,  1.6782e+00,  7.5639e-01, -6.1476e-01,\n",
      "        -1.4390e-01, -1.0592e+00, -1.8597e+00, -2.2371e+00,  9.5881e-01,\n",
      "        -2.9390e+00,  1.0333e+00, -1.3223e-01,  2.8091e+00,  4.2364e-03,\n",
      "        -1.9504e+00,  1.2026e+00, -8.3588e-01,  1.6420e+00,  1.2749e+00,\n",
      "        -2.0219e+00, -3.6535e-01,  4.8953e-01, -8.4399e-01,  4.0901e+00,\n",
      "         1.3393e+00,  2.4545e-01, -5.4414e-01, -5.1616e-01, -1.1052e+00,\n",
      "         6.5651e-01,  8.6133e-01, -5.8406e-01, -1.8734e+00, -2.9276e+00,\n",
      "        -1.9539e+00, -1.8495e-01, -4.5784e-01,  1.4273e+00, -3.1059e-01,\n",
      "        -1.0124e+00,  1.8306e+00, -5.6004e-01, -1.4836e+00, -6.1996e-01,\n",
      "        -4.7811e-01, -1.1323e+00, -4.0378e-01, -6.2506e-01,  6.4526e-01,\n",
      "        -3.2231e-01, -8.3162e-01, -6.2635e-01, -2.2149e+00, -9.5680e-01,\n",
      "        -1.4664e+00, -2.7411e+00,  2.4084e-01, -1.4390e+00,  7.8258e-01,\n",
      "        -2.0244e+00, -1.8533e+00, -6.3657e-01, -2.1858e-01, -4.7848e-01,\n",
      "        -1.6760e+00, -1.6754e+00,  3.4767e-01,  8.2679e-02, -4.1586e-01,\n",
      "        -3.8501e-01, -1.5365e+00, -9.5093e-01, -1.8828e-01, -6.9520e-01,\n",
      "        -6.4737e-01, -8.9460e-01, -1.2263e-01, -5.1371e-01, -1.3384e+00,\n",
      "        -9.4226e-02,  6.4032e-01, -8.9130e-01,  1.5101e+00,  7.0099e-01,\n",
      "         1.9739e-01,  1.5980e-01,  1.5253e+00,  1.9147e+00,  6.8314e-01,\n",
      "        -2.4697e+00,  2.8316e-02, -1.4930e+00, -1.1235e+00, -9.3965e-01,\n",
      "         4.9000e-02, -2.1124e+00,  1.0395e+00,  3.0403e-01, -2.7906e+00,\n",
      "        -2.2773e+00,  3.6142e-01, -1.4844e+00, -1.7919e+00, -1.5418e+00,\n",
      "        -2.1018e+00, -3.2151e+00,  2.1195e+00,  3.0513e-01, -2.2135e+00,\n",
      "        -1.7017e-01, -1.7837e+00, -1.2872e+00, -1.7264e+00, -1.0828e+00,\n",
      "         5.7907e-02, -6.7133e-02,  8.3089e-01,  1.2480e+00,  1.1956e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-1.3781e+00, -1.7283e-01, -2.0377e+00, -7.2789e-01, -1.8356e-01,\n",
      "        -6.3790e-01, -1.3366e-01, -1.0339e+00, -6.7516e-01,  2.8133e-01,\n",
      "         2.4829e-02, -3.1211e-02,  6.8329e-01,  6.4849e-01,  8.3511e-01,\n",
      "        -3.9013e-03, -5.2738e-01,  4.4999e-01,  3.5544e-01,  9.7115e-01,\n",
      "        -1.4025e+00, -1.1960e+00, -1.3503e+00, -2.5278e-02, -8.7758e-01,\n",
      "        -5.4254e-01, -5.4982e-01,  6.1912e-02, -3.3681e-01,  5.1515e-01,\n",
      "        -1.1717e-01,  9.2660e-01,  4.9550e-02, -3.5780e-01,  2.4043e-01,\n",
      "        -1.0405e+00,  2.6348e-01, -6.4582e-01,  1.9112e-01,  1.9892e-01,\n",
      "         8.5318e-01,  1.6907e-01,  8.8829e-01,  6.8710e-01,  4.2678e-01,\n",
      "         9.2252e-01,  1.3868e+00,  2.0842e-01,  6.8294e-01, -5.3784e-01,\n",
      "        -2.7733e-01, -1.6782e+00,  1.0503e+00,  1.0082e+00,  1.5090e+00,\n",
      "         6.4351e-01,  4.4856e-01, -6.2254e-02,  9.1650e-01,  1.1942e+00,\n",
      "         1.7924e+00,  1.6972e+00,  4.4367e-01,  1.7714e+00,  1.2072e+00,\n",
      "         4.9509e-01,  1.3491e+00,  1.6546e+00,  7.8537e-01,  4.3774e-01,\n",
      "        -5.8952e-01,  1.2452e+00, -8.1970e-01,  6.1518e-02, -4.9918e-01,\n",
      "         1.5504e-02,  2.4755e+00,  1.3229e+00,  1.9060e+00,  1.0615e+00,\n",
      "        -1.3939e+00, -1.5196e+00,  3.0035e-01, -1.1071e+00, -2.0258e-01,\n",
      "         6.5128e-01, -7.3448e-01, -3.0859e-01, -3.2885e-01, -1.2477e+00,\n",
      "        -9.6952e-01, -1.0713e+00, -5.0558e-01, -9.7118e-01,  6.7265e-01,\n",
      "        -1.9773e+00, -7.6220e-01, -1.1810e+00, -1.8339e+00, -2.0226e-01,\n",
      "        -1.2677e+00, -1.1434e+00,  2.4852e-01,  1.0945e+00,  1.4533e+00,\n",
      "         7.9192e-01,  1.1261e+00, -1.3387e+00, -1.1723e+00, -1.3979e+00,\n",
      "        -1.1788e+00, -3.9972e-02,  7.9186e-01,  5.8216e-01,  1.2140e+00,\n",
      "        -1.3833e+00, -1.1419e+00, -6.4155e-01, -5.5893e-01, -8.8150e-01,\n",
      "        -8.3811e-01, -1.4111e+00, -1.5801e+00, -1.4892e+00,  1.7688e-01,\n",
      "         3.7857e-01, -6.6112e-01, -1.4825e+00, -1.9272e+00, -4.1356e-01,\n",
      "        -8.6864e-01, -1.2408e+00, -1.2247e+00, -9.2949e-01, -1.1147e+00,\n",
      "        -1.5462e+00, -1.7730e+00, -1.2280e+00, -4.1893e-01, -1.4450e+00,\n",
      "        -3.7314e-01, -1.0756e+00, -1.2979e+00, -1.8961e+00, -6.8218e-01,\n",
      "        -1.3192e+00, -2.0686e+00, -2.4345e+00, -2.3009e+00, -1.9337e+00,\n",
      "        -5.8943e-01,  1.5551e+00, -1.6662e+00, -4.9687e-01, -2.3750e-01,\n",
      "        -5.4516e-01, -1.3591e-01,  4.0773e-02, -2.1745e-01,  5.4797e-01,\n",
      "        -1.3760e+00,  5.9718e-01,  8.8427e-01,  9.3904e-02,  1.0431e+00,\n",
      "        -3.0164e-01,  4.1794e-02, -8.7523e-01,  8.1646e-01, -3.6100e-01,\n",
      "        -7.1884e-01,  1.4266e+00,  3.6305e-01,  3.1557e-01,  7.4771e-01,\n",
      "        -1.7869e+00, -4.9648e-01, -1.8851e-01,  9.2794e-01, -3.0631e-01,\n",
      "        -7.2683e-02, -4.2984e-01,  7.4598e-01, -6.8453e-01,  6.7864e-01,\n",
      "         4.6103e-01,  5.0454e-01,  7.5821e-03, -4.9975e-01, -8.6178e-02,\n",
      "        -1.7451e+00,  9.5133e-01,  1.8925e-02, -9.8029e-01, -2.4035e+00,\n",
      "         6.1667e-01,  1.5621e-02, -1.1846e+00, -1.2045e+00, -2.1361e-01,\n",
      "        -1.9100e+00, -3.1237e-01,  1.5603e-01,  5.2120e-01, -5.1054e-01,\n",
      "        -7.8982e-01, -6.3280e-01,  5.8482e-01,  1.0546e+00,  1.0763e+00,\n",
      "        -6.7218e-01,  4.1047e-01,  2.9784e-01,  8.8903e-02, -9.7603e-01,\n",
      "         5.9756e-01, -1.3330e+00, -5.0122e-01, -1.5328e-01,  1.5679e-01,\n",
      "        -1.5514e+00, -7.2316e-01, -4.3287e-01,  1.1040e+00,  4.0809e-01,\n",
      "         3.6667e-01, -1.6219e+00,  7.0857e-01, -1.8042e+00, -3.8192e-01,\n",
      "         5.9726e-01,  3.5641e-01,  2.1808e-01, -1.6395e+00,  2.0994e-01,\n",
      "         6.4741e-01,  4.7660e-01,  1.0046e+00,  7.0750e-03, -2.9981e-01,\n",
      "         4.7750e-01,  1.4126e-01,  1.0109e-01, -2.1010e-01, -1.9798e+00,\n",
      "        -1.7650e-02, -4.8273e-01, -1.0393e+00,  5.7686e-01,  5.8325e-01,\n",
      "         1.0304e+00,  2.8536e-01, -1.5097e+00,  9.6256e-01,  4.4513e-01,\n",
      "        -1.8084e+00, -1.3198e+00, -3.9848e-01,  2.1688e-01,  8.9741e-01,\n",
      "         1.7041e-01, -5.5137e-01, -6.7690e-01,  1.2511e+00,  3.1267e-01,\n",
      "         6.7294e-01, -5.6433e-01, -4.7214e-01, -1.1347e+00,  6.3588e-01,\n",
      "         1.7422e-01,  9.6692e-01,  1.1502e+00,  1.5842e+00,  4.2051e-01,\n",
      "         3.5526e-02,  1.1427e+00,  1.3481e+00,  8.5234e-01,  7.2671e-01,\n",
      "         1.4102e+00,  1.9261e+00,  1.7570e+00,  9.8947e-01,  2.1424e+00,\n",
      "         2.5849e+00,  1.1989e+00,  1.4188e+00,  4.9912e-01, -6.3217e-03,\n",
      "         3.0336e-01,  1.9706e-01, -5.5180e-02,  2.0107e+00, -8.3080e-01,\n",
      "        -5.7517e-01, -8.0241e-01, -1.1028e-01,  2.3066e+00,  2.5925e+00,\n",
      "        -4.6728e-01,  1.0057e+00, -1.5033e-01,  2.7262e-01,  4.2837e-02,\n",
      "         7.2785e-01,  1.5066e+00,  2.2191e-02,  1.5107e-01,  2.7360e-01,\n",
      "         1.4898e+00,  5.0325e-01,  6.6142e-01,  1.3235e+00,  1.5357e+00,\n",
      "         1.1856e+00,  4.3038e-01,  3.6840e-01, -6.7952e-02,  1.8306e-01,\n",
      "         1.6225e-02, -1.3544e+00, -1.0929e+00, -1.0629e+00,  5.8279e-02,\n",
      "        -9.6317e-01, -5.6103e-01,  1.1585e+00,  1.3000e-01, -1.4188e+00,\n",
      "         8.8874e-01,  5.2844e-01, -9.5703e-02,  1.0521e+00,  7.2148e-01,\n",
      "         2.5175e-01,  3.0560e-01,  3.0235e-01,  5.5431e-01, -1.3913e+00,\n",
      "        -5.7859e-02,  3.0211e-01, -4.2996e-01,  6.1852e-02, -1.0077e+00,\n",
      "        -1.1312e+00, -8.3048e-01, -1.2700e+00, -1.2956e+00, -1.0692e+00,\n",
      "        -7.2686e-01, -8.3225e-01, -2.4948e-01,  5.5072e-01, -3.1078e-01,\n",
      "        -4.5027e-01,  1.4429e+00,  1.8110e-02,  9.8798e-01,  8.4885e-01,\n",
      "         6.2806e-01,  1.3446e+00,  1.6097e+00,  1.1231e+00, -3.9755e-01,\n",
      "         1.7266e-01, -2.4501e-01, -1.7148e-01,  8.4118e-02, -5.7114e-01,\n",
      "         3.6023e-01, -2.3762e-01,  1.3338e+00,  8.6965e-01,  1.2587e+00,\n",
      "        -4.6796e-01,  4.5668e-01,  1.0167e+00,  4.3534e-01, -6.3246e-01,\n",
      "         2.9999e-01,  5.4816e-01,  1.1471e+00,  1.1228e+00,  1.2648e-01,\n",
      "        -5.8198e-01, -1.3181e+00,  1.9137e-01,  1.0846e-01, -9.8697e-01,\n",
      "         3.4650e-01, -5.9928e-01, -2.1053e+00, -2.0388e+00, -1.4137e+00,\n",
      "        -4.8850e-01, -1.4002e+00, -1.0467e-01, -1.5475e-01,  2.9394e-01,\n",
      "        -1.2919e+00,  1.4658e+00,  9.1389e-01, -1.8892e+00, -1.6454e+00,\n",
      "        -8.6061e-01, -2.1063e-02, -1.1350e+00, -2.6013e+00,  6.7029e-01,\n",
      "         3.2004e-01, -7.2482e-01,  1.1344e+00,  5.9230e-02, -4.3364e-01,\n",
      "        -7.3374e-01, -1.6522e-01, -5.3350e-01,  1.0505e+00,  2.9110e+00,\n",
      "         7.6447e-01,  7.6927e-01, -6.2003e-01,  1.8892e-02, -2.6896e-01,\n",
      "        -9.8289e-02, -2.1041e-01,  3.1557e-01,  7.2052e-01,  9.7333e-01,\n",
      "        -8.9811e-01, -1.2959e+00,  2.1202e-01, -9.1533e-01,  1.1594e+00,\n",
      "         3.6430e-01, -8.2424e-01,  1.3606e-01,  3.9457e-01, -1.4395e+00,\n",
      "         9.1877e-01,  9.4611e-01, -1.8624e-01,  4.3414e-01, -1.3737e+00,\n",
      "        -9.3573e-01,  1.7343e+00,  1.2042e+00,  1.9557e+00, -1.0982e+00,\n",
      "        -2.1739e+00, -2.7149e-01, -1.3048e-01, -4.1610e-01, -1.0233e+00,\n",
      "         1.6002e+00, -9.5900e-01,  4.9785e-01, -3.8509e-02, -9.4399e-01,\n",
      "        -9.0226e-02, -1.7094e-01,  3.7379e-01,  3.8709e-01,  1.2151e+00,\n",
      "        -7.3896e-01, -1.6786e+00, -1.1040e+00, -1.5362e-01, -1.9240e-01,\n",
      "         1.5779e+00, -6.6647e-01, -9.6600e-01,  6.8448e-01,  5.8282e-01,\n",
      "         1.7767e+00, -1.2298e+00, -3.7930e-01,  1.4604e+00, -3.4052e-01,\n",
      "        -1.1570e+00,  3.8058e-01, -2.0978e-01, -3.6008e-01, -1.9192e+00,\n",
      "         2.8371e-01, -1.0833e-01,  1.6174e+00,  8.0178e-01,  1.0686e+00,\n",
      "         1.0186e-01, -6.6707e-01,  1.4001e+00, -1.0541e+00,  5.6544e-01,\n",
      "        -1.0503e+00, -9.4919e-03,  1.9364e-01, -1.2522e-01,  6.1900e-01,\n",
      "         1.5371e-01,  6.7507e-01,  6.5397e-01,  5.9984e-01,  1.8802e+00,\n",
      "         1.3308e+00, -4.2972e-02,  3.3378e-01,  1.6781e+00, -1.9116e+00,\n",
      "        -1.6887e+00, -9.5447e-01,  1.1114e+00,  2.8227e-01,  6.4635e-01,\n",
      "         1.8112e+00, -1.9707e-01, -5.5320e-01, -2.1620e-01,  8.2113e-01,\n",
      "        -8.5233e-01, -6.9016e-01, -7.7592e-01,  7.8784e-01, -4.0062e-01,\n",
      "        -9.4899e-01,  1.4299e-01,  3.2001e-01,  5.1166e-01, -1.1650e-01,\n",
      "         6.2467e-01,  3.8417e-01,  7.1238e-01,  2.9766e-01, -1.3825e+00,\n",
      "        -9.8066e-01, -1.4861e+00, -1.7045e+00, -7.9450e-01,  2.9040e+00,\n",
      "        -1.3928e+00,  4.9338e-01,  1.2775e+00,  9.3835e-01, -9.0872e-01,\n",
      "         2.9109e-01,  4.9862e-01, -3.0017e+00, -1.2168e+00,  7.8375e-01,\n",
      "        -5.3924e-01,  9.0330e-01,  6.1079e-01,  1.9678e-01, -1.9251e+00,\n",
      "        -1.5134e+00,  1.1087e+00, -1.0988e-02,  9.1830e-01,  4.8384e-01,\n",
      "        -1.6047e+00,  3.0673e-02,  3.1509e-01,  1.3431e+00,  5.1188e-02,\n",
      "        -5.0950e-01, -4.5782e-01, -6.4067e-01,  8.6051e-01, -1.6544e+00,\n",
      "        -4.7804e-02, -4.9778e-01,  8.6480e-01, -2.3935e-01,  9.8916e-01,\n",
      "        -5.8985e-01, -1.9549e+00, -1.8269e-01, -1.2133e+00,  1.3706e-01,\n",
      "         1.6183e-01, -8.4033e-01, -1.4195e+00,  1.2810e+00, -3.1330e-01,\n",
      "         7.1136e-01, -7.9097e-01,  2.1389e+00,  1.1920e+00,  6.4982e-01,\n",
      "        -3.9022e-01,  5.9372e-01, -3.9003e-01,  8.2295e-01,  8.1526e-01,\n",
      "        -7.8687e-01,  1.2657e+00,  4.2133e-01, -1.0129e-02,  1.4042e+00,\n",
      "         1.9794e+00, -1.0135e+00, -8.0639e-01, -1.4321e+00,  1.7072e+00,\n",
      "         1.3334e+00,  2.1616e+00, -3.3828e-01,  8.9155e-01, -7.4159e-01,\n",
      "        -1.1096e-01,  2.5699e+00, -2.0957e+00,  7.9689e-01,  3.4845e-02,\n",
      "        -8.7496e-03,  5.0875e-01, -9.8890e-01,  6.4306e-01,  2.6183e+00,\n",
      "         2.1674e+00,  4.0318e-02,  2.1277e+00,  7.4386e-01, -1.0098e+00,\n",
      "        -2.7384e+00,  3.7248e+00, -1.0595e+00, -1.4960e+00,  8.2418e-01,\n",
      "         6.5369e-01,  5.6123e-01,  8.8191e-01,  1.6544e+00,  1.3692e-01,\n",
      "        -3.1637e-01,  1.6212e-01,  3.9056e-02, -1.2759e+00, -9.8496e-01,\n",
      "         1.8417e-01, -3.6553e-01, -5.6846e-01,  7.0649e-01,  1.9171e+00,\n",
      "        -2.2081e+00,  4.1564e-01,  1.5702e-02, -4.6274e-01,  4.6203e-01,\n",
      "         1.7591e+00, -7.9794e-01,  2.4525e-01,  7.5027e-01, -9.5061e-01,\n",
      "        -2.0249e-01, -4.2558e-01, -8.9106e-01,  7.9282e-01, -7.3032e-01,\n",
      "        -8.8067e-01, -1.7749e+00,  2.6947e-01, -1.3824e+00,  1.0224e+00,\n",
      "        -1.9224e-01,  1.4165e+00, -9.5574e-02, -1.0731e+00,  1.2138e+00,\n",
      "        -8.6906e-02, -1.8646e-01, -6.1844e-01,  1.9561e+00,  2.6381e+00,\n",
      "        -1.5204e+00, -9.3818e-01,  3.4712e+00,  2.0516e-01,  2.1442e-01,\n",
      "         2.3275e-01,  1.4655e+00,  9.8843e-01,  3.6095e-01,  2.9493e-01,\n",
      "        -1.4392e+00, -1.3893e-02, -1.0981e+00, -3.9313e-01, -1.4958e+00,\n",
      "        -1.9752e+00, -1.2626e+00,  6.4474e-01, -9.8945e-01, -2.2460e+00,\n",
      "         1.7540e+00,  1.7228e+00, -2.6897e-01, -1.3862e+00,  1.2624e-01,\n",
      "         1.4673e+00, -5.7023e-01, -7.2061e-01,  1.9989e+00,  8.1660e-01,\n",
      "        -1.9304e+00, -1.4064e-01, -7.2599e-01,  5.7568e-01,  7.4678e-01,\n",
      "         1.3709e+00,  1.7573e+00, -1.6928e-01, -5.2684e-01,  7.9288e-01,\n",
      "        -1.3479e+00,  9.6610e-01, -1.5227e+00, -1.9774e-01,  6.2748e-01,\n",
      "         1.3885e+00,  5.3361e-01,  1.2607e+00, -8.6030e-01, -1.8190e+00,\n",
      "         1.7564e+00,  1.1091e-02, -1.6147e+00,  1.1236e-01, -2.3175e+00,\n",
      "        -9.3428e-01,  5.5748e-01, -2.5083e-01,  1.3728e+00, -1.9627e+00,\n",
      "         3.0223e-01,  3.0045e-01,  7.6822e-01,  1.3756e+00, -9.7643e-02,\n",
      "         4.6224e-02,  1.9691e+00, -7.7246e-01,  5.8888e-01, -1.0199e+00,\n",
      "         1.4615e-01, -8.4874e-02,  6.1144e-02,  7.0951e-01,  1.2985e+00,\n",
      "         1.4406e+00, -8.7447e-02, -5.3930e-02,  1.2102e+00,  2.0552e-01,\n",
      "        -1.0313e+00,  2.5059e-02, -9.8463e-01, -7.6173e-02,  9.9435e-02,\n",
      "         7.6274e-02,  2.6593e+00, -4.9848e-01,  1.8298e+00,  1.1297e+00,\n",
      "         8.3674e-01, -1.5435e+00,  1.2338e+00, -5.0311e-01,  1.8395e+00,\n",
      "         7.6137e-01, -2.8668e-01,  1.2277e+00,  2.5502e+00,  1.1387e+00,\n",
      "        -2.8014e-01,  7.9818e-01,  4.3307e-01,  1.0795e+00, -1.7428e+00,\n",
      "        -6.9620e-01, -1.7508e+00,  8.5702e-01,  3.0578e+00,  1.6400e+00,\n",
      "         1.0067e+00,  1.0754e+00, -9.5294e-02, -7.9262e-01,  8.7090e-01,\n",
      "        -1.2003e+00,  2.4266e-01,  5.9856e-01, -3.9511e-01,  1.6089e+00,\n",
      "        -1.2366e+00,  3.9344e-01,  6.4499e-01, -5.6360e-01,  1.4467e+00,\n",
      "        -1.3559e+00, -7.9200e-01, -1.3187e+00, -1.2114e+00,  7.8314e-01,\n",
      "         7.2701e-01,  1.0100e+00, -4.9861e-01,  4.7898e-01, -7.6600e-02,\n",
      "         1.5868e-01,  1.4951e+00, -1.3471e+00,  6.2604e-01, -2.1364e+00,\n",
      "         4.5510e-01,  6.7259e-01, -1.9394e+00,  1.8472e+00, -2.6923e-02,\n",
      "        -3.0994e+00, -2.0242e+00, -1.5885e+00,  6.8761e-02,  1.0497e+00,\n",
      "         2.6897e-01,  2.8214e-01,  6.9693e-01,  6.4208e-01, -1.5947e+00,\n",
      "        -4.4491e-01,  9.8287e-01, -1.7910e+00, -1.8122e+00,  2.6027e-01,\n",
      "         6.2133e-01,  8.6786e-01,  1.0229e+00,  3.5811e-01, -1.4697e-01,\n",
      "        -5.6164e-01,  2.4179e-01, -5.7094e-01,  1.1488e+00,  1.1802e+00,\n",
      "         1.5793e+00,  2.1211e+00, -7.2960e-01, -1.0557e-01,  1.4246e+00,\n",
      "         1.3819e+00,  1.3799e+00,  1.5940e+00, -6.1815e-01,  5.1329e-01,\n",
      "         2.9696e+00, -2.7326e+00, -3.4949e-01,  1.4393e+00,  1.4912e+00,\n",
      "        -1.3970e+00,  4.7635e-01,  9.5277e-01,  2.9972e-01, -1.5437e+00,\n",
      "        -2.1867e+00,  1.5338e-02, -6.5607e-01,  1.2431e+00, -5.0634e-01,\n",
      "         3.8759e-01, -1.5709e+00,  1.2132e+00, -5.0284e-01, -2.6015e+00,\n",
      "        -7.4326e-01,  1.1642e+00, -3.4000e-01, -4.9829e-02,  4.7495e-01,\n",
      "         2.3005e-01, -2.7563e-03,  1.0083e+00,  1.8938e+00,  6.7882e-01,\n",
      "         1.0805e+00, -1.2105e+00, -3.3045e-01, -4.3760e-01,  7.7851e-01,\n",
      "        -2.1617e+00, -5.9320e-01,  5.2634e-01,  1.9967e+00,  3.2125e-01,\n",
      "        -1.7263e+00,  1.4268e+00, -5.3506e-01,  1.5163e+00,  1.1864e+00,\n",
      "        -8.4156e-01, -2.8952e-01,  4.6104e-01, -6.6635e-02,  3.3943e+00,\n",
      "         1.9156e+00,  2.3149e-02,  8.1253e-01,  6.2278e-01, -8.5907e-01,\n",
      "         1.0666e+00,  1.2358e+00,  2.2860e-01, -6.2501e-01, -2.4852e+00,\n",
      "        -8.8608e-01, -2.1824e-01, -4.3024e-01,  1.0561e+00, -3.0778e-01,\n",
      "        -5.8711e-01,  1.6874e+00, -1.2115e-01, -1.4533e+00, -8.2972e-01,\n",
      "        -1.1880e+00, -1.9471e+00, -5.9441e-01, -7.9660e-01,  5.4658e-01,\n",
      "        -4.8553e-01, -1.1162e+00, -4.3506e-01, -2.0924e+00, -1.5497e+00,\n",
      "        -1.8076e+00, -1.1726e+00, -1.5043e-01, -1.3178e+00, -9.9728e-03,\n",
      "        -1.4334e+00, -1.4923e+00, -4.9230e-01, -1.6094e-01, -1.0862e-01,\n",
      "        -7.6712e-01, -8.5774e-01,  7.0746e-01,  7.3973e-01,  2.6328e-02,\n",
      "         7.2166e-01, -2.7108e-01, -4.2954e-01,  9.0659e-01,  6.4062e-01,\n",
      "        -1.0033e-01, -6.2535e-01,  9.8219e-01,  5.9854e-01, -1.7042e+00,\n",
      "        -5.1182e-01,  1.0581e-01, -1.3811e+00,  7.4451e-01, -2.6225e-01,\n",
      "        -3.1122e-01,  1.3203e+00,  4.8036e-01,  1.6383e+00,  2.8576e-01,\n",
      "        -1.3588e+00,  1.1760e+00, -5.7547e-01, -1.1000e+00, -7.7993e-01,\n",
      "         8.6464e-01, -1.3764e+00,  5.1975e-01,  9.1886e-01, -1.4338e+00,\n",
      "        -7.5330e-01,  7.0932e-03, -3.7107e-01, -1.3919e+00, -5.8875e-01,\n",
      "        -2.6540e-01, -1.4611e+00,  1.7404e+00,  9.4284e-01, -6.5256e-01,\n",
      "         1.0785e+00, -1.1992e+00, -8.6737e-01, -2.2894e+00, -9.1933e-01,\n",
      "        -4.5263e-01, -4.2965e-01,  5.6580e-01,  1.1220e+00,  1.4595e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-1.3513e+00, -5.7337e-01, -2.9155e+00, -1.2205e+00, -7.8822e-01,\n",
      "        -8.3299e-01, -4.0514e-01, -1.2942e+00, -7.8359e-01,  5.9496e-01,\n",
      "         9.1254e-02, -9.0692e-02,  9.9225e-01,  9.1738e-01,  1.3157e+00,\n",
      "         7.0957e-03, -3.3140e-01,  6.1324e-01,  5.2650e-01,  1.6268e+00,\n",
      "        -1.7904e+00, -1.3680e+00, -1.9810e+00,  5.6672e-02, -1.4186e+00,\n",
      "        -3.0700e-01, -1.3988e-01,  4.2688e-01,  1.8072e-01,  9.0423e-01,\n",
      "        -4.2909e-01,  5.7034e-01,  3.0053e-01, -2.6820e-01,  4.9182e-01,\n",
      "        -1.0847e+00,  4.8408e-01, -5.1142e-01,  4.3249e-01, -3.8859e-02,\n",
      "         6.0754e-01,  9.9495e-01,  1.3821e+00,  5.3876e-01,  7.0858e-01,\n",
      "         1.3361e+00,  1.2197e+00,  3.4408e-01,  4.5150e-01, -1.0196e+00,\n",
      "        -7.9770e-01, -2.6276e+00,  1.6183e+00,  1.6446e+00,  2.0602e+00,\n",
      "         1.1818e-01,  9.9008e-01,  2.4103e-01,  8.4317e-01,  8.8523e-01,\n",
      "         1.9323e+00,  1.7685e+00,  4.8453e-01,  2.3315e+00,  8.8490e-01,\n",
      "         4.4005e-01,  1.8075e+00,  2.0530e+00,  1.4301e+00,  5.4469e-01,\n",
      "        -6.7840e-01,  1.8351e+00, -8.2863e-01,  2.4587e-01, -5.2852e-01,\n",
      "         5.5998e-01,  3.5055e+00,  1.8546e+00,  2.4201e+00,  1.2286e+00,\n",
      "        -1.4867e+00, -1.9526e+00,  4.3939e-01, -8.5297e-01, -5.2423e-01,\n",
      "         9.6507e-01, -9.4596e-01,  1.5175e-01, -6.6417e-01, -1.9537e+00,\n",
      "        -1.3105e+00, -8.3830e-01, -6.4554e-01, -1.1334e+00,  4.5179e-01,\n",
      "        -1.9870e+00, -1.1278e+00, -1.4394e+00, -1.6360e+00, -2.3012e-01,\n",
      "        -1.5912e+00, -1.6225e+00,  9.2477e-01,  1.3932e+00,  2.0986e+00,\n",
      "         9.6222e-01,  1.5645e+00, -2.1760e+00, -1.8982e+00, -1.4871e+00,\n",
      "        -1.1431e+00, -4.7324e-01,  3.6978e-01,  2.7178e-01,  1.3778e+00,\n",
      "        -1.6245e+00, -1.4188e+00, -1.3109e+00, -4.0577e-01, -6.9518e-01,\n",
      "        -4.0998e-01, -1.2968e+00, -1.5092e+00, -1.5483e+00,  3.3144e-01,\n",
      "         7.0482e-01, -8.1086e-01, -2.0274e+00, -2.1461e+00, -1.1103e+00,\n",
      "        -1.1597e+00, -1.3523e+00, -1.7083e+00, -1.4555e+00, -1.1580e+00,\n",
      "        -1.8256e+00, -2.1944e+00, -1.4552e+00, -6.1053e-01, -1.2918e+00,\n",
      "         2.8760e-01, -1.1799e+00, -1.1335e+00, -2.0661e+00, -6.9110e-01,\n",
      "        -1.2148e+00, -2.5078e+00, -3.0451e+00, -2.6203e+00, -2.4576e+00,\n",
      "        -4.1587e-01,  2.2776e+00, -1.4748e+00, -6.2005e-01,  2.7201e-01,\n",
      "        -3.7851e-01,  3.7511e-01,  4.5443e-01,  2.7506e-01,  7.2653e-01,\n",
      "        -1.3352e+00,  5.3943e-01,  1.1839e+00,  6.0767e-02,  1.4250e+00,\n",
      "        -1.8177e-01, -1.1633e-02, -9.5773e-01,  1.1505e+00, -6.2610e-01,\n",
      "        -5.9604e-01,  1.7567e+00,  4.1128e-01,  4.5442e-01,  1.1285e+00,\n",
      "        -1.6033e+00, -3.4361e-01,  1.5510e-01,  7.6862e-01,  6.8659e-02,\n",
      "         2.4614e-01, -5.8389e-01,  1.4389e+00, -7.2594e-01,  1.0133e+00,\n",
      "         1.0575e+00,  1.2215e+00,  4.5169e-01, -5.3142e-01,  3.8474e-02,\n",
      "        -1.9225e+00,  9.9193e-01,  5.4440e-01, -5.2804e-01, -2.2414e+00,\n",
      "         9.1912e-01,  1.2421e-01, -1.0139e+00, -9.6674e-01, -1.6268e-02,\n",
      "        -1.9693e+00,  1.0158e-01,  2.7395e-01,  6.8621e-01, -3.8512e-01,\n",
      "        -2.9390e-01, -3.2993e-01,  9.4924e-01,  1.5297e+00,  1.1087e+00,\n",
      "        -2.7439e-01,  5.6656e-01,  4.9322e-01,  2.5587e-01, -7.2736e-01,\n",
      "         1.0802e+00, -1.2956e+00, -1.7624e-01,  6.3815e-03,  5.2814e-01,\n",
      "        -1.4531e+00, -6.9968e-01, -4.3195e-01,  1.7713e+00,  5.5206e-01,\n",
      "         6.6174e-01, -1.2676e+00,  1.4488e+00, -1.9711e+00, -4.4883e-01,\n",
      "         7.1212e-01,  4.8454e-01,  5.0149e-01, -1.5093e+00,  5.4596e-01,\n",
      "         9.4726e-01,  5.7345e-01,  1.5038e+00,  3.0357e-01, -1.3082e-01,\n",
      "         8.3701e-01,  4.7921e-01,  1.5074e-01,  3.1682e-02, -1.8063e+00,\n",
      "         3.4122e-01, -4.3198e-01, -9.6792e-01,  6.3689e-01,  5.9008e-01,\n",
      "         1.0881e+00,  3.9916e-01, -9.7336e-01,  1.1187e+00,  9.8238e-01,\n",
      "        -1.8681e+00, -1.1851e+00, -3.6413e-01,  3.1911e-01,  1.4734e+00,\n",
      "         3.7292e-01, -3.0215e-01, -1.3463e-01,  1.7563e+00,  5.0565e-01,\n",
      "         7.9669e-01, -4.7194e-01, -7.9625e-01, -4.9982e-01,  3.2628e-01,\n",
      "        -9.8462e-02,  7.5974e-01,  9.3710e-01,  1.7584e+00,  3.8034e-01,\n",
      "         9.6629e-02,  1.7680e+00,  1.2321e+00,  1.0692e+00,  6.7851e-01,\n",
      "         1.5958e+00,  2.3000e+00,  2.0151e+00,  1.1107e+00,  2.8730e+00,\n",
      "         3.0898e+00,  1.2657e+00,  1.7189e+00,  4.2546e-01, -3.3676e-01,\n",
      "         2.8423e-01, -2.4349e-03, -3.4219e-01,  2.4108e+00, -9.1487e-01,\n",
      "        -4.6437e-01, -1.0119e+00, -1.1820e-01,  3.1605e+00,  3.4035e+00,\n",
      "         2.4187e-01,  1.0885e+00,  7.9220e-01,  7.1422e-01,  1.5691e-01,\n",
      "         1.7875e+00,  2.2707e+00,  2.4297e-02, -4.3619e-02,  5.7781e-02,\n",
      "         1.5371e+00,  3.0699e-01,  8.7469e-01,  1.6907e+00,  1.9668e+00,\n",
      "         1.1739e+00,  3.6913e-01, -1.3506e-01, -5.7375e-01,  2.5237e-01,\n",
      "         4.2247e-01, -1.5831e+00, -1.2107e+00, -1.1561e+00, -2.5432e-01,\n",
      "        -1.5681e+00, -7.5366e-01,  1.1741e+00,  1.3043e-01, -1.2113e+00,\n",
      "         1.2653e+00,  5.1898e-01, -1.9992e-01,  1.7531e+00,  1.2620e+00,\n",
      "         6.5686e-01,  9.0382e-01,  8.6677e-01,  1.2127e+00, -1.4142e+00,\n",
      "         1.6204e-01,  6.8419e-01, -3.1136e-01,  1.8601e-01, -1.0884e+00,\n",
      "        -1.5929e+00, -8.9761e-01, -1.6236e+00, -1.8469e+00, -1.4490e+00,\n",
      "        -9.9853e-01, -7.0487e-01, -2.6172e-01,  8.3237e-01, -3.1571e-01,\n",
      "        -1.0981e-01,  2.0872e+00,  6.7835e-01,  1.6426e+00,  1.6148e+00,\n",
      "         1.3274e+00,  2.0114e+00,  1.9648e+00,  1.2380e+00, -6.7090e-02,\n",
      "         3.8301e-01, -2.1581e-01, -2.6751e-02,  5.3162e-01, -2.4511e-01,\n",
      "         7.9982e-01,  2.7477e-01,  1.7490e+00,  1.3347e+00,  1.9473e+00,\n",
      "        -5.0466e-02,  9.4385e-01,  1.3570e+00,  8.4581e-01, -2.4753e-01,\n",
      "         6.9891e-01,  1.0059e+00,  1.3523e+00,  1.3388e+00,  6.6602e-01,\n",
      "        -1.0393e+00, -1.5851e+00,  5.1321e-01,  3.4378e-01, -1.3982e+00,\n",
      "         2.0595e-01, -9.4654e-01, -2.5104e+00, -2.7857e+00, -1.5874e+00,\n",
      "        -5.7663e-01, -2.0969e+00,  6.4863e-02, -5.1745e-02, -6.4123e-04,\n",
      "        -1.6420e+00,  1.7654e+00,  8.6343e-01, -2.5302e+00, -1.8670e+00,\n",
      "        -1.3956e+00, -3.6527e-01, -1.1552e+00, -2.8470e+00,  3.6984e-01,\n",
      "         1.7144e-01, -1.1294e+00,  1.1100e+00,  6.0699e-01, -3.5756e-01,\n",
      "        -8.2061e-01,  8.1087e-02, -1.1158e+00,  1.1984e+00,  3.6546e+00,\n",
      "         9.3739e-01, -2.1082e-02, -3.3018e-01, -2.3142e-04, -4.7440e-01,\n",
      "        -1.0716e+00, -2.8997e-01, -2.4524e-01,  3.7085e-01,  9.9388e-01,\n",
      "        -9.0742e-01, -1.1853e+00,  3.9357e-01, -1.1840e+00,  1.0882e+00,\n",
      "         2.1522e-01, -6.7410e-01, -2.6787e-01,  2.5947e-01, -1.0667e+00,\n",
      "         5.4082e-01,  8.1302e-01, -7.2001e-01,  4.8421e-01, -1.5927e+00,\n",
      "        -9.5212e-01,  1.6996e+00,  1.5326e+00,  1.5410e+00, -1.4413e+00,\n",
      "        -2.4577e+00, -1.4445e-01, -3.5229e-01, -6.0598e-01, -1.2952e+00,\n",
      "         1.8728e+00, -9.4617e-01,  6.0377e-01,  9.3080e-02, -9.8461e-01,\n",
      "        -4.7599e-01, -5.0137e-01,  2.5436e-01,  7.1398e-02,  1.9281e+00,\n",
      "        -7.6484e-01, -1.8425e+00, -1.5141e+00, -5.0353e-01, -2.9858e-01,\n",
      "         1.6695e+00, -9.1313e-01, -1.3999e+00,  1.2588e+00,  4.4899e-01,\n",
      "         1.9780e+00, -1.7021e+00,  1.0980e-01,  1.8048e+00, -6.2072e-01,\n",
      "        -1.0107e+00,  7.2929e-01,  3.4956e-01, -1.0784e+00, -2.1228e+00,\n",
      "         8.4559e-01, -1.7311e-01,  2.5099e+00,  6.1689e-01,  4.6184e-01,\n",
      "        -5.3093e-02, -5.9879e-01,  1.2986e+00, -1.3951e+00,  6.0934e-02,\n",
      "        -1.2553e+00, -1.4952e-01, -7.4447e-01, -2.2372e-01,  4.5711e-01,\n",
      "         6.2879e-02, -8.0918e-03,  6.4231e-01,  2.1575e-01,  2.5456e+00,\n",
      "         1.2471e+00, -6.4439e-01,  3.1095e-01,  2.2072e+00, -2.0822e+00,\n",
      "        -1.9842e+00, -7.4333e-01,  1.2171e+00, -2.0233e-01,  8.6398e-01,\n",
      "         2.1539e+00, -4.4757e-01, -9.7937e-01, -3.7878e-01,  6.3606e-01,\n",
      "        -7.9501e-01, -3.4427e-01, -8.8993e-01,  9.2318e-01, -8.2566e-01,\n",
      "        -1.5202e+00,  5.2804e-01,  7.2377e-01,  7.2248e-01,  5.6443e-02,\n",
      "         7.2677e-01,  6.5169e-01,  8.9453e-01,  6.9004e-01, -1.7259e+00,\n",
      "        -8.7930e-01, -1.6216e+00, -2.0792e+00, -1.5840e+00,  3.4054e+00,\n",
      "        -1.7950e+00,  4.3833e-01,  1.4804e+00,  1.2844e+00, -6.4654e-01,\n",
      "        -5.3437e-02,  5.8989e-01, -3.3223e+00, -7.9238e-01,  7.3608e-01,\n",
      "        -5.7427e-02,  1.7144e+00,  1.3990e-01,  2.0323e-01, -2.3497e+00,\n",
      "        -1.9273e+00,  1.1565e+00, -5.3933e-01,  8.4510e-01,  7.8238e-01,\n",
      "        -1.7526e+00, -1.0843e-01, -7.0466e-01,  1.5255e+00,  1.7221e-01,\n",
      "        -8.5171e-01, -8.6723e-01, -7.4940e-01,  8.0322e-01, -2.1660e+00,\n",
      "        -3.1677e-01, -4.5264e-01,  5.8052e-01,  2.0345e-01,  1.0505e+00,\n",
      "        -9.5678e-02, -1.8748e+00, -5.6704e-01, -2.4749e+00, -4.3035e-01,\n",
      "        -6.2920e-01, -1.1951e+00, -1.7107e+00,  1.4514e+00,  1.9217e-01,\n",
      "         5.8704e-01, -6.9896e-01,  2.2558e+00,  1.5081e+00,  9.7000e-01,\n",
      "         5.6856e-01,  1.9929e-01,  2.9960e-01,  9.7268e-01,  4.5716e-01,\n",
      "        -9.6558e-01,  1.2625e+00,  9.5649e-01,  4.6703e-01,  1.3557e+00,\n",
      "         2.1630e+00, -1.9860e+00, -6.9217e-01, -1.2844e+00,  1.4504e+00,\n",
      "         2.1109e+00,  1.7283e+00, -8.7775e-01,  6.6272e-01, -7.6285e-01,\n",
      "        -3.8244e-01,  2.8057e+00, -2.3287e+00,  1.6181e+00, -5.2985e-01,\n",
      "         1.5687e-01,  3.7068e-01, -1.2620e+00,  7.3409e-01,  2.0405e+00,\n",
      "         2.8390e+00, -2.7842e-02,  2.5970e+00,  1.1646e+00, -1.0594e+00,\n",
      "        -2.8860e+00,  4.6966e+00, -8.3310e-01, -1.8344e+00,  1.1332e+00,\n",
      "         1.0288e+00,  9.3032e-01,  1.4805e+00,  2.1216e+00,  2.7278e-02,\n",
      "        -2.0584e-01,  6.3587e-02, -4.4958e-01, -1.3171e+00, -1.0122e+00,\n",
      "         2.7499e-01, -6.5683e-01, -7.1682e-01,  1.3367e-01,  1.8901e+00,\n",
      "        -2.9926e+00,  1.6063e-01,  4.1678e-02, -3.3484e-01, -6.8556e-02,\n",
      "         1.9472e+00, -6.5493e-01,  4.0530e-01,  3.7040e-01, -8.2998e-01,\n",
      "        -4.0697e-01, -2.8054e-01, -1.4070e+00,  1.1537e+00, -7.9541e-01,\n",
      "        -8.9081e-01, -1.7206e+00,  1.2159e+00, -1.9086e+00,  1.1455e+00,\n",
      "        -3.8475e-01,  1.5220e+00,  1.0480e-01, -1.4748e+00,  7.3648e-01,\n",
      "        -1.8624e-01, -4.1375e-01, -1.0991e+00,  2.7168e+00,  3.5145e+00,\n",
      "        -1.3359e+00, -7.0647e-01,  3.7263e+00,  8.7215e-01,  3.8955e-01,\n",
      "         3.9567e-01,  2.0219e+00,  4.4188e-01,  3.2756e-01,  5.7868e-01,\n",
      "        -1.2227e+00,  9.6577e-01, -1.4990e+00, -2.7075e-01, -2.3062e+00,\n",
      "        -2.2196e+00, -1.3240e+00,  7.5686e-01, -1.2361e+00, -2.6204e+00,\n",
      "         1.6518e+00,  1.3504e+00, -3.7901e-01, -2.0019e+00,  1.4616e-01,\n",
      "         1.7368e+00, -1.0025e+00, -6.1946e-01,  1.6761e+00,  6.8884e-01,\n",
      "        -2.0278e+00, -4.0849e-02, -8.6787e-01,  4.4492e-01,  1.0897e+00,\n",
      "         1.6611e+00,  1.4429e+00, -9.7495e-02, -3.7966e-01,  1.1340e+00,\n",
      "        -1.5088e+00,  2.2026e-01, -1.4007e+00, -4.5734e-01,  4.2130e-01,\n",
      "         1.3232e+00,  2.6465e-01,  1.3537e+00, -1.3900e+00, -2.4875e+00,\n",
      "         1.2673e+00,  5.7555e-01, -2.1108e+00, -5.8117e-01, -2.5357e+00,\n",
      "        -1.2094e+00,  6.3461e-01,  3.9256e-01,  1.3833e+00, -1.8831e+00,\n",
      "        -3.3387e-01,  2.4370e-01,  3.5087e-01,  1.6995e+00,  2.0144e-01,\n",
      "         1.8429e-01,  1.9022e+00, -5.3277e-01,  5.8695e-01, -1.4286e+00,\n",
      "         2.8170e-01,  4.7962e-01, -2.0891e-01,  1.0046e+00,  1.1880e+00,\n",
      "         1.2088e+00, -1.6832e-01, -4.3623e-01,  1.0383e+00,  8.2223e-01,\n",
      "        -1.6416e+00, -8.1712e-02, -7.5781e-01,  1.2580e-01,  5.9208e-01,\n",
      "         8.0565e-03,  3.6819e+00, -6.3554e-01,  2.3216e+00,  1.6150e+00,\n",
      "         7.0014e-01, -1.6208e+00,  1.3678e+00, -7.7582e-01,  1.8866e+00,\n",
      "         6.1480e-01, -4.2845e-01,  1.3879e+00,  2.7442e+00,  1.3787e+00,\n",
      "        -1.1612e+00,  1.9114e-01,  1.0322e+00,  1.4471e+00, -2.0150e+00,\n",
      "        -1.2081e+00, -1.6873e+00,  1.0484e+00,  3.3677e+00,  1.9660e+00,\n",
      "         1.7788e+00,  1.1646e+00, -2.4376e-01, -8.4299e-01,  8.3049e-01,\n",
      "        -1.1101e+00, -9.2795e-02,  2.6180e-01, -8.2222e-01,  1.0297e+00,\n",
      "        -1.6444e+00,  7.1387e-01,  6.8660e-01, -1.0988e-01,  1.6529e+00,\n",
      "        -1.5158e+00, -4.3734e-01, -1.8235e+00, -1.5728e+00,  1.0042e+00,\n",
      "         3.4797e-01,  1.0139e+00, -3.5572e-01,  7.6377e-01,  3.9941e-01,\n",
      "         1.0270e+00,  1.9283e+00, -1.6404e+00,  7.6733e-01, -2.3952e+00,\n",
      "        -1.3077e-01,  7.7512e-01, -2.0911e+00,  1.2564e+00, -6.4210e-01,\n",
      "        -3.7927e+00, -2.5017e+00, -1.8031e+00,  5.9953e-02,  6.2942e-01,\n",
      "        -6.7275e-03,  4.7943e-01,  5.9370e-01,  5.6984e-01, -2.0743e+00,\n",
      "        -3.5290e-01,  1.1877e+00, -2.6813e+00, -2.1480e+00,  1.8472e-01,\n",
      "         2.8368e-01,  1.1900e+00,  1.4279e+00,  8.6968e-01, -7.1791e-01,\n",
      "        -1.0211e+00,  1.0983e-02, -5.5658e-01,  1.4966e+00,  1.3973e+00,\n",
      "         1.5377e+00,  2.0254e+00, -1.0631e+00,  5.1958e-01,  1.4148e+00,\n",
      "         1.3902e+00,  1.6897e+00,  1.2369e+00, -1.0327e+00, -1.4862e-01,\n",
      "         3.3442e+00, -2.8397e+00, -7.8912e-01,  1.5516e+00,  1.7222e+00,\n",
      "        -1.4662e+00,  4.7772e-01,  8.8035e-01, -1.4375e-01, -1.3243e+00,\n",
      "        -2.4173e+00, -1.6072e-01, -6.8325e-01,  1.5090e+00, -1.0828e+00,\n",
      "         6.9646e-02, -1.8562e+00,  1.6974e+00, -1.1217e+00, -2.6640e+00,\n",
      "        -1.0777e+00,  1.3007e+00, -5.1615e-01,  2.2972e-01, -1.8937e-01,\n",
      "         7.4734e-02, -4.7596e-01,  1.0912e+00,  1.2817e+00,  1.4681e-01,\n",
      "         4.7600e-01, -1.3614e+00, -1.2438e+00, -1.3235e+00,  1.0516e+00,\n",
      "        -2.4935e+00,  2.7513e-03,  3.0851e-01,  2.5498e+00,  1.1299e-01,\n",
      "        -2.3261e+00,  1.3166e+00, -8.0148e-01,  1.3364e+00,  1.0277e+00,\n",
      "        -1.5782e+00, -1.7651e-01,  3.4251e-01, -3.5850e-01,  3.6337e+00,\n",
      "         1.5519e+00,  1.5573e-01,  2.7067e-01, -2.3431e-01, -1.2065e+00,\n",
      "         1.1937e+00,  9.9270e-01, -1.6324e-01, -1.2851e+00, -2.7104e+00,\n",
      "        -1.4960e+00, -2.0712e-01, -4.5474e-01,  1.3362e+00, -2.7098e-01,\n",
      "        -8.8962e-01,  1.8842e+00, -6.8723e-02, -1.4230e+00, -7.8616e-01,\n",
      "        -7.7977e-01, -1.6630e+00, -3.6468e-01, -7.2133e-01,  6.7899e-01,\n",
      "        -3.7837e-01, -1.0231e+00, -1.4453e-01, -2.1345e+00, -1.1335e+00,\n",
      "        -1.9118e+00, -2.0076e+00, -3.1718e-01, -1.6037e+00,  2.6781e-01,\n",
      "        -1.8052e+00, -1.7603e+00, -5.1618e-01, -9.0646e-02, -2.2542e-01,\n",
      "        -1.1308e+00, -1.2678e+00,  6.8780e-01,  4.4039e-01,  1.0888e-01,\n",
      "         3.5563e-01, -9.4008e-01, -6.8178e-01,  4.8622e-01, -4.3500e-02,\n",
      "        -4.9864e-01, -9.3692e-01,  7.0673e-01,  2.1743e-01, -1.6081e+00,\n",
      "        -8.4534e-03,  3.7933e-01, -1.1449e+00,  1.1312e+00,  1.0292e-01,\n",
      "        -1.3075e-01,  1.0818e+00,  1.1050e+00,  1.7846e+00,  6.6471e-01,\n",
      "        -2.0248e+00,  6.5793e-01, -1.1774e+00, -1.3919e+00, -1.2555e+00,\n",
      "         5.5659e-01, -1.9096e+00,  8.2016e-01,  7.1800e-01, -2.2232e+00,\n",
      "        -1.7170e+00,  1.1577e-01, -1.1068e+00, -1.8390e+00, -1.3000e+00,\n",
      "        -1.0974e+00, -2.2236e+00,  2.0206e+00,  7.4244e-01, -1.2532e+00,\n",
      "         8.7354e-01, -1.6217e+00, -8.4376e-01, -1.9935e+00, -8.5017e-01,\n",
      "        -7.0707e-02, -2.0065e-01,  8.3040e-01,  1.2434e+00,  1.3879e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-7.0290e-01, -7.8656e-01, -2.7595e+00, -1.0837e+00, -8.2986e-01,\n",
      "        -7.2450e-01, -5.2009e-01, -1.6701e+00, -8.5074e-01,  3.8261e-01,\n",
      "         4.5039e-01, -4.1621e-02,  9.9775e-01,  1.1332e+00,  1.7289e+00,\n",
      "         5.1244e-02, -3.3867e-01,  8.2132e-01,  7.8611e-01,  1.6348e+00,\n",
      "        -1.5655e+00, -1.2022e+00, -2.0377e+00,  1.4515e-01, -1.4679e+00,\n",
      "        -3.7767e-01, -3.5667e-01,  3.7524e-01,  4.1851e-01,  8.1905e-01,\n",
      "        -4.9907e-01,  1.8487e-01,  3.4817e-02, -1.1354e-01,  6.1372e-01,\n",
      "        -1.0825e+00,  5.1475e-01, -4.7030e-01,  2.0127e-02, -1.5893e-01,\n",
      "         2.9182e-01,  1.0156e+00,  1.2073e+00,  1.3351e-01,  4.8242e-01,\n",
      "         1.0639e+00,  9.1893e-01,  3.3357e-01,  1.4317e-01, -1.4028e+00,\n",
      "        -1.0428e+00, -2.6331e+00,  1.4343e+00,  1.7559e+00,  1.9568e+00,\n",
      "        -6.0097e-02,  8.6345e-01,  2.7506e-01,  3.0226e-01,  6.6663e-01,\n",
      "         1.3788e+00,  1.3915e+00,  2.0626e-01,  1.8225e+00,  5.4961e-01,\n",
      "         2.3759e-01,  1.3814e+00,  1.7786e+00,  1.2465e+00,  4.1262e-01,\n",
      "        -8.4562e-01,  1.5654e+00, -8.2439e-01, -4.6610e-01, -8.5427e-01,\n",
      "         4.6977e-01,  3.1195e+00,  1.1520e+00,  2.1066e+00,  7.3755e-01,\n",
      "        -1.4131e+00, -1.9826e+00,  3.7065e-01, -8.6157e-01, -7.9209e-01,\n",
      "         8.8360e-01, -9.0069e-01,  6.1961e-01, -6.2920e-01, -1.6101e+00,\n",
      "        -1.0290e+00, -8.1268e-01, -5.7291e-01, -9.4645e-01,  1.0068e-01,\n",
      "        -1.6568e+00, -1.1089e+00, -1.3739e+00, -1.6585e+00, -8.1636e-02,\n",
      "        -1.7087e+00, -1.6786e+00,  9.4812e-01,  1.4408e+00,  2.3173e+00,\n",
      "         1.1798e+00,  1.5907e+00, -2.2906e+00, -2.3914e+00, -9.5008e-01,\n",
      "        -1.2249e+00, -1.2095e+00,  1.0084e-01, -4.0085e-01,  8.2843e-01,\n",
      "        -1.5215e+00, -1.5842e+00, -1.4565e+00, -1.1521e-01, -5.6393e-01,\n",
      "        -5.3328e-01, -9.5258e-01, -1.2850e+00, -1.4317e+00,  2.7714e-01,\n",
      "         4.5848e-01, -9.8423e-01, -2.2264e+00, -2.1978e+00, -1.5248e+00,\n",
      "        -1.4193e+00, -1.2963e+00, -1.6435e+00, -1.6222e+00, -1.0291e+00,\n",
      "        -2.0524e+00, -2.1217e+00, -1.3458e+00, -7.4535e-01, -1.1621e+00,\n",
      "         8.1429e-01, -1.4212e+00, -1.1144e+00, -1.9308e+00, -9.7225e-01,\n",
      "        -4.3764e-01, -2.1740e+00, -2.9994e+00, -2.6438e+00, -2.1114e+00,\n",
      "        -4.8006e-01,  2.1164e+00, -1.4704e+00, -8.3896e-01,  2.6940e-01,\n",
      "        -5.4995e-01,  1.7467e-01,  2.5887e-01,  3.3468e-01,  5.6095e-01,\n",
      "        -1.4320e+00,  4.1675e-01,  1.2639e+00,  1.0197e-01,  1.4468e+00,\n",
      "        -7.1572e-02, -6.7782e-02, -1.0078e+00,  1.0757e+00, -6.2890e-01,\n",
      "        -7.6863e-01,  1.5908e+00,  2.2171e-01,  3.1564e-01,  1.3046e+00,\n",
      "        -1.4971e+00, -5.9937e-01, -7.2791e-02,  4.8210e-01,  1.5671e-01,\n",
      "         5.0297e-01, -5.1580e-01,  1.4089e+00, -6.0332e-01,  7.9001e-01,\n",
      "         7.5746e-01,  1.0240e+00,  4.3380e-02, -6.4331e-01,  1.2772e-01,\n",
      "        -1.9447e+00,  9.1204e-01,  3.6461e-01, -4.6839e-01, -2.1742e+00,\n",
      "         6.8870e-01, -1.8446e-01, -1.0229e+00, -1.0302e+00, -1.3270e-01,\n",
      "        -1.9026e+00, -2.7245e-01,  2.3690e-01,  4.6274e-01, -3.4129e-01,\n",
      "        -1.2826e-01,  4.1051e-03,  9.8535e-01,  1.6182e+00,  1.1460e+00,\n",
      "        -4.3268e-01,  3.8277e-01,  3.3734e-01, -1.0782e-01, -7.1609e-01,\n",
      "         1.2139e+00, -1.1559e+00, -2.7220e-01, -2.1610e-01,  3.0859e-01,\n",
      "        -1.4390e+00, -7.6053e-01, -1.8838e-01,  1.7313e+00,  3.9585e-01,\n",
      "         6.1762e-01, -1.0523e+00,  1.5269e+00, -1.9071e+00, -3.5907e-01,\n",
      "         8.2991e-01,  4.8499e-01,  4.5538e-01, -1.3682e+00,  6.6227e-01,\n",
      "         1.0847e+00,  5.0202e-01,  1.4740e+00,  5.9554e-01, -5.5590e-02,\n",
      "         9.2454e-01,  5.6564e-01, -1.3057e-02,  2.8747e-01, -1.4708e+00,\n",
      "         4.0520e-01, -3.3026e-01, -8.1988e-01,  7.0954e-01,  5.8432e-01,\n",
      "         1.0998e+00,  3.6622e-01, -8.9011e-01,  1.1333e+00,  1.1132e+00,\n",
      "        -1.5802e+00, -8.6378e-01, -1.9442e-01,  4.8667e-01,  1.4203e+00,\n",
      "         6.2834e-01, -1.5672e-01, -3.4333e-01,  1.8525e+00,  5.4941e-01,\n",
      "         8.3210e-01, -2.4605e-01, -7.1108e-01, -4.2901e-01,  2.9488e-01,\n",
      "        -1.1323e-01,  5.2213e-01,  7.2668e-01,  1.6667e+00,  8.1986e-03,\n",
      "        -2.4696e-01,  1.7409e+00,  7.4840e-01,  8.9065e-01,  4.3749e-01,\n",
      "         1.2994e+00,  2.2525e+00,  2.0862e+00,  9.5198e-01,  2.4952e+00,\n",
      "         3.0115e+00,  9.7203e-01,  1.8230e+00,  6.6387e-02, -5.0880e-01,\n",
      "         1.8811e-01, -3.2543e-02, -5.0935e-01,  2.2278e+00, -9.4425e-01,\n",
      "        -2.3544e-01, -7.8790e-01, -2.3003e-01,  2.6654e+00,  2.9848e+00,\n",
      "         5.9330e-01,  6.3885e-01,  1.1663e+00,  8.2981e-01,  7.1554e-02,\n",
      "         2.1937e+00,  2.2620e+00, -6.1098e-02, -2.2871e-01, -4.6312e-01,\n",
      "         9.9521e-01,  4.4093e-02,  4.8667e-01,  1.4171e+00,  1.6370e+00,\n",
      "         6.9404e-01, -8.9115e-02, -4.6737e-01, -1.1697e+00, -4.4517e-01,\n",
      "         4.6966e-01, -1.8338e+00, -1.5051e+00, -1.1217e+00, -7.8135e-01,\n",
      "        -1.9219e+00, -7.1657e-01,  4.4740e-01, -2.3910e-01, -9.5151e-01,\n",
      "         1.2558e+00,  4.1551e-01,  7.1251e-02,  1.8824e+00,  8.5200e-01,\n",
      "         4.8473e-01,  8.5047e-01,  9.1628e-01,  1.2124e+00, -1.4379e+00,\n",
      "         1.0398e-01,  5.6096e-01, -2.8634e-01, -1.1034e-01, -1.1450e+00,\n",
      "        -1.6600e+00, -1.0851e+00, -1.4091e+00, -1.8172e+00, -1.5875e+00,\n",
      "        -9.5895e-01, -6.4078e-01, -2.5077e-01,  7.8852e-01, -4.7940e-01,\n",
      "        -5.1557e-02,  1.6288e+00,  4.0699e-01,  1.2354e+00,  1.3610e+00,\n",
      "         1.0302e+00,  1.6994e+00,  1.6293e+00,  7.1736e-01, -2.6555e-01,\n",
      "         1.3498e-01, -8.5487e-02, -3.9696e-01,  5.1787e-01, -3.6916e-01,\n",
      "         9.2836e-01,  2.9092e-01,  1.6212e+00,  1.1523e+00,  1.7153e+00,\n",
      "        -1.3348e-01,  1.1509e+00,  1.0400e+00,  7.8072e-01, -2.2328e-01,\n",
      "         5.1699e-01,  5.2038e-01,  1.0955e+00,  1.2080e+00,  7.8235e-01,\n",
      "        -1.5680e+00, -1.6279e+00,  2.4048e-01,  4.8050e-01, -1.4485e+00,\n",
      "        -2.1749e-01, -7.6321e-01, -2.1321e+00, -2.6876e+00, -1.1662e+00,\n",
      "        -4.3775e-01, -2.2681e+00,  3.3327e-01, -1.6307e-01,  8.0591e-02,\n",
      "        -1.3643e+00,  1.7999e+00,  7.8606e-01, -2.6205e+00, -1.6180e+00,\n",
      "        -1.5371e+00, -8.1080e-01, -6.7353e-01, -2.1644e+00,  4.1844e-01,\n",
      "        -1.5232e-02, -8.5801e-01,  1.0688e+00,  1.4592e+00,  6.1221e-01,\n",
      "        -7.4588e-01,  1.5584e-02, -1.3580e+00,  1.3338e+00,  3.5200e+00,\n",
      "         1.1542e+00, -6.7891e-01, -1.1920e-01,  4.0411e-01, -1.9884e-01,\n",
      "        -1.7908e+00, -1.9440e-01, -5.4727e-01,  4.4674e-01,  7.6518e-01,\n",
      "        -1.0286e+00, -7.7544e-01,  5.7334e-01, -6.7769e-01,  1.0216e+00,\n",
      "         1.0473e-01, -3.4613e-01, -6.0227e-01,  1.8802e-01, -8.0962e-01,\n",
      "         2.8845e-01,  4.8020e-01, -1.0822e+00,  7.7361e-01, -1.3620e+00,\n",
      "        -1.1524e+00,  1.6139e+00,  1.9071e+00,  7.4881e-01, -1.5289e+00,\n",
      "        -2.0930e+00, -1.2164e-01, -7.1823e-02, -7.2799e-01, -1.0566e+00,\n",
      "         1.7590e+00, -8.2885e-01,  8.1913e-01, -8.8732e-02, -1.0952e+00,\n",
      "        -5.8192e-01, -6.7339e-01, -3.4982e-01, -2.4843e-01,  1.7820e+00,\n",
      "        -1.4147e-01, -1.3585e+00, -1.3607e+00, -5.7426e-01,  2.3941e-01,\n",
      "         1.2144e+00, -5.8276e-01, -1.5711e+00,  1.2864e+00,  4.7744e-01,\n",
      "         2.1845e+00, -2.0929e+00,  4.9297e-01,  1.4713e+00, -6.2140e-01,\n",
      "        -3.7426e-01,  9.4830e-01,  9.9750e-01, -1.3858e+00, -1.8656e+00,\n",
      "         1.6069e+00, -1.2206e-01,  3.3957e+00,  1.6829e-01,  2.8777e-01,\n",
      "        -2.3956e-01, -5.3764e-01,  6.2322e-01, -1.8662e+00, -3.8152e-01,\n",
      "        -1.6883e+00, -4.3764e-01, -1.1525e+00, -2.5062e-01,  4.0562e-01,\n",
      "        -1.0990e-01, -3.5267e-01,  3.9174e-01, -1.2672e-01,  2.8720e+00,\n",
      "         1.1767e+00, -1.2427e+00,  4.1690e-01,  3.1740e+00, -2.0502e+00,\n",
      "        -1.6558e+00, -3.8730e-01,  9.0969e-01, -3.1160e-01,  8.0464e-01,\n",
      "         1.8957e+00, -5.6320e-01, -1.0358e+00, -1.2706e-01,  7.9255e-02,\n",
      "        -5.1214e-01,  2.2109e-01, -1.0137e+00,  1.0033e+00, -1.1084e+00,\n",
      "        -1.6217e+00,  9.4361e-01,  1.5304e+00,  7.6524e-01,  4.2240e-01,\n",
      "         8.4103e-01,  1.1285e+00,  6.9909e-01,  9.6803e-01, -1.6056e+00,\n",
      "        -6.1849e-01, -1.2288e+00, -2.1378e+00, -1.6230e+00,  3.0674e+00,\n",
      "        -1.9205e+00,  5.5703e-01,  1.2499e+00,  1.5659e+00,  8.3551e-02,\n",
      "        -1.2436e-01,  7.8595e-01, -3.0640e+00, -4.0716e-01,  3.2778e-01,\n",
      "         4.4484e-01,  2.0154e+00, -1.2864e-01,  2.0135e-01, -2.3241e+00,\n",
      "        -2.0130e+00,  8.1713e-01, -7.9706e-01,  7.7463e-01,  1.1980e+00,\n",
      "        -1.1906e+00,  2.6933e-01, -1.3905e+00,  1.5797e+00, -2.8800e-01,\n",
      "        -6.0803e-01, -1.0474e+00, -2.2590e-01,  8.0357e-01, -1.5498e+00,\n",
      "        -7.2284e-02, -1.1747e-01,  1.9423e-01,  6.7049e-01,  1.1284e+00,\n",
      "         7.5509e-01, -1.9546e+00, -7.7023e-01, -2.8604e+00, -7.1994e-01,\n",
      "        -8.5893e-01, -1.0873e+00, -1.4941e+00,  8.9240e-01,  9.5894e-02,\n",
      "         7.2317e-01, -1.9032e-01,  1.9625e+00,  1.0954e+00,  1.3237e+00,\n",
      "         1.8379e+00,  1.8592e-01,  1.0810e+00,  1.5584e+00, -3.0725e-01,\n",
      "        -5.7832e-01,  8.8472e-01,  1.3304e+00,  1.0824e+00,  1.0642e+00,\n",
      "         1.7148e+00, -2.1626e+00, -4.9565e-01, -1.1322e+00,  6.8797e-01,\n",
      "         2.6008e+00,  1.5165e+00, -9.9733e-01,  6.8879e-01, -2.6116e-01,\n",
      "        -1.3991e-01,  2.7606e+00, -2.0624e+00,  2.3979e+00, -5.6615e-01,\n",
      "         6.9123e-01, -1.9324e-01, -6.2688e-01,  4.8760e-01,  1.3346e+00,\n",
      "         3.8474e+00,  3.8520e-01,  3.1376e+00,  7.7327e-01, -8.3720e-01,\n",
      "        -2.3357e+00,  4.6024e+00, -5.1399e-01, -1.9497e+00,  1.1325e+00,\n",
      "         8.8311e-01,  1.0698e+00,  2.3280e+00,  2.3562e+00, -5.7385e-02,\n",
      "        -5.1194e-02,  1.5410e-01, -6.5946e-01, -1.1552e+00, -8.8387e-01,\n",
      "         3.9900e-01, -7.7353e-01, -6.1296e-01, -1.6160e-01,  1.2457e+00,\n",
      "        -3.2856e+00, -4.2716e-02,  2.5266e-03, -6.4626e-01, -2.8914e-01,\n",
      "         2.3654e+00, -6.7234e-01,  1.0720e+00,  1.1690e-01, -2.7927e-02,\n",
      "        -4.8803e-01,  3.7029e-01, -1.1099e+00,  1.3415e+00, -6.1556e-01,\n",
      "        -7.5327e-01, -1.6796e+00,  2.2205e+00, -2.3551e+00,  1.6326e+00,\n",
      "        -1.0145e-01,  1.5178e+00,  2.7909e-01, -1.6129e+00,  1.3458e-01,\n",
      "         1.6082e-01, -4.3357e-01, -1.0040e+00,  3.4038e+00,  3.2718e+00,\n",
      "        -8.2683e-01, -4.1714e-01,  2.7118e+00,  1.5463e+00,  2.2125e-01,\n",
      "         3.5918e-01,  2.9403e+00,  6.1788e-02,  5.5759e-01,  8.6864e-01,\n",
      "        -6.4171e-01,  1.3497e+00, -1.8245e+00,  1.4670e-01, -2.4080e+00,\n",
      "        -1.9905e+00, -8.6800e-01,  6.7166e-01, -1.2814e+00, -2.7365e+00,\n",
      "         1.1900e+00,  8.8859e-01, -1.6450e-01, -2.3336e+00,  2.0507e-01,\n",
      "         1.5982e+00, -1.1625e+00, -6.0793e-01,  1.5247e+00,  6.0279e-01,\n",
      "        -1.5771e+00,  2.4689e-01, -7.8538e-01,  1.6412e-02,  1.4364e+00,\n",
      "         1.4099e+00,  7.9047e-01, -1.9852e-01,  1.2138e-01,  1.2456e+00,\n",
      "        -1.3878e+00, -4.2065e-01, -1.3282e+00, -7.5182e-01,  5.2057e-01,\n",
      "         1.2232e+00,  2.6368e-01,  1.1054e+00, -1.6657e+00, -2.5452e+00,\n",
      "         7.0016e-01,  1.5154e-01, -1.8974e+00, -8.5613e-01, -2.5148e+00,\n",
      "        -1.3181e+00,  3.9106e-01,  1.0240e+00,  8.7166e-01, -1.0968e+00,\n",
      "        -1.9146e-01, -3.2632e-02,  1.6749e-01,  1.3695e+00,  2.9098e-01,\n",
      "         8.4453e-01,  1.4012e+00,  4.6886e-01,  6.7168e-01, -9.4217e-01,\n",
      "         8.7703e-01,  7.5407e-01,  8.9811e-03,  1.3165e+00,  6.6624e-01,\n",
      "         1.0794e+00, -1.9977e-01, -4.1670e-01,  8.8257e-01,  1.4750e+00,\n",
      "        -1.6304e+00,  6.0655e-02, -2.1623e-01,  4.1979e-01,  1.2914e+00,\n",
      "         4.4603e-02,  4.5295e+00, -4.9513e-01,  2.5164e+00,  2.1843e+00,\n",
      "         4.9959e-01, -1.0850e+00,  1.3171e+00, -8.1299e-01,  1.3302e+00,\n",
      "         6.3250e-01, -3.7620e-01,  1.2921e+00,  2.2151e+00,  1.1753e+00,\n",
      "        -1.1885e+00,  1.3745e-01,  6.0454e-01,  1.7163e+00, -1.7706e+00,\n",
      "        -1.4105e+00, -1.2715e+00,  1.5756e+00,  2.8580e+00,  2.0962e+00,\n",
      "         2.4254e+00,  1.2827e+00, -4.5900e-01, -8.4552e-01,  5.9302e-01,\n",
      "        -8.9745e-01, -2.2734e-01,  9.1730e-02, -3.8688e-01,  8.7995e-01,\n",
      "        -1.9192e+00,  1.0962e+00,  1.0951e+00,  3.8611e-02,  1.5166e+00,\n",
      "        -1.2306e+00,  5.8488e-02, -1.6312e+00, -1.6683e+00,  1.1870e+00,\n",
      "         2.9648e-01,  8.3756e-01,  2.0729e-01,  4.1967e-01,  7.3605e-01,\n",
      "         2.0525e+00,  2.1439e+00, -1.4424e+00,  1.0222e+00, -1.9525e+00,\n",
      "        -9.2423e-01,  3.6842e-01, -1.8446e+00,  1.0169e+00, -3.5290e-01,\n",
      "        -3.2970e+00, -2.5374e+00, -1.5284e+00,  5.5399e-01,  5.1970e-01,\n",
      "        -8.6219e-02,  9.5715e-01,  6.3104e-01,  4.5892e-01, -1.9323e+00,\n",
      "         1.7768e-01,  1.3751e+00, -3.0055e+00, -1.7611e+00,  4.7902e-01,\n",
      "        -1.2967e-01,  1.3287e+00,  1.6631e+00,  1.3088e+00, -1.1028e+00,\n",
      "        -1.0442e+00,  3.4019e-01,  2.0585e-02,  1.3638e+00,  1.6567e+00,\n",
      "         1.4324e+00,  1.7317e+00, -6.2347e-01,  1.2634e+00,  1.1424e+00,\n",
      "         1.4031e+00,  1.9459e+00,  1.1721e+00, -1.5051e+00, -7.1278e-01,\n",
      "         2.9062e+00, -2.5649e+00, -1.4429e+00,  1.1463e+00,  1.9624e+00,\n",
      "        -1.5622e+00,  1.0054e+00,  8.1739e-01, -4.9262e-01, -1.0309e+00,\n",
      "        -2.0572e+00,  8.3713e-02, -3.0370e-01,  1.0179e+00, -9.6286e-01,\n",
      "         7.0756e-02, -1.7295e+00,  2.1926e+00, -1.3455e+00, -2.3439e+00,\n",
      "        -1.0958e+00,  1.1885e+00, -5.8339e-01,  9.4315e-01, -6.1114e-01,\n",
      "         7.1386e-02, -7.5460e-01,  1.5466e+00,  6.2921e-01, -5.5783e-01,\n",
      "         4.6861e-02, -8.6298e-01, -1.6647e+00, -1.7617e+00,  1.0276e+00,\n",
      "        -2.7159e+00,  1.1897e+00,  3.2161e-01,  2.5886e+00, -8.7821e-02,\n",
      "        -2.1504e+00,  1.0120e+00, -8.4461e-01,  1.5131e+00,  1.0341e+00,\n",
      "        -1.7422e+00, -6.4428e-01,  5.0219e-01, -3.9443e-01,  3.2976e+00,\n",
      "         1.2528e+00,  3.5280e-01, -1.6535e-01, -6.2649e-01, -5.6949e-01,\n",
      "         6.9155e-01,  7.8575e-01, -3.6060e-01, -1.4606e+00, -2.5834e+00,\n",
      "        -1.7146e+00, -6.5264e-02, -3.0807e-01,  1.7341e+00, -2.4980e-01,\n",
      "        -8.2817e-01,  1.6319e+00, -2.6510e-01, -1.0901e+00, -2.3284e-01,\n",
      "        -5.2373e-01, -1.0068e+00, -3.1662e-01, -5.9897e-01,  1.0003e+00,\n",
      "        -3.2019e-01, -6.6591e-01, -3.7498e-01, -1.5617e+00, -6.5600e-01,\n",
      "        -1.2832e+00, -1.8242e+00,  2.3697e-01, -1.0888e+00,  8.6895e-01,\n",
      "        -1.7085e+00, -1.3905e+00, -5.2107e-01,  9.5444e-02, -1.9614e-01,\n",
      "        -1.2441e+00, -1.5567e+00,  3.8159e-01,  3.0903e-01, -1.4800e-01,\n",
      "         1.5984e-01, -1.0057e+00, -6.8956e-01,  2.8601e-01, -4.6335e-01,\n",
      "        -3.9217e-01, -4.9206e-01,  1.0375e-01, -1.4960e-01, -1.1857e+00,\n",
      "        -1.5394e-01,  6.4625e-01, -7.5854e-01,  1.6195e+00,  6.6893e-01,\n",
      "         5.1749e-01,  4.7768e-01,  1.2964e+00,  1.7532e+00,  3.0407e-01,\n",
      "        -2.1301e+00,  2.2361e-01, -1.3008e+00, -1.1185e+00, -1.1806e+00,\n",
      "         2.6048e-01, -1.9056e+00,  8.2063e-01,  3.3599e-01, -2.3569e+00,\n",
      "        -2.0221e+00,  3.5198e-01, -1.0856e+00, -1.4547e+00, -1.3525e+00,\n",
      "        -1.6494e+00, -2.8505e+00,  1.9030e+00,  2.5213e-01, -1.7899e+00,\n",
      "         2.1641e-02, -1.5496e+00, -1.0756e+00, -1.6093e+00, -1.0405e+00,\n",
      "        -5.9231e-02, -1.5648e-01,  7.3201e-01,  1.1115e+00,  1.2615e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-1.2241e+00, -1.7927e-01, -2.6039e+00, -1.2960e+00, -5.9373e-01,\n",
      "        -7.2172e-01, -3.4179e-01, -7.4332e-01, -2.2521e-01,  8.0996e-01,\n",
      "         7.1469e-01,  9.1674e-01,  1.4480e+00,  1.3281e+00,  1.5592e+00,\n",
      "         4.4380e-01,  2.5048e-01,  7.1483e-01,  6.0853e-01,  1.8306e+00,\n",
      "        -1.1936e+00, -5.5693e-01, -1.3435e+00,  3.2779e-01, -6.5578e-01,\n",
      "        -1.6833e-01,  1.1786e-01,  6.6930e-01, -5.0179e-02,  8.6102e-01,\n",
      "        -1.2646e-01,  1.2689e+00,  4.0254e-01, -4.9398e-01,  3.0657e-01,\n",
      "        -9.4132e-01,  4.9550e-01, -3.7000e-01,  7.8470e-01,  2.8994e-01,\n",
      "         1.4153e+00,  9.1424e-01,  1.6773e+00,  8.1524e-01,  8.9037e-01,\n",
      "         1.2130e+00,  1.9728e+00,  8.4366e-01,  6.5766e-01, -9.7863e-01,\n",
      "        -7.5363e-01, -2.2288e+00,  1.7763e+00,  1.5119e+00,  1.6976e+00,\n",
      "         1.0672e+00,  7.7887e-01,  4.0232e-01,  8.7491e-01,  1.6405e+00,\n",
      "         2.1096e+00,  1.8146e+00,  3.5685e-01,  2.3254e+00,  1.5476e+00,\n",
      "         4.2506e-01,  1.7312e+00,  1.7761e+00,  1.1757e+00,  8.0749e-01,\n",
      "         2.4505e-02,  1.8279e+00, -5.4339e-01,  6.5464e-01, -1.3790e-01,\n",
      "         7.0647e-01,  3.0706e+00,  1.9207e+00,  2.9723e+00,  1.6119e+00,\n",
      "        -1.0977e+00, -1.3228e+00,  7.5992e-01, -6.5116e-01,  1.6824e-02,\n",
      "         1.2392e+00, -2.6865e-01,  1.6220e-01, -2.9759e-01, -1.3029e+00,\n",
      "        -5.9032e-01, -2.8210e-01,  1.9405e-01, -7.9700e-01,  1.5160e+00,\n",
      "        -1.2370e+00, -4.8353e-01, -9.0765e-01, -1.7069e+00,  9.8108e-02,\n",
      "        -1.0195e+00, -1.5693e+00,  8.6979e-01,  1.6999e+00,  1.7975e+00,\n",
      "         9.6837e-01,  1.5491e+00, -1.5797e+00, -1.3562e+00, -1.3379e+00,\n",
      "        -4.6507e-01,  4.9354e-01,  6.9857e-01,  1.2413e+00,  1.9415e+00,\n",
      "        -1.0396e+00, -9.6526e-01, -8.3055e-01, -6.4544e-01, -6.9667e-01,\n",
      "        -8.8847e-02, -1.7319e+00, -1.6968e+00, -1.5862e+00,  4.2729e-01,\n",
      "         1.0212e+00, -3.4812e-01, -1.3418e+00, -1.5189e+00, -4.2649e-01,\n",
      "        -7.5073e-01, -1.0761e+00, -1.4221e+00, -6.3139e-01, -7.6540e-01,\n",
      "        -1.1772e+00, -1.5377e+00, -1.0905e+00, -2.8063e-01, -1.0301e+00,\n",
      "        -5.1878e-02, -7.7906e-01, -9.2701e-01, -1.6344e+00, -5.4818e-01,\n",
      "        -1.2665e+00, -2.1368e+00, -3.0927e+00, -2.9459e+00, -2.3117e+00,\n",
      "        -5.4909e-01,  2.0550e+00, -1.7046e+00, -5.6568e-01,  4.3988e-02,\n",
      "        -3.9814e-01,  1.4151e-01,  1.6103e-01,  9.3119e-02,  7.3633e-01,\n",
      "        -1.6263e+00,  6.1071e-01,  1.1125e+00,  9.8118e-02,  1.0083e+00,\n",
      "        -3.3253e-01,  5.1758e-02, -9.0174e-01,  9.1749e-01, -7.2166e-01,\n",
      "        -6.9462e-01,  1.7119e+00,  3.2370e-01,  3.2890e-01,  7.6745e-01,\n",
      "        -1.7044e+00, -5.3265e-01, -1.9571e-01,  9.1252e-01, -2.1892e-02,\n",
      "         1.4580e-01, -5.9859e-01,  1.4072e+00, -6.5738e-01,  9.0425e-01,\n",
      "         1.0407e+00,  1.0095e+00,  4.0132e-01, -5.5796e-01, -7.7642e-02,\n",
      "        -1.8218e+00,  8.7065e-01,  3.9925e-01, -6.1631e-01, -2.1621e+00,\n",
      "         8.9007e-01,  1.2201e-01, -1.2908e+00, -9.1522e-01, -1.5011e-01,\n",
      "        -2.0090e+00,  1.4277e-03,  2.1749e-01,  3.1544e-01, -4.3829e-01,\n",
      "        -5.8089e-01, -5.2990e-01,  6.9353e-01,  1.1747e+00,  1.1553e+00,\n",
      "        -4.3352e-01,  5.9010e-01,  1.1701e-01,  1.3803e-01, -9.1454e-01,\n",
      "         7.7111e-01, -1.3502e+00, -2.7832e-01, -2.0686e-02,  3.0285e-01,\n",
      "        -1.4093e+00, -6.9555e-01, -8.2035e-01,  1.3949e+00,  3.0439e-01,\n",
      "         6.0942e-01, -1.6200e+00,  9.0768e-01, -2.0710e+00, -5.9165e-01,\n",
      "         4.8398e-01,  8.2683e-02,  1.6139e-01, -1.6996e+00,  4.3224e-01,\n",
      "         7.9496e-01,  5.7180e-01,  1.2033e+00,  1.8733e-01, -2.8282e-01,\n",
      "         6.6386e-01,  2.6853e-01,  1.5239e-01, -1.4473e-01, -2.0966e+00,\n",
      "         2.0126e-01, -5.8607e-01, -1.0371e+00,  4.5411e-01,  2.8025e-01,\n",
      "         8.6355e-01,  2.1101e-01, -1.2910e+00,  9.8156e-01,  7.4805e-01,\n",
      "        -1.7912e+00, -1.4095e+00, -7.5168e-01, -1.3960e-01,  1.0381e+00,\n",
      "         4.0381e-02, -5.4777e-01, -8.3406e-02,  1.4555e+00,  4.4434e-01,\n",
      "         7.8727e-01, -5.0587e-01, -9.2382e-01, -8.8158e-01,  4.3567e-01,\n",
      "        -2.0906e-01,  9.5044e-01,  1.0560e+00,  1.3537e+00,  3.7195e-01,\n",
      "         1.1079e-01,  1.2937e+00,  1.5422e+00,  1.2031e+00,  6.6067e-01,\n",
      "         1.6164e+00,  2.2505e+00,  1.9331e+00,  9.8268e-01,  2.5076e+00,\n",
      "         2.8685e+00,  1.1741e+00,  1.4382e+00,  5.6991e-01, -2.5565e-01,\n",
      "         2.3611e-01, -3.5656e-02, -3.2059e-01,  2.2863e+00, -7.9683e-01,\n",
      "        -6.2625e-01, -1.2403e+00, -3.1434e-01,  3.3846e+00,  3.4737e+00,\n",
      "         4.7249e-01,  2.1284e+00,  7.1316e-01,  1.0498e+00,  1.0637e+00,\n",
      "         1.6229e+00,  2.4201e+00,  9.5321e-01,  9.2478e-01,  1.1575e+00,\n",
      "         2.3025e+00,  1.2104e+00,  1.5932e+00,  2.0534e+00,  2.2090e+00,\n",
      "         2.0502e+00,  1.2903e+00,  1.2964e+00,  9.3394e-01,  1.0516e+00,\n",
      "         1.0035e+00, -5.3653e-01, -6.4407e-02, -5.0329e-01,  8.3781e-01,\n",
      "        -1.0287e-01,  3.3459e-01,  1.1556e+00,  4.0348e-01, -1.3213e+00,\n",
      "         1.5345e+00,  9.0652e-01, -1.8914e-01,  2.0221e+00,  1.3370e+00,\n",
      "         1.0325e+00,  1.1959e+00,  8.3117e-01,  1.3051e+00, -1.5840e+00,\n",
      "         1.4872e-02,  5.9253e-01, -2.5636e-01, -4.4488e-02, -1.3572e+00,\n",
      "        -1.5628e+00, -1.0037e+00, -1.5976e+00, -1.7224e+00, -1.2434e+00,\n",
      "        -7.0703e-01, -1.0693e+00, -3.5759e-01,  6.0986e-01, -3.2482e-01,\n",
      "        -6.0935e-01,  2.3882e+00,  9.2148e-01,  2.0597e+00,  1.8612e+00,\n",
      "         1.1014e+00,  1.9099e+00,  2.2562e+00,  1.5998e+00,  5.5333e-01,\n",
      "         4.9628e-01, -7.4646e-02,  1.5471e-01,  5.1276e-01, -3.1831e-01,\n",
      "         9.5397e-01,  2.0023e-01,  1.6123e+00,  1.5330e+00,  1.6717e+00,\n",
      "        -1.5725e-01,  8.7965e-01,  1.7699e+00,  1.1673e+00,  1.5150e-01,\n",
      "         1.0790e+00,  1.2592e+00,  2.1022e+00,  1.4640e+00,  6.1831e-01,\n",
      "        -8.4705e-01, -1.6952e+00,  6.6107e-01,  1.7543e-01, -1.1821e+00,\n",
      "         5.0342e-01, -9.8622e-01, -1.9574e+00, -2.1043e+00, -1.6975e+00,\n",
      "        -5.1035e-01, -1.7630e+00,  3.4525e-01, -7.3033e-02,  1.1384e-01,\n",
      "        -1.6255e+00,  1.2804e+00,  8.5734e-01, -2.3429e+00, -1.7882e+00,\n",
      "        -9.9845e-01, -9.5427e-01, -1.5156e+00, -3.2460e+00,  2.3728e-01,\n",
      "         5.1397e-01, -1.0724e+00,  6.6502e-01, -1.3942e-01, -7.6536e-01,\n",
      "        -1.0433e+00,  2.1983e-01, -7.4627e-01,  1.2766e+00,  3.7953e+00,\n",
      "         5.2105e-01,  2.9465e-01, -5.0472e-01, -2.8131e-01, -7.3579e-01,\n",
      "        -8.4158e-01, -5.0832e-01, -3.8909e-01,  2.6495e-01,  1.3855e+00,\n",
      "        -6.1898e-01, -1.4073e+00,  1.4010e-01, -1.1020e+00,  1.2512e+00,\n",
      "        -1.5258e-01, -9.8030e-01, -2.1658e-01, -1.3914e-03, -1.2731e+00,\n",
      "         9.4613e-01,  6.5355e-01, -4.3911e-01,  6.1748e-01, -1.7294e+00,\n",
      "        -1.0452e+00,  1.5473e+00,  9.7031e-01,  1.7419e+00, -1.6877e+00,\n",
      "        -2.5315e+00, -5.2767e-01, -1.3354e-02, -8.1295e-01, -1.6973e+00,\n",
      "         1.9861e+00, -8.1041e-01,  4.6409e-01, -5.6921e-01, -6.2820e-01,\n",
      "        -9.0606e-01, -5.8636e-01,  5.0910e-01,  1.9434e-01,  1.5643e+00,\n",
      "        -1.1057e+00, -1.9071e+00, -1.6454e+00, -3.7051e-01, -7.5079e-01,\n",
      "         1.4891e+00, -1.1425e+00, -1.3843e+00,  9.7236e-01,  6.7563e-01,\n",
      "         1.4062e+00, -1.8932e+00, -5.4725e-01,  1.4044e+00, -6.0795e-01,\n",
      "        -1.3695e+00,  4.7818e-01, -2.5332e-01, -9.7750e-01, -2.1138e+00,\n",
      "         2.5985e-01, -2.1803e-01,  2.0056e+00,  9.0610e-01,  6.5783e-01,\n",
      "         1.9334e-01, -6.6759e-01,  1.0950e+00, -1.5209e+00,  3.1260e-01,\n",
      "        -1.5916e+00,  1.6269e-01, -5.8550e-01, -3.7832e-01,  3.4399e-01,\n",
      "        -2.7488e-01,  1.8385e-01,  1.0583e+00,  2.3904e-01,  1.6659e+00,\n",
      "         6.9714e-01,  8.7397e-02,  3.8305e-01,  1.7835e+00, -2.0243e+00,\n",
      "        -2.2043e+00, -1.2917e+00,  1.2032e+00,  6.5744e-02,  5.1129e-01,\n",
      "         1.8246e+00, -5.2101e-01, -8.2994e-01, -6.6644e-01,  3.2192e-01,\n",
      "        -1.0872e+00, -9.6324e-01, -2.6643e-01,  6.5273e-01, -7.2930e-01,\n",
      "        -1.6891e+00, -1.7673e-01,  1.4206e-01,  4.1947e-01, -3.9263e-03,\n",
      "         7.8525e-01,  4.9576e-01,  1.1199e-02,  6.8599e-01, -1.5471e+00,\n",
      "        -1.1671e+00, -2.1103e+00, -2.1066e+00, -1.3004e+00,  3.1766e+00,\n",
      "        -1.9563e+00,  2.4288e-01,  1.4257e+00,  1.1699e+00, -1.2044e+00,\n",
      "         1.0081e-01,  3.3387e-01, -3.2897e+00, -1.4539e+00,  8.9803e-01,\n",
      "        -9.4751e-01,  1.1462e+00,  3.1735e-01, -2.2998e-02, -2.5637e+00,\n",
      "        -1.8810e+00,  4.7119e-01, -6.4178e-02,  7.4416e-01,  5.6115e-01,\n",
      "        -1.4970e+00, -7.1219e-01, -6.9401e-01,  1.5685e+00, -5.1788e-01,\n",
      "        -9.7684e-01, -5.6794e-01, -7.4278e-01,  6.8170e-01, -2.3066e+00,\n",
      "        -3.5965e-01, -1.3353e+00,  4.4896e-01, -4.4034e-01,  1.4684e+00,\n",
      "        -1.1167e+00, -2.2695e+00, -5.7024e-01, -1.9563e+00, -4.2371e-01,\n",
      "        -5.0215e-01, -1.2555e+00, -1.6920e+00,  1.3145e+00,  5.7364e-01,\n",
      "         6.4184e-01, -1.0298e+00,  2.0696e+00,  1.1465e+00,  6.6047e-01,\n",
      "        -1.9210e-01,  2.8433e-01, -3.1874e-02,  7.8474e-01,  3.8699e-01,\n",
      "        -9.7089e-01,  1.0293e+00,  5.6021e-01, -5.3609e-01,  1.6440e+00,\n",
      "         2.2847e+00, -1.6461e+00, -2.6360e-01, -1.9499e+00,  1.4965e+00,\n",
      "         1.7518e+00,  1.7305e+00, -7.2149e-01,  7.1906e-01, -1.2257e+00,\n",
      "        -3.8660e-01,  2.3337e+00, -2.6294e+00,  1.0862e+00, -4.3952e-01,\n",
      "        -3.2171e-03,  7.2513e-01, -1.4015e+00,  5.8703e-01,  2.2781e+00,\n",
      "         2.0955e+00, -7.4879e-02,  2.5771e+00,  1.0729e+00, -1.3271e+00,\n",
      "        -3.2982e+00,  4.1455e+00, -1.4731e+00, -1.8349e+00,  9.7940e-01,\n",
      "         1.2139e+00,  6.6492e-01,  9.1859e-01,  1.7942e+00, -4.5287e-01,\n",
      "        -1.4590e-01,  1.6503e-02, -5.6490e-01, -1.2837e+00, -1.1230e+00,\n",
      "         2.6180e-02, -1.6810e-01, -8.1405e-01,  4.5484e-01,  2.3841e+00,\n",
      "        -2.3716e+00,  2.1106e-01, -1.4904e-01, -6.1355e-01, -7.9919e-02,\n",
      "         2.1650e+00, -9.2658e-01, -2.9937e-01,  2.1355e-01, -1.4853e+00,\n",
      "        -1.8970e-01, -8.1250e-01, -1.5001e+00,  1.4049e+00, -9.2464e-01,\n",
      "        -1.3707e+00, -2.2181e+00,  6.8537e-01, -2.0056e+00,  7.7129e-01,\n",
      "        -5.6742e-01,  1.2735e+00, -2.5020e-02, -1.3573e+00,  4.5835e-01,\n",
      "        -4.4096e-01, -5.4722e-01, -1.0757e+00,  2.5098e+00,  3.7315e+00,\n",
      "        -2.0474e+00, -6.6067e-01,  3.8487e+00,  2.7999e-01,  6.7952e-01,\n",
      "         5.5663e-01,  1.5745e+00,  6.4313e-01,  1.1253e-01,  5.0197e-01,\n",
      "        -1.4514e+00,  2.7919e-01, -1.7864e+00, -6.5126e-01, -2.0210e+00,\n",
      "        -2.5425e+00, -1.4511e+00,  6.1300e-01, -1.1675e+00, -2.7167e+00,\n",
      "         1.5583e+00,  1.4588e+00, -7.6355e-01, -1.9706e+00, -7.5928e-02,\n",
      "         1.8166e+00, -6.5257e-01, -4.7651e-01,  1.4992e+00,  4.7427e-01,\n",
      "        -2.3436e+00, -8.8639e-01, -1.1823e+00,  9.8415e-02,  1.0383e+00,\n",
      "         1.6730e+00,  1.5969e+00, -4.7767e-02, -6.0513e-01,  1.2085e+00,\n",
      "        -1.7075e+00,  5.4556e-01, -1.6279e+00, -5.5207e-01,  7.2413e-01,\n",
      "         1.5475e+00,  2.0841e-01,  1.7008e+00, -7.2104e-01, -2.6846e+00,\n",
      "         1.2805e+00,  2.5943e-01, -2.0428e+00, -4.4077e-01, -2.4808e+00,\n",
      "        -1.1036e+00,  6.4115e-01, -1.2842e-01,  1.3772e+00, -2.3557e+00,\n",
      "         2.4481e-01,  1.6126e-01,  6.4800e-01,  1.4251e+00, -6.4796e-01,\n",
      "         1.1587e-02,  1.6374e+00, -8.4842e-01,  4.7155e-02, -1.6793e+00,\n",
      "        -2.3853e-01,  4.0989e-02, -3.9175e-02,  8.9250e-01,  1.3337e+00,\n",
      "         7.7356e-01, -3.0148e-01,  9.9434e-02,  8.6379e-01,  4.2095e-01,\n",
      "        -1.4157e+00, -4.8482e-01, -1.6408e+00,  2.3581e-02,  8.9075e-02,\n",
      "        -1.8645e-01,  3.0317e+00, -1.1307e+00,  1.8054e+00,  7.9677e-01,\n",
      "         5.9605e-01, -1.8427e+00,  1.6286e+00, -2.6922e-01,  2.2000e+00,\n",
      "         8.8090e-01, -7.4131e-01,  1.6666e+00,  2.5973e+00,  1.5768e+00,\n",
      "        -8.6129e-01,  4.4633e-01,  7.4377e-01,  9.8467e-01, -1.9174e+00,\n",
      "        -1.4052e+00, -1.6651e+00,  4.7827e-01,  3.4038e+00,  2.0814e+00,\n",
      "         1.0732e+00,  7.7445e-01, -4.7947e-01, -8.6501e-01,  5.0825e-01,\n",
      "        -1.1975e+00,  5.9171e-02,  1.8961e-01, -7.6842e-01,  9.0274e-01,\n",
      "        -1.4923e+00,  5.7302e-01,  2.5357e-01, -2.4502e-01,  9.6838e-01,\n",
      "        -1.7214e+00, -8.8203e-01, -1.9639e+00, -1.9014e+00,  7.4643e-01,\n",
      "         5.9328e-01,  1.5453e+00, -1.0983e+00,  6.2053e-01, -6.5837e-02,\n",
      "         2.0314e-01,  1.5009e+00, -1.5881e+00,  4.8558e-01, -2.2919e+00,\n",
      "         5.0136e-01,  8.9107e-01, -2.3111e+00,  1.8093e+00, -5.0324e-01,\n",
      "        -3.6755e+00, -2.3369e+00, -1.9351e+00, -1.4628e-01,  9.4362e-01,\n",
      "        -2.0797e-01,  5.2021e-01,  2.4712e-01,  9.4365e-01, -2.1785e+00,\n",
      "        -6.5421e-01,  4.5634e-01, -2.2696e+00, -2.3806e+00, -1.1921e-02,\n",
      "         2.1116e-01,  9.4747e-01,  1.0756e+00,  4.8184e-01, -6.3646e-01,\n",
      "        -7.0934e-01, -1.7692e-02, -7.7465e-01,  1.2435e+00,  1.2159e+00,\n",
      "         1.9397e+00,  1.7900e+00, -1.3250e+00, -2.5776e-02,  1.2825e+00,\n",
      "         1.3770e+00,  1.0020e+00,  1.9019e+00, -5.5940e-01, -7.4873e-02,\n",
      "         3.4621e+00, -2.9870e+00, -7.6564e-01,  1.4560e+00,  1.4227e+00,\n",
      "        -1.8802e+00,  1.1658e-01,  8.8328e-01, -3.2994e-01, -1.8433e+00,\n",
      "        -2.7914e+00, -5.3773e-01, -1.2027e+00,  9.2814e-01, -1.1470e+00,\n",
      "         1.2439e-01, -1.8912e+00,  9.7735e-01, -8.7743e-01, -2.9594e+00,\n",
      "        -8.1054e-01,  7.0078e-01, -8.4849e-01, -1.7743e-01, -3.8940e-03,\n",
      "        -4.8713e-02, -5.2868e-01,  8.0029e-01,  1.4099e+00,  1.0763e-01,\n",
      "         6.9342e-01, -1.6975e+00, -9.4795e-01, -9.1990e-01,  9.2969e-01,\n",
      "        -1.8949e+00, -6.3079e-01,  2.6112e-01,  2.3770e+00,  7.6794e-02,\n",
      "        -2.1103e+00,  9.3765e-01, -7.9404e-01,  1.3198e+00,  6.5213e-01,\n",
      "        -1.2805e+00, -7.0185e-01,  5.7236e-01, -2.2463e-01,  3.3037e+00,\n",
      "         1.3685e+00,  1.5772e-01,  6.3726e-01,  1.2387e-01, -1.1086e+00,\n",
      "         1.1628e+00,  1.3248e+00, -2.2120e-01, -1.3658e+00, -2.9828e+00,\n",
      "        -1.7240e+00, -3.2838e-01, -1.0672e+00,  7.3090e-01, -4.4666e-01,\n",
      "        -5.9905e-01,  1.1697e+00, -3.3771e-01, -1.4956e+00, -8.8579e-01,\n",
      "        -1.0445e+00, -2.0379e+00, -5.2305e-01, -6.7730e-01,  8.9020e-01,\n",
      "        -5.7233e-01, -1.1693e+00, -2.2790e-01, -1.9474e+00, -1.4161e+00,\n",
      "        -1.9636e+00, -1.6943e+00,  2.3957e-01, -1.5748e+00,  2.3618e-01,\n",
      "        -1.3853e+00, -1.3525e+00, -5.4133e-01,  1.2571e-02, -2.3019e-02,\n",
      "        -4.4328e-01, -5.0811e-01,  1.0667e+00,  1.0710e+00,  6.5014e-01,\n",
      "         7.3803e-01, -1.1768e-01,  1.3815e-01,  9.0649e-01,  8.0322e-01,\n",
      "         1.3885e-01, -5.5523e-01,  1.3586e+00,  5.8044e-01, -1.5106e+00,\n",
      "        -2.7227e-01, -2.4308e-01, -1.2134e+00,  6.5359e-01, -3.0066e-01,\n",
      "        -6.6319e-01,  1.1599e+00,  6.8720e-01,  1.5393e+00,  4.9876e-01,\n",
      "        -2.0553e+00,  1.1821e+00, -1.0953e+00, -1.3451e+00, -1.5879e+00,\n",
      "         3.1579e-01, -1.9291e+00,  1.2997e-01,  3.5013e-01, -2.2601e+00,\n",
      "        -1.4460e+00,  9.7634e-02, -8.7687e-01, -1.9577e+00, -4.9399e-01,\n",
      "         5.0386e-02, -1.0660e+00,  2.1126e+00,  1.4243e+00, -3.3118e-01,\n",
      "         1.6562e+00, -1.1678e+00, -5.5007e-01, -2.0695e+00, -3.1222e-01,\n",
      "        -1.8774e-01, -2.6611e-01,  1.0303e+00,  1.5955e+00,  1.4849e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-1.2543e+00, -1.8568e-01, -1.5882e+00, -7.2414e-01, -6.7324e-03,\n",
      "        -3.3530e-01,  2.1363e-01, -5.4989e-01, -2.5809e-01,  1.4065e-01,\n",
      "         2.9697e-01,  6.8419e-01,  9.8429e-01,  1.0627e+00,  9.7814e-01,\n",
      "         9.7330e-02,  1.6902e-01,  3.7092e-01,  4.8092e-01,  1.1974e+00,\n",
      "        -6.8985e-01, -7.2066e-01, -8.8097e-01,  2.8279e-01, -5.1177e-01,\n",
      "        -5.1668e-01, -6.4848e-01, -1.3369e-01, -5.3867e-01, -9.4702e-03,\n",
      "        -2.3135e-01,  1.1303e+00,  1.9810e-01, -2.8036e-01,  6.2591e-01,\n",
      "        -1.0281e+00,  1.0161e-01, -6.2130e-01,  3.7765e-01,  3.5880e-01,\n",
      "         1.0982e+00,  1.9514e-01,  1.0121e+00,  8.3717e-01,  3.2376e-01,\n",
      "         7.4941e-01,  1.5855e+00,  2.7685e-01,  5.1250e-01, -5.2603e-01,\n",
      "        -3.2362e-01, -1.6115e+00,  7.5240e-01,  7.4311e-01,  1.1687e+00,\n",
      "         1.0061e+00,  2.2109e-01,  2.5669e-01,  8.6078e-01,  1.3808e+00,\n",
      "         1.9296e+00,  1.3363e+00,  1.7570e-01,  1.5632e+00,  1.4328e+00,\n",
      "         2.6063e-01,  1.3055e+00,  1.3728e+00,  6.9156e-01,  9.9705e-01,\n",
      "         2.3024e-01,  1.3952e+00, -5.3638e-01,  7.0593e-01, -1.9357e-01,\n",
      "         2.7594e-01,  2.4614e+00,  1.5660e+00,  2.4131e+00,  1.4142e+00,\n",
      "        -1.1659e+00, -1.3243e+00,  3.0288e-01, -9.5421e-01,  1.0353e-01,\n",
      "         7.0833e-01, -5.1536e-01, -1.6242e-01, -1.7739e-01, -9.2996e-01,\n",
      "        -6.1826e-01, -6.0836e-01,  2.6971e-01, -6.7956e-01,  1.4394e+00,\n",
      "        -1.3253e+00, -3.4608e-01, -6.8292e-01, -1.5321e+00,  2.9912e-02,\n",
      "        -6.2967e-01, -1.2419e+00,  5.1763e-01,  1.5079e+00,  1.4243e+00,\n",
      "         4.9845e-01,  1.6216e+00, -8.7900e-01, -5.3091e-01, -1.1967e+00,\n",
      "        -6.9653e-01,  2.2336e-01,  6.7914e-01,  8.7709e-01,  1.2551e+00,\n",
      "        -1.0992e+00, -8.0075e-01, -3.9719e-01, -5.9705e-01, -6.4361e-01,\n",
      "        -5.6223e-01, -1.5277e+00, -1.4648e+00, -1.3073e+00,  3.4075e-01,\n",
      "         6.5389e-01, -6.3296e-02, -1.0882e+00, -1.1724e+00, -2.8795e-01,\n",
      "        -6.6485e-01, -1.0216e+00, -1.0636e+00, -5.2574e-01, -9.0454e-01,\n",
      "        -8.6971e-01, -1.1205e+00, -4.5761e-01, -4.8871e-01, -9.0108e-01,\n",
      "        -2.6981e-01, -7.1399e-01, -9.8284e-01, -1.1664e+00, -4.5829e-01,\n",
      "        -7.7924e-01, -1.7476e+00, -2.0105e+00, -2.1940e+00, -1.9194e+00,\n",
      "        -3.8827e-01,  1.4347e+00, -1.8991e+00, -3.4109e-01, -3.3159e-01,\n",
      "        -3.9654e-01, -3.6862e-01, -2.0154e-01, -1.7181e-01,  3.4643e-01,\n",
      "        -1.0873e+00,  7.4012e-01,  9.0999e-01,  1.9262e-01,  6.8512e-01,\n",
      "        -2.8197e-01, -1.6520e-01, -1.1004e+00,  6.0424e-01, -5.8853e-01,\n",
      "        -3.4004e-01,  1.6363e+00,  4.5416e-01,  5.5971e-02,  8.8267e-01,\n",
      "        -1.6097e+00, -7.1765e-01, -7.2975e-02,  1.1045e+00, -1.5579e-01,\n",
      "        -5.2221e-03, -2.1818e-01,  8.9064e-01, -4.7923e-01,  7.0824e-01,\n",
      "         7.3916e-01,  7.5070e-01,  2.3136e-01, -3.3961e-01,  9.2977e-02,\n",
      "        -1.5801e+00,  9.6466e-01,  4.1518e-01, -7.6819e-01, -2.1432e+00,\n",
      "         8.3862e-01,  3.9506e-01, -1.0950e+00, -6.9904e-01,  2.0025e-01,\n",
      "        -1.7486e+00, -8.4635e-02,  3.7216e-01,  5.9277e-01, -2.6960e-01,\n",
      "        -7.7354e-01, -8.0917e-01,  2.8263e-01,  7.1098e-01,  1.0630e+00,\n",
      "        -6.1044e-01,  1.4254e-01, -1.3780e-01, -5.7543e-02, -1.1179e+00,\n",
      "         1.3079e-01, -1.5895e+00, -3.9562e-01, -3.0724e-01,  1.8505e-01,\n",
      "        -1.2333e+00, -5.4031e-01, -9.5704e-01,  1.4059e+00,  5.8207e-01,\n",
      "         3.9375e-01, -1.5165e+00,  4.5392e-01, -1.4941e+00, -1.0066e-01,\n",
      "         6.1470e-01,  1.4366e-02, -3.3685e-02, -1.2720e+00,  3.3734e-02,\n",
      "         5.4668e-01,  4.3442e-01,  1.1436e+00, -1.4291e-01, -3.8292e-01,\n",
      "         2.8763e-01, -4.3314e-02, -7.5336e-02, -3.2460e-01, -1.7964e+00,\n",
      "         1.9745e-01, -5.1684e-01, -1.0897e+00,  4.8307e-01,  5.0309e-01,\n",
      "         1.0278e+00,  8.9582e-02, -1.2754e+00,  8.6792e-01,  5.6984e-01,\n",
      "        -1.6138e+00, -1.0834e+00, -7.2006e-01, -2.7177e-02,  7.2219e-01,\n",
      "         5.3716e-02, -3.1972e-01, -6.8475e-01,  1.2941e+00,  4.6120e-01,\n",
      "         9.0383e-01, -2.1986e-01, -4.1867e-01, -1.0596e+00,  4.9486e-01,\n",
      "        -5.4802e-02,  8.2430e-01,  9.9727e-01,  1.0390e+00,  1.1996e-01,\n",
      "         8.4178e-02,  8.0210e-01,  1.3497e+00,  7.1497e-01,  5.6839e-01,\n",
      "         1.3197e+00,  1.8519e+00,  1.4315e+00,  8.9533e-01,  2.0917e+00,\n",
      "         2.2996e+00,  7.6351e-01,  8.3040e-01,  1.4039e-02, -3.2608e-01,\n",
      "        -1.8429e-01, -3.8060e-01, -3.9829e-01,  1.2515e+00, -4.6830e-01,\n",
      "        -4.5342e-01, -8.8052e-01, -2.1362e-01,  2.3735e+00,  2.4169e+00,\n",
      "         1.1786e-01,  1.4805e+00,  1.6125e-01,  9.9991e-01,  7.9474e-01,\n",
      "         7.1992e-01,  1.4640e+00,  8.1609e-01,  1.1043e+00,  1.2763e+00,\n",
      "         2.1550e+00,  1.2231e+00,  1.3162e+00,  1.7834e+00,  1.9690e+00,\n",
      "         1.7173e+00,  1.0330e+00,  1.2943e+00,  9.7580e-01,  6.1656e-01,\n",
      "         5.1239e-01, -3.9662e-01, -9.1420e-02, -4.4376e-01,  8.1883e-01,\n",
      "         2.0565e-01,  1.5884e-01,  9.3222e-01,  5.5229e-01, -1.5251e+00,\n",
      "         1.2419e+00,  6.3507e-01, -2.1970e-01,  1.7463e+00,  1.1234e+00,\n",
      "         5.4874e-01,  7.5924e-01,  7.9848e-01,  7.0224e-01, -1.6343e+00,\n",
      "        -2.1623e-01,  4.7151e-03, -2.6945e-01, -1.8253e-01, -1.2535e+00,\n",
      "        -1.0911e+00, -6.9343e-01, -1.4270e+00, -1.4277e+00, -8.6628e-01,\n",
      "        -2.4322e-01, -1.1880e+00, -4.8728e-01,  2.4533e-01, -6.7468e-02,\n",
      "        -8.4302e-01,  1.9451e+00,  6.6320e-01,  1.5283e+00,  1.4943e+00,\n",
      "         7.8904e-01,  1.5648e+00,  1.9031e+00,  1.3267e+00,  4.5981e-01,\n",
      "         2.3987e-02, -2.2891e-02,  1.0078e-01,  5.8995e-02, -5.4702e-01,\n",
      "         3.3163e-01, -4.3871e-01,  1.5021e+00,  1.1282e+00,  1.1338e+00,\n",
      "        -4.5642e-01,  2.6904e-01,  1.3152e+00,  9.2899e-01, -4.3924e-02,\n",
      "         6.3093e-01,  8.0319e-01,  1.5448e+00,  9.4538e-01,  3.9721e-02,\n",
      "        -5.4739e-01, -1.5252e+00, -5.3441e-02, -2.4291e-01, -5.8324e-01,\n",
      "         2.7312e-01, -3.9607e-01, -1.5044e+00, -1.4830e+00, -1.3481e+00,\n",
      "        -3.5558e-01, -8.3097e-01,  1.8021e-01, -1.3463e-01,  1.1237e+00,\n",
      "        -5.5628e-01,  1.3021e+00,  8.7466e-01, -1.2989e+00, -1.8314e+00,\n",
      "        -5.6253e-01, -8.8184e-02, -1.1417e+00, -2.4133e+00,  4.5764e-01,\n",
      "         1.5160e-01, -2.0067e-01,  7.4733e-01, -2.8681e-01, -1.9546e-01,\n",
      "        -1.2626e+00, -3.6947e-01,  1.0320e-01,  8.2340e-01,  2.3704e+00,\n",
      "         3.8176e-01,  5.6206e-01, -6.3345e-01, -1.7794e-01, -3.3800e-01,\n",
      "        -3.7553e-01, -5.7722e-01,  8.0353e-02,  2.6640e-01,  4.1086e-01,\n",
      "        -6.4982e-01, -9.3300e-01,  7.4305e-02, -7.5968e-01,  1.5193e+00,\n",
      "         1.3877e-02, -9.7971e-01,  2.4715e-04, -1.1391e-01, -1.0257e+00,\n",
      "         7.0937e-01,  4.0716e-01,  1.5048e-01,  6.1559e-01, -1.4457e+00,\n",
      "        -9.0578e-01,  1.8506e+00,  6.4894e-01,  1.6971e+00, -1.1899e+00,\n",
      "        -2.0941e+00, -1.4310e-01,  5.7861e-02, -7.2417e-01, -1.2913e+00,\n",
      "         1.4383e+00, -4.8768e-01,  4.5893e-01, -6.9750e-01, -5.6529e-01,\n",
      "        -2.9008e-01,  1.5572e-01,  8.6769e-01,  4.9963e-01,  7.3699e-01,\n",
      "        -7.0045e-01, -1.6471e+00, -1.0587e+00, -1.6739e-01, -5.0347e-01,\n",
      "         1.5274e+00, -9.4259e-01, -8.4021e-01,  2.2252e-01,  6.3805e-01,\n",
      "         9.4630e-01, -9.9994e-01, -6.4369e-01,  1.1429e+00, -2.7083e-01,\n",
      "        -1.2184e+00,  2.3276e-01, -7.3178e-01, -3.8070e-01, -1.4148e+00,\n",
      "        -1.6651e-01, -3.3976e-02,  1.2000e+00,  1.1036e+00,  6.9990e-01,\n",
      "         3.5518e-01, -9.5047e-01,  1.0764e+00, -9.7858e-01,  5.7898e-01,\n",
      "        -1.5586e+00,  3.6422e-01,  2.1364e-01, -1.2131e-01,  2.6078e-01,\n",
      "         1.3359e-01,  1.3159e+00,  5.7035e-01,  4.4456e-01,  7.5032e-01,\n",
      "         7.5857e-01,  6.8824e-01,  9.3899e-02,  1.3940e+00, -1.8568e+00,\n",
      "        -1.8104e+00, -1.0672e+00,  8.8480e-01,  5.6091e-01,  1.2521e-02,\n",
      "         9.7967e-01,  7.5747e-02, -5.4309e-01, -8.0786e-01,  2.2508e-01,\n",
      "        -8.2636e-01, -1.0243e+00, -7.3582e-01,  5.2314e-01,  3.4183e-02,\n",
      "        -1.1525e+00, -3.2559e-01, -2.0677e-01,  1.2408e-01, -1.7124e-01,\n",
      "         3.9082e-01,  5.5990e-02,  1.9362e-01,  5.1765e-01, -1.3428e+00,\n",
      "        -1.1263e+00, -1.7403e+00, -1.3441e+00, -3.0395e-01,  2.6443e+00,\n",
      "        -1.3392e+00,  5.8449e-01,  1.0330e+00,  5.9482e-01, -1.0621e+00,\n",
      "         4.4329e-01,  2.7773e-01, -3.0207e+00, -1.8203e+00,  9.0883e-01,\n",
      "        -1.4015e+00,  1.8771e-01,  5.8227e-01, -1.8238e-01, -1.9052e+00,\n",
      "        -1.6091e+00,  6.7900e-01,  4.8031e-01,  1.0489e+00,  4.2778e-01,\n",
      "        -1.8443e+00, -7.2483e-01, -5.6581e-02,  1.2062e+00,  8.1078e-02,\n",
      "        -9.3421e-01, -3.0924e-01, -8.1825e-01,  1.3809e+00, -1.5746e+00,\n",
      "        -1.3401e-01, -1.3948e+00,  5.6658e-01, -7.0711e-01,  5.2320e-01,\n",
      "        -1.1663e+00, -1.8659e+00,  3.7245e-02, -5.1764e-01,  3.1723e-01,\n",
      "         1.4438e-01, -1.0440e+00, -1.4383e+00,  1.0762e+00,  2.3410e-01,\n",
      "         5.2103e-01, -1.1010e+00,  1.4701e+00,  1.2515e+00,  4.4990e-01,\n",
      "        -8.7837e-01,  6.2767e-01, -6.8648e-01,  5.7669e-01,  7.1256e-01,\n",
      "        -1.1348e+00,  6.2294e-01,  1.7879e-02, -5.6666e-01,  1.2654e+00,\n",
      "         1.9863e+00, -3.5676e-01, -2.3697e-01, -1.5941e+00,  1.3224e+00,\n",
      "         8.7803e-01,  1.7652e+00, -1.8267e-01,  7.9327e-01, -1.0908e+00,\n",
      "         1.9208e-01,  1.8595e+00, -1.8527e+00,  4.7410e-01,  5.9629e-01,\n",
      "        -3.0201e-01,  1.0330e+00, -8.2656e-01,  2.8907e-01,  2.7383e+00,\n",
      "         1.7674e+00, -3.6563e-01,  1.6866e+00,  5.2898e-01, -1.1302e+00,\n",
      "        -2.4955e+00,  3.0140e+00, -1.2537e+00, -1.3586e+00,  5.6518e-01,\n",
      "         7.2973e-01,  7.9219e-02,  4.4977e-01,  8.9536e-01, -5.4288e-01,\n",
      "        -4.6087e-01,  5.6757e-01, -5.7791e-01, -8.6638e-01, -8.1641e-01,\n",
      "         1.3699e-01, -1.5099e-01, -1.9727e-01,  8.4824e-01,  2.2927e+00,\n",
      "        -1.2674e+00,  8.7820e-01, -2.1132e-01, -8.4413e-01,  4.8572e-01,\n",
      "         1.7451e+00, -1.2037e+00,  1.6323e-01, -1.3347e-02, -1.2225e+00,\n",
      "         1.6630e-01, -5.2034e-01, -9.4799e-01,  7.8115e-01, -8.8089e-01,\n",
      "        -1.3248e+00, -2.3067e+00, -7.5088e-02, -1.0126e+00,  5.0419e-01,\n",
      "        -4.2417e-01,  7.5865e-01,  3.3717e-01, -9.0634e-01,  1.4607e+00,\n",
      "        -4.7438e-01, -3.3669e-01,  2.7360e-02,  1.6630e+00,  3.0616e+00,\n",
      "        -1.9562e+00, -9.8971e-01,  3.0351e+00, -1.3443e-01,  7.4532e-01,\n",
      "         5.8695e-02,  1.1531e+00,  1.0022e+00,  4.0089e-01,  1.8037e-01,\n",
      "        -1.7245e+00, -4.5515e-01, -1.0074e+00, -6.3099e-01, -7.0436e-01,\n",
      "        -1.9744e+00, -8.7635e-01,  4.1003e-01, -7.9293e-01, -2.1504e+00,\n",
      "         1.3989e+00,  1.6382e+00, -2.2294e-01, -8.9831e-01,  4.5458e-01,\n",
      "         1.1685e+00,  3.6089e-01, -4.1193e-01,  1.4541e+00,  2.5232e-01,\n",
      "        -2.2911e+00, -6.3327e-01, -8.4906e-01,  1.8722e-02,  6.9540e-01,\n",
      "         1.1223e+00,  1.4196e+00, -2.7496e-01, -4.2923e-01,  9.3427e-01,\n",
      "        -1.0722e+00,  8.0857e-01, -1.8602e+00, -3.3256e-01,  5.0528e-01,\n",
      "         9.4587e-01,  4.7839e-01,  8.8519e-01, -6.4866e-01, -1.5428e+00,\n",
      "         1.2104e+00, -6.0342e-01, -1.5214e+00,  4.2745e-01, -2.2250e+00,\n",
      "        -1.1883e+00,  5.4763e-01, -7.2051e-01,  1.0885e+00, -2.1074e+00,\n",
      "         1.0584e+00,  2.6835e-01,  5.3822e-01,  9.4240e-01, -5.6298e-01,\n",
      "        -6.5677e-02,  2.0858e+00, -1.1299e+00,  1.8636e-01, -1.1936e+00,\n",
      "        -4.0030e-01, -5.1658e-01,  4.5273e-01,  7.4054e-01,  1.5702e+00,\n",
      "         1.4154e+00, -2.0516e-01,  1.6437e-01,  9.8935e-01, -3.7142e-01,\n",
      "        -1.1422e+00, -3.5585e-01, -1.4714e+00, -1.8861e-01, -4.0936e-01,\n",
      "        -1.5916e-01,  1.8009e+00, -7.5288e-01,  1.2231e+00,  6.7548e-01,\n",
      "         3.5727e-01, -1.7470e+00,  1.0859e+00, -8.6736e-01,  1.6598e+00,\n",
      "         4.7865e-01, -6.6173e-01,  1.7023e+00,  1.8402e+00,  1.0571e+00,\n",
      "         5.0577e-01,  1.0091e+00,  2.2074e-01,  4.7200e-01, -1.8960e+00,\n",
      "        -3.3985e-01, -1.8291e+00,  2.0559e-01,  2.7161e+00,  1.6431e+00,\n",
      "         4.4362e-01,  6.3900e-01, -1.3845e-01, -6.0117e-01,  6.0727e-01,\n",
      "        -1.0024e+00,  3.0426e-01,  3.1380e-01,  2.5080e-02,  2.2120e+00,\n",
      "        -9.8138e-01,  2.6112e-01,  8.9735e-01, -8.2672e-01,  7.8679e-01,\n",
      "        -1.3830e+00, -7.3080e-01, -1.2757e+00, -1.6670e+00,  5.0004e-01,\n",
      "         3.1106e-01,  9.8043e-01, -8.1833e-01,  3.8249e-01, -5.9708e-01,\n",
      "        -2.6907e-01,  1.3801e+00, -1.5643e+00,  2.7845e-01, -1.6131e+00,\n",
      "         5.9813e-01,  4.9008e-01, -1.9365e+00,  1.7055e+00,  3.1390e-01,\n",
      "        -3.3426e+00, -1.8527e+00, -1.3756e+00,  3.6932e-02,  1.4147e+00,\n",
      "         6.8986e-02,  6.3160e-02,  2.7620e-01,  5.0999e-01, -1.5116e+00,\n",
      "        -1.4185e-01,  6.2272e-01, -1.4117e+00, -1.6221e+00,  5.1691e-01,\n",
      "         3.2398e-01,  6.4371e-01,  8.1318e-01, -1.6265e-02, -3.3464e-01,\n",
      "        -1.4735e-01,  4.7334e-01, -3.4059e-01,  1.3063e+00,  8.2753e-01,\n",
      "         1.8422e+00,  1.8368e+00, -8.6526e-01, -4.2382e-01,  9.8382e-01,\n",
      "         1.1154e+00,  6.9971e-01,  1.5205e+00, -7.1366e-02,  1.2992e+00,\n",
      "         2.6251e+00, -3.0588e+00, -1.6016e-01,  1.4054e+00,  9.1963e-01,\n",
      "        -1.8628e+00,  1.9697e-01,  4.4013e-01, -1.6187e-01, -2.1113e+00,\n",
      "        -2.3185e+00, -6.5292e-01, -1.3442e+00,  7.3929e-01, -1.0633e-01,\n",
      "         3.0168e-01, -1.1375e+00,  8.4130e-01, -4.7805e-02, -2.7180e+00,\n",
      "        -2.9270e-01,  6.9362e-01, -8.0970e-01, -3.4333e-01,  9.9053e-01,\n",
      "         3.0230e-01, -1.5726e-01,  9.8721e-01,  1.5745e+00,  1.0601e+00,\n",
      "         1.4262e+00, -1.6771e+00,  5.5645e-01, -5.4570e-01,  8.3174e-01,\n",
      "        -1.9001e+00, -1.0840e+00,  6.9088e-01,  1.6957e+00,  4.4972e-01,\n",
      "        -1.3098e+00,  1.0940e+00, -3.9683e-01,  1.2853e+00,  5.1067e-01,\n",
      "        -1.2882e+00, -1.0405e+00,  3.4505e-01,  2.1236e-01,  2.8626e+00,\n",
      "         1.9980e+00,  1.9492e-01,  6.2715e-01,  6.5919e-01, -9.3029e-01,\n",
      "         6.3354e-01,  1.5188e+00, -3.3836e-01, -5.4206e-01, -2.3048e+00,\n",
      "        -5.2631e-01, -4.3203e-01, -7.9852e-01,  4.1820e-01, -5.0629e-01,\n",
      "        -5.7234e-01,  1.0704e+00, -4.8850e-01, -1.4074e+00, -1.0404e+00,\n",
      "        -1.4412e+00, -2.3198e+00, -6.5170e-01, -1.0569e+00,  8.0446e-01,\n",
      "        -1.0309e+00, -1.5590e+00, -6.0315e-01, -2.1035e+00, -1.8616e+00,\n",
      "        -1.9796e+00, -1.0249e+00,  1.5712e-01, -1.4697e+00, -1.4356e-01,\n",
      "        -1.2299e+00, -9.6721e-01, -6.7098e-01, -1.8773e-01, -1.1139e-02,\n",
      "        -3.9997e-01, -4.2726e-01,  2.7148e-01,  8.4967e-01,  1.8448e-01,\n",
      "         7.4454e-01,  1.3544e-01,  2.0758e-03,  1.1107e+00,  1.1512e+00,\n",
      "         4.0141e-01, -3.5978e-01,  9.6730e-01,  6.7383e-01, -1.5906e+00,\n",
      "        -8.6927e-01, -5.4628e-01, -1.4480e+00,  3.0657e-01, -5.6560e-01,\n",
      "        -5.8855e-01,  1.0490e+00, -1.1320e-01,  9.1847e-01, -1.0091e-01,\n",
      "        -1.0007e+00,  1.3201e+00,  2.5954e-02, -8.0538e-01, -1.0733e+00,\n",
      "         7.3416e-01, -9.1896e-01,  4.2134e-01,  1.0188e+00, -1.1620e+00,\n",
      "        -3.5023e-01, -3.0773e-01,  3.8202e-03, -1.3477e+00, -3.1939e-01,\n",
      "         6.0036e-01, -7.4208e-01,  1.8472e+00,  1.2154e+00, -2.5827e-01,\n",
      "         1.3394e+00, -1.0867e+00, -1.0945e+00, -2.4628e+00, -9.8475e-01,\n",
      "        -9.6668e-01, -5.3455e-01, -1.0253e-01,  1.3740e+00,  1.4061e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-9.5331e-01, -2.6164e-01, -2.4287e+00, -1.1310e+00, -6.8657e-01,\n",
      "        -6.2315e-01, -3.2593e-01, -1.0852e+00, -3.0306e-01,  9.1312e-01,\n",
      "         9.4702e-01,  8.8831e-01,  1.6331e+00,  1.4668e+00,  1.9399e+00,\n",
      "         1.9458e-01,  2.8793e-01,  9.6097e-01,  6.3941e-01,  2.0398e+00,\n",
      "        -1.3184e+00, -3.7969e-01, -1.3939e+00,  4.9859e-01, -9.4823e-01,\n",
      "        -2.3669e-01, -1.5776e-01,  3.7195e-01, -5.6268e-02,  9.6718e-01,\n",
      "        -2.8988e-01,  9.8192e-01,  4.2328e-01, -8.8865e-02,  4.3794e-01,\n",
      "        -1.0138e+00,  5.5122e-01, -3.5267e-01,  6.2521e-01,  8.9505e-02,\n",
      "         9.6176e-01,  9.7994e-01,  1.5285e+00,  6.8531e-01,  6.1812e-01,\n",
      "         9.7563e-01,  1.5451e+00,  9.2452e-01,  2.3745e-01, -1.2873e+00,\n",
      "        -8.9384e-01, -2.5866e+00,  1.5294e+00,  1.5211e+00,  1.8783e+00,\n",
      "         6.8141e-01,  6.6549e-01,  4.9152e-01,  6.4102e-01,  1.3153e+00,\n",
      "         1.8256e+00,  1.5417e+00,  2.5382e-01,  2.3360e+00,  1.2580e+00,\n",
      "         4.5919e-01,  1.7087e+00,  1.7731e+00,  1.2622e+00,  7.3567e-01,\n",
      "        -1.3528e-01,  1.4489e+00, -3.8032e-01,  4.2531e-01, -2.8824e-01,\n",
      "         7.1733e-01,  3.0536e+00,  1.6842e+00,  2.6580e+00,  1.1885e+00,\n",
      "        -1.3159e+00, -1.7067e+00,  5.8971e-01, -8.2851e-01, -2.4836e-01,\n",
      "         1.0317e+00, -5.7809e-01,  4.6142e-01, -2.1805e-01, -1.2326e+00,\n",
      "        -6.0094e-01, -4.2772e-01,  2.2761e-01, -6.4872e-01,  1.3215e+00,\n",
      "        -9.1288e-01, -6.2448e-01, -1.1978e+00, -1.6951e+00,  9.5704e-02,\n",
      "        -1.2097e+00, -1.8635e+00,  8.0742e-01,  1.5088e+00,  1.9783e+00,\n",
      "         9.9761e-01,  1.6408e+00, -1.6804e+00, -1.7273e+00, -1.1097e+00,\n",
      "        -7.3907e-01, -1.5862e-01,  6.2619e-01,  6.9073e-01,  1.3324e+00,\n",
      "        -9.8477e-01, -1.2327e+00, -8.6215e-01, -4.9308e-01, -6.1602e-01,\n",
      "        -2.2070e-01, -1.2774e+00, -1.5453e+00, -1.3364e+00,  4.1710e-01,\n",
      "         8.8028e-01, -4.3380e-01, -1.4767e+00, -1.5140e+00, -4.7184e-01,\n",
      "        -8.7052e-01, -1.0714e+00, -1.4404e+00, -9.9258e-01, -8.1250e-01,\n",
      "        -1.5145e+00, -1.6110e+00, -1.1537e+00, -4.2736e-01, -9.0791e-01,\n",
      "         5.7266e-01, -1.0367e+00, -7.7189e-01, -1.6442e+00, -4.6904e-01,\n",
      "        -1.0248e+00, -1.9801e+00, -2.9045e+00, -2.9261e+00, -2.2564e+00,\n",
      "        -4.9045e-01,  2.1466e+00, -1.4249e+00, -5.4241e-01,  3.2585e-01,\n",
      "        -3.6450e-01,  2.1516e-01,  2.2377e-01,  2.8540e-01,  3.5322e-01,\n",
      "        -1.4552e+00,  4.4438e-01,  1.1693e+00,  2.2660e-02,  9.7933e-01,\n",
      "        -4.5689e-01, -1.1576e-01, -9.8338e-01,  7.3139e-01, -5.4264e-01,\n",
      "        -6.1496e-01,  1.5724e+00,  2.9856e-01,  1.8636e-01,  1.1272e+00,\n",
      "        -1.4730e+00, -4.9083e-01, -2.3809e-01,  5.5566e-01,  3.7782e-02,\n",
      "         3.2115e-01, -5.9472e-01,  1.4652e+00, -8.7722e-01,  5.7289e-01,\n",
      "         1.0224e+00,  1.1652e+00,  3.8759e-01, -2.8141e-01,  1.1656e-01,\n",
      "        -1.6264e+00,  8.3901e-01,  5.6338e-01, -6.2780e-01, -1.9264e+00,\n",
      "         7.4286e-01,  2.3462e-02, -1.3108e+00, -8.6344e-01, -1.0328e-01,\n",
      "        -1.9446e+00, -1.5395e-01,  2.1005e-01,  4.7697e-01, -2.6506e-01,\n",
      "        -4.9952e-01, -6.7013e-01,  9.0203e-01,  1.3215e+00,  8.4562e-01,\n",
      "        -6.9686e-01,  1.1248e-01,  1.9181e-01, -1.6808e-01, -9.9990e-01,\n",
      "         8.5053e-01, -1.2967e+00, -3.0399e-01, -6.7481e-02,  3.3445e-01,\n",
      "        -1.4836e+00, -9.9591e-01, -5.1630e-01,  1.5849e+00,  2.1011e-01,\n",
      "         5.6842e-01, -1.3849e+00,  1.1329e+00, -1.9431e+00, -3.2586e-01,\n",
      "         5.8777e-01,  2.1948e-01,  1.8732e-01, -1.5790e+00,  2.9395e-01,\n",
      "         1.0558e+00,  3.1539e-01,  1.1581e+00,  2.8696e-01, -2.4298e-01,\n",
      "         6.7573e-01,  2.3638e-01, -9.5669e-02, -1.2931e-01, -1.6554e+00,\n",
      "         4.3078e-01, -4.3862e-01, -8.5555e-01,  7.2686e-01,  6.0367e-01,\n",
      "         1.1459e+00,  2.9699e-01, -1.0323e+00,  9.9337e-01,  1.1427e+00,\n",
      "        -1.7571e+00, -1.2747e+00, -4.7529e-01,  1.7830e-01,  1.3018e+00,\n",
      "         3.4932e-01, -2.5244e-01,  7.5287e-02,  1.7190e+00,  5.6066e-01,\n",
      "         8.3851e-01, -4.5519e-01, -9.3694e-01, -6.7554e-01,  5.7654e-01,\n",
      "         1.0479e-01,  8.3442e-01,  8.7563e-01,  1.5292e+00,  2.2956e-01,\n",
      "         3.5814e-02,  1.4822e+00,  1.1869e+00,  1.0126e+00,  5.6000e-01,\n",
      "         1.4200e+00,  2.3438e+00,  2.0501e+00,  1.1793e+00,  2.6161e+00,\n",
      "         3.0657e+00,  1.0617e+00,  1.5740e+00,  2.5178e-01, -2.0892e-01,\n",
      "         1.4505e-01, -1.2187e-01, -3.9174e-01,  2.1298e+00, -8.7452e-01,\n",
      "        -5.1920e-01, -7.5671e-01, -2.5889e-01,  3.0267e+00,  3.4556e+00,\n",
      "         5.0849e-01,  1.4082e+00,  7.8239e-01,  1.0453e+00,  6.6157e-01,\n",
      "         1.5508e+00,  2.0474e+00,  5.1506e-01,  6.3085e-01,  9.0665e-01,\n",
      "         1.7543e+00,  8.8410e-01,  1.1339e+00,  1.8842e+00,  1.9583e+00,\n",
      "         1.6545e+00,  1.0508e+00,  9.0934e-01,  2.1298e-01,  7.0139e-01,\n",
      "         1.1048e+00, -7.9031e-01, -4.9048e-01, -6.3547e-01,  5.1116e-01,\n",
      "        -5.9143e-01,  7.2745e-03,  7.7473e-01,  3.0654e-01, -1.1808e+00,\n",
      "         1.5984e+00,  8.4485e-01,  1.5574e-01,  2.5214e+00,  1.2680e+00,\n",
      "         1.0175e+00,  1.1569e+00,  1.0449e+00,  1.3879e+00, -1.7292e+00,\n",
      "        -9.0296e-02,  4.7583e-01, -2.7118e-01, -3.2099e-01, -1.5159e+00,\n",
      "        -1.7113e+00, -1.1520e+00, -1.8528e+00, -1.8906e+00, -1.3874e+00,\n",
      "        -7.3583e-01, -9.1373e-01, -2.8098e-01,  7.6399e-01, -2.5848e-01,\n",
      "        -3.9788e-01,  2.3078e+00,  9.5247e-01,  1.9332e+00,  2.0422e+00,\n",
      "         1.1919e+00,  1.8273e+00,  2.0861e+00,  9.7918e-01,  4.9379e-01,\n",
      "         1.8340e-01, -2.4918e-02,  9.2840e-02,  9.0445e-01, -1.8641e-01,\n",
      "         1.1833e+00,  5.5886e-01,  1.7509e+00,  1.7443e+00,  2.0395e+00,\n",
      "         4.8965e-03,  1.1402e+00,  1.8052e+00,  1.3053e+00,  2.8349e-01,\n",
      "         1.2074e+00,  1.3134e+00,  2.0258e+00,  1.3491e+00,  9.6511e-01,\n",
      "        -1.2884e+00, -1.8220e+00,  4.3405e-01,  4.0413e-01, -1.1235e+00,\n",
      "         2.1253e-01, -8.6832e-01, -1.8498e+00, -2.3629e+00, -1.3166e+00,\n",
      "        -2.5692e-01, -1.6706e+00,  7.5239e-01, -1.2582e-01, -2.4000e-02,\n",
      "        -1.4997e+00,  1.3693e+00,  7.9437e-01, -2.2362e+00, -1.8176e+00,\n",
      "        -1.4336e+00, -8.3517e-01, -1.1494e+00, -2.5414e+00,  3.1623e-01,\n",
      "         3.2807e-01, -8.8996e-01,  7.3307e-01,  5.7057e-01, -3.4979e-01,\n",
      "        -9.4106e-01,  1.8089e-01, -1.0213e+00,  1.4444e+00,  3.5152e+00,\n",
      "         7.2684e-01, -2.8396e-01, -4.2758e-01, -5.0427e-01, -9.3897e-01,\n",
      "        -1.5059e+00, -3.8872e-01, -7.2576e-01,  1.1921e-01,  9.3937e-01,\n",
      "        -1.0631e+00, -1.1993e+00,  4.0711e-01, -9.6358e-01,  1.0336e+00,\n",
      "        -2.9474e-01, -7.5324e-01, -2.5238e-01,  1.8011e-01, -8.1131e-01,\n",
      "         4.9550e-01,  5.7358e-01, -6.2922e-01,  5.8499e-01, -1.6414e+00,\n",
      "        -1.2278e+00,  1.6792e+00,  1.0807e+00,  1.6185e+00, -1.6652e+00,\n",
      "        -2.3437e+00, -1.0413e-02, -1.9921e-01, -8.7450e-01, -1.4825e+00,\n",
      "         1.6859e+00, -6.2134e-01,  4.5160e-01, -6.0421e-01, -1.1329e+00,\n",
      "        -7.8667e-01, -5.8690e-01,  2.0817e-01, -9.3502e-02,  1.4793e+00,\n",
      "        -7.1645e-01, -1.8425e+00, -1.4383e+00, -5.8793e-01, -3.0522e-01,\n",
      "         1.5171e+00, -1.0087e+00, -1.4105e+00,  1.0291e+00,  3.4666e-01,\n",
      "         1.4880e+00, -1.9264e+00, -1.2744e-02,  1.3903e+00, -7.1715e-01,\n",
      "        -9.0547e-01,  8.8981e-01,  1.4988e-01, -1.4129e+00, -1.5406e+00,\n",
      "         5.8717e-01, -2.0927e-01,  2.6413e+00,  3.2215e-01,  1.1767e-01,\n",
      "        -2.2522e-01, -7.4652e-01,  9.1848e-01, -1.6361e+00,  1.1813e-01,\n",
      "        -1.7993e+00,  1.5821e-01, -9.2835e-01, -5.3133e-01,  3.1895e-01,\n",
      "        -2.3183e-01, -2.0916e-01,  3.4321e-01,  2.4939e-01,  1.9120e+00,\n",
      "         6.8346e-01, -4.6119e-01,  2.2697e-01,  2.2887e+00, -2.2233e+00,\n",
      "        -1.9404e+00, -8.9739e-01,  9.2001e-01,  2.0565e-02,  1.8008e-01,\n",
      "         1.4188e+00, -6.9003e-01, -7.9955e-01, -7.7769e-01,  1.6683e-01,\n",
      "        -1.0426e+00, -7.3979e-01, -7.3441e-01,  6.1438e-01, -8.7439e-01,\n",
      "        -1.8292e+00,  2.5409e-01,  6.0905e-01,  2.3852e-01,  2.9444e-01,\n",
      "         6.7145e-01,  7.0760e-01,  2.4288e-01,  8.3862e-01, -1.5884e+00,\n",
      "        -1.0338e+00, -1.7371e+00, -2.0511e+00, -1.6022e+00,  2.6442e+00,\n",
      "        -1.7566e+00,  3.7393e-01,  1.4238e+00,  1.1372e+00, -6.4602e-01,\n",
      "        -3.0082e-02,  5.4072e-01, -3.4077e+00, -1.0091e+00,  5.8101e-01,\n",
      "        -6.2889e-01,  1.5197e+00,  5.0331e-02, -4.8154e-02, -2.4283e+00,\n",
      "        -1.9968e+00,  5.7512e-01, -3.3609e-01,  9.5495e-01,  6.0886e-01,\n",
      "        -1.4903e+00, -4.2492e-01, -1.1481e+00,  1.7602e+00, -5.8789e-01,\n",
      "        -1.1569e+00, -6.9074e-01, -5.5153e-01,  8.3383e-01, -1.9622e+00,\n",
      "        -4.8487e-01, -9.4188e-01,  4.4311e-01, -1.8149e-01,  1.4008e+00,\n",
      "        -3.6279e-01, -1.9916e+00, -6.2356e-01, -2.2891e+00, -5.0316e-01,\n",
      "        -8.8540e-01, -1.1727e+00, -1.7534e+00,  1.3900e+00,  6.0920e-01,\n",
      "         5.4399e-01, -6.4796e-01,  1.8392e+00,  9.9967e-01,  7.5025e-01,\n",
      "         8.5455e-01,  2.0675e-01,  5.4402e-01,  1.1639e+00,  1.4833e-01,\n",
      "        -1.0300e+00,  8.4573e-01,  5.4595e-01, -2.3253e-03,  1.4727e+00,\n",
      "         2.0137e+00, -1.7418e+00, -1.6662e-01, -1.7436e+00,  1.0712e+00,\n",
      "         2.1397e+00,  1.3415e+00, -8.6071e-01,  6.0011e-01, -8.3059e-01,\n",
      "        -2.7638e-01,  2.3690e+00, -2.3619e+00,  1.6619e+00, -3.6877e-01,\n",
      "         7.5544e-02,  3.5664e-01, -1.1137e+00,  5.1130e-01,  1.9214e+00,\n",
      "         2.9088e+00, -8.2254e-02,  2.5223e+00,  1.1692e+00, -1.2698e+00,\n",
      "        -2.9347e+00,  4.3996e+00, -1.1631e+00, -1.8843e+00,  1.0396e+00,\n",
      "         6.9707e-01,  7.5416e-01,  1.2814e+00,  2.1587e+00, -4.9926e-01,\n",
      "        -7.0515e-02,  4.9811e-02, -7.6482e-01, -1.2684e+00, -1.0547e+00,\n",
      "        -3.5943e-02, -4.5068e-01, -6.2149e-01,  2.4443e-01,  2.2229e+00,\n",
      "        -2.7179e+00, -3.6905e-02, -1.6913e-01, -6.7595e-01, -2.6656e-01,\n",
      "         2.2398e+00, -1.0443e+00,  1.3501e-01, -1.2682e-01, -1.1225e+00,\n",
      "        -4.0274e-01, -5.5593e-01, -1.1277e+00,  1.3423e+00, -7.1573e-01,\n",
      "        -1.3512e+00, -2.2176e+00,  1.3238e+00, -2.0517e+00,  1.1877e+00,\n",
      "        -7.3526e-01,  1.2054e+00,  1.7905e-01, -1.3987e+00,  8.4412e-02,\n",
      "        -4.9355e-01, -7.3089e-01, -1.2649e+00,  2.7972e+00,  3.9782e+00,\n",
      "        -1.6582e+00, -4.3159e-01,  3.5642e+00,  5.2206e-01,  6.9254e-01,\n",
      "         3.4554e-01,  2.2941e+00,  4.5798e-01,  4.1191e-01,  8.5540e-01,\n",
      "        -1.3091e+00,  7.8547e-01, -2.0979e+00, -2.1548e-01, -2.0942e+00,\n",
      "        -2.3391e+00, -1.3418e+00,  8.1225e-01, -1.1857e+00, -2.6558e+00,\n",
      "         1.1790e+00,  1.5141e+00, -4.6873e-01, -2.1109e+00,  1.6179e-01,\n",
      "         1.7432e+00, -7.6224e-01, -3.8542e-01,  1.2260e+00,  2.9167e-01,\n",
      "        -2.4487e+00, -7.1434e-01, -1.1100e+00,  2.0654e-01,  1.2635e+00,\n",
      "         1.7307e+00,  1.3011e+00,  8.6287e-02, -2.1413e-01,  1.3918e+00,\n",
      "        -1.3478e+00, -1.7449e-01, -1.5204e+00, -6.3040e-01,  5.4646e-01,\n",
      "         1.2743e+00,  9.1980e-03,  1.3982e+00, -1.2210e+00, -2.5150e+00,\n",
      "         9.4367e-01,  1.2816e-01, -2.2176e+00, -6.0539e-01, -2.4027e+00,\n",
      "        -1.4814e+00,  4.9449e-01,  4.1402e-01,  1.1718e+00, -1.8124e+00,\n",
      "        -3.9178e-02,  7.4070e-02,  3.6603e-01,  1.3239e+00, -4.4944e-01,\n",
      "         3.3643e-01,  1.4415e+00, -2.7731e-01,  1.6184e-01, -1.1829e+00,\n",
      "         2.6489e-01,  2.2120e-01, -1.6429e-01,  1.0571e+00,  1.1440e+00,\n",
      "         6.9722e-01, -2.3157e-01,  2.1390e-02,  6.4977e-01,  7.0077e-01,\n",
      "        -1.6769e+00, -4.9570e-01, -1.1844e+00,  9.9408e-02,  4.8560e-01,\n",
      "        -2.5605e-02,  3.5162e+00, -9.0503e-01,  1.9399e+00,  1.2917e+00,\n",
      "         2.3309e-01, -1.4459e+00,  1.7065e+00, -8.4201e-01,  1.8866e+00,\n",
      "         3.2238e-01, -7.3793e-01,  1.5511e+00,  2.3802e+00,  1.0538e+00,\n",
      "        -9.5331e-01,  2.5303e-01,  6.9560e-01,  1.1519e+00, -2.0726e+00,\n",
      "        -1.3017e+00, -1.3543e+00,  8.2518e-01,  3.2896e+00,  2.2633e+00,\n",
      "         1.3473e+00,  8.3617e-01, -4.1760e-01, -1.1220e+00,  5.7263e-01,\n",
      "        -9.8421e-01, -1.2052e-01,  1.3711e-01, -4.9903e-01,  5.9103e-01,\n",
      "        -1.7528e+00,  7.7163e-01,  4.0502e-01, -1.4946e-01,  9.4455e-01,\n",
      "        -1.3236e+00, -1.4525e-01, -1.9987e+00, -1.9653e+00,  9.1990e-01,\n",
      "         2.6002e-01,  9.8753e-01, -3.3922e-01,  2.9704e-01,  3.3460e-01,\n",
      "         1.0572e+00,  1.7908e+00, -1.3154e+00,  6.9004e-01, -1.9145e+00,\n",
      "        -2.3588e-01,  8.3206e-01, -2.0722e+00,  1.4829e+00, -5.4542e-01,\n",
      "        -3.6670e+00, -2.4867e+00, -1.6689e+00,  3.7467e-02,  6.4842e-01,\n",
      "        -4.5098e-01,  6.4765e-01,  4.3435e-01,  5.8141e-01, -2.1887e+00,\n",
      "        -2.7166e-01,  4.8405e-01, -2.5999e+00, -2.1101e+00,  1.2537e-01,\n",
      "         1.7433e-01,  9.7146e-01,  1.0669e+00,  7.9268e-01, -7.9592e-01,\n",
      "        -7.6158e-01, -5.2043e-02, -2.1899e-01,  1.2669e+00,  1.3562e+00,\n",
      "         1.8190e+00,  1.9102e+00, -8.7595e-01,  4.9983e-01,  1.0066e+00,\n",
      "         1.2688e+00,  1.3012e+00,  1.4720e+00, -1.0845e+00, -4.7150e-01,\n",
      "         3.3350e+00, -2.9177e+00, -1.1135e+00,  1.1062e+00,  1.2920e+00,\n",
      "        -1.8826e+00,  2.6102e-01,  1.0608e+00, -4.6866e-01, -1.4991e+00,\n",
      "        -2.4119e+00, -5.5575e-01, -1.0810e+00,  1.1403e+00, -1.2140e+00,\n",
      "        -1.0148e-01, -1.5955e+00,  1.3511e+00, -9.1380e-01, -2.6960e+00,\n",
      "        -6.5971e-01,  6.8638e-01, -7.4294e-01,  1.8962e-01, -2.7039e-01,\n",
      "         1.9186e-02, -8.4438e-01,  1.0452e+00,  1.1905e+00, -2.9218e-01,\n",
      "         3.3640e-01, -1.3100e+00, -1.1064e+00, -1.5563e+00,  7.8038e-01,\n",
      "        -2.2179e+00,  1.3492e-01,  2.2979e-01,  2.3905e+00, -2.8376e-01,\n",
      "        -1.9265e+00,  9.0289e-01, -9.7339e-01,  1.2649e+00,  5.8354e-01,\n",
      "        -1.6438e+00, -7.5612e-01,  5.1411e-01, -3.1241e-01,  3.3337e+00,\n",
      "         1.1972e+00,  1.5832e-01,  1.3583e-01, -2.8900e-01, -9.7476e-01,\n",
      "         9.0392e-01,  1.0857e+00, -5.8181e-01, -1.3393e+00, -2.7870e+00,\n",
      "        -1.8563e+00, -3.5699e-01, -7.4356e-01,  1.1604e+00, -3.7347e-01,\n",
      "        -8.4129e-01,  1.4414e+00, -2.6690e-01, -1.2486e+00, -5.0971e-01,\n",
      "        -7.5071e-01, -1.4723e+00, -1.4491e-01, -3.3222e-01,  1.0645e+00,\n",
      "        -4.9888e-01, -1.0354e+00, -3.2084e-01, -1.7580e+00, -1.1336e+00,\n",
      "        -1.3977e+00, -1.6203e+00,  6.7101e-01, -1.1766e+00,  5.1025e-01,\n",
      "        -1.4033e+00, -1.3154e+00, -4.0834e-01,  1.8222e-01,  1.8648e-01,\n",
      "        -7.6610e-01, -6.6896e-01,  7.7316e-01,  9.5253e-01,  5.2042e-01,\n",
      "         3.9276e-01, -4.8903e-01, -3.3792e-02,  7.7348e-01,  2.6204e-01,\n",
      "        -4.6151e-03, -1.0674e-01,  1.0040e+00, -9.9631e-02, -1.0783e+00,\n",
      "        -1.5491e-01,  2.2542e-01, -8.9497e-01,  1.3643e+00,  3.2711e-01,\n",
      "        -7.2063e-02,  7.8318e-01,  7.4931e-01,  1.4255e+00,  7.2333e-01,\n",
      "        -2.1542e+00,  8.4480e-01, -1.1784e+00, -1.1127e+00, -1.2622e+00,\n",
      "         2.0069e-01, -1.8159e+00,  4.1038e-01,  3.9185e-01, -2.4117e+00,\n",
      "        -1.6387e+00,  2.1097e-01, -9.7213e-01, -1.7133e+00, -9.9327e-01,\n",
      "        -7.7185e-01, -1.8378e+00,  1.9028e+00,  8.4275e-01, -9.3790e-01,\n",
      "         8.5794e-01, -1.1816e+00, -6.6090e-01, -1.8017e+00, -4.0477e-01,\n",
      "        -1.5719e-01, -1.0956e-01,  8.3491e-01,  1.3488e+00,  1.2792e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n",
      "tensor([-1.0327e+00, -1.5568e-02, -2.2941e+00, -8.6799e-01, -5.5411e-01,\n",
      "        -7.3027e-01, -2.7788e-01, -7.5893e-01, -2.2504e-01,  4.2742e-01,\n",
      "         8.1188e-01,  6.3918e-01,  1.5337e+00,  1.3354e+00,  1.9086e+00,\n",
      "         5.5945e-01,  3.1137e-01,  1.0524e+00,  8.6498e-01,  1.7047e+00,\n",
      "        -1.0382e+00, -6.3694e-01, -1.3851e+00,  3.4040e-01, -8.4171e-01,\n",
      "        -1.9855e-01, -1.0959e-01,  4.4381e-01,  4.2311e-02,  9.5448e-01,\n",
      "         6.6981e-02,  1.0934e+00,  4.5561e-01, -3.7837e-01,  1.2191e-01,\n",
      "        -8.3442e-01,  5.1358e-01, -3.3274e-01,  3.5071e-01,  4.0138e-02,\n",
      "         1.1160e+00,  6.8161e-01,  1.4220e+00,  4.9577e-01,  5.4328e-01,\n",
      "         9.5350e-01,  1.6306e+00,  5.5521e-01,  4.3543e-01, -1.0321e+00,\n",
      "        -6.4787e-01, -2.3051e+00,  1.5193e+00,  1.4121e+00,  1.4936e+00,\n",
      "         7.8456e-01,  6.8613e-01,  2.9564e-01,  8.1796e-01,  1.3567e+00,\n",
      "         1.6891e+00,  1.5196e+00,  2.9122e-01,  1.8455e+00,  1.2917e+00,\n",
      "         3.0896e-01,  1.2058e+00,  1.6331e+00,  7.8843e-01,  1.1045e-01,\n",
      "        -3.2573e-01,  1.2623e+00, -5.1133e-01,  2.4154e-01, -2.6201e-01,\n",
      "         5.2567e-01,  2.4393e+00,  1.3444e+00,  2.4533e+00,  9.2554e-01,\n",
      "        -9.3938e-01, -1.1359e+00,  7.4530e-01, -7.0448e-01,  1.0416e-01,\n",
      "         9.7101e-01, -2.6788e-01,  2.3491e-01,  8.3282e-02, -1.1350e+00,\n",
      "        -3.4570e-01, -2.9855e-01,  2.9961e-01, -5.2377e-01,  1.2499e+00,\n",
      "        -8.8140e-01, -3.3062e-01, -8.8533e-01, -1.5952e+00,  7.4437e-02,\n",
      "        -1.1239e+00, -1.5824e+00,  3.7983e-01,  1.0237e+00,  1.5091e+00,\n",
      "         7.2063e-01,  9.5083e-01, -1.5593e+00, -1.5951e+00, -1.4273e+00,\n",
      "        -1.0960e+00,  6.3803e-02,  3.4205e-01,  7.8684e-01,  1.4045e+00,\n",
      "        -1.2946e+00, -1.3907e+00, -9.2925e-01, -6.4841e-01, -8.5218e-01,\n",
      "        -5.2319e-01, -1.6375e+00, -1.6627e+00, -1.6278e+00,  1.9762e-01,\n",
      "         4.9814e-01, -7.8709e-01, -1.1745e+00, -1.3107e+00, -1.5939e-01,\n",
      "        -5.3568e-01, -7.8756e-01, -1.2736e+00, -6.8450e-01, -6.6367e-01,\n",
      "        -1.3357e+00, -1.2652e+00, -1.2190e+00, -3.4916e-01, -1.0912e+00,\n",
      "         3.5084e-02, -8.3515e-01, -1.0111e+00, -1.5388e+00, -4.2889e-01,\n",
      "        -1.1789e+00, -1.9897e+00, -2.7943e+00, -2.4827e+00, -1.9866e+00,\n",
      "        -6.2984e-01,  1.9788e+00, -1.1292e+00, -1.8899e-01,  3.3633e-01,\n",
      "        -1.9080e-01,  2.4755e-01,  4.1313e-01,  2.2220e-01,  7.8341e-01,\n",
      "        -1.2933e+00,  7.2702e-01,  1.1686e+00,  3.1816e-01,  1.1993e+00,\n",
      "        -1.3397e-01,  1.1072e-01, -7.9785e-01,  9.9500e-01, -5.1558e-01,\n",
      "        -6.7404e-01,  1.5274e+00,  4.5485e-01,  3.4644e-01,  8.8772e-01,\n",
      "        -1.6188e+00, -3.5540e-01, -2.0554e-01,  9.5338e-01,  9.2683e-02,\n",
      "         3.2852e-01, -6.7781e-01,  1.1063e+00, -8.5253e-01,  6.8492e-01,\n",
      "         7.0680e-01,  8.5521e-01,  4.3002e-01, -5.6112e-01, -2.2241e-01,\n",
      "        -1.6246e+00,  7.4720e-01,  4.1142e-01, -5.3664e-01, -1.9490e+00,\n",
      "         8.0730e-01,  5.2863e-02, -1.1331e+00, -9.2273e-01, -8.6626e-02,\n",
      "        -1.5936e+00, -1.0300e-01,  2.4226e-01,  4.6598e-01, -1.6094e-01,\n",
      "        -3.6031e-01, -5.4885e-01,  9.7474e-01,  1.3972e+00,  1.2095e+00,\n",
      "        -3.9878e-01,  6.0800e-01,  4.4211e-01,  3.4177e-01, -5.3394e-01,\n",
      "         9.3293e-01, -1.0061e+00, -2.1904e-01,  2.9113e-02,  4.7820e-01,\n",
      "        -1.1558e+00, -8.1632e-01, -4.0720e-01,  1.5477e+00,  3.5760e-01,\n",
      "         6.5142e-01, -1.2443e+00,  1.1029e+00, -1.7977e+00, -4.0229e-01,\n",
      "         5.5917e-01,  3.0322e-01,  3.3030e-01, -1.5381e+00,  5.7537e-01,\n",
      "         9.0788e-01,  7.1772e-01,  1.2915e+00,  3.4352e-01, -4.9793e-02,\n",
      "         7.8246e-01,  4.1229e-01,  3.2319e-01,  6.7402e-02, -1.6366e+00,\n",
      "         2.3742e-01, -3.3957e-01, -8.7873e-01,  6.4950e-01,  5.0641e-01,\n",
      "         1.0486e+00,  3.4297e-01, -1.0694e+00,  9.3981e-01,  9.7052e-01,\n",
      "        -1.5311e+00, -1.1043e+00, -2.9986e-01,  2.1915e-01,  1.2627e+00,\n",
      "         3.0346e-01, -3.2255e-01, -5.9036e-02,  1.5658e+00,  4.8386e-01,\n",
      "         7.3670e-01, -4.2291e-01, -7.0231e-01, -9.2080e-01,  5.2816e-01,\n",
      "         8.2730e-02,  8.7453e-01,  8.1853e-01,  1.5278e+00,  4.6164e-01,\n",
      "         2.1847e-02,  1.1934e+00,  1.2657e+00,  7.4894e-01,  4.5144e-01,\n",
      "         1.1616e+00,  1.8255e+00,  1.6506e+00,  9.3731e-01,  2.2525e+00,\n",
      "         2.5060e+00,  1.1094e+00,  1.3497e+00,  2.2462e-01, -2.3731e-01,\n",
      "         2.5864e-01, -5.4193e-03, -2.1129e-01,  1.8916e+00, -7.8700e-01,\n",
      "        -4.9855e-01, -9.8945e-01, -2.6046e-01,  2.4813e+00,  2.8450e+00,\n",
      "         1.6038e-01,  1.7015e+00,  4.3609e-01,  5.4107e-01,  6.3645e-01,\n",
      "         1.2135e+00,  1.8138e+00,  1.9086e-01,  3.8419e-01,  4.9866e-01,\n",
      "         1.5603e+00,  7.3099e-01,  1.0493e+00,  1.6588e+00,  1.7486e+00,\n",
      "         1.5379e+00,  9.6301e-01,  7.7434e-01,  2.5864e-01,  8.0185e-01,\n",
      "         7.9986e-01, -1.0009e+00, -6.9002e-01, -5.4592e-01,  4.3070e-01,\n",
      "        -6.1968e-01, -3.2024e-01,  8.3007e-01,  1.5324e-02, -1.6224e+00,\n",
      "         1.1843e+00,  6.3317e-01, -1.4301e-01,  1.3520e+00,  6.7835e-01,\n",
      "         4.3757e-01,  6.6918e-01,  4.2982e-01,  9.0245e-01, -1.3624e+00,\n",
      "        -1.4429e-01,  2.7693e-01, -6.5521e-01, -2.3911e-01, -1.2496e+00,\n",
      "        -1.4606e+00, -9.1572e-01, -1.5338e+00, -1.7393e+00, -1.3761e+00,\n",
      "        -9.3381e-01, -7.5833e-01, -1.8511e-01,  5.3548e-01, -5.9359e-01,\n",
      "        -6.5260e-01,  1.7089e+00,  2.7715e-01,  1.3755e+00,  1.3199e+00,\n",
      "         7.0643e-01,  1.4256e+00,  1.5498e+00,  9.0613e-01,  3.2191e-01,\n",
      "         4.7838e-01,  3.5413e-02,  1.4934e-01,  5.7471e-01, -1.2285e-01,\n",
      "         7.5334e-01,  3.4429e-01,  1.4958e+00,  1.3077e+00,  1.6910e+00,\n",
      "        -6.8256e-02,  1.0494e+00,  1.4193e+00,  9.9573e-01,  4.3174e-02,\n",
      "         8.4330e-01,  1.0258e+00,  1.6208e+00,  1.1627e+00,  5.0196e-01,\n",
      "        -9.8780e-01, -1.6889e+00,  5.2363e-01,  2.9991e-01, -9.6860e-01,\n",
      "         1.7551e-01, -6.5342e-01, -1.9951e+00, -2.2008e+00, -1.4867e+00,\n",
      "        -3.8604e-01, -1.7584e+00, -1.0767e-02, -7.1872e-02,  1.1577e-01,\n",
      "        -1.2261e+00,  1.1423e+00,  8.4813e-01, -2.2269e+00, -1.5452e+00,\n",
      "        -9.2904e-01, -8.6628e-01, -1.1700e+00, -2.6251e+00,  4.4968e-01,\n",
      "        -9.7510e-02, -8.9751e-01,  6.6751e-01,  1.8258e-01, -5.7542e-01,\n",
      "        -1.0872e+00,  3.0974e-01, -5.0424e-01,  1.1854e+00,  3.0204e+00,\n",
      "         8.2697e-01,  4.2431e-01, -4.4811e-01, -4.0537e-01, -8.1725e-01,\n",
      "        -7.0003e-01, -3.1494e-01, -2.7478e-01,  2.1218e-01,  1.2048e+00,\n",
      "        -7.1142e-01, -1.3458e+00,  3.2239e-01, -8.1243e-01,  1.0133e+00,\n",
      "         7.6811e-03, -6.1598e-01, -4.9939e-02,  1.7742e-01, -1.3895e+00,\n",
      "         7.4132e-01,  6.6975e-01, -4.9421e-01,  2.8604e-01, -1.4191e+00,\n",
      "        -7.3412e-01,  1.3577e+00,  1.0445e+00,  1.9313e+00, -1.2544e+00,\n",
      "        -1.7666e+00, -3.8133e-01, -2.9237e-01, -7.9704e-01, -1.5959e+00,\n",
      "         1.7944e+00, -8.0774e-01,  3.5904e-01, -4.8694e-01, -7.5929e-01,\n",
      "        -5.7468e-01, -6.3153e-01,  4.0815e-01, -1.8001e-02,  9.0753e-01,\n",
      "        -9.0598e-01, -1.6108e+00, -1.4674e+00, -4.1417e-01, -3.2141e-01,\n",
      "         1.2992e+00, -8.5330e-01, -9.3929e-01,  7.6421e-01,  5.6281e-01,\n",
      "         1.7874e+00, -1.5943e+00, -6.0025e-01,  1.0712e+00, -4.6845e-01,\n",
      "        -1.1733e+00,  3.5234e-01, -7.0271e-02, -1.2064e+00, -1.8461e+00,\n",
      "         6.6230e-01, -1.0206e-01,  2.1489e+00,  8.4462e-01,  6.4014e-01,\n",
      "        -2.9238e-01, -7.0706e-01,  9.0912e-01, -1.4101e+00,  6.0511e-01,\n",
      "        -1.2622e+00, -7.2302e-02, -5.2928e-01, -4.9440e-01,  4.4485e-01,\n",
      "        -3.9452e-01,  4.1501e-01,  6.6983e-01,  3.7890e-01,  1.6947e+00,\n",
      "         9.8885e-01, -2.5339e-01,  3.3204e-01,  1.6193e+00, -1.9839e+00,\n",
      "        -1.8333e+00, -6.6245e-01,  9.5943e-01,  1.4584e-02,  3.5132e-01,\n",
      "         1.7304e+00, -6.2825e-01, -6.5864e-01, -2.3739e-01,  2.7054e-01,\n",
      "        -8.8601e-01, -7.0940e-01, -1.7515e-01,  6.4943e-01, -8.4106e-01,\n",
      "        -1.3874e+00,  1.7731e-02,  1.7849e-01,  5.9864e-01, -3.5576e-02,\n",
      "         7.0355e-01,  4.9053e-01,  2.9582e-01,  2.6336e-01, -1.3434e+00,\n",
      "        -8.1792e-01, -1.8098e+00, -1.9898e+00, -1.1709e+00,  2.3906e+00,\n",
      "        -1.6999e+00,  3.7731e-01,  1.4598e+00,  9.5859e-01, -7.9835e-01,\n",
      "         2.0965e-03,  3.6131e-01, -2.8674e+00, -1.0788e+00,  6.6728e-01,\n",
      "        -6.0614e-01,  1.0458e+00,  4.1945e-01, -1.3572e-01, -1.9964e+00,\n",
      "        -1.6678e+00,  5.8460e-01, -2.5016e-02,  9.7470e-01,  3.0719e-01,\n",
      "        -1.3434e+00, -3.4722e-01, -3.8957e-01,  1.3518e+00, -8.1103e-02,\n",
      "        -8.0713e-01, -5.6708e-01, -6.5842e-01,  6.2250e-01, -2.0132e+00,\n",
      "        -3.8151e-01, -9.2001e-01,  6.6677e-01, -5.7004e-02,  1.5770e+00,\n",
      "        -5.7330e-01, -2.0450e+00, -2.4805e-01, -1.4790e+00, -2.0345e-01,\n",
      "        -3.5254e-01, -9.9519e-01, -1.6442e+00,  1.3130e+00,  1.9411e-01,\n",
      "         5.9968e-01, -7.7801e-01,  1.7998e+00,  7.9961e-01,  6.6659e-01,\n",
      "         1.9691e-01,  3.3826e-01,  5.5304e-02,  8.5042e-01,  4.2560e-01,\n",
      "        -8.1946e-01,  9.8551e-01,  2.9297e-01,  7.7233e-02,  9.4253e-01,\n",
      "         1.9328e+00, -1.1673e+00, -2.7407e-01, -1.6995e+00,  1.6822e+00,\n",
      "         1.7014e+00,  1.6170e+00, -5.6197e-01,  5.9216e-01, -8.1515e-01,\n",
      "        -1.5752e-01,  2.1487e+00, -2.2935e+00,  1.3027e+00, -1.6774e-01,\n",
      "        -4.5460e-03,  5.7053e-01, -1.2344e+00,  6.7040e-01,  2.0793e+00,\n",
      "         2.1425e+00,  1.8761e-01,  2.3466e+00,  8.2896e-01, -1.3749e+00,\n",
      "        -2.8232e+00,  3.7699e+00, -1.0811e+00, -1.8117e+00,  1.1166e+00,\n",
      "         7.2280e-01,  5.8598e-01,  1.0963e+00,  1.8968e+00, -3.1577e-01,\n",
      "        -3.7108e-02, -1.9351e-01, -3.7395e-01, -1.0942e+00, -8.4795e-01,\n",
      "        -2.0723e-01, -3.0914e-01, -5.5865e-01,  3.1820e-01,  1.7881e+00,\n",
      "        -1.9549e+00,  1.8258e-01, -2.0407e-01, -6.5054e-01, -1.8634e-01,\n",
      "         1.7462e+00, -9.3008e-01,  3.8304e-03,  3.5794e-01, -1.1562e+00,\n",
      "        -1.7963e-01, -6.2818e-01, -1.0564e+00,  8.8990e-01, -7.7672e-01,\n",
      "        -1.0769e+00, -1.8589e+00,  6.8723e-01, -1.9067e+00,  8.8532e-01,\n",
      "        -4.7823e-01,  1.1865e+00,  2.5717e-01, -1.2494e+00,  9.3516e-01,\n",
      "        -1.7297e-01, -1.8038e-01, -7.6146e-01,  2.2433e+00,  2.6982e+00,\n",
      "        -1.7639e+00, -8.2907e-01,  3.3083e+00,  3.0822e-01,  6.0647e-01,\n",
      "         3.1177e-01,  1.5859e+00,  5.6772e-01,  3.0035e-01,  7.3829e-01,\n",
      "        -1.1625e+00,  1.5509e-01, -1.6012e+00, -5.7733e-01, -1.5946e+00,\n",
      "        -2.1884e+00, -1.4317e+00,  4.0249e-01, -8.7844e-01, -2.4053e+00,\n",
      "         1.4089e+00,  1.4192e+00, -2.5234e-01, -2.0608e+00, -6.5004e-02,\n",
      "         1.6399e+00, -6.2369e-01, -5.5136e-01,  1.3428e+00,  6.5498e-01,\n",
      "        -1.9558e+00, -5.2496e-01, -8.7861e-01,  4.3697e-01,  6.4370e-01,\n",
      "         1.3343e+00,  1.5860e+00, -1.2231e-01, -4.6062e-01,  1.1452e+00,\n",
      "        -1.5468e+00,  2.4246e-01, -1.4367e+00, -3.0660e-01,  6.9695e-01,\n",
      "         1.1603e+00,  1.1344e-01,  1.3139e+00, -5.8024e-01, -2.1085e+00,\n",
      "         1.2973e+00, -2.6874e-02, -1.8224e+00, -4.4001e-01, -2.3048e+00,\n",
      "        -1.1246e+00,  7.6003e-01,  7.0709e-02,  1.5701e+00, -2.0130e+00,\n",
      "         2.7458e-01,  3.4117e-01,  6.5529e-01,  1.3814e+00, -2.4455e-01,\n",
      "         2.3010e-02,  1.4446e+00, -6.6952e-01,  1.8922e-01, -1.2002e+00,\n",
      "         1.1470e-01, -3.1603e-02, -5.6130e-02,  6.8212e-01,  1.1821e+00,\n",
      "         1.1670e+00, -4.9781e-02,  1.5957e-01,  5.0333e-01,  2.9574e-01,\n",
      "        -1.3637e+00, -1.9699e-01, -1.1878e+00,  3.0178e-01,  1.7234e-01,\n",
      "        -1.3940e-01,  3.1585e+00, -1.0408e+00,  1.8280e+00,  1.0925e+00,\n",
      "         2.6775e-01, -1.6820e+00,  1.2617e+00, -1.8579e-01,  1.6130e+00,\n",
      "         4.7905e-01, -5.2291e-01,  1.3285e+00,  2.3145e+00,  1.0105e+00,\n",
      "        -5.7612e-01,  2.8265e-01,  4.7260e-01,  9.9573e-01, -1.8367e+00,\n",
      "        -9.3909e-01, -1.4674e+00,  5.9758e-01,  2.9340e+00,  1.6805e+00,\n",
      "         1.1897e+00,  6.9106e-01, -4.9485e-01, -1.2020e+00,  7.3876e-01,\n",
      "        -1.3653e+00, -3.6077e-02,  3.0408e-01, -5.4232e-01,  9.0616e-01,\n",
      "        -1.4770e+00,  4.9488e-01,  2.7182e-01, -6.4318e-01,  1.2352e+00,\n",
      "        -1.4407e+00, -5.1638e-01, -1.4802e+00, -1.6560e+00,  8.1839e-01,\n",
      "         9.5164e-01,  1.0047e+00, -5.6644e-01,  6.0591e-01, -3.3096e-02,\n",
      "         3.2383e-01,  1.2142e+00, -1.4031e+00,  4.2401e-01, -1.8751e+00,\n",
      "         3.8039e-01,  1.0696e+00, -1.8031e+00,  1.6820e+00, -4.6165e-01,\n",
      "        -3.0961e+00, -2.1843e+00, -1.5127e+00, -2.7862e-02,  9.7484e-01,\n",
      "        -2.9303e-01,  6.4245e-01,  3.6305e-01,  5.3400e-01, -1.7827e+00,\n",
      "        -6.1790e-01,  6.4539e-01, -2.2197e+00, -1.9995e+00,  1.0640e-01,\n",
      "         4.1284e-01,  1.0685e+00,  1.1742e+00,  4.6129e-01, -4.1207e-01,\n",
      "        -6.7055e-01,  1.1311e-01, -4.5020e-01,  1.3315e+00,  1.1020e+00,\n",
      "         1.4687e+00,  1.9745e+00, -1.0113e+00,  1.0723e-01,  1.3224e+00,\n",
      "         1.1481e+00,  1.2634e+00,  1.8300e+00, -8.4883e-01, -1.9188e-02,\n",
      "         2.8495e+00, -2.6950e+00, -1.1109e+00,  1.2142e+00,  1.2830e+00,\n",
      "        -1.7596e+00,  1.7523e-01,  9.2137e-01,  1.2323e-01, -1.4389e+00,\n",
      "        -2.5069e+00, -2.5420e-01, -9.9826e-01,  8.6668e-01, -9.7690e-01,\n",
      "         1.4050e-01, -1.5818e+00,  1.2684e+00, -9.9678e-01, -2.3479e+00,\n",
      "        -7.6057e-01,  8.6252e-01, -6.2527e-01, -2.3686e-01,  2.1184e-01,\n",
      "         1.5811e-01, -4.2922e-01,  8.7692e-01,  1.5051e+00, -1.1398e-01,\n",
      "         7.8597e-01, -1.3934e+00, -9.1324e-01, -8.0903e-01,  8.2522e-01,\n",
      "        -1.8598e+00, -4.6931e-01,  3.7755e-01,  1.9321e+00,  1.2584e-02,\n",
      "        -1.7489e+00,  8.9083e-01, -6.7218e-01,  1.2538e+00,  8.9639e-01,\n",
      "        -1.0096e+00, -4.1875e-01,  5.6990e-01, -1.9927e-01,  3.1927e+00,\n",
      "         1.3426e+00,  9.9609e-02,  5.0422e-01,  3.3102e-01, -9.8968e-01,\n",
      "         9.9953e-01,  1.1406e+00, -1.1185e-01, -1.2725e+00, -2.4960e+00,\n",
      "        -1.2532e+00, -3.0580e-01, -8.8416e-01,  8.9197e-01, -3.6442e-01,\n",
      "        -5.7478e-01,  1.2451e+00, -2.9489e-01, -1.5536e+00, -7.3709e-01,\n",
      "        -9.8845e-01, -1.5938e+00, -4.3349e-01, -7.9768e-01,  7.3642e-01,\n",
      "        -6.6855e-01, -1.2654e+00, -4.7561e-01, -1.9761e+00, -1.4026e+00,\n",
      "        -1.8137e+00, -1.2803e+00, -6.2138e-02, -1.3609e+00,  2.0388e-01,\n",
      "        -1.4853e+00, -1.4007e+00, -6.8215e-01,  9.5205e-02,  1.3789e-01,\n",
      "        -6.4763e-01, -5.6409e-01,  7.5611e-01,  9.3323e-01,  3.9077e-01,\n",
      "         5.5222e-01, -3.5186e-01, -4.2951e-02,  6.8306e-01,  3.4264e-01,\n",
      "        -3.3935e-01, -3.4050e-01,  1.2646e+00,  2.6316e-01, -1.4895e+00,\n",
      "        -3.9877e-01,  5.1751e-02, -1.3613e+00,  6.5633e-01, -3.6079e-01,\n",
      "        -5.7995e-01,  1.3384e+00,  5.6529e-01,  1.3507e+00,  3.5514e-01,\n",
      "        -1.7484e+00,  1.3143e+00, -1.0451e+00, -1.3638e+00, -1.1303e+00,\n",
      "         6.1045e-01, -1.6861e+00,  2.6266e-01,  2.9953e-01, -1.8813e+00,\n",
      "        -1.1630e+00,  3.2481e-01, -7.0263e-01, -1.6433e+00, -7.6835e-01,\n",
      "        -4.6204e-01, -1.2152e+00,  1.4721e+00,  9.8698e-01, -3.7846e-01,\n",
      "         1.2293e+00, -1.3324e+00, -5.0959e-01, -2.0929e+00, -6.1843e-01,\n",
      "        -2.1222e-01, -4.8103e-01,  7.2255e-01,  1.0226e+00,  1.2464e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>) tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#  feature    .\n",
    "for i in range(10):\n",
    "    print(first_feature[0][0][i],first_label[0][0][i].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a75340c-7752-49f4-948f-4c2777412d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = 10\n",
    "h = 10\n",
    "cols = 32\n",
    "rows = 16\n",
    "def feature_print(pic):\n",
    "    #print(\"test with 'after pooling 4 feature'\")\n",
    "    fig = plt.figure(figsize=(64,32))\n",
    "    ax = []\n",
    "    for i in range(cols*rows):\n",
    "        ch = pic[i,:,:]\n",
    "        ax.append(fig.add_subplot(rows,cols,i+1))\n",
    "        ax[-1].set_title(str(i)+\"th ch (14x14)\")\n",
    "        plt.imshow(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c4590c-472f-4dda-ba88-766fbd184048",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20734/3476020286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_20734/3158745465.py\u001b[0m in \u001b[0;36mfeature_print\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"th ch (14x14)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 4608x2304 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_print(first_feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46670422-0d8b-44fe-979c-f89924c10a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
